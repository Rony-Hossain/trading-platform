# JupyterHub Deployment Configuration
# Deploy with: helm install jupyterhub jupyterhub/jupyterhub --values deployment.yaml

# Hub configuration
hub:
  image:
    name: jupyterhub/k8s-hub
    tag: "3.2.1"

  # Database for hub state
  db:
    type: postgres
    url: "postgresql://jupyterhub:password@postgres:5432/jupyterhub"

  # Authentication
  config:
    JupyterHub:
      authenticator_class: oauthenticator.generic.GenericOAuthenticator
      admin_access: true

    GenericOAuthenticator:
      client_id: "trading-platform-jupyterhub"
      client_secret: "${OAUTH_CLIENT_SECRET}"
      oauth_callback_url: "https://jupyter.trading-platform.com/hub/oauth_callback"
      authorize_url: "https://auth.trading-platform.com/oauth/authorize"
      token_url: "https://auth.trading-platform.com/oauth/token"
      userdata_url: "https://auth.trading-platform.com/oauth/userinfo"
      scope:
        - openid
        - profile
        - email
      username_claim: "preferred_username"

    Authenticator:
      admin_users:
        - admin
        - data-science-lead
      allowed_users:
        - quant-team
        - research-team
        - risk-team

# Proxy configuration
proxy:
  secretToken: "${JUPYTERHUB_PROXY_SECRET_TOKEN}"
  service:
    type: LoadBalancer
  https:
    enabled: true
    hosts:
      - jupyter.trading-platform.com
    letsencrypt:
      contactEmail: devops@trading-platform.com

# Single-user notebook configuration
singleuser:
  # Docker image for user notebooks
  image:
    name: trading-platform/jupyterhub-notebook
    tag: "latest"
    pullPolicy: Always

  # Resource limits
  cpu:
    guarantee: 1.0
    limit: 2.0
  memory:
    guarantee: 2G
    limit: 4G

  # Storage for user workspaces
  storage:
    capacity: 10Gi
    dynamic:
      storageClass: standard

  # Default environment
  defaultUrl: "/lab"  # JupyterLab interface

  # Pre-installed Python packages
  extraEnv:
    FEATURE_STORE_URL: "postgresql://features_user:password@postgres:5432/features"
    TRADING_API_URL: "https://api.trading-platform.com"
    JUPYTER_ENABLE_LAB: "yes"

  # Custom notebook image build
  cmd: "start-singleuser.sh"

  # Lifecycle hooks
  lifecycleHooks:
    postStart:
      exec:
        command:
          - "sh"
          - "-c"
          - >
            pip install --quiet
            pandas numpy scipy matplotlib seaborn plotly
            scikit-learn tensorflow torch
            yfinance ta-lib backtrader vectorbt
            psycopg2-binary sqlalchemy
            structlog
            jupyterlab-git jupyterlab-variableinspector

# Scheduling
scheduling:
  userScheduler:
    enabled: true
  podPriority:
    enabled: true
  userPlaceholder:
    enabled: true
    replicas: 2  # Pre-warm 2 notebook servers

# Culling idle notebooks
cull:
  enabled: true
  timeout: 3600  # 1 hour
  every: 600  # Check every 10 minutes
  concurrency: 10

# Resource management
prePuller:
  hook:
    enabled: true
  continuous:
    enabled: true

# Ingress configuration
ingress:
  enabled: true
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
  hosts:
    - jupyter.trading-platform.com
  tls:
    - hosts:
        - jupyter.trading-platform.com
      secretName: jupyterhub-tls

# Custom notebook image Dockerfile
---
# File: infrastructure/jupyterhub/Dockerfile.notebook
FROM jupyterhub/k8s-singleuser-sample:3.2.1

USER root

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    git \
    vim \
    curl \
    wget \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Install TA-Lib for technical analysis
RUN wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz && \
    tar -xzf ta-lib-0.4.0-src.tar.gz && \
    cd ta-lib/ && \
    ./configure --prefix=/usr && \
    make && \
    make install && \
    cd .. && \
    rm -rf ta-lib*

USER ${NB_USER}

# Install Python packages
RUN pip install --no-cache-dir \
    # Data analysis
    pandas==2.1.0 \
    numpy==1.25.0 \
    scipy==1.11.0 \
    # Visualization
    matplotlib==3.7.2 \
    seaborn==0.12.2 \
    plotly==5.16.0 \
    # Machine learning
    scikit-learn==1.3.0 \
    tensorflow==2.13.0 \
    torch==2.0.1 \
    # Finance
    yfinance==0.2.28 \
    TA-Lib==0.4.28 \
    backtrader==1.9.78.123 \
    vectorbt==0.25.6 \
    # Database
    psycopg2-binary==2.9.7 \
    sqlalchemy==2.0.20 \
    # Logging
    structlog==23.1.0 \
    # JupyterLab extensions
    jupyterlab-git==0.44.0 \
    jupyterlab-variableinspector==3.0.9 \
    jupyterlab-execute-time==3.0.1

# Create directories
RUN mkdir -p /home/${NB_USER}/work/notebooks && \
    mkdir -p /home/${NB_USER}/work/data && \
    mkdir -p /home/${NB_USER}/work/models

# Copy example notebooks
COPY notebooks/ /home/${NB_USER}/work/notebooks/

# Copy feature store utilities
COPY utils/ /home/${NB_USER}/work/utils/

USER ${NB_USER}
WORKDIR /home/${NB_USER}

---
# File: infrastructure/jupyterhub/notebooks/01_feature_exploration.ipynb
# Example notebook for exploring the feature store

# Cell 1: Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sqlalchemy import create_engine
import os

# Cell 2: Connect to feature store
feature_store_url = os.getenv('FEATURE_STORE_URL')
engine = create_engine(feature_store_url)

print("Connected to feature store")

# Cell 3: List available features
query = """
SELECT DISTINCT feature_name, data_type, description
FROM feature_metadata
ORDER BY feature_name
"""

features = pd.read_sql(query, engine)
print(f"Available features: {len(features)}")
features.head(20)

# Cell 4: Get historical features for a symbol
symbol = "AAPL"
start_date = "2024-01-01"
end_date = "2024-12-31"

query = f"""
SELECT timestamp, feature_name, feature_value
FROM features
WHERE symbol = '{symbol}'
  AND timestamp >= '{start_date}'
  AND timestamp <= '{end_date}'
  AND feature_name IN ('sma_20', 'rsi_14', 'sentiment_z')
ORDER BY timestamp, feature_name
"""

data = pd.read_sql(query, engine)
data_pivot = data.pivot(index='timestamp', columns='feature_name', values='feature_value')

print(f"Retrieved {len(data_pivot)} rows for {symbol}")
data_pivot.head()

# Cell 5: Visualize features
fig, axes = plt.subplots(3, 1, figsize=(12, 10))

data_pivot['sma_20'].plot(ax=axes[0], title='SMA 20')
data_pivot['rsi_14'].plot(ax=axes[1], title='RSI 14')
data_pivot['sentiment_z'].plot(ax=axes[2], title='Sentiment Z-Score')

plt.tight_layout()
plt.show()

# Cell 6: Feature correlations
correlation_matrix = data_pivot.corr()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Feature Correlations')
plt.show()

---
# File: infrastructure/jupyterhub/utils/feature_store.py
# Feature Store utility functions

import pandas as pd
from sqlalchemy import create_engine
from typing import List, Optional
import logging

logger = logging.getLogger(__name__)


class FeatureStore:
    """Feature Store client for Jupyter notebooks"""

    def __init__(self, url: str):
        self.engine = create_engine(url)
        logger.info("FeatureStore initialized")

    def get_historical_features(
        self,
        symbols: List[str],
        feature_names: List[str],
        start_date: str,
        end_date: str
    ) -> pd.DataFrame:
        """
        Get historical features for symbols

        Args:
            symbols: List of ticker symbols
            feature_names: List of feature names
            start_date: Start date (YYYY-MM-DD)
            end_date: End date (YYYY-MM-DD)

        Returns:
            DataFrame with multi-index (timestamp, symbol, feature_name)
        """
        symbols_str = "', '".join(symbols)
        features_str = "', '".join(feature_names)

        query = f"""
        SELECT timestamp, symbol, feature_name, feature_value
        FROM features
        WHERE symbol IN ('{symbols_str}')
          AND feature_name IN ('{features_str}')
          AND timestamp >= '{start_date}'
          AND timestamp <= '{end_date}'
        ORDER BY timestamp, symbol, feature_name
        """

        df = pd.read_sql(query, self.engine)

        logger.info(
            f"Retrieved {len(df)} rows for {len(symbols)} symbols, "
            f"{len(feature_names)} features"
        )

        return df

    def get_latest_features(
        self,
        symbols: List[str],
        feature_names: List[str]
    ) -> pd.DataFrame:
        """Get latest feature values"""
        symbols_str = "', '".join(symbols)
        features_str = "', '".join(feature_names)

        query = f"""
        SELECT DISTINCT ON (symbol, feature_name)
            timestamp, symbol, feature_name, feature_value
        FROM features
        WHERE symbol IN ('{symbols_str}')
          AND feature_name IN ('{features_str}')
        ORDER BY symbol, feature_name, timestamp DESC
        """

        return pd.read_sql(query, self.engine)

    def list_features(self) -> pd.DataFrame:
        """List all available features"""
        query = """
        SELECT feature_name, data_type, description, category
        FROM feature_metadata
        ORDER BY category, feature_name
        """

        return pd.read_sql(query, self.engine)

    @property
    def is_pit_validated(self) -> bool:
        """Check if features are PIT-validated"""
        return True  # All features in store are PIT-validated
