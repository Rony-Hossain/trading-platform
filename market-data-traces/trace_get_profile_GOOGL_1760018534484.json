{"traceEvents": [{"ph": "M", "pid": 1, "tid": 1, "name": "process_name", "args": {"name": "MainProcess"}}, {"ph": "M", "pid": 1, "tid": 1, "name": "thread_name", "args": {"name": "MainThread"}}, {"pid": 1, "tid": 1, "ts": 38293908862.859, "ph": "X", "dur": 43.291255805776785, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293908855.955, "ph": "X", "dur": 50.5483599298268, "name": "FinnhubProvider.get_company_profile (/app/app/providers/finnhub_provider.py:124)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293908844.018, "ph": "X", "dur": 62.73174278972899, "name": "MarketDataService.get_company_profile (/app/app/services/market_data.py:280)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293941070.689, "ph": "X", "dur": 33.982870651535094, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293941132.882, "ph": "X", "dur": 4.87269019291422, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293941145.939, "ph": "X", "dur": 11.281769436451393, "name": "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293941165.789, "ph": "X", "dur": 51.66046031059056, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293941129.943, "ph": "X", "dur": 87.74157102478871, "name": "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293941128.792, "ph": "X", "dur": 89.13763543720302, "name": "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293941126.344, "ph": "X", "dur": 91.78722978700083, "name": "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294040527.464, "ph": "X", "dur": 36.62252073563307, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294040591.137, "ph": "X", "dur": 5.003623024628582, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294040605.131, "ph": "X", "dur": 8.955363721897903, "name": "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294040623.562, "ph": "X", "dur": 55.628774784026255, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294040589.312, "ph": "X", "dur": 90.17128661077926, "name": "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294040588.048, "ph": "X", "dur": 91.63419858706463, "name": "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294040585.836, "ph": "X", "dur": 94.04955023371099, "name": "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294140945.714, "ph": "X", "dur": 31.203448388434023, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294141001.252, "ph": "X", "dur": 4.347301488440135, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294141013.59, "ph": "X", "dur": 7.7603944603022725, "name": "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294141030.274, "ph": "X", "dur": 47.03969151540523, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294140999.337, "ph": "X", "dur": 78.24479728145589, "name": "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294140998.082, "ph": "X", "dur": 79.6889256447527, "name": "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294140996.17, "ph": "X", "dur": 81.80539686119877, "name": "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294241269.074, "ph": "X", "dur": 22.961309500895783, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294241309.282, "ph": "X", "dur": 2.774450130251158, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294241317.526, "ph": "X", "dur": 4.464975299221398, "name": "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294241327.266, "ph": "X", "dur": 28.057745672056065, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294241308.212, "ph": "X", "dur": 47.218135838796535, "name": "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294241307.67, "ph": "X", "dur": 47.83191801615796, "name": "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294241306.237, "ph": "X", "dur": 49.326872626364974, "name": "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294341671.184, "ph": "X", "dur": 29.621205223750763, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294341724.227, "ph": "X", "dur": 3.9047816647979263, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294341736.668, "ph": "X", "dur": 7.667581313770573, "name": "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294341752.612, "ph": "X", "dur": 46.154651868120816, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294341722.722, "ph": "X", "dur": 76.37914254432762, "name": "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294341721.68, "ph": "X", "dur": 77.62935772647785, "name": "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294341719.406, "ph": "X", "dur": 80.1038225081176, "name": "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294442038.684, "ph": "X", "dur": 37.762244076674115, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294442111.643, "ph": "X", "dur": 6.0102036971330834, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294442127.975, "ph": "X", "dur": 8.92387354718179, "name": "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294442148.319, "ph": "X", "dur": 59.37555311603807, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294442108.204, "ph": "X", "dur": 100.01003260234496, "name": "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294442106.049, "ph": "X", "dur": 102.35301209306482, "name": "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294442102.859, "ph": "X", "dur": 105.87051985477513, "name": "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294542512.198, "ph": "X", "dur": 47.86561802769626, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294542603.715, "ph": "X", "dur": 6.3599103742435945, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294542664.828, "ph": "X", "dur": 10.414960942949987, "name": "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294542686.841, "ph": "X", "dur": 64.51894831966975, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294542601.257, "ph": "X", "dur": 150.7567253869627, "name": "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294542599.906, "ph": "X", "dur": 152.61464569521334, "name": "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294542596.803, "ph": "X", "dur": 156.2354633283607, "name": "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294643347.352, "ph": "X", "dur": 40.946066478234734, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294643420.66, "ph": "X", "dur": 6.26378247247862, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294643438.707, "ph": "X", "dur": 9.817752541754944, "name": "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294643460.732, "ph": "X", "dur": 63.728379196533666, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294643417.912, "ph": "X", "dur": 107.24559081737868, "name": "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294643415.687, "ph": "X", "dur": 110.03329996856294, "name": "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294643412.597, "ph": "X", "dur": 113.60108151797792, "name": "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294743804.678, "ph": "X", "dur": 32.626030842714535, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294743862.061, "ph": "X", "dur": 4.125212887810712, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294743874.218, "ph": "X", "dur": 7.07589750463099, "name": "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294743889.469, "ph": "X", "dur": 45.62650086761901, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294743860.528, "ph": "X", "dur": 74.80629118613865, "name": "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294743859.578, "ph": "X", "dur": 75.91894402610795, "name": "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294743857.121, "ph": "X", "dur": 78.56467116146693, "name": "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294770162.726, "ph": "X", "dur": 33.634821352041214, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294770235.234, "ph": "X", "dur": 4.066099752817308, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294770259.422, "ph": "X", "dur": 45.37071225545128, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294770232.087, "ph": "X", "dur": 73.02074303381453, "name": "MarketDataService._real_time_broadcast_task (/app/app/services/market_data.py:246)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782492.325, "ph": "X", "dur": 31.629946895115403, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782564.198, "ph": "X", "dur": 5.077100098966177, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782603.787, "ph": "X", "dur": 32.487363582122526, "name": "ListCommands.llen (/usr/local/lib/python3.11/site-packages/redis/commands/core.py:2665)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782641.839, "ph": "X", "dur": 0.9087953931228903, "name": "Redis.initialize (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:399)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782666.244, "ph": "X", "dur": 31.804524004067883, "name": "deprecated_args.<locals>.decorator.<locals>.wrapper (/usr/local/lib/python3.11/site-packages/redis/utils.py:169)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782717.984, "ph": "X", "dur": 0.48616410088033035, "name": "Lock.acquire.<locals>.<genexpr> (/usr/local/lib/python3.11/asyncio/locks.py:100)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782710.793, "ph": "X", "dur": 9.810018112877302, "name": "Lock.acquire (/usr/local/lib/python3.11/asyncio/locks.py:93)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782706.413, "ph": "X", "dur": 14.56779679103799, "name": "_ContextManagerMixin.__aenter__ (/usr/local/lib/python3.11/asyncio/locks.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782722.914, "ph": "X", "dur": 7.816192840062402, "name": "ConnectionPool.get_available_connection (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1156)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782743.241, "ph": "X", "dur": 2.145199095134458, "name": "AbstractConnection.is_connected (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:259)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782740.087, "ph": "X", "dur": 5.976503685594788, "name": "AbstractConnection.connect_check_health (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:298)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782737.021, "ph": "X", "dur": 10.090667389294584, "name": "AbstractConnection.connect (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:294)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782778.694, "ph": "X", "dur": 10.039841142384368, "name": "Timeout.__init__ (/usr/local/lib/python3.11/asyncio/timeouts.py:33)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782759.346, "ph": "X", "dur": 29.80627905760862, "name": "timeout (/usr/local/lib/python3.11/asyncio/timeouts.py:129)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782797.849, "ph": "X", "dur": 4.835675426142649, "name": "current_task (/usr/local/lib/python3.11/asyncio/tasks.py:35)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782807.804, "ph": "X", "dur": 8.947076833814716, "name": "Timeout.reschedule (/usr/local/lib/python3.11/asyncio/timeouts.py:50)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782791.573, "ph": "X", "dur": 25.612561228310224, "name": "Timeout.__aenter__ (/usr/local/lib/python3.11/asyncio/timeouts.py:85)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782820.965, "ph": "X", "dur": 0.7861494494917161, "name": "StreamReader.at_eof (/usr/local/lib/python3.11/asyncio/streams.py:493)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782823.845, "ph": "X", "dur": 6.802430197885804, "name": "Timeout.__aexit__ (/usr/local/lib/python3.11/asyncio/timeouts.py:97)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782753.192, "ph": "X", "dur": 79.61765840723727, "name": "_AsyncRESPBase.can_read_destructive (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:242)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782749.126, "ph": "X", "dur": 84.2058321092954, "name": "AbstractConnection.can_read_destructive (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:563)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782733.109, "ph": "X", "dur": 100.85860994206335, "name": "ConnectionPool.ensure_connection (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1180)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782840.375, "ph": "X", "dur": 0.834765859579749, "name": "Lock._wake_up_first (/usr/local/lib/python3.11/asyncio/locks.py:142)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782838.358, "ph": "X", "dur": 3.311440478041705, "name": "Lock.release (/usr/local/lib/python3.11/asyncio/locks.py:125)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782836.364, "ph": "X", "dur": 15.589293862092228, "name": "_ContextManagerMixin.__aexit__ (/usr/local/lib/python3.11/asyncio/locks.py:20)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782699.532, "ph": "X", "dur": 152.8323146221984, "name": "ConnectionPool.get_connection (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1139)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782863.951, "ph": "X", "dur": 0.5253887044740843, "name": "AbstractBackoff.reset (/usr/local/lib/python3.11/site-packages/redis/backoff.py:13)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782865.692, "ph": "X", "dur": 6.715141643409563, "name": "Redis.execute_command.<locals>.<lambda> (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:678)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782906.742, "ph": "X", "dur": 4.620216335979776, "name": "Encoder.encode (/usr/local/lib/python3.11/site-packages/redis/_parsers/encoders.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782918.462, "ph": "X", "dur": 8.508976683816872, "name": "Encoder.encode (/usr/local/lib/python3.11/site-packages/redis/_parsers/encoders.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782878.393, "ph": "X", "dur": 54.58075967110582, "name": "AbstractConnection.pack_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:630)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782936.723, "ph": "X", "dur": 0.623726443061242, "name": "AbstractConnection.is_connected (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:259)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782939.089, "ph": "X", "dur": 1.0607216746479935, "name": "AbstractConnection.check_health (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:504)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782944.785, "ph": "X", "dur": 7.867571546178165, "name": "StreamWriter.writelines (/usr/local/lib/python3.11/asyncio/streams.py:348)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782956.882, "ph": "X", "dur": 2.0733793984135, "name": "StreamReader.exception (/usr/local/lib/python3.11/asyncio/streams.py:460)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782964.278, "ph": "X", "dur": 1.5048988759068407, "name": "FlowControlMixin._drain_helper (/usr/local/lib/python3.11/asyncio/streams.py:164)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782954.82, "ph": "X", "dur": 11.463528515075971, "name": "StreamWriter.drain (/usr/local/lib/python3.11/asyncio/streams.py:369)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782935.916, "ph": "X", "dur": 30.842140068007048, "name": "AbstractConnection.send_packed_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:516)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782876.015, "ph": "X", "dur": 91.31763946228686, "name": "AbstractConnection.send_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:557)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782977.633, "ph": "X", "dur": 3.2048158513713596, "name": "Connection._host_error (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:781)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783010.052, "ph": "X", "dur": 9.775213182927915, "name": "StreamReader._wait_for_data (/usr/local/lib/python3.11/asyncio/streams.py:519)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783005.049, "ph": "X", "dur": 15.14014452798347, "name": "StreamReader.readuntil (/usr/local/lib/python3.11/asyncio/streams.py:578)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783001.768, "ph": "X", "dur": 18.58196537853399, "name": "StreamReader.readline (/usr/local/lib/python3.11/asyncio/streams.py:547)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782994.845, "ph": "X", "dur": 25.69653502755319, "name": "_AsyncRESPBase._readline (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:273)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782991.438, "ph": "X", "dur": 29.320667415933833, "name": "_AsyncRESP2Parser._read_response (/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py:87)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782985.842, "ph": "X", "dur": 35.13143333986488, "name": "_AsyncRESP2Parser.read_response (/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py:74)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782976.095, "ph": "X", "dur": 45.11437118407802, "name": "AbstractConnection.read_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:572)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782971.25, "ph": "X", "dur": 50.1771073437, "name": "Redis.parse_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:689)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782873.119, "ph": "X", "dur": 148.5214754413243, "name": "Redis._send_command_parse_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:647)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782860.849, "ph": "X", "dur": 160.99489938413802, "name": "Retry.call_with_retry (/usr/local/lib/python3.11/site-packages/redis/asyncio/retry.py:37)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782638.205, "ph": "X", "dur": 383.95307080309084, "name": "Redis.execute_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:667)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782591.745, "ph": "X", "dur": 430.6999589395567, "name": "CollectorJobs.backfill_depth (/app/app/services/data_collector.py:97)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294782561.721, "ph": "X", "dur": 461.0338365384616, "name": "DataCollectorService.metrics_reporter (/app/app/services/data_collector.py:447)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783543.649, "ph": "X", "dur": 3.1893469936160765, "name": "StreamReaderProtocol._stream_reader (/usr/local/lib/python3.11/asyncio/streams.py:211)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783555.986, "ph": "X", "dur": 9.138780178139118, "name": "StreamReader._wakeup_waiter (/usr/local/lib/python3.11/asyncio/streams.py:472)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783549.617, "ph": "X", "dur": 20.553139823921512, "name": "StreamReader.feed_data (/usr/local/lib/python3.11/asyncio/streams.py:497)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783540.572, "ph": "X", "dur": 29.912351225073415, "name": "StreamReaderProtocol.data_received (/usr/local/lib/python3.11/asyncio/streams.py:284)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783584.051, "ph": "X", "dur": 1.549648071556053, "name": "StreamReader._wait_for_data (/usr/local/lib/python3.11/asyncio/streams.py:519)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783597.861, "ph": "X", "dur": 0.7187494264151247, "name": "StreamReader._maybe_resume_transport (/usr/local/lib/python3.11/asyncio/streams.py:484)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783583.497, "ph": "X", "dur": 18.45379484284736, "name": "StreamReader.readuntil (/usr/local/lib/python3.11/asyncio/streams.py:578)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783582.876, "ph": "X", "dur": 20.130508531678952, "name": "StreamReader.readline (/usr/local/lib/python3.11/asyncio/streams.py:547)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783582.474, "ph": "X", "dur": 29.976436492916733, "name": "_AsyncRESPBase._readline (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:273)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783582.203, "ph": "X", "dur": 36.62859778689407, "name": "_AsyncRESP2Parser._read_response (/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py:87)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783621.003, "ph": "X", "dur": 1.7855481523241223, "name": "_AsyncRESPBase._clear (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:225)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783581.502, "ph": "X", "dur": 41.633325729933745, "name": "_AsyncRESP2Parser.read_response (/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py:74)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783581.084, "ph": "X", "dur": 46.85240784472521, "name": "AbstractConnection.read_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:572)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783635.812, "ph": "X", "dur": 9.286286786019854, "name": "CaseInsensitiveDict.__contains__ (/usr/local/lib/python3.11/site-packages/redis/client.py:88)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783580.701, "ph": "X", "dur": 64.87473204804127, "name": "Redis.parse_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:689)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783580.47, "ph": "X", "dur": 65.811702860647, "name": "Redis._send_command_parse_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:647)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783580.18, "ph": "X", "dur": 66.94258685439931, "name": "Retry.call_with_retry (/usr/local/lib/python3.11/site-packages/redis/asyncio/retry.py:37)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783676.404, "ph": "X", "dur": 1.5927398895886276, "name": "AfterConnectionReleasedEvent.__init__ (/usr/local/lib/python3.11/site-packages/redis/event.py:98)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783690.688, "ph": "X", "dur": 0.2950132157614732, "name": "AfterConnectionReleasedEvent.connection (/usr/local/lib/python3.11/site-packages/redis/event.py:101)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783692.667, "ph": "X", "dur": 1.533074295389678, "name": "AbstractConnection.re_auth (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:717)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783689.014, "ph": "X", "dur": 5.617405201989999, "name": "AsyncReAuthConnectionListener.listen (/usr/local/lib/python3.11/site-packages/redis/event.py:243)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783679.853, "ph": "X", "dur": 15.500900389204897, "name": "EventDispatcher.dispatch_async (/usr/local/lib/python3.11/site-packages/redis/event.py:86)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783665.732, "ph": "X", "dur": 30.425585827025493, "name": "ConnectionPool.release (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1196)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783579.793, "ph": "X", "dur": 116.82744327836556, "name": "Redis.execute_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:667)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783578.503, "ph": "X", "dur": 119.77536559915812, "name": "CollectorJobs.backfill_depth (/app/app/services/data_collector.py:97)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783707.732, "ph": "X", "dur": 3.5904323768423487, "name": "MetricWrapperBase._is_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:67)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783706.133, "ph": "X", "dur": 5.7571773809930935, "name": "MetricWrapperBase._raise_if_not_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783717.657, "ph": "X", "dur": 9.406722892828846, "name": "MutexValue.set (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:22)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783703.891, "ph": "X", "dur": 23.64414907895043, "name": "Gauge.set (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:409)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783733.863, "ph": "X", "dur": 3.8086537630329516, "name": "ListCommands.llen (/usr/local/lib/python3.11/site-packages/redis/commands/core.py:2665)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783739.639, "ph": "X", "dur": 0.43644277238120566, "name": "Redis.initialize (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:399)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783743.052, "ph": "X", "dur": 12.924783113744692, "name": "deprecated_args.<locals>.decorator.<locals>.wrapper (/usr/local/lib/python3.11/site-packages/redis/utils.py:169)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783766.284, "ph": "X", "dur": 0.38285422944326014, "name": "Lock.acquire.<locals>.<genexpr> (/usr/local/lib/python3.11/asyncio/locks.py:100)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783761.987, "ph": "X", "dur": 5.868774140513351, "name": "Lock.acquire (/usr/local/lib/python3.11/asyncio/locks.py:93)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783759.864, "ph": "X", "dur": 8.384120903363515, "name": "_ContextManagerMixin.__aenter__ (/usr/local/lib/python3.11/asyncio/locks.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783769.823, "ph": "X", "dur": 2.795996039267445, "name": "ConnectionPool.get_available_connection (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1156)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783778.31, "ph": "X", "dur": 1.6010267776718152, "name": "AbstractConnection.is_connected (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:259)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783777.047, "ph": "X", "dur": 3.299838834725242, "name": "AbstractConnection.connect_check_health (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:298)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783775.26, "ph": "X", "dur": 5.759387217815277, "name": "AbstractConnection.connect (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:294)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783797.186, "ph": "X", "dur": 4.550606476081001, "name": "Timeout.__init__ (/usr/local/lib/python3.11/asyncio/timeouts.py:33)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783785.424, "ph": "X", "dur": 16.62957454613503, "name": "timeout (/usr/local/lib/python3.11/asyncio/timeouts.py:129)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783807.107, "ph": "X", "dur": 3.894837399098101, "name": "current_task (/usr/local/lib/python3.11/asyncio/tasks.py:35)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783815.041, "ph": "X", "dur": 7.362623832309276, "name": "Timeout.reschedule (/usr/local/lib/python3.11/asyncio/timeouts.py:50)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783804.138, "ph": "X", "dur": 18.694114597259794, "name": "Timeout.__aenter__ (/usr/local/lib/python3.11/asyncio/timeouts.py:85)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783824.346, "ph": "X", "dur": 0.29004108291156067, "name": "StreamReader.at_eof (/usr/local/lib/python3.11/asyncio/streams.py:493)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783826.252, "ph": "X", "dur": 5.130688641904123, "name": "Timeout.__aexit__ (/usr/local/lib/python3.11/asyncio/timeouts.py:97)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783783.499, "ph": "X", "dur": 48.88490526192831, "name": "_AsyncRESPBase.can_read_destructive (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:242)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783782.186, "ph": "X", "dur": 50.48758941721676, "name": "AbstractConnection.can_read_destructive (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:563)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783773.961, "ph": "X", "dur": 59.34130064529423, "name": "ConnectionPool.ensure_connection (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1180)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783837.826, "ph": "X", "dur": 0.48671656008587616, "name": "Lock._wake_up_first (/usr/local/lib/python3.11/asyncio/locks.py:142)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783836.348, "ph": "X", "dur": 2.2927057030151943, "name": "Lock.release (/usr/local/lib/python3.11/asyncio/locks.py:125)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783835.167, "ph": "X", "dur": 3.85395541788771, "name": "_ContextManagerMixin.__aexit__ (/usr/local/lib/python3.11/asyncio/locks.py:20)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783756.986, "ph": "X", "dur": 82.55895121756329, "name": "ConnectionPool.get_connection (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1139)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783844.421, "ph": "X", "dur": 0.23479516235697773, "name": "AbstractBackoff.reset (/usr/local/lib/python3.11/site-packages/redis/backoff.py:13)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783845.592, "ph": "X", "dur": 4.80915738427645, "name": "Redis.execute_command.<locals>.<lambda> (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:678)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783870.316, "ph": "X", "dur": 2.4225336163184643, "name": "Encoder.encode (/usr/local/lib/python3.11/site-packages/redis/_parsers/encoders.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783877.399, "ph": "X", "dur": 5.336755925572717, "name": "Encoder.encode (/usr/local/lib/python3.11/site-packages/redis/_parsers/encoders.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783854.718, "ph": "X", "dur": 32.542057043471566, "name": "AbstractConnection.pack_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:630)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783890.106, "ph": "X", "dur": 0.493898529757972, "name": "AbstractConnection.is_connected (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:259)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783891.789, "ph": "X", "dur": 0.6629510466549959, "name": "AbstractConnection.check_health (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:504)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783895.572, "ph": "X", "dur": 3.424142155973054, "name": "StreamWriter.writelines (/usr/local/lib/python3.11/asyncio/streams.py:348)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783901.704, "ph": "X", "dur": 0.5209690308297177, "name": "StreamReader.exception (/usr/local/lib/python3.11/asyncio/streams.py:460)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783905.053, "ph": "X", "dur": 0.887249484106603, "name": "FlowControlMixin._drain_helper (/usr/local/lib/python3.11/asyncio/streams.py:164)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783900.426, "ph": "X", "dur": 5.957167613400684, "name": "StreamWriter.drain (/usr/local/lib/python3.11/asyncio/streams.py:369)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783889.581, "ph": "X", "dur": 26.87824526821572, "name": "AbstractConnection.send_packed_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:516)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783853.342, "ph": "X", "dur": 63.624516865891046, "name": "AbstractConnection.send_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:557)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783923.233, "ph": "X", "dur": 1.8501858793729846, "name": "Connection._host_error (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:781)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783940.172, "ph": "X", "dur": 5.994734839377801, "name": "StreamReader._wait_for_data (/usr/local/lib/python3.11/asyncio/streams.py:519)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783937.258, "ph": "X", "dur": 9.070827695856982, "name": "StreamReader.readuntil (/usr/local/lib/python3.11/asyncio/streams.py:578)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783935.634, "ph": "X", "dur": 10.870739787525295, "name": "StreamReader.readline (/usr/local/lib/python3.11/asyncio/streams.py:547)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783931.576, "ph": "X", "dur": 15.056723187946048, "name": "_AsyncRESPBase._readline (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:273)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783930.167, "ph": "X", "dur": 16.587587646513544, "name": "_AsyncRESP2Parser._read_response (/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py:87)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783927.805, "ph": "X", "dur": 19.094095062074974, "name": "_AsyncRESP2Parser.read_response (/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py:74)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783922.207, "ph": "X", "dur": 24.815915053913137, "name": "AbstractConnection.read_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:572)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783919.897, "ph": "X", "dur": 27.25557490560352, "name": "Redis.parse_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:689)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783850.861, "ph": "X", "dur": 96.40689366377507, "name": "Redis._send_command_parse_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:647)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783843.052, "ph": "X", "dur": 104.32915867130225, "name": "Retry.call_with_retry (/usr/local/lib/python3.11/site-packages/redis/asyncio/retry.py:37)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783738.145, "ph": "X", "dur": 209.351101186359, "name": "Redis.execute_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:667)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783731.11, "ph": "X", "dur": 216.53251839924923, "name": "CollectorJobs.backfill_depth (/app/app/services/data_collector.py:97)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294783577.037, "ph": "X", "dur": 370.82443024249966, "name": "DataCollectorService.metrics_reporter (/app/app/services/data_collector.py:447)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784510.824, "ph": "X", "dur": 2.5009828235059723, "name": "StreamReaderProtocol._stream_reader (/usr/local/lib/python3.11/asyncio/streams.py:211)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784520.496, "ph": "X", "dur": 9.183529373788332, "name": "StreamReader._wakeup_waiter (/usr/local/lib/python3.11/asyncio/streams.py:472)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784515.376, "ph": "X", "dur": 19.13166228805209, "name": "StreamReader.feed_data (/usr/local/lib/python3.11/asyncio/streams.py:497)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784507.8, "ph": "X", "dur": 27.073263367773396, "name": "StreamReaderProtocol.data_received (/usr/local/lib/python3.11/asyncio/streams.py:284)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784548.845, "ph": "X", "dur": 1.1966266392122678, "name": "StreamReader._wait_for_data (/usr/local/lib/python3.11/asyncio/streams.py:519)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784559.069, "ph": "X", "dur": 0.6944412213711082, "name": "StreamReader._maybe_resume_transport (/usr/local/lib/python3.11/asyncio/streams.py:484)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784548.13, "ph": "X", "dur": 13.928049031015918, "name": "StreamReader.readuntil (/usr/local/lib/python3.11/asyncio/streams.py:578)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784547.628, "ph": "X", "dur": 15.289308513480844, "name": "StreamReader.readline (/usr/local/lib/python3.11/asyncio/streams.py:547)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784547.247, "ph": "X", "dur": 23.11599807844862, "name": "_AsyncRESPBase._readline (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:273)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784546.973, "ph": "X", "dur": 28.104152245321917, "name": "_AsyncRESP2Parser._read_response (/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py:87)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784576.859, "ph": "X", "dur": 1.5209201928676699, "name": "_AsyncRESPBase._clear (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:225)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784546.436, "ph": "X", "dur": 32.262512685465374, "name": "_AsyncRESP2Parser.read_response (/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py:74)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784545.857, "ph": "X", "dur": 36.36341736823208, "name": "AbstractConnection.read_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:572)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784587.544, "ph": "X", "dur": 6.236711971406874, "name": "CaseInsensitiveDict.__contains__ (/usr/local/lib/python3.11/site-packages/redis/client.py:88)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784545.349, "ph": "X", "dur": 48.82081999408499, "name": "Redis.parse_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:689)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784544.805, "ph": "X", "dur": 50.05998599212429, "name": "Redis._send_command_parse_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:647)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784544.358, "ph": "X", "dur": 51.11186831948355, "name": "Retry.call_with_retry (/usr/local/lib/python3.11/site-packages/redis/asyncio/retry.py:37)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784607.291, "ph": "X", "dur": 1.0944216861862892, "name": "AfterConnectionReleasedEvent.__init__ (/usr/local/lib/python3.11/site-packages/redis/event.py:98)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784615.505, "ph": "X", "dur": 0.217668926985057, "name": "AfterConnectionReleasedEvent.connection (/usr/local/lib/python3.11/site-packages/redis/event.py:101)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784617.042, "ph": "X", "dur": 0.7734428877641619, "name": "AbstractConnection.re_auth (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:717)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784614.379, "ph": "X", "dur": 3.878816082137272, "name": "AsyncReAuthConnectionListener.listen (/usr/local/lib/python3.11/site-packages/redis/event.py:243)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784609.953, "ph": "X", "dur": 8.979119467736375, "name": "EventDispatcher.dispatch_async (/usr/local/lib/python3.11/site-packages/redis/event.py:86)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784600.206, "ph": "X", "dur": 19.851516632878308, "name": "ConnectionPool.release (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1196)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784543.808, "ph": "X", "dur": 76.67415576008909, "name": "Redis.execute_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:667)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784542.874, "ph": "X", "dur": 78.99503688258713, "name": "CollectorJobs.backfill_depth (/app/app/services/data_collector.py:97)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784628.284, "ph": "X", "dur": 2.430268045196106, "name": "MetricWrapperBase._is_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:67)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784627.024, "ph": "X", "dur": 4.201452258176037, "name": "MetricWrapperBase._raise_if_not_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784634.632, "ph": "X", "dur": 5.292559189129051, "name": "MutexValue.set (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:22)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784625.66, "ph": "X", "dur": 14.619727956359299, "name": "Gauge.set (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:409)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784645.962, "ph": "X", "dur": 2.7954435800618995, "name": "ListCommands.llen (/usr/local/lib/python3.11/site-packages/redis/commands/core.py:2665)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784665.631, "ph": "X", "dur": 0.4016378424318184, "name": "Redis.initialize (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:399)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784668.85, "ph": "X", "dur": 11.95245491198403, "name": "deprecated_args.<locals>.decorator.<locals>.wrapper (/usr/local/lib/python3.11/site-packages/redis/utils.py:169)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784689.539, "ph": "X", "dur": 0.38616898467653515, "name": "Lock.acquire.<locals>.<genexpr> (/usr/local/lib/python3.11/asyncio/locks.py:100)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784686.086, "ph": "X", "dur": 4.8334655893204665, "name": "Lock.acquire (/usr/local/lib/python3.11/asyncio/locks.py:93)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784684.24, "ph": "X", "dur": 7.055456514025795, "name": "_ContextManagerMixin.__aenter__ (/usr/local/lib/python3.11/asyncio/locks.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784692.753, "ph": "X", "dur": 2.5065074155614306, "name": "ConnectionPool.get_available_connection (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1156)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784700.456, "ph": "X", "dur": 1.2441381308892092, "name": "AbstractConnection.is_connected (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:259)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784699.358, "ph": "X", "dur": 2.8120173562282744, "name": "AbstractConnection.connect_check_health (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:298)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784697.486, "ph": "X", "dur": 5.2638313104406675, "name": "AbstractConnection.connect (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:294)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784717.548, "ph": "X", "dur": 3.7633521081781938, "name": "Timeout.__init__ (/usr/local/lib/python3.11/asyncio/timeouts.py:33)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784706.892, "ph": "X", "dur": 14.778836207556497, "name": "timeout (/usr/local/lib/python3.11/asyncio/timeouts.py:129)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784726.725, "ph": "X", "dur": 3.6534127262745733, "name": "current_task (/usr/local/lib/python3.11/asyncio/tasks.py:35)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784734.25, "ph": "X", "dur": 7.544382910933854, "name": "Timeout.reschedule (/usr/local/lib/python3.11/asyncio/timeouts.py:50)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784724.116, "ph": "X", "dur": 18.06154880690982, "name": "Timeout.__aenter__ (/usr/local/lib/python3.11/asyncio/timeouts.py:85)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784743.476, "ph": "X", "dur": 0.28341157244501075, "name": "StreamReader.at_eof (/usr/local/lib/python3.11/asyncio/streams.py:493)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784745.119, "ph": "X", "dur": 5.472660890136992, "name": "Timeout.__aexit__ (/usr/local/lib/python3.11/asyncio/timeouts.py:97)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784705.332, "ph": "X", "dur": 46.09609119233296, "name": "_AsyncRESPBase.can_read_destructive (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:242)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784704.068, "ph": "X", "dur": 47.77004258513683, "name": "AbstractConnection.can_read_destructive (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:563)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784696.441, "ph": "X", "dur": 55.98179621637004, "name": "ConnectionPool.ensure_connection (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1180)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784756.241, "ph": "X", "dur": 0.6148870957725087, "name": "Lock._wake_up_first (/usr/local/lib/python3.11/asyncio/locks.py:142)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784754.797, "ph": "X", "dur": 2.382756553519165, "name": "Lock.release (/usr/local/lib/python3.11/asyncio/locks.py:125)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784753.865, "ph": "X", "dur": 3.7152881572957064, "name": "_ContextManagerMixin.__aexit__ (/usr/local/lib/python3.11/asyncio/locks.py:20)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784681.931, "ph": "X", "dur": 76.06037358272768, "name": "ConnectionPool.get_connection (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1139)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784762.318, "ph": "X", "dur": 0.4276034250924724, "name": "AbstractBackoff.reset (/usr/local/lib/python3.11/site-packages/redis/backoff.py:13)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784763.559, "ph": "X", "dur": 3.710316024445794, "name": "Redis.execute_command.<locals>.<lambda> (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:678)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784784.257, "ph": "X", "dur": 2.217018791855416, "name": "Encoder.encode (/usr/local/lib/python3.11/site-packages/redis/_parsers/encoders.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784790.804, "ph": "X", "dur": 4.8307032932927365, "name": "Encoder.encode (/usr/local/lib/python3.11/site-packages/redis/_parsers/encoders.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784770.787, "ph": "X", "dur": 29.379228091721693, "name": "AbstractConnection.pack_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:630)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784802.658, "ph": "X", "dur": 0.5281510005018134, "name": "AbstractConnection.is_connected (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:259)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784804.309, "ph": "X", "dur": 0.5756624921787548, "name": "AbstractConnection.check_health (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:504)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784807.474, "ph": "X", "dur": 3.6373914093137447, "name": "StreamWriter.writelines (/usr/local/lib/python3.11/asyncio/streams.py:348)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784813.455, "ph": "X", "dur": 0.6933363029600166, "name": "StreamReader.exception (/usr/local/lib/python3.11/asyncio/streams.py:460)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784816.752, "ph": "X", "dur": 0.8579691462126738, "name": "FlowControlMixin._drain_helper (/usr/local/lib/python3.11/asyncio/streams.py:164)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784812.409, "ph": "X", "dur": 5.60525109946799, "name": "StreamWriter.drain (/usr/local/lib/python3.11/asyncio/streams.py:369)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784802.135, "ph": "X", "dur": 16.28097278743561, "name": "AbstractConnection.send_packed_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:516)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784769.648, "ph": "X", "dur": 49.31471852384297, "name": "AbstractConnection.send_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:557)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784824.054, "ph": "X", "dur": 1.5695366029557027, "name": "Connection._host_error (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:781)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784838.602, "ph": "X", "dur": 5.800821658231214, "name": "StreamReader._wait_for_data (/usr/local/lib/python3.11/asyncio/streams.py:519)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784835.729, "ph": "X", "dur": 8.818353838922537, "name": "StreamReader.readuntil (/usr/local/lib/python3.11/asyncio/streams.py:578)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784834.397, "ph": "X", "dur": 10.296182213757632, "name": "StreamReader.readline (/usr/local/lib/python3.11/asyncio/streams.py:547)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784830.835, "ph": "X", "dur": 13.980532655542772, "name": "_AsyncRESPBase._readline (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:273)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784829.748, "ph": "X", "dur": 15.19152323409923, "name": "_AsyncRESP2Parser._read_response (/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py:87)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784827.813, "ph": "X", "dur": 17.258273122046184, "name": "_AsyncRESP2Parser.read_response (/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py:74)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784823.328, "ph": "X", "dur": 21.8729648659705, "name": "AbstractConnection.read_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:572)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784821.532, "ph": "X", "dur": 23.795522901269987, "name": "Redis.parse_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:689)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784767.698, "ph": "X", "dur": 77.73708727155928, "name": "Redis._send_command_parse_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:647)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784761.12, "ph": "X", "dur": 84.42681579151373, "name": "Retry.call_with_retry (/usr/local/lib/python3.11/site-packages/redis/asyncio/retry.py:37)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784664.225, "ph": "X", "dur": 189.9183486312844, "name": "Redis.execute_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:667)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784643.272, "ph": "X", "dur": 211.09245260223943, "name": "CollectorJobs.backfill_depth (/app/app/services/data_collector.py:97)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294784542.188, "ph": "X", "dur": 312.38750531668404, "name": "DataCollectorService.metrics_reporter (/app/app/services/data_collector.py:447)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786129.295, "ph": "X", "dur": 3.374973286679475, "name": "StreamReaderProtocol._stream_reader (/usr/local/lib/python3.11/asyncio/streams.py:211)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786145.251, "ph": "X", "dur": 16.70305162047262, "name": "StreamReader._wakeup_waiter (/usr/local/lib/python3.11/asyncio/streams.py:472)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786136.494, "ph": "X", "dur": 33.4525098142111, "name": "StreamReader.feed_data (/usr/local/lib/python3.11/asyncio/streams.py:497)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786126.066, "ph": "X", "dur": 44.51716278288298, "name": "StreamReaderProtocol.data_received (/usr/local/lib/python3.11/asyncio/streams.py:284)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786200.64, "ph": "X", "dur": 2.6987632190913793, "name": "StreamReader._wait_for_data (/usr/local/lib/python3.11/asyncio/streams.py:519)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786216.548, "ph": "X", "dur": 1.406561137319683, "name": "StreamReader._maybe_resume_transport (/usr/local/lib/python3.11/asyncio/streams.py:484)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786199.649, "ph": "X", "dur": 21.84092223204884, "name": "StreamReader.readuntil (/usr/local/lib/python3.11/asyncio/streams.py:578)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786198.791, "ph": "X", "dur": 24.165670568985693, "name": "StreamReader.readline (/usr/local/lib/python3.11/asyncio/streams.py:547)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786197.743, "ph": "X", "dur": 43.017788499031596, "name": "_AsyncRESPBase._readline (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:273)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786196.14, "ph": "X", "dur": 51.21904540535944, "name": "_AsyncRESP2Parser._read_response (/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py:87)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786250.232, "ph": "X", "dur": 3.521927435354666, "name": "_AsyncRESPBase._clear (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:225)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786195.148, "ph": "X", "dur": 59.14186287209218, "name": "_AsyncRESP2Parser.read_response (/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py:74)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786193.883, "ph": "X", "dur": 66.81054910427386, "name": "AbstractConnection.read_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:572)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786267.724, "ph": "X", "dur": 5.91131349934038, "name": "CaseInsensitiveDict.__contains__ (/usr/local/lib/python3.11/site-packages/redis/client.py:88)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786192.337, "ph": "X", "dur": 81.69214272406188, "name": "Redis.parse_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:689)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786191.279, "ph": "X", "dur": 83.5898400951118, "name": "Redis._send_command_parse_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:647)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786190.301, "ph": "X", "dur": 85.56653913255478, "name": "Retry.call_with_retry (/usr/local/lib/python3.11/site-packages/redis/asyncio/retry.py:37)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786287.426, "ph": "X", "dur": 0.8148773281800992, "name": "AfterConnectionReleasedEvent.__init__ (/usr/local/lib/python3.11/site-packages/redis/event.py:98)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786295.103, "ph": "X", "dur": 0.28727878688383157, "name": "AfterConnectionReleasedEvent.connection (/usr/local/lib/python3.11/site-packages/redis/event.py:101)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786296.887, "ph": "X", "dur": 0.704937946276479, "name": "AbstractConnection.re_auth (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:717)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786293.877, "ph": "X", "dur": 4.19703258453167, "name": "AsyncReAuthConnectionListener.listen (/usr/local/lib/python3.11/site-packages/redis/event.py:243)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786289.507, "ph": "X", "dur": 9.250929396864922, "name": "EventDispatcher.dispatch_async (/usr/local/lib/python3.11/site-packages/redis/event.py:86)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786280.761, "ph": "X", "dur": 18.757094946692018, "name": "ConnectionPool.release (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1196)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786188.683, "ph": "X", "dur": 111.28903974276862, "name": "Redis.execute_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:667)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786186.904, "ph": "X", "dur": 114.31596372995422, "name": "CollectorJobs.backfill_depth (/app/app/services/data_collector.py:97)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786307.326, "ph": "X", "dur": 2.3363499802533148, "name": "MetricWrapperBase._is_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:67)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786306.331, "ph": "X", "dur": 3.693189789073873, "name": "MetricWrapperBase._raise_if_not_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786313.516, "ph": "X", "dur": 5.1378706115762185, "name": "MutexValue.set (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:22)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786304.99, "ph": "X", "dur": 14.080527771746567, "name": "Gauge.set (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:409)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786324.873, "ph": "X", "dur": 31.465314051862745, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294786185.318, "ph": "X", "dur": 171.22312911561352, "name": "DataCollectorService.metrics_reporter (/app/app/services/data_collector.py:447)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294844512.461, "ph": "X", "dur": 26.165572893061597, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294844558.103, "ph": "X", "dur": 3.509220873627112, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294844618.982, "ph": "X", "dur": 9.907803392258915, "name": "_format_timetuple_and_zone (/usr/local/lib/python3.11/email/utils.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294844597.205, "ph": "X", "dur": 31.90672895709386, "name": "format_datetime (/usr/local/lib/python3.11/email/utils.py:271)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294844577.835, "ph": "X", "dur": 51.94994893429658, "name": "formatdate (/usr/local/lib/python3.11/email/utils.py:242)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294844567.699, "ph": "X", "dur": 72.12133944718592, "name": "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294844644.875, "ph": "X", "dur": 29.749375759437395, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294844556.651, "ph": "X", "dur": 118.07876337892688, "name": "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294844555.898, "ph": "X", "dur": 118.95938335256692, "name": "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294844554.167, "ph": "X", "dur": 120.75211347456315, "name": "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908831.887, "ph": "X", "dur": 33.83702142127099, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908942.185, "ph": "X", "dur": 4.631265520090692, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909056.967, "ph": "X", "dur": 8.369756964019324, "name": "ABCMeta.__instancecheck__ (<frozen abc>:117)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909041.181, "ph": "X", "dur": 25.46173986519621, "name": "iscoroutine (/usr/local/lib/python3.11/asyncio/coroutines.py:34)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909077.283, "ph": "X", "dur": 3.675511094496407, "name": "ismethod (/usr/local/lib/python3.11/inspect.py:300)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909083.762, "ph": "X", "dur": 3.192661748849351, "name": "_unwrap_partial (/usr/local/lib/python3.11/functools.py:421)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909088.096, "ph": "X", "dur": 1.9573629652488755, "name": "isfunction (/usr/local/lib/python3.11/inspect.py:378)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909095.254, "ph": "X", "dur": 1.3728611257813874, "name": "isclass (/usr/local/lib/python3.11/inspect.py:292)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909091.951, "ph": "X", "dur": 16.374338393172852, "name": "_signature_is_functionlike (/usr/local/lib/python3.11/inspect.py:2075)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909075.368, "ph": "X", "dur": 33.919890302102864, "name": "_has_code_flag (/usr/local/lib/python3.11/inspect.py:391)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909072.167, "ph": "X", "dur": 37.4931964435733, "name": "iscoroutinefunction (/usr/local/lib/python3.11/inspect.py:409)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909068.165, "ph": "X", "dur": 43.903933064727106, "name": "iscoroutinefunction (/usr/local/lib/python3.11/asyncio/coroutines.py:21)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909147.333, "ph": "X", "dur": 4.8936836427249615, "name": "RLock (/usr/local/lib/python3.11/threading.py:90)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909145.057, "ph": "X", "dur": 20.75644481156238, "name": "Condition.__init__ (/usr/local/lib/python3.11/threading.py:243)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909137.972, "ph": "X", "dur": 32.270799573548565, "name": "Future.__init__ (/usr/local/lib/python3.11/concurrent/futures/_base.py:328)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909174.951, "ph": "X", "dur": 2.215361414238778, "name": "_WorkItem.__init__ (/usr/local/lib/python3.11/concurrent/futures/thread.py:47)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909273.427, "ph": "X", "dur": 3.4882274238163706, "name": "Condition.__enter__ (/usr/local/lib/python3.11/threading.py:271)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909283.031, "ph": "X", "dur": 1.9844334663206211, "name": "Condition.__exit__ (/usr/local/lib/python3.11/threading.py:274)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909269.988, "ph": "X", "dur": 16.013582531951425, "name": "Semaphore.acquire (/usr/local/lib/python3.11/threading.py:440)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909264.045, "ph": "X", "dur": 22.51160770758148, "name": "ThreadPoolExecutor._adjust_thread_count (/usr/local/lib/python3.11/concurrent/futures/thread.py:180)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909122.383, "ph": "X", "dur": 166.25375856172877, "name": "ThreadPoolExecutor.submit (/usr/local/lib/python3.11/concurrent/futures/thread.py:161)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909294.863, "ph": "X", "dur": 7.985797816164972, "name": "isfuture (/usr/local/lib/python3.11/asyncio/base_futures.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909317.479, "ph": "X", "dur": 1.9617826388932422, "name": "isfuture (/usr/local/lib/python3.11/asyncio/base_futures.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909321.004, "ph": "X", "dur": 1.995482650431538, "name": "isfuture (/usr/local/lib/python3.11/asyncio/base_futures.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909323.812, "ph": "X", "dur": 1.4734087011907284, "name": "isfuture (/usr/local/lib/python3.11/asyncio/base_futures.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909326.012, "ph": "X", "dur": 0.7165395895929414, "name": "isfuture (/usr/local/lib/python3.11/asyncio/base_futures.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909328.46, "ph": "X", "dur": 1.6026841552884528, "name": "_get_loop (/usr/local/lib/python3.11/asyncio/futures.py:299)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909339.351, "ph": "X", "dur": 2.2562433954491694, "name": "Condition.__enter__ (/usr/local/lib/python3.11/threading.py:271)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909346.627, "ph": "X", "dur": 1.4341840975969744, "name": "Condition.__exit__ (/usr/local/lib/python3.11/threading.py:274)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909338.08, "ph": "X", "dur": 10.663567585445609, "name": "Future.add_done_callback (/usr/local/lib/python3.11/concurrent/futures/_base.py:408)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909316.651, "ph": "X", "dur": 32.56968000374886, "name": "_chain_future (/usr/local/lib/python3.11/asyncio/futures.py:365)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909292.676, "ph": "X", "dur": 56.97843262317472, "name": "wrap_future (/usr/local/lib/python3.11/asyncio/futures.py:409)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294909008.217, "ph": "X", "dur": 345.1588329304571, "name": "to_thread (/usr/local/lib/python3.11/asyncio/threads.py:12)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908941.322, "ph": "X", "dur": 412.4953231984106, "name": "FinnhubProvider.get_company_profile (/app/app/providers/finnhub_provider.py:124)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908938.73, "ph": "X", "dur": 415.81947023817986, "name": "MarketDataService.get_company_profile (/app/app/services/market_data.py:280)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908936.39, "ph": "X", "dur": 418.5950252868421, "name": "get_company_profile (/app/app/main.py:596)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908934.086, "ph": "X", "dur": 421.1297081218864, "name": "run_endpoint_function (/usr/local/lib/python3.11/site-packages/fastapi/routing.py:280)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908922.872, "ph": "X", "dur": 432.637433373406, "name": "get_request_handler.<locals>.app (/usr/local/lib/python3.11/site-packages/fastapi/routing.py:316)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908919.126, "ph": "X", "dur": 436.5217740475987, "name": "request_response.<locals>.app.<locals>.app (/usr/local/lib/python3.11/site-packages/fastapi/routing.py:103)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908917.827, "ph": "X", "dur": 438.0647926086882, "name": "wrap_app_handling_exceptions.<locals>.wrapped_app (/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py:31)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908915.247, "ph": "X", "dur": 441.0707231460631, "name": "request_response.<locals>.app (/usr/local/lib/python3.11/site-packages/fastapi/routing.py:100)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908911.277, "ph": "X", "dur": 445.31360984465505, "name": "Route.handle (/usr/local/lib/python3.11/site-packages/starlette/routing.py:281)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908910.725, "ph": "X", "dur": 446.1997544103506, "name": "Router.app (/usr/local/lib/python3.11/site-packages/starlette/routing.py:718)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908909.51, "ph": "X", "dur": 447.7626615028397, "name": "Router.__call__ (/usr/local/lib/python3.11/site-packages/starlette/routing.py:712)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908908.579, "ph": "X", "dur": 448.8620553218759, "name": "AsyncExitStackMiddleware.__call__ (/usr/local/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:15)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908907.649, "ph": "X", "dur": 450.16530658775855, "name": "wrap_app_handling_exceptions.<locals>.wrapped_app (/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py:31)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908905.898, "ph": "X", "dur": 452.2276368020611, "name": "ExceptionMiddleware.__call__ (/usr/local/lib/python3.11/site-packages/starlette/middleware/exceptions.py:47)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908904.869, "ph": "X", "dur": 453.6016028462536, "name": "CORSMiddleware.__call__ (/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py:75)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908903.161, "ph": "X", "dur": 455.73961997171597, "name": "ServerErrorMiddleware.__call__ (/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py:149)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908901.111, "ph": "X", "dur": 473.2801997477961, "name": "Starlette.__call__ (/usr/local/lib/python3.11/site-packages/starlette/applications.py:109)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908898.886, "ph": "X", "dur": 475.8944367084389, "name": "FastAPI.__call__ (/usr/local/lib/python3.11/site-packages/fastapi/applications.py:1130)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908897.776, "ph": "X", "dur": 477.79821113074985, "name": "ProxyHeadersMiddleware.__call__ (/usr/local/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py:27)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294908889.223, "ph": "X", "dur": 486.65689449167724, "name": "RequestResponseCycle.run_asgi (/usr/local/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py:407)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935835.795, "ph": "X", "dur": 10.486228180465398, "name": "isfuture (/usr/local/lib/python3.11/asyncio/base_futures.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935863.5, "ph": "X", "dur": 9.475227834316529, "name": "Condition.__enter__ (/usr/local/lib/python3.11/threading.py:271)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935879.964, "ph": "X", "dur": 3.602586479364357, "name": "Condition.__exit__ (/usr/local/lib/python3.11/threading.py:274)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935857.951, "ph": "X", "dur": 26.355066400563818, "name": "Future.done (/usr/local/lib/python3.11/concurrent/futures/_base.py:393)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935890.634, "ph": "X", "dur": 1.1220446464635807, "name": "Condition.__enter__ (/usr/local/lib/python3.11/threading.py:271)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935893.782, "ph": "X", "dur": 0.7938838783693576, "name": "Condition.__exit__ (/usr/local/lib/python3.11/threading.py:274)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935889.731, "ph": "X", "dur": 5.204165716241718, "name": "Future.cancelled (/usr/local/lib/python3.11/concurrent/futures/_base.py:383)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935898.891, "ph": "X", "dur": 0.8226117570577408, "name": "Condition.__enter__ (/usr/local/lib/python3.11/threading.py:271)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935901.687, "ph": "X", "dur": 0.6629510466549959, "name": "Condition.__exit__ (/usr/local/lib/python3.11/threading.py:274)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935897.865, "ph": "X", "dur": 4.909152500480245, "name": "Future.exception (/usr/local/lib/python3.11/concurrent/futures/_base.py:463)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935906.362, "ph": "X", "dur": 0.7309035289371331, "name": "Condition.__enter__ (/usr/local/lib/python3.11/threading.py:271)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935909.629, "ph": "X", "dur": 0.8005133888359076, "name": "Future.__get_result (/usr/local/lib/python3.11/concurrent/futures/_base.py:398)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935911.307, "ph": "X", "dur": 0.7110149975374832, "name": "Condition.__exit__ (/usr/local/lib/python3.11/threading.py:274)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935905.568, "ph": "X", "dur": 6.8880613747454085, "name": "Future.result (/usr/local/lib/python3.11/concurrent/futures/_base.py:428)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935852.536, "ph": "X", "dur": 81.8203132597485, "name": "_copy_future_state (/usr/local/lib/python3.11/asyncio/futures.py:345)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935829.601, "ph": "X", "dur": 105.45949020584902, "name": "_chain_future.<locals>._set_state (/usr/local/lib/python3.11/asyncio/futures.py:381)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935949.353, "ph": "X", "dur": 0.9397331086334567, "name": "_chain_future.<locals>._call_check_cancel (/usr/local/lib/python3.11/asyncio/futures.py:387)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935992.536, "ph": "X", "dur": 1.705441567519977, "name": "to_thread (/usr/local/lib/python3.11/asyncio/threads.py:12)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935991.417, "ph": "X", "dur": 28.67705244147294, "name": "FinnhubProvider.get_company_profile (/app/app/providers/finnhub_provider.py:124)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935989.582, "ph": "X", "dur": 32.74591449031798, "name": "MarketDataService.get_company_profile (/app/app/services/market_data.py:280)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294936029.681, "ph": "B", "name": "maybe_trace (/app/app/observability/trace.py:6)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294936025.42, "ph": "B", "name": "_GeneratorContextManager.__exit__ (/usr/local/lib/python3.11/contextlib.py:141)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935988.561, "ph": "B", "name": "get_company_profile (/app/app/main.py:596)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935987.411, "ph": "B", "name": "run_endpoint_function (/usr/local/lib/python3.11/site-packages/fastapi/routing.py:280)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935986.742, "ph": "B", "name": "get_request_handler.<locals>.app (/usr/local/lib/python3.11/site-packages/fastapi/routing.py:316)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935985.66, "ph": "B", "name": "request_response.<locals>.app.<locals>.app (/usr/local/lib/python3.11/site-packages/fastapi/routing.py:103)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935984.674, "ph": "B", "name": "wrap_app_handling_exceptions.<locals>.wrapped_app (/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py:31)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935984.058, "ph": "B", "name": "request_response.<locals>.app (/usr/local/lib/python3.11/site-packages/fastapi/routing.py:100)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935982.246, "ph": "B", "name": "Route.handle (/usr/local/lib/python3.11/site-packages/starlette/routing.py:281)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935981.669, "ph": "B", "name": "Router.app (/usr/local/lib/python3.11/site-packages/starlette/routing.py:718)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935980.597, "ph": "B", "name": "Router.__call__ (/usr/local/lib/python3.11/site-packages/starlette/routing.py:712)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935979.695, "ph": "B", "name": "AsyncExitStackMiddleware.__call__ (/usr/local/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:15)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935978.93, "ph": "B", "name": "wrap_app_handling_exceptions.<locals>.wrapped_app (/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py:31)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935977.393, "ph": "B", "name": "ExceptionMiddleware.__call__ (/usr/local/lib/python3.11/site-packages/starlette/middleware/exceptions.py:47)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935976.311, "ph": "B", "name": "CORSMiddleware.__call__ (/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py:75)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935975.376, "ph": "B", "name": "ServerErrorMiddleware.__call__ (/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py:149)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935973.334, "ph": "B", "name": "Starlette.__call__ (/usr/local/lib/python3.11/site-packages/starlette/applications.py:109)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935972.073, "ph": "B", "name": "FastAPI.__call__ (/usr/local/lib/python3.11/site-packages/fastapi/applications.py:1130)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935970.691, "ph": "B", "name": "ProxyHeadersMiddleware.__call__ (/usr/local/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py:27)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38294935968.473, "ph": "B", "name": "RequestResponseCycle.run_asgi (/usr/local/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py:407)", "cat": "FEE"}], "viztracer_metadata": {"version": "1.0.4", "overflow": false, "baseTimeNanoseconds": 1759980240576414830}, "file_info": {"files": {"/usr/local/lib/python3.11/asyncio/tasks.py": ["\"\"\"Support for tasks, coroutines and the scheduler.\"\"\"\n\n__all__ = (\n    'Task', 'create_task',\n    'FIRST_COMPLETED', 'FIRST_EXCEPTION', 'ALL_COMPLETED',\n    'wait', 'wait_for', 'as_completed', 'sleep',\n    'gather', 'shield', 'ensure_future', 'run_coroutine_threadsafe',\n    'current_task', 'all_tasks',\n    '_register_task', '_unregister_task', '_enter_task', '_leave_task',\n)\n\nimport concurrent.futures\nimport contextvars\nimport functools\nimport inspect\nimport itertools\nimport types\nimport warnings\nimport weakref\nfrom types import GenericAlias\n\nfrom . import base_tasks\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import futures\nfrom .coroutines import _is_coroutine\n\n# Helper to generate new task names\n# This uses itertools.count() instead of a \"+= 1\" operation because the latter\n# is not thread safe. See bpo-11866 for a longer explanation.\n_task_name_counter = itertools.count(1).__next__\n\n\ndef current_task(loop=None):\n    \"\"\"Return a currently executed task.\"\"\"\n    if loop is None:\n        loop = events.get_running_loop()\n    return _current_tasks.get(loop)\n\n\ndef all_tasks(loop=None):\n    \"\"\"Return a set of all tasks for the loop.\"\"\"\n    if loop is None:\n        loop = events.get_running_loop()\n    # Looping over a WeakSet (_all_tasks) isn't safe as it can be updated from another\n    # thread while we do so. Therefore we cast it to list prior to filtering. The list\n    # cast itself requires iteration, so we repeat it several times ignoring\n    # RuntimeErrors (which are not very likely to occur). See issues 34970 and 36607 for\n    # details.\n    i = 0\n    while True:\n        try:\n            tasks = list(_all_tasks)\n        except RuntimeError:\n            i += 1\n            if i >= 1000:\n                raise\n        else:\n            break\n    return {t for t in tasks\n            if futures._get_loop(t) is loop and not t.done()}\n\n\ndef _set_task_name(task, name):\n    if name is not None:\n        try:\n            set_name = task.set_name\n        except AttributeError:\n            warnings.warn(\"Task.set_name() was added in Python 3.8, \"\n                      \"the method support will be mandatory for third-party \"\n                      \"task implementations since 3.13.\",\n                      DeprecationWarning, stacklevel=3)\n        else:\n            set_name(name)\n\n\nclass Task(futures._PyFuture):  # Inherit Python Task implementation\n                                # from a Python Future implementation.\n\n    \"\"\"A coroutine wrapped in a Future.\"\"\"\n\n    # An important invariant maintained while a Task not done:\n    # _fut_waiter is either None or a Future.  The Future\n    # can be either done() or not done().\n    # The task can be in any of 3 states:\n    #\n    # - 1: _fut_waiter is not None and not _fut_waiter.done():\n    #      __step() is *not* scheduled and the Task is waiting for _fut_waiter.\n    # - 2: (_fut_waiter is None or _fut_waiter.done()) and __step() is scheduled:\n    #       the Task is waiting for __step() to be executed.\n    # - 3:  _fut_waiter is None and __step() is *not* scheduled:\n    #       the Task is currently executing (in __step()).\n    #\n    # * In state 1, one of the callbacks of __fut_waiter must be __wakeup().\n    # * The transition from 1 to 2 happens when _fut_waiter becomes done(),\n    #   as it schedules __wakeup() to be called (which calls __step() so\n    #   we way that __step() is scheduled).\n    # * It transitions from 2 to 3 when __step() is executed, and it clears\n    #   _fut_waiter to None.\n\n    # If False, don't log a message if the task is destroyed while its\n    # status is still pending\n    _log_destroy_pending = True\n\n    def __init__(self, coro, *, loop=None, name=None, context=None):\n        super().__init__(loop=loop)\n        if self._source_traceback:\n            del self._source_traceback[-1]\n        if not coroutines.iscoroutine(coro):\n            # raise after Future.__init__(), attrs are required for __del__\n            # prevent logging for pending task in __del__\n            self._log_destroy_pending = False\n            raise TypeError(f\"a coroutine was expected, got {coro!r}\")\n\n        if name is None:\n            self._name = f'Task-{_task_name_counter()}'\n        else:\n            self._name = str(name)\n\n        self._num_cancels_requested = 0\n        self._must_cancel = False\n        self._fut_waiter = None\n        self._coro = coro\n        if context is None:\n            self._context = contextvars.copy_context()\n        else:\n            self._context = context\n\n        self._loop.call_soon(self.__step, context=self._context)\n        _register_task(self)\n\n    def __del__(self):\n        if self._state == futures._PENDING and self._log_destroy_pending:\n            context = {\n                'task': self,\n                'message': 'Task was destroyed but it is pending!',\n            }\n            if self._source_traceback:\n                context['source_traceback'] = self._source_traceback\n            self._loop.call_exception_handler(context)\n        super().__del__()\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    def __repr__(self):\n        return base_tasks._task_repr(self)\n\n    def get_coro(self):\n        return self._coro\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, value):\n        self._name = str(value)\n\n    def set_result(self, result):\n        raise RuntimeError('Task does not support set_result operation')\n\n    def set_exception(self, exception):\n        raise RuntimeError('Task does not support set_exception operation')\n\n    def get_stack(self, *, limit=None):\n        \"\"\"Return the list of stack frames for this task's coroutine.\n\n        If the coroutine is not done, this returns the stack where it is\n        suspended.  If the coroutine has completed successfully or was\n        cancelled, this returns an empty list.  If the coroutine was\n        terminated by an exception, this returns the list of traceback\n        frames.\n\n        The frames are always ordered from oldest to newest.\n\n        The optional limit gives the maximum number of frames to\n        return; by default all available frames are returned.  Its\n        meaning differs depending on whether a stack or a traceback is\n        returned: the newest frames of a stack are returned, but the\n        oldest frames of a traceback are returned.  (This matches the\n        behavior of the traceback module.)\n\n        For reasons beyond our control, only one stack frame is\n        returned for a suspended coroutine.\n        \"\"\"\n        return base_tasks._task_get_stack(self, limit)\n\n    def print_stack(self, *, limit=None, file=None):\n        \"\"\"Print the stack or traceback for this task's coroutine.\n\n        This produces output similar to that of the traceback module,\n        for the frames retrieved by get_stack().  The limit argument\n        is passed to get_stack().  The file argument is an I/O stream\n        to which the output is written; by default output is written\n        to sys.stderr.\n        \"\"\"\n        return base_tasks._task_print_stack(self, limit, file)\n\n    def cancel(self, msg=None):\n        \"\"\"Request that this task cancel itself.\n\n        This arranges for a CancelledError to be thrown into the\n        wrapped coroutine on the next cycle through the event loop.\n        The coroutine then has a chance to clean up or even deny\n        the request using try/except/finally.\n\n        Unlike Future.cancel, this does not guarantee that the\n        task will be cancelled: the exception might be caught and\n        acted upon, delaying cancellation of the task or preventing\n        cancellation completely.  The task may also return a value or\n        raise a different exception.\n\n        Immediately after this method is called, Task.cancelled() will\n        not return True (unless the task was already cancelled).  A\n        task will be marked as cancelled when the wrapped coroutine\n        terminates with a CancelledError exception (even if cancel()\n        was not called).\n\n        This also increases the task's count of cancellation requests.\n        \"\"\"\n        self._log_traceback = False\n        if self.done():\n            return False\n        self._num_cancels_requested += 1\n        # These two lines are controversial.  See discussion starting at\n        # https://github.com/python/cpython/pull/31394#issuecomment-1053545331\n        # Also remember that this is duplicated in _asynciomodule.c.\n        # if self._num_cancels_requested > 1:\n        #     return False\n        if self._fut_waiter is not None:\n            if self._fut_waiter.cancel(msg=msg):\n                # Leave self._fut_waiter; it may be a Task that\n                # catches and ignores the cancellation so we may have\n                # to cancel it again later.\n                return True\n        # It must be the case that self.__step is already scheduled.\n        self._must_cancel = True\n        self._cancel_message = msg\n        return True\n\n    def cancelling(self):\n        \"\"\"Return the count of the task's cancellation requests.\n\n        This count is incremented when .cancel() is called\n        and may be decremented using .uncancel().\n        \"\"\"\n        return self._num_cancels_requested\n\n    def uncancel(self):\n        \"\"\"Decrement the task's count of cancellation requests.\n\n        This should be called by the party that called `cancel()` on the task\n        beforehand.\n\n        Returns the remaining number of cancellation requests.\n        \"\"\"\n        if self._num_cancels_requested > 0:\n            self._num_cancels_requested -= 1\n        return self._num_cancels_requested\n\n    def __step(self, exc=None):\n        if self.done():\n            raise exceptions.InvalidStateError(\n                f'_step(): already done: {self!r}, {exc!r}')\n        if self._must_cancel:\n            if not isinstance(exc, exceptions.CancelledError):\n                exc = self._make_cancelled_error()\n            self._must_cancel = False\n        coro = self._coro\n        self._fut_waiter = None\n\n        _enter_task(self._loop, self)\n        # Call either coro.throw(exc) or coro.send(None).\n        try:\n            if exc is None:\n                # We use the `send` method directly, because coroutines\n                # don't have `__iter__` and `__next__` methods.\n                result = coro.send(None)\n            else:\n                result = coro.throw(exc)\n        except StopIteration as exc:\n            if self._must_cancel:\n                # Task is cancelled right before coro stops.\n                self._must_cancel = False\n                super().cancel(msg=self._cancel_message)\n            else:\n                super().set_result(exc.value)\n        except exceptions.CancelledError as exc:\n            # Save the original exception so we can chain it later.\n            self._cancelled_exc = exc\n            super().cancel()  # I.e., Future.cancel(self).\n        except (KeyboardInterrupt, SystemExit) as exc:\n            super().set_exception(exc)\n            raise\n        except BaseException as exc:\n            super().set_exception(exc)\n        else:\n            blocking = getattr(result, '_asyncio_future_blocking', None)\n            if blocking is not None:\n                # Yielded Future must come from Future.__iter__().\n                if futures._get_loop(result) is not self._loop:\n                    new_exc = RuntimeError(\n                        f'Task {self!r} got Future '\n                        f'{result!r} attached to a different loop')\n                    self._loop.call_soon(\n                        self.__step, new_exc, context=self._context)\n                elif blocking:\n                    if result is self:\n                        new_exc = RuntimeError(\n                            f'Task cannot await on itself: {self!r}')\n                        self._loop.call_soon(\n                            self.__step, new_exc, context=self._context)\n                    else:\n                        result._asyncio_future_blocking = False\n                        result.add_done_callback(\n                            self.__wakeup, context=self._context)\n                        self._fut_waiter = result\n                        if self._must_cancel:\n                            if self._fut_waiter.cancel(\n                                    msg=self._cancel_message):\n                                self._must_cancel = False\n                else:\n                    new_exc = RuntimeError(\n                        f'yield was used instead of yield from '\n                        f'in task {self!r} with {result!r}')\n                    self._loop.call_soon(\n                        self.__step, new_exc, context=self._context)\n\n            elif result is None:\n                # Bare yield relinquishes control for one event loop iteration.\n                self._loop.call_soon(self.__step, context=self._context)\n            elif inspect.isgenerator(result):\n                # Yielding a generator is just wrong.\n                new_exc = RuntimeError(\n                    f'yield was used instead of yield from for '\n                    f'generator in task {self!r} with {result!r}')\n                self._loop.call_soon(\n                    self.__step, new_exc, context=self._context)\n            else:\n                # Yielding something else is an error.\n                new_exc = RuntimeError(f'Task got bad yield: {result!r}')\n                self._loop.call_soon(\n                    self.__step, new_exc, context=self._context)\n        finally:\n            _leave_task(self._loop, self)\n            self = None  # Needed to break cycles when an exception occurs.\n\n    def __wakeup(self, future):\n        try:\n            future.result()\n        except BaseException as exc:\n            # This may also be a cancellation.\n            self.__step(exc)\n        else:\n            # Don't pass the value of `future.result()` explicitly,\n            # as `Future.__iter__` and `Future.__await__` don't need it.\n            # If we call `_step(value, None)` instead of `_step()`,\n            # Python eval loop would use `.send(value)` method call,\n            # instead of `__next__()`, which is slower for futures\n            # that return non-generator iterators from their `__iter__`.\n            self.__step()\n        self = None  # Needed to break cycles when an exception occurs.\n\n\n_PyTask = Task\n\n\ntry:\n    import _asyncio\nexcept ImportError:\n    pass\nelse:\n    # _CTask is needed for tests.\n    Task = _CTask = _asyncio.Task\n\n\ndef create_task(coro, *, name=None, context=None):\n    \"\"\"Schedule the execution of a coroutine object in a spawn task.\n\n    Return a Task object.\n    \"\"\"\n    loop = events.get_running_loop()\n    if context is None:\n        # Use legacy API if context is not needed\n        task = loop.create_task(coro)\n    else:\n        task = loop.create_task(coro, context=context)\n\n    _set_task_name(task, name)\n    return task\n\n\n# wait() and as_completed() similar to those in PEP 3148.\n\nFIRST_COMPLETED = concurrent.futures.FIRST_COMPLETED\nFIRST_EXCEPTION = concurrent.futures.FIRST_EXCEPTION\nALL_COMPLETED = concurrent.futures.ALL_COMPLETED\n\n\nasync def wait(fs, *, timeout=None, return_when=ALL_COMPLETED):\n    \"\"\"Wait for the Futures or Tasks given by fs to complete.\n\n    The fs iterable must not be empty.\n\n    Coroutines will be wrapped in Tasks.\n\n    Returns two sets of Future: (done, pending).\n\n    Usage:\n\n        done, pending = await asyncio.wait(fs)\n\n    Note: This does not raise TimeoutError! Futures that aren't done\n    when the timeout occurs are returned in the second set.\n    \"\"\"\n    if futures.isfuture(fs) or coroutines.iscoroutine(fs):\n        raise TypeError(f\"expect a list of futures, not {type(fs).__name__}\")\n    if not fs:\n        raise ValueError('Set of Tasks/Futures is empty.')\n    if return_when not in (FIRST_COMPLETED, FIRST_EXCEPTION, ALL_COMPLETED):\n        raise ValueError(f'Invalid return_when value: {return_when}')\n\n    fs = set(fs)\n\n    if any(coroutines.iscoroutine(f) for f in fs):\n        raise TypeError(\"Passing coroutines is forbidden, use tasks explicitly.\")\n\n    loop = events.get_running_loop()\n    return await _wait(fs, timeout, return_when, loop)\n\n\ndef _release_waiter(waiter, *args):\n    if not waiter.done():\n        waiter.set_result(None)\n\n\nasync def wait_for(fut, timeout):\n    \"\"\"Wait for the single Future or coroutine to complete, with timeout.\n\n    Coroutine will be wrapped in Task.\n\n    Returns result of the Future or coroutine.  When a timeout occurs,\n    it cancels the task and raises TimeoutError.  To avoid the task\n    cancellation, wrap it in shield().\n\n    If the wait is cancelled, the task is also cancelled.\n\n    This function is a coroutine.\n    \"\"\"\n    loop = events.get_running_loop()\n\n    if timeout is None:\n        return await fut\n\n    if timeout <= 0:\n        fut = ensure_future(fut, loop=loop)\n\n        if fut.done():\n            return fut.result()\n\n        await _cancel_and_wait(fut, loop=loop)\n        try:\n            return fut.result()\n        except exceptions.CancelledError as exc:\n            raise exceptions.TimeoutError() from exc\n\n    waiter = loop.create_future()\n    timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\n    cb = functools.partial(_release_waiter, waiter)\n\n    fut = ensure_future(fut, loop=loop)\n    fut.add_done_callback(cb)\n\n    try:\n        # wait until the future completes or the timeout\n        try:\n            await waiter\n        except exceptions.CancelledError:\n            if fut.done():\n                return fut.result()\n            else:\n                fut.remove_done_callback(cb)\n                # We must ensure that the task is not running\n                # after wait_for() returns.\n                # See https://bugs.python.org/issue32751\n                await _cancel_and_wait(fut, loop=loop)\n                raise\n\n        if fut.done():\n            return fut.result()\n        else:\n            fut.remove_done_callback(cb)\n            # We must ensure that the task is not running\n            # after wait_for() returns.\n            # See https://bugs.python.org/issue32751\n            await _cancel_and_wait(fut, loop=loop)\n            # In case task cancellation failed with some\n            # exception, we should re-raise it\n            # See https://bugs.python.org/issue40607\n            try:\n                return fut.result()\n            except exceptions.CancelledError as exc:\n                raise exceptions.TimeoutError() from exc\n    finally:\n        timeout_handle.cancel()\n\n\nasync def _wait(fs, timeout, return_when, loop):\n    \"\"\"Internal helper for wait().\n\n    The fs argument must be a collection of Futures.\n    \"\"\"\n    assert fs, 'Set of Futures is empty.'\n    waiter = loop.create_future()\n    timeout_handle = None\n    if timeout is not None:\n        timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\n    counter = len(fs)\n\n    def _on_completion(f):\n        nonlocal counter\n        counter -= 1\n        if (counter <= 0 or\n            return_when == FIRST_COMPLETED or\n            return_when == FIRST_EXCEPTION and (not f.cancelled() and\n                                                f.exception() is not None)):\n            if timeout_handle is not None:\n                timeout_handle.cancel()\n            if not waiter.done():\n                waiter.set_result(None)\n\n    for f in fs:\n        f.add_done_callback(_on_completion)\n\n    try:\n        await waiter\n    finally:\n        if timeout_handle is not None:\n            timeout_handle.cancel()\n        for f in fs:\n            f.remove_done_callback(_on_completion)\n\n    done, pending = set(), set()\n    for f in fs:\n        if f.done():\n            done.add(f)\n        else:\n            pending.add(f)\n    return done, pending\n\n\nasync def _cancel_and_wait(fut, loop):\n    \"\"\"Cancel the *fut* future or task and wait until it completes.\"\"\"\n\n    waiter = loop.create_future()\n    cb = functools.partial(_release_waiter, waiter)\n    fut.add_done_callback(cb)\n\n    try:\n        fut.cancel()\n        # We cannot wait on *fut* directly to make\n        # sure _cancel_and_wait itself is reliably cancellable.\n        await waiter\n    finally:\n        fut.remove_done_callback(cb)\n\n\n# This is *not* a @coroutine!  It is just an iterator (yielding Futures).\ndef as_completed(fs, *, timeout=None):\n    \"\"\"Return an iterator whose values are coroutines.\n\n    When waiting for the yielded coroutines you'll get the results (or\n    exceptions!) of the original Futures (or coroutines), in the order\n    in which and as soon as they complete.\n\n    This differs from PEP 3148; the proper way to use this is:\n\n        for f in as_completed(fs):\n            result = await f  # The 'await' may raise.\n            # Use result.\n\n    If a timeout is specified, the 'await' will raise\n    TimeoutError when the timeout occurs before all Futures are done.\n\n    Note: The futures 'f' are not necessarily members of fs.\n    \"\"\"\n    if futures.isfuture(fs) or coroutines.iscoroutine(fs):\n        raise TypeError(f\"expect an iterable of futures, not {type(fs).__name__}\")\n\n    from .queues import Queue  # Import here to avoid circular import problem.\n    done = Queue()\n\n    loop = events._get_event_loop()\n    todo = {ensure_future(f, loop=loop) for f in set(fs)}\n    timeout_handle = None\n\n    def _on_timeout():\n        for f in todo:\n            f.remove_done_callback(_on_completion)\n            done.put_nowait(None)  # Queue a dummy value for _wait_for_one().\n        todo.clear()  # Can't do todo.remove(f) in the loop.\n\n    def _on_completion(f):\n        if not todo:\n            return  # _on_timeout() was here first.\n        todo.remove(f)\n        done.put_nowait(f)\n        if not todo and timeout_handle is not None:\n            timeout_handle.cancel()\n\n    async def _wait_for_one():\n        f = await done.get()\n        if f is None:\n            # Dummy value from _on_timeout().\n            raise exceptions.TimeoutError\n        return f.result()  # May raise f.exception().\n\n    for f in todo:\n        f.add_done_callback(_on_completion)\n    if todo and timeout is not None:\n        timeout_handle = loop.call_later(timeout, _on_timeout)\n    for _ in range(len(todo)):\n        yield _wait_for_one()\n\n\n@types.coroutine\ndef __sleep0():\n    \"\"\"Skip one event loop run cycle.\n\n    This is a private helper for 'asyncio.sleep()', used\n    when the 'delay' is set to 0.  It uses a bare 'yield'\n    expression (which Task.__step knows how to handle)\n    instead of creating a Future object.\n    \"\"\"\n    yield\n\n\nasync def sleep(delay, result=None):\n    \"\"\"Coroutine that completes after a given time (in seconds).\"\"\"\n    if delay <= 0:\n        await __sleep0()\n        return result\n\n    loop = events.get_running_loop()\n    future = loop.create_future()\n    h = loop.call_later(delay,\n                        futures._set_result_unless_cancelled,\n                        future, result)\n    try:\n        return await future\n    finally:\n        h.cancel()\n\n\ndef ensure_future(coro_or_future, *, loop=None):\n    \"\"\"Wrap a coroutine or an awaitable in a future.\n\n    If the argument is a Future, it is returned directly.\n    \"\"\"\n    return _ensure_future(coro_or_future, loop=loop)\n\n\ndef _ensure_future(coro_or_future, *, loop=None):\n    if futures.isfuture(coro_or_future):\n        if loop is not None and loop is not futures._get_loop(coro_or_future):\n            raise ValueError('The future belongs to a different loop than '\n                            'the one specified as the loop argument')\n        return coro_or_future\n    called_wrap_awaitable = False\n    if not coroutines.iscoroutine(coro_or_future):\n        if inspect.isawaitable(coro_or_future):\n            coro_or_future = _wrap_awaitable(coro_or_future)\n            called_wrap_awaitable = True\n        else:\n            raise TypeError('An asyncio.Future, a coroutine or an awaitable '\n                            'is required')\n\n    if loop is None:\n        loop = events._get_event_loop(stacklevel=4)\n    try:\n        return loop.create_task(coro_or_future)\n    except RuntimeError:\n        if not called_wrap_awaitable:\n            coro_or_future.close()\n        raise\n\n\n@types.coroutine\ndef _wrap_awaitable(awaitable):\n    \"\"\"Helper for asyncio.ensure_future().\n\n    Wraps awaitable (an object with __await__) into a coroutine\n    that will later be wrapped in a Task by ensure_future().\n    \"\"\"\n    return (yield from awaitable.__await__())\n\n_wrap_awaitable._is_coroutine = _is_coroutine\n\n\nclass _GatheringFuture(futures.Future):\n    \"\"\"Helper for gather().\n\n    This overrides cancel() to cancel all the children and act more\n    like Task.cancel(), which doesn't immediately mark itself as\n    cancelled.\n    \"\"\"\n\n    def __init__(self, children, *, loop):\n        assert loop is not None\n        super().__init__(loop=loop)\n        self._children = children\n        self._cancel_requested = False\n\n    def cancel(self, msg=None):\n        if self.done():\n            return False\n        ret = False\n        for child in self._children:\n            if child.cancel(msg=msg):\n                ret = True\n        if ret:\n            # If any child tasks were actually cancelled, we should\n            # propagate the cancellation request regardless of\n            # *return_exceptions* argument.  See issue 32684.\n            self._cancel_requested = True\n        return ret\n\n\ndef gather(*coros_or_futures, return_exceptions=False):\n    \"\"\"Return a future aggregating results from the given coroutines/futures.\n\n    Coroutines will be wrapped in a future and scheduled in the event\n    loop. They will not necessarily be scheduled in the same order as\n    passed in.\n\n    All futures must share the same event loop.  If all the tasks are\n    done successfully, the returned future's result is the list of\n    results (in the order of the original sequence, not necessarily\n    the order of results arrival).  If *return_exceptions* is True,\n    exceptions in the tasks are treated the same as successful\n    results, and gathered in the result list; otherwise, the first\n    raised exception will be immediately propagated to the returned\n    future.\n\n    Cancellation: if the outer Future is cancelled, all children (that\n    have not completed yet) are also cancelled.  If any child is\n    cancelled, this is treated as if it raised CancelledError --\n    the outer Future is *not* cancelled in this case.  (This is to\n    prevent the cancellation of one child to cause other children to\n    be cancelled.)\n\n    If *return_exceptions* is False, cancelling gather() after it\n    has been marked done won't cancel any submitted awaitables.\n    For instance, gather can be marked done after propagating an\n    exception to the caller, therefore, calling ``gather.cancel()``\n    after catching an exception (raised by one of the awaitables) from\n    gather won't cancel any other awaitables.\n    \"\"\"\n    if not coros_or_futures:\n        loop = events._get_event_loop()\n        outer = loop.create_future()\n        outer.set_result([])\n        return outer\n\n    def _done_callback(fut):\n        nonlocal nfinished\n        nfinished += 1\n\n        if outer is None or outer.done():\n            if not fut.cancelled():\n                # Mark exception retrieved.\n                fut.exception()\n            return\n\n        if not return_exceptions:\n            if fut.cancelled():\n                # Check if 'fut' is cancelled first, as\n                # 'fut.exception()' will *raise* a CancelledError\n                # instead of returning it.\n                exc = fut._make_cancelled_error()\n                outer.set_exception(exc)\n                return\n            else:\n                exc = fut.exception()\n                if exc is not None:\n                    outer.set_exception(exc)\n                    return\n\n        if nfinished == nfuts:\n            # All futures are done; create a list of results\n            # and set it to the 'outer' future.\n            results = []\n\n            for fut in children:\n                if fut.cancelled():\n                    # Check if 'fut' is cancelled first, as 'fut.exception()'\n                    # will *raise* a CancelledError instead of returning it.\n                    # Also, since we're adding the exception return value\n                    # to 'results' instead of raising it, don't bother\n                    # setting __context__.  This also lets us preserve\n                    # calling '_make_cancelled_error()' at most once.\n                    res = exceptions.CancelledError(\n                        '' if fut._cancel_message is None else\n                        fut._cancel_message)\n                else:\n                    res = fut.exception()\n                    if res is None:\n                        res = fut.result()\n                results.append(res)\n\n            if outer._cancel_requested:\n                # If gather is being cancelled we must propagate the\n                # cancellation regardless of *return_exceptions* argument.\n                # See issue 32684.\n                exc = fut._make_cancelled_error()\n                outer.set_exception(exc)\n            else:\n                outer.set_result(results)\n\n    arg_to_fut = {}\n    children = []\n    nfuts = 0\n    nfinished = 0\n    loop = None\n    outer = None  # bpo-46672\n    for arg in coros_or_futures:\n        if arg not in arg_to_fut:\n            fut = _ensure_future(arg, loop=loop)\n            if loop is None:\n                loop = futures._get_loop(fut)\n            if fut is not arg:\n                # 'arg' was not a Future, therefore, 'fut' is a new\n                # Future created specifically for 'arg'.  Since the caller\n                # can't control it, disable the \"destroy pending task\"\n                # warning.\n                fut._log_destroy_pending = False\n\n            nfuts += 1\n            arg_to_fut[arg] = fut\n            fut.add_done_callback(_done_callback)\n\n        else:\n            # There's a duplicate Future object in coros_or_futures.\n            fut = arg_to_fut[arg]\n\n        children.append(fut)\n\n    outer = _GatheringFuture(children, loop=loop)\n    return outer\n\n\ndef shield(arg):\n    \"\"\"Wait for a future, shielding it from cancellation.\n\n    The statement\n\n        task = asyncio.create_task(something())\n        res = await shield(task)\n\n    is exactly equivalent to the statement\n\n        res = await something()\n\n    *except* that if the coroutine containing it is cancelled, the\n    task running in something() is not cancelled.  From the POV of\n    something(), the cancellation did not happen.  But its caller is\n    still cancelled, so the yield-from expression still raises\n    CancelledError.  Note: If something() is cancelled by other means\n    this will still cancel shield().\n\n    If you want to completely ignore cancellation (not recommended)\n    you can combine shield() with a try/except clause, as follows:\n\n        task = asyncio.create_task(something())\n        try:\n            res = await shield(task)\n        except CancelledError:\n            res = None\n\n    Save a reference to tasks passed to this function, to avoid\n    a task disappearing mid-execution. The event loop only keeps\n    weak references to tasks. A task that isn't referenced elsewhere\n    may get garbage collected at any time, even before it's done.\n    \"\"\"\n    inner = _ensure_future(arg)\n    if inner.done():\n        # Shortcut.\n        return inner\n    loop = futures._get_loop(inner)\n    outer = loop.create_future()\n\n    def _inner_done_callback(inner):\n        if outer.cancelled():\n            if not inner.cancelled():\n                # Mark inner's result as retrieved.\n                inner.exception()\n            return\n\n        if inner.cancelled():\n            outer.cancel()\n        else:\n            exc = inner.exception()\n            if exc is not None:\n                outer.set_exception(exc)\n            else:\n                outer.set_result(inner.result())\n\n\n    def _outer_done_callback(outer):\n        if not inner.done():\n            inner.remove_done_callback(_inner_done_callback)\n\n    inner.add_done_callback(_inner_done_callback)\n    outer.add_done_callback(_outer_done_callback)\n    return outer\n\n\ndef run_coroutine_threadsafe(coro, loop):\n    \"\"\"Submit a coroutine object to a given event loop.\n\n    Return a concurrent.futures.Future to access the result.\n    \"\"\"\n    if not coroutines.iscoroutine(coro):\n        raise TypeError('A coroutine object is required')\n    future = concurrent.futures.Future()\n\n    def callback():\n        try:\n            futures._chain_future(ensure_future(coro, loop=loop), future)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            if future.set_running_or_notify_cancel():\n                future.set_exception(exc)\n            raise\n\n    loop.call_soon_threadsafe(callback)\n    return future\n\n\n# WeakSet containing all alive tasks.\n_all_tasks = weakref.WeakSet()\n\n# Dictionary containing tasks that are currently active in\n# all running event loops.  {EventLoop: Task}\n_current_tasks = {}\n\n\ndef _register_task(task):\n    \"\"\"Register a new task in asyncio as executed by loop.\"\"\"\n    _all_tasks.add(task)\n\n\ndef _enter_task(loop, task):\n    current_task = _current_tasks.get(loop)\n    if current_task is not None:\n        raise RuntimeError(f\"Cannot enter into task {task!r} while another \"\n                           f\"task {current_task!r} is being executed.\")\n    _current_tasks[loop] = task\n\n\ndef _leave_task(loop, task):\n    current_task = _current_tasks.get(loop)\n    if current_task is not task:\n        raise RuntimeError(f\"Leaving task {task!r} does not match \"\n                           f\"the current task {current_task!r}.\")\n    del _current_tasks[loop]\n\n\ndef _unregister_task(task):\n    \"\"\"Unregister a task.\"\"\"\n    _all_tasks.discard(task)\n\n\n_py_register_task = _register_task\n_py_unregister_task = _unregister_task\n_py_enter_task = _enter_task\n_py_leave_task = _leave_task\n\n\ntry:\n    from _asyncio import (_register_task, _unregister_task,\n                          _enter_task, _leave_task,\n                          _all_tasks, _current_tasks)\nexcept ImportError:\n    pass\nelse:\n    _c_register_task = _register_task\n    _c_unregister_task = _unregister_task\n    _c_enter_task = _enter_task\n    _c_leave_task = _leave_task\n", 990], "/app/app/providers/finnhub_provider.py": ["import finnhub\nimport logging\nimport asyncio\nfrom datetime import datetime, timedelta\nfrom typing import Dict, Optional\nfrom .base import DataProvider\n\nlogger = logging.getLogger(__name__)\n\nclass FinnhubProvider(DataProvider):\n    def __init__(self, api_key: str = \"demo\"):\n        super().__init__(\"Finnhub\")\n        self.api_key = api_key\n        self.client = finnhub.Client(api_key=api_key)\n        self.rate_limit_delay = 1.0  # 60 calls/minute = 1 call/second for free tier\n        \n    async def get_price(self, symbol: str) -> Optional[Dict]:\n        \"\"\"Get current stock price from Finnhub\"\"\"\n        try:\n            # Add rate limiting\n            await asyncio.sleep(self.rate_limit_delay)\n            \n            # Get current price quote\n            quote = await asyncio.to_thread(self.client.quote, symbol)\n            \n            if not quote or 'c' not in quote:\n                return None\n            \n            current_price = float(quote['c'])  # Current price\n            previous_close = float(quote['pc'])  # Previous close\n            \n            if current_price == 0:\n                return None\n            \n            change = current_price - previous_close\n            change_percent = (change / previous_close * 100) if previous_close != 0 else 0\n            \n            self.mark_available()\n            return {\n                \"symbol\": symbol.upper(),\n                \"price\": round(current_price, 2),\n                \"change\": round(change, 2),\n                \"change_percent\": round(change_percent, 2),\n                \"high\": round(float(quote.get('h', current_price)), 2),  # Day high\n                \"low\": round(float(quote.get('l', current_price)), 2),   # Day low\n                \"open\": round(float(quote.get('o', current_price)), 2),  # Day open\n                \"previous_close\": round(previous_close, 2),\n                \"timestamp\": datetime.now().isoformat(),\n                \"source\": self.name\n            }\n            \n        except Exception as e:\n            error_msg = f\"Finnhub error for {symbol}: {e}\"\n            self.mark_unavailable(error_msg)\n            logger.error(error_msg)\n            return None\n    \n    async def get_historical(self, symbol: str, period: str = \"1mo\") -> Optional[Dict]:\n        \"\"\"Get historical data from Finnhub\"\"\"\n        try:\n            await asyncio.sleep(self.rate_limit_delay)\n            \n            # Convert period to date range\n            end_date = datetime.now()\n            \n            period_map = {\n                \"1d\": 1,\n                \"5d\": 5,\n                \"1mo\": 30,\n                \"3mo\": 90,\n                \"6mo\": 180,\n                \"1y\": 365\n            }\n            \n            days = period_map.get(period, 30)\n            start_date = end_date - timedelta(days=days)\n            \n            # Convert to Unix timestamps\n            start_ts = int(start_date.timestamp())\n            end_ts = int(end_date.timestamp())\n            \n            # Get candle data (daily resolution)\n            candles = await asyncio.to_thread(\n                self.client.stock_candles, \n                symbol, \n                'D',  # Daily resolution\n                start_ts, \n                end_ts\n            )\n            \n            if not candles or candles['s'] != 'ok':\n                return None\n            \n            data = []\n            timestamps = candles['t']\n            opens = candles['o']\n            highs = candles['h']\n            lows = candles['l']\n            closes = candles['c']\n            volumes = candles['v']\n            \n            for i in range(len(timestamps)):\n                date = datetime.fromtimestamp(timestamps[i])\n                data.append({\n                    \"date\": date.strftime(\"%Y-%m-%d\"),\n                    \"open\": round(float(opens[i]), 2),\n                    \"high\": round(float(highs[i]), 2),\n                    \"low\": round(float(lows[i]), 2),\n                    \"close\": round(float(closes[i]), 2),\n                    \"volume\": int(volumes[i])\n                })\n            \n            return {\n                \"symbol\": symbol.upper(),\n                \"period\": period,\n                \"data\": data,\n                \"source\": self.name\n            }\n            \n        except Exception as e:\n            logger.error(f\"Finnhub historical error for {symbol}: {e}\")\n            return None\n\n    async def get_company_profile(self, symbol: str) -> Optional[Dict]:\n        \"\"\"Get company profile data\"\"\"\n        try:\n            await asyncio.sleep(self.rate_limit_delay)\n            \n            profile = await asyncio.to_thread(self.client.company_profile2, symbol=symbol)\n            \n            if not profile:\n                return None\n                \n            return {\n                \"symbol\": symbol.upper(),\n                \"name\": profile.get('name'),\n                \"country\": profile.get('country'),\n                \"currency\": profile.get('currency'),\n                \"exchange\": profile.get('exchange'),\n                \"ipo\": profile.get('ipo'),\n                \"market_cap\": profile.get('marketCapitalization'),\n                \"shares_outstanding\": profile.get('shareOutstanding'),\n                \"logo\": profile.get('logo'),\n                \"phone\": profile.get('phone'),\n                \"weburl\": profile.get('weburl'),\n                \"industry\": profile.get('finnhubIndustry'),\n                \"source\": self.name\n            }\n            \n        except Exception as e:\n            logger.error(f\"Finnhub company profile error for {symbol}: {e}\")\n            return None\n\n    async def get_news_sentiment(self, symbol: str) -> Optional[Dict]:\n        \"\"\"Get news sentiment for a symbol\"\"\"\n        try:\n            await asyncio.sleep(self.rate_limit_delay)\n            \n            sentiment = await asyncio.to_thread(self.client.news_sentiment, symbol)\n            \n            if not sentiment:\n                return None\n                \n            return {\n                \"symbol\": symbol.upper(),\n                \"sentiment\": {\n                    \"buzz\": sentiment.get('buzz', {}),\n                    \"sentiment\": sentiment.get('sentiment', {}),\n                    \"company_news_score\": sentiment.get('companyNewsScore'),\n                    \"sector_average_bullishness\": sentiment.get('sectorAverageBullishness'),\n                    \"sector_average_news_score\": sentiment.get('sectorAverageNewsScore')\n                },\n                \"source\": self.name\n            }\n            \n        except Exception as e:\n            logger.error(f\"Finnhub news sentiment error for {symbol}: {e}\")\n            return None\n\n    async def get_intraday(self, symbol: str, interval: str = \"1m\") -> Optional[Dict]:\n        \"\"\"Get intraday minute data from Finnhub\"\"\"\n        try:\n            await asyncio.sleep(self.rate_limit_delay)\n\n            end_date = datetime.now()\n            # fetch last trading day window (~6.5h = 390 minutes)\n            start_date = end_date - timedelta(hours=8)\n            start_ts = int(start_date.timestamp())\n            end_ts = int(end_date.timestamp())\n\n            resolution = '1'  # 1-minute\n            candles = await asyncio.to_thread(self.client.stock_candles, symbol, resolution, start_ts, end_ts)\n            if not candles or candles.get('s') != 'ok':\n                return None\n\n            data = []\n            for i, ts in enumerate(candles['t']):\n                date = datetime.fromtimestamp(ts)\n                data.append({\n                    \"timestamp\": date.isoformat(),\n                    \"open\": round(float(candles['o'][i]), 4),\n                    \"high\": round(float(candles['h'][i]), 4),\n                    \"low\": round(float(candles['l'][i]), 4),\n                    \"close\": round(float(candles['c'][i]), 4),\n                    \"volume\": int(candles['v'][i])\n                })\n\n            return {\"symbol\": symbol.upper(), \"interval\": interval, \"data\": data, \"source\": self.name}\n        except Exception as e:\n            logger.error(f\"Finnhub intraday error for {symbol}: {e}\")\n            return None\n", 211], "/app/app/services/market_data.py": ["import asyncio\nimport logging\nimport os\nimport time\nfrom datetime import datetime, timezone\nfrom typing import Dict, List, Optional\n\nfrom fastapi import HTTPException\n\nfrom ..circuit_breaker import CircuitBreakerManager\nfrom ..core.config import get_settings\nfrom ..providers import FinnhubProvider, YahooFinanceProvider\nfrom ..providers.registry import ProviderRegistry\nfrom .cache import DataCache\nfrom .database import db_service\nfrom .data_collector import DataCollectorService\nfrom .macro_data_service import MacroFactorService\nfrom .options_service import options_service\nfrom .websocket import ConnectionManager\n\nlogger = logging.getLogger(__name__)\n\nclass MarketDataService:\n    SUPPORTED_INTRADAY_INTERVALS = {\n        \"1m\",\n        \"2m\",\n        \"5m\",\n        \"15m\",\n        \"30m\",\n        \"60m\",\n        \"90m\",\n    }\n\n    def __init__(self):\n        self.settings = get_settings()\n        self.breakers = CircuitBreakerManager()\n        self.registry = ProviderRegistry(self.breakers)\n\n        self._init_providers()\n        self.providers = [entry.adapter for entry in self.registry.providers.values()]\n\n        # Cache TTL configurable via env/settings\n        cache_ttl = int(os.getenv(\"CACHE_TTL_SECONDS\", self.settings.cache_ttl_seconds))\n        self.cache = DataCache(ttl_seconds=cache_ttl)\n        self.metrics_cache = DataCache(ttl_seconds=int(os.getenv(\"OPTIONS_METRICS_CACHE_TTL\", \"300\")))\n        self.options_metrics_ttl = int(os.getenv(\"OPTIONS_METRICS_TTL\", \"900\"))\n        self.update_interval = int(os.getenv(\"WEBSOCKET_UPDATE_INTERVAL\", \"5\"))\n        self.connection_manager = ConnectionManager()\n        self.macro_service = MacroFactorService(\n            providers=self.providers,\n            cache_ttl_seconds=getattr(self.settings, \"macro_cache_ttl_seconds\", 300),\n            refresh_interval_seconds=getattr(self.settings, \"macro_refresh_interval_seconds\", 900),\n        )\n        self.data_collector = DataCollectorService(db=db_service, registry=self.registry)\n        self.background_tasks_running = False\n\n    def _init_providers(self) -> None:\n        \"\"\"Register provider adapters with the routing registry.\"\"\"\n        finnhub_key = (\n            os.getenv(\"FINNHUB_API_KEY\")\n            or self.settings.finnhub.api_key\n            or self.settings.finnhub_api_key\n        )\n\n        if finnhub_key and finnhub_key not in {\"your_finnhub_api_key_here\", \"\"}:\n            try:\n                finnhub_provider = FinnhubProvider(api_key=finnhub_key)\n                self.registry.register(\"finnhub\", finnhub_provider, {\"bars_1m\", \"eod\", \"quotes_l1\"})\n                logger.info(\"Finnhub provider registered\")\n            except Exception as exc:  # pragma: no cover - defensive\n                logger.warning(\"Failed to initialize Finnhub provider: %s\", exc)\n        else:\n            logger.warning(\"No valid Finnhub API key provided; Finnhub disabled\")\n\n        if self.settings.yfinance_enabled:\n            try:\n                yahoo_provider = YahooFinanceProvider()\n                self.registry.register(\"yfinance\", yahoo_provider, {\"bars_1m\", \"eod\", \"options_chain\"})\n                logger.info(\"Yahoo Finance provider registered\")\n            except Exception as exc:  # pragma: no cover\n                logger.warning(\"Failed to initialize Yahoo Finance provider: %s\", exc)\n\n    def reload_configuration(self) -> None:\n        \"\"\"Reload providers and settings after a hot reload.\"\"\"\n        self.settings = get_settings()\n        self.registry.providers.clear()\n        self._init_providers()\n        self.providers = [entry.adapter for entry in self.registry.providers.values()]\n        if hasattr(self.macro_service, \"providers\"):\n            self.macro_service.providers = self.providers\n\n    async def _try_providers(\n        self,\n        *,\n        capability: str,\n        endpoint: str,\n        executor,\n        provider_hint: Optional[str] = None,\n    ):\n        ranked = self.registry.rank(capability, provider_hint=provider_hint)\n        if not ranked:\n            raise HTTPException(status_code=503, detail=f\"No providers available for {capability}\")\n\n        last_error: Optional[str] = None\n        for provider_name in ranked:\n            entry = self.registry.providers[provider_name]\n            adapter = entry.adapter\n            if not adapter.enabled:\n                continue\n\n            self.registry.record_selection(capability, provider_name)\n            started = time.perf_counter()\n\n            try:\n                result = await executor(adapter)\n                elapsed_ms = (time.perf_counter() - started) * 1000.0\n                self.registry.record_outcome(provider_name, elapsed_ms, error=False, endpoint=endpoint)\n                if result:\n                    return result, provider_name\n                last_error = f\"{provider_name}: empty result\"\n            except HTTPException:\n                raise\n            except Exception as exc:  # pragma: no cover - provider failure paths\n                elapsed_ms = (time.perf_counter() - started) * 1000.0\n                self.registry.record_outcome(provider_name, elapsed_ms, error=True, endpoint=endpoint)\n                self.registry.record_error(provider_name, endpoint, exc.__class__.__name__)\n                last_error = f\"{provider_name}: {exc}\"\n\n        raise HTTPException(\n            status_code=502,\n            detail=f\"All providers failed for {endpoint}. last_error={last_error}\",\n        )\n    \n    async def get_stock_price(self, symbol: str) -> Dict:\n        \"\"\"Get stock price with caching and health-aware routing.\"\"\"\n        symbol = symbol.upper()\n        cache_key = f\"price:{symbol}\"\n\n        cached_data = self.cache.get(cache_key)\n        if cached_data:\n            logger.info(\"Cache hit for %s\", symbol)\n            return cached_data\n\n        data, provider_used = await self._try_providers(\n            capability=\"quotes_l1\",\n            endpoint=\"quotes\",\n            executor=lambda adapter: adapter.get_price_safe(symbol),\n        )\n\n        if isinstance(data, dict):\n            data[\"provider_used\"] = provider_used\n        self.cache.set(cache_key, data)\n        logger.info(\"Fetched %s from %s\", symbol, provider_used)\n        return data\n    \n    async def get_historical_data(self, symbol: str, period: str = \"1mo\") -> Dict:\n        \"\"\"Get historical data.\"\"\"\n        symbol = symbol.upper()\n        data, provider_used = await self._try_providers(\n            capability=\"eod\",\n            endpoint=\"historical\",\n            executor=lambda adapter: adapter.get_historical_safe(symbol, period),\n        )\n\n        if isinstance(data, dict):\n            data[\"provider_used\"] = provider_used\n        return data\n\n    def _normalize_intraday_interval(self, interval: str) -> str:\n        interval_normalized = (interval or \"1m\").strip().lower()\n\n        # Map common aliases to supported values\n        alias_map = {\n            \"1\": \"1m\",\n            \"5\": \"5m\",\n            \"15\": \"15m\",\n            \"30\": \"30m\",\n            \"60\": \"60m\",\n            \"1min\": \"1m\",\n            \"5min\": \"5m\",\n            \"15min\": \"15m\",\n            \"30min\": \"30m\",\n            \"60min\": \"60m\",\n            \"1hour\": \"60m\",\n            \"1hr\": \"60m\",\n            \"1h\": \"60m\",\n        }\n\n        resolved = alias_map.get(interval_normalized, interval_normalized)\n        if resolved not in self.SUPPORTED_INTRADAY_INTERVALS:\n            raise HTTPException(\n                status_code=400,\n                detail=f\"Unsupported intraday interval '{interval}'\"\n            )\n        return resolved\n\n    async def get_intraday_data(self, symbol: str, interval: str = \"1m\") -> Dict:\n        \"\"\"Get intraday data with provider fallback and validation.\"\"\"\n        normalized_interval = self._normalize_intraday_interval(interval)\n\n        capability = \"bars_1m\" if normalized_interval.endswith(\"m\") else \"eod\"\n        data, provider_used = await self._try_providers(\n            capability=capability,\n            endpoint=\"intraday\",\n            executor=lambda adapter: adapter.get_intraday_safe(symbol, normalized_interval),\n        )\n\n        if isinstance(data, dict):\n            data[\"provider_used\"] = provider_used\n        return data\n    \n    async def start_background_tasks(self):\n        \"\"\"Start background tasks for cache cleanup and real-time updates\"\"\"\n        if self.background_tasks_running:\n            return\n\n        self.background_tasks_running = True\n\n        # Initialize database connections for macro storage\n        try:\n            await db_service.initialize()\n            logger.info(\"Database service initialized\")\n        except Exception as exc:\n            logger.error(f\"Failed to initialize database service: {exc}\")\n\n        # Start cache cleanup task\n        asyncio.create_task(self._cache_cleanup_task())\n\n        # Start macro refresh task\n        asyncio.create_task(self._macro_refresh_task())\n\n        # Start real-time data broadcasting\n        asyncio.create_task(self._real_time_broadcast_task())\n\n        # Start data collector (M2/M3: RLC consumer + gap detection + backfill)\n        asyncio.create_task(self.data_collector.run())\n\n        logger.info(\"Background tasks started (including data collector)\")\n    \n    async def _cache_cleanup_task(self):\n        \"\"\"Clean up expired cache entries\"\"\"\n        while self.background_tasks_running:\n            self.cache.clear_expired()\n            await asyncio.sleep(60)  # Clean up every minute\n    \n    async def _real_time_broadcast_task(self):\n        \"\"\"Broadcast real-time data to WebSocket clients\"\"\"\n        while self.background_tasks_running:\n            if self.connection_manager.symbol_subscribers:\n                for symbol in list(self.connection_manager.symbol_subscribers.keys()):\n                    try:\n                        data = await self.get_stock_price(symbol)\n                        await self.connection_manager.broadcast_to_symbol(symbol, data)\n                    except Exception as e:\n                        logger.error(f\"Error broadcasting {symbol}: {e}\")\n\n            await asyncio.sleep(self.update_interval)  # Update interval configurable\n\n    async def _macro_refresh_task(self):\n        \"\"\"Periodically refresh macro factors from configured providers.\"\"\"\n        interval = getattr(\n            self.macro_service,\n            \"refresh_interval_seconds\",\n            getattr(self.settings, \"macro_refresh_interval_seconds\", 900),\n        )\n        interval = max(60, interval or 900)\n\n        try:\n            await self.macro_service.refresh_all()\n        except Exception as exc:\n            logger.error(f\"Initial macro refresh failed: {exc}\")\n\n        while self.background_tasks_running:\n            await asyncio.sleep(interval)\n            try:\n                await self.macro_service.refresh_all()\n            except Exception as exc:\n                logger.error(f\"Macro refresh task failed: {exc}\")\n\n    async def get_company_profile(self, symbol: str) -> Dict:\n        \"\"\"Get company profile data (Finnhub only)\"\"\"\n        for provider in self.providers:\n            if hasattr(provider, 'get_company_profile'):\n                data = await provider.get_company_profile(symbol)\n                if data:\n                    return data\n        \n        raise HTTPException(\n            status_code=503,\n            detail=f\"Unable to fetch company profile for {symbol}\"\n        )\n    \n    async def get_news_sentiment(self, symbol: str) -> Dict:\n        \"\"\"Get news sentiment data (Finnhub only)\"\"\"\n        for provider in self.providers:\n            if hasattr(provider, 'get_news_sentiment'):\n                data = await provider.get_news_sentiment(symbol)\n                if data:\n                    return data\n        \n        raise HTTPException(\n            status_code=503,\n            detail=f\"Unable to fetch news sentiment for {symbol}\"\n        )\n    \n\n    async def get_options_metrics(self, symbol: str) -> Dict:\n        \"\"\"Return cached or freshly computed options metrics for a symbol.\"\"\"\n        symbol = symbol.upper()\n        cache_key = f\"options_metrics:{symbol}\"\n        cached = self.metrics_cache.get(cache_key)\n        if cached:\n            return cached\n\n        latest = await db_service.get_latest_options_metrics(symbol)\n        if latest:\n            try:\n                timestamp = datetime.fromisoformat(latest['as_of'])\n                if timestamp.tzinfo is None:\n                    timestamp = timestamp.replace(tzinfo=timezone.utc)\n                if (datetime.now(timezone.utc) - timestamp).total_seconds() < self.options_metrics_ttl:\n                    self.metrics_cache.set(cache_key, latest)\n                    return latest\n            except Exception:\n                pass\n\n        chain = await options_service.fetch_options_chain(symbol)\n        metrics = options_service.calculate_chain_metrics(chain)\n        record = metrics.to_db_record()\n        await db_service.store_options_metrics(record)\n        result = metrics.to_dict()\n        result['metadata'].setdefault('source', 'provider')\n        self.metrics_cache.set(cache_key, result)\n        return result\n\n    async def get_options_metrics_history(self, symbol: str, limit: int = 50) -> List[Dict]:\n        \"\"\"Return recent history of stored options metrics for a symbol.\"\"\"\n        history = await db_service.get_options_metrics_history(symbol.upper(), limit)\n        return history\n\n    async def get_stats(self) -> Dict:\n        macro_stats = await self.macro_service.stats()\n        return {\n            \"providers\": [\n                {\n                    \"name\": provider.name,\n                    \"available\": provider.is_available,\n                    \"last_error\": provider.last_error,\n                }\n                for provider in self.providers\n            ],\n            \"cache\": self.cache.stats(),\n            \"websocket\": self.connection_manager.stats(),\n            \"database\": {\n                \"connected\": db_service.pool is not None,\n                \"storage_enabled\": self.settings.store_historical_data,\n            },\n            \"macro\": macro_stats,\n            \"options_metrics\": {\n                \"cache\": self.metrics_cache.stats(),\n                \"ttl_seconds\": self.options_metrics_ttl,\n            },\n        }\n\n    async def get_macro_snapshot(self, factors: Optional[List[str]] = None) -> Dict:\n        \"\"\"Return the latest macro factor snapshot.\"\"\"\n        return await self.macro_service.get_snapshot(factors)\n\n    async def get_macro_history(self, factor_key: str, lookback_days: int = 30) -> Dict:\n        \"\"\"Return macro factor history for the requested lookback window.\"\"\"\n        return await self.macro_service.get_history(factor_key, lookback_days)\n\n    def list_macro_factors(self) -> List[str]:\n        \"\"\"List available macro factor keys.\"\"\"\n        return self.macro_service.available_factors()\n\n    async def refresh_macro_factors(self, factor_key: Optional[str] = None) -> Dict:\n        \"\"\"Refresh macro factors from providers.\"\"\"\n        if factor_key:\n            return await self.macro_service.refresh_factor(factor_key)\n        return await self.macro_service.refresh_all()\n    \n    async def get_unusual_options_activity(self, symbol: str, lookback_days: int = 20) -> Dict:\n        \"\"\"Get unusual options activity for a symbol\"\"\"\n        try:\n            unusual_activities = await options_service.detect_unusual_activity(symbol.upper(), lookback_days)\n            \n            return {\n                \"symbol\": symbol.upper(),\n                \"lookback_days\": lookback_days,\n                \"unusual_activities_count\": len(unusual_activities),\n                \"unusual_activities\": [\n                    {\n                        \"contract_symbol\": activity.contract_symbol,\n                        \"strike\": activity.strike,\n                        \"expiry\": activity.expiry.isoformat(),\n                        \"option_type\": activity.option_type,\n                        \"volume\": activity.volume,\n                        \"avg_volume_20d\": activity.avg_volume_20d,\n                        \"volume_ratio\": activity.volume_ratio,\n                        \"open_interest\": activity.open_interest,\n                        \"volume_spike\": activity.volume_spike,\n                        \"large_single_trades\": activity.large_single_trades,\n                        \"sweep_activity\": activity.sweep_activity,\n                        \"unusual_volume_vs_oi\": activity.unusual_volume_vs_oi,\n                        \"underlying_price\": activity.underlying_price,\n                        \"strike_distance_pct\": activity.strike_distance_pct,\n                        \"days_to_expiration\": activity.days_to_expiration,\n                        \"unusual_score\": activity.unusual_score,\n                        \"confidence_level\": activity.confidence_level,\n                        \"large_trades\": activity.large_trades,\n                        \"timestamp\": activity.timestamp.isoformat()\n                    }\n                    for activity in unusual_activities\n                ],\n                \"analysis_timestamp\": datetime.now().isoformat()\n            }\n        except Exception as e:\n            logger.error(f\"Error getting unusual options activity for {symbol}: {e}\")\n            return {\n                \"symbol\": symbol.upper(),\n                \"error\": str(e),\n                \"unusual_activities_count\": 0,\n                \"unusual_activities\": []\n            }\n    \n    async def get_options_flow_analysis(self, symbol: str) -> Dict:\n        \"\"\"Get comprehensive options flow analysis\"\"\"\n        try:\n            flow_analysis = await options_service.analyze_options_flow(symbol.upper())\n            \n            return {\n                \"symbol\": symbol.upper(),\n                \"timestamp\": flow_analysis.timestamp.isoformat(),\n                \"flow_metrics\": {\n                    \"total_call_volume\": flow_analysis.total_call_volume,\n                    \"total_put_volume\": flow_analysis.total_put_volume,\n                    \"call_put_ratio\": flow_analysis.call_put_ratio,\n                    \"large_trades_count\": flow_analysis.large_trades_count,\n                    \"block_trades_value\": flow_analysis.block_trades_value,\n                    \"sweep_trades_count\": flow_analysis.sweep_trades_count\n                },\n                \"sentiment_analysis\": {\n                    \"flow_sentiment\": flow_analysis.flow_sentiment,\n                    \"smart_money_score\": flow_analysis.smart_money_score,\n                    \"call_premium_bought\": flow_analysis.call_premium_bought,\n                    \"put_premium_bought\": flow_analysis.put_premium_bought,\n                    \"net_premium_flow\": flow_analysis.net_premium_flow\n                },\n                \"unusual_activities_summary\": {\n                    \"count\": len(flow_analysis.unusual_activities),\n                    \"top_activities\": [\n                        {\n                            \"contract_symbol\": activity.contract_symbol,\n                            \"unusual_score\": activity.unusual_score,\n                            \"volume_ratio\": activity.volume_ratio,\n                            \"volume\": activity.volume,\n                            \"strike\": activity.strike,\n                            \"option_type\": activity.option_type\n                        }\n                        for activity in flow_analysis.unusual_activities[:5]  # Top 5\n                    ]\n                }\n            }\n        except Exception as e:\n            logger.error(f\"Error getting options flow analysis for {symbol}: {e}\")\n            return {\n                \"symbol\": symbol.upper(),\n                \"error\": str(e),\n                \"flow_metrics\": {},\n                \"sentiment_analysis\": {},\n                \"unusual_activities_summary\": {\"count\": 0, \"top_activities\": []}\n            }\n\n", 474], "/usr/local/lib/python3.11/asyncio/futures.py": ["\"\"\"A Future class similar to the one in PEP 3148.\"\"\"\n\n__all__ = (\n    'Future', 'wrap_future', 'isfuture',\n)\n\nimport concurrent.futures\nimport contextvars\nimport logging\nimport sys\nfrom types import GenericAlias\n\nfrom . import base_futures\nfrom . import events\nfrom . import exceptions\nfrom . import format_helpers\n\n\nisfuture = base_futures.isfuture\n\n\n_PENDING = base_futures._PENDING\n_CANCELLED = base_futures._CANCELLED\n_FINISHED = base_futures._FINISHED\n\n\nSTACK_DEBUG = logging.DEBUG - 1  # heavy-duty debugging\n\n\nclass Future:\n    \"\"\"This class is *almost* compatible with concurrent.futures.Future.\n\n    Differences:\n\n    - This class is not thread-safe.\n\n    - result() and exception() do not take a timeout argument and\n      raise an exception when the future isn't done yet.\n\n    - Callbacks registered with add_done_callback() are always called\n      via the event loop's call_soon().\n\n    - This class is not compatible with the wait() and as_completed()\n      methods in the concurrent.futures package.\n\n    (In Python 3.4 or later we may be able to unify the implementations.)\n    \"\"\"\n\n    # Class variables serving as defaults for instance variables.\n    _state = _PENDING\n    _result = None\n    _exception = None\n    _loop = None\n    _source_traceback = None\n    _cancel_message = None\n    # A saved CancelledError for later chaining as an exception context.\n    _cancelled_exc = None\n\n    # This field is used for a dual purpose:\n    # - Its presence is a marker to declare that a class implements\n    #   the Future protocol (i.e. is intended to be duck-type compatible).\n    #   The value must also be not-None, to enable a subclass to declare\n    #   that it is not compatible by setting this to None.\n    # - It is set by __iter__() below so that Task._step() can tell\n    #   the difference between\n    #   `await Future()` or`yield from Future()` (correct) vs.\n    #   `yield Future()` (incorrect).\n    _asyncio_future_blocking = False\n\n    __log_traceback = False\n\n    def __init__(self, *, loop=None):\n        \"\"\"Initialize the future.\n\n        The optional event_loop argument allows explicitly setting the event\n        loop object used by the future. If it's not provided, the future uses\n        the default event loop.\n        \"\"\"\n        if loop is None:\n            self._loop = events._get_event_loop()\n        else:\n            self._loop = loop\n        self._callbacks = []\n        if self._loop.get_debug():\n            self._source_traceback = format_helpers.extract_stack(\n                sys._getframe(1))\n\n    def __repr__(self):\n        return base_futures._future_repr(self)\n\n    def __del__(self):\n        if not self.__log_traceback:\n            # set_exception() was not called, or result() or exception()\n            # has consumed the exception\n            return\n        exc = self._exception\n        context = {\n            'message':\n                f'{self.__class__.__name__} exception was never retrieved',\n            'exception': exc,\n            'future': self,\n        }\n        if self._source_traceback:\n            context['source_traceback'] = self._source_traceback\n        self._loop.call_exception_handler(context)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    @property\n    def _log_traceback(self):\n        return self.__log_traceback\n\n    @_log_traceback.setter\n    def _log_traceback(self, val):\n        if val:\n            raise ValueError('_log_traceback can only be set to False')\n        self.__log_traceback = False\n\n    def get_loop(self):\n        \"\"\"Return the event loop the Future is bound to.\"\"\"\n        loop = self._loop\n        if loop is None:\n            raise RuntimeError(\"Future object is not initialized.\")\n        return loop\n\n    def _make_cancelled_error(self):\n        \"\"\"Create the CancelledError to raise if the Future is cancelled.\n\n        This should only be called once when handling a cancellation since\n        it erases the saved context exception value.\n        \"\"\"\n        if self._cancelled_exc is not None:\n            exc = self._cancelled_exc\n            self._cancelled_exc = None\n            return exc\n\n        if self._cancel_message is None:\n            exc = exceptions.CancelledError()\n        else:\n            exc = exceptions.CancelledError(self._cancel_message)\n        exc.__context__ = self._cancelled_exc\n        # Remove the reference since we don't need this anymore.\n        self._cancelled_exc = None\n        return exc\n\n    def cancel(self, msg=None):\n        \"\"\"Cancel the future and schedule callbacks.\n\n        If the future is already done or cancelled, return False.  Otherwise,\n        change the future's state to cancelled, schedule the callbacks and\n        return True.\n        \"\"\"\n        self.__log_traceback = False\n        if self._state != _PENDING:\n            return False\n        self._state = _CANCELLED\n        self._cancel_message = msg\n        self.__schedule_callbacks()\n        return True\n\n    def __schedule_callbacks(self):\n        \"\"\"Internal: Ask the event loop to call all callbacks.\n\n        The callbacks are scheduled to be called as soon as possible. Also\n        clears the callback list.\n        \"\"\"\n        callbacks = self._callbacks[:]\n        if not callbacks:\n            return\n\n        self._callbacks[:] = []\n        for callback, ctx in callbacks:\n            self._loop.call_soon(callback, self, context=ctx)\n\n    def cancelled(self):\n        \"\"\"Return True if the future was cancelled.\"\"\"\n        return self._state == _CANCELLED\n\n    # Don't implement running(); see http://bugs.python.org/issue18699\n\n    def done(self):\n        \"\"\"Return True if the future is done.\n\n        Done means either that a result / exception are available, or that the\n        future was cancelled.\n        \"\"\"\n        return self._state != _PENDING\n\n    def result(self):\n        \"\"\"Return the result this future represents.\n\n        If the future has been cancelled, raises CancelledError.  If the\n        future's result isn't yet available, raises InvalidStateError.  If\n        the future is done and has an exception set, this exception is raised.\n        \"\"\"\n        if self._state == _CANCELLED:\n            exc = self._make_cancelled_error()\n            raise exc\n        if self._state != _FINISHED:\n            raise exceptions.InvalidStateError('Result is not ready.')\n        self.__log_traceback = False\n        if self._exception is not None:\n            raise self._exception.with_traceback(self._exception_tb)\n        return self._result\n\n    def exception(self):\n        \"\"\"Return the exception that was set on this future.\n\n        The exception (or None if no exception was set) is returned only if\n        the future is done.  If the future has been cancelled, raises\n        CancelledError.  If the future isn't done yet, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state == _CANCELLED:\n            exc = self._make_cancelled_error()\n            raise exc\n        if self._state != _FINISHED:\n            raise exceptions.InvalidStateError('Exception is not set.')\n        self.__log_traceback = False\n        return self._exception\n\n    def add_done_callback(self, fn, *, context=None):\n        \"\"\"Add a callback to be run when the future becomes done.\n\n        The callback is called with a single argument - the future object. If\n        the future is already done when this is called, the callback is\n        scheduled with call_soon.\n        \"\"\"\n        if self._state != _PENDING:\n            self._loop.call_soon(fn, self, context=context)\n        else:\n            if context is None:\n                context = contextvars.copy_context()\n            self._callbacks.append((fn, context))\n\n    # New method not in PEP 3148.\n\n    def remove_done_callback(self, fn):\n        \"\"\"Remove all instances of a callback from the \"call when done\" list.\n\n        Returns the number of callbacks removed.\n        \"\"\"\n        filtered_callbacks = [(f, ctx)\n                              for (f, ctx) in self._callbacks\n                              if f != fn]\n        removed_count = len(self._callbacks) - len(filtered_callbacks)\n        if removed_count:\n            self._callbacks[:] = filtered_callbacks\n        return removed_count\n\n    # So-called internal methods (note: no set_running_or_notify_cancel()).\n\n    def set_result(self, result):\n        \"\"\"Mark the future done and set its result.\n\n        If the future is already done when this method is called, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state != _PENDING:\n            raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n        self._result = result\n        self._state = _FINISHED\n        self.__schedule_callbacks()\n\n    def set_exception(self, exception):\n        \"\"\"Mark the future done and set an exception.\n\n        If the future is already done when this method is called, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state != _PENDING:\n            raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n        if isinstance(exception, type):\n            exception = exception()\n        if type(exception) is StopIteration:\n            raise TypeError(\"StopIteration interacts badly with generators \"\n                            \"and cannot be raised into a Future\")\n        self._exception = exception\n        self._exception_tb = exception.__traceback__\n        self._state = _FINISHED\n        self.__schedule_callbacks()\n        self.__log_traceback = True\n\n    def __await__(self):\n        if not self.done():\n            self._asyncio_future_blocking = True\n            yield self  # This tells Task to wait for completion.\n        if not self.done():\n            raise RuntimeError(\"await wasn't used with future\")\n        return self.result()  # May raise too.\n\n    __iter__ = __await__  # make compatible with 'yield from'.\n\n\n# Needed for testing purposes.\n_PyFuture = Future\n\n\ndef _get_loop(fut):\n    # Tries to call Future.get_loop() if it's available.\n    # Otherwise fallbacks to using the old '_loop' property.\n    try:\n        get_loop = fut.get_loop\n    except AttributeError:\n        pass\n    else:\n        return get_loop()\n    return fut._loop\n\n\ndef _set_result_unless_cancelled(fut, result):\n    \"\"\"Helper setting the result only if the future was not cancelled.\"\"\"\n    if fut.cancelled():\n        return\n    fut.set_result(result)\n\n\ndef _convert_future_exc(exc):\n    exc_class = type(exc)\n    if exc_class is concurrent.futures.CancelledError:\n        return exceptions.CancelledError(*exc.args)\n    elif exc_class is concurrent.futures.TimeoutError:\n        return exceptions.TimeoutError(*exc.args)\n    elif exc_class is concurrent.futures.InvalidStateError:\n        return exceptions.InvalidStateError(*exc.args)\n    else:\n        return exc\n\n\ndef _set_concurrent_future_state(concurrent, source):\n    \"\"\"Copy state from a future to a concurrent.futures.Future.\"\"\"\n    assert source.done()\n    if source.cancelled():\n        concurrent.cancel()\n    if not concurrent.set_running_or_notify_cancel():\n        return\n    exception = source.exception()\n    if exception is not None:\n        concurrent.set_exception(_convert_future_exc(exception))\n    else:\n        result = source.result()\n        concurrent.set_result(result)\n\n\ndef _copy_future_state(source, dest):\n    \"\"\"Internal helper to copy state from another Future.\n\n    The other Future may be a concurrent.futures.Future.\n    \"\"\"\n    assert source.done()\n    if dest.cancelled():\n        return\n    assert not dest.done()\n    if source.cancelled():\n        dest.cancel()\n    else:\n        exception = source.exception()\n        if exception is not None:\n            dest.set_exception(_convert_future_exc(exception))\n        else:\n            result = source.result()\n            dest.set_result(result)\n\n\ndef _chain_future(source, destination):\n    \"\"\"Chain two futures so that when one completes, so does the other.\n\n    The result (or exception) of source will be copied to destination.\n    If destination is cancelled, source gets cancelled too.\n    Compatible with both asyncio.Future and concurrent.futures.Future.\n    \"\"\"\n    if not isfuture(source) and not isinstance(source,\n                                               concurrent.futures.Future):\n        raise TypeError('A future is required for source argument')\n    if not isfuture(destination) and not isinstance(destination,\n                                                    concurrent.futures.Future):\n        raise TypeError('A future is required for destination argument')\n    source_loop = _get_loop(source) if isfuture(source) else None\n    dest_loop = _get_loop(destination) if isfuture(destination) else None\n\n    def _set_state(future, other):\n        if isfuture(future):\n            _copy_future_state(other, future)\n        else:\n            _set_concurrent_future_state(future, other)\n\n    def _call_check_cancel(destination):\n        if destination.cancelled():\n            if source_loop is None or source_loop is dest_loop:\n                source.cancel()\n            else:\n                source_loop.call_soon_threadsafe(source.cancel)\n\n    def _call_set_state(source):\n        if (destination.cancelled() and\n                dest_loop is not None and dest_loop.is_closed()):\n            return\n        if dest_loop is None or dest_loop is source_loop:\n            _set_state(destination, source)\n        else:\n            if dest_loop.is_closed():\n                return\n            dest_loop.call_soon_threadsafe(_set_state, destination, source)\n\n    destination.add_done_callback(_call_check_cancel)\n    source.add_done_callback(_call_set_state)\n\n\ndef wrap_future(future, *, loop=None):\n    \"\"\"Wrap concurrent.futures.Future object.\"\"\"\n    if isfuture(future):\n        return future\n    assert isinstance(future, concurrent.futures.Future), \\\n        f'concurrent.futures.Future is expected, got {future!r}'\n    if loop is None:\n        loop = events._get_event_loop()\n    new_future = loop.create_future()\n    _chain_future(future, new_future)\n    return new_future\n\n\ntry:\n    import _asyncio\nexcept ImportError:\n    pass\nelse:\n    # _CFuture is needed for tests.\n    Future = _CFuture = _asyncio.Future\n", 428], "/usr/local/lib/python3.11/site-packages/uvicorn/server.py": ["from __future__ import annotations\n\nimport asyncio\nimport contextlib\nimport logging\nimport os\nimport platform\nimport signal\nimport socket\nimport sys\nimport threading\nimport time\nfrom collections.abc import Generator, Sequence\nfrom email.utils import formatdate\nfrom types import FrameType\nfrom typing import TYPE_CHECKING, Union\n\nimport click\n\nfrom uvicorn._compat import asyncio_run\nfrom uvicorn.config import Config\n\nif TYPE_CHECKING:\n    from uvicorn.protocols.http.h11_impl import H11Protocol\n    from uvicorn.protocols.http.httptools_impl import HttpToolsProtocol\n    from uvicorn.protocols.websockets.websockets_impl import WebSocketProtocol\n    from uvicorn.protocols.websockets.websockets_sansio_impl import WebSocketsSansIOProtocol\n    from uvicorn.protocols.websockets.wsproto_impl import WSProtocol\n\n    Protocols = Union[H11Protocol, HttpToolsProtocol, WSProtocol, WebSocketProtocol, WebSocketsSansIOProtocol]\n\nHANDLED_SIGNALS = (\n    signal.SIGINT,  # Unix signal 2. Sent by Ctrl+C.\n    signal.SIGTERM,  # Unix signal 15. Sent by `kill <pid>`.\n)\nif sys.platform == \"win32\":  # pragma: py-not-win32\n    HANDLED_SIGNALS += (signal.SIGBREAK,)  # Windows signal 21. Sent by Ctrl+Break.\n\nlogger = logging.getLogger(\"uvicorn.error\")\n\n\nclass ServerState:\n    \"\"\"\n    Shared servers state that is available between all protocol instances.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.total_requests = 0\n        self.connections: set[Protocols] = set()\n        self.tasks: set[asyncio.Task[None]] = set()\n        self.default_headers: list[tuple[bytes, bytes]] = []\n\n\nclass Server:\n    def __init__(self, config: Config) -> None:\n        self.config = config\n        self.server_state = ServerState()\n\n        self.started = False\n        self.should_exit = False\n        self.force_exit = False\n        self.last_notified = 0.0\n\n        self._captured_signals: list[int] = []\n\n    def run(self, sockets: list[socket.socket] | None = None) -> None:\n        return asyncio_run(self.serve(sockets=sockets), loop_factory=self.config.get_loop_factory())\n\n    async def serve(self, sockets: list[socket.socket] | None = None) -> None:\n        with self.capture_signals():\n            await self._serve(sockets)\n\n    async def _serve(self, sockets: list[socket.socket] | None = None) -> None:\n        process_id = os.getpid()\n\n        config = self.config\n        if not config.loaded:\n            config.load()\n\n        self.lifespan = config.lifespan_class(config)\n\n        message = \"Started server process [%d]\"\n        color_message = \"Started server process [\" + click.style(\"%d\", fg=\"cyan\") + \"]\"\n        logger.info(message, process_id, extra={\"color_message\": color_message})\n\n        await self.startup(sockets=sockets)\n        if self.should_exit:\n            return\n        await self.main_loop()\n        await self.shutdown(sockets=sockets)\n\n        message = \"Finished server process [%d]\"\n        color_message = \"Finished server process [\" + click.style(\"%d\", fg=\"cyan\") + \"]\"\n        logger.info(message, process_id, extra={\"color_message\": color_message})\n\n    async def startup(self, sockets: list[socket.socket] | None = None) -> None:\n        await self.lifespan.startup()\n        if self.lifespan.should_exit:\n            self.should_exit = True\n            return\n\n        config = self.config\n\n        def create_protocol(\n            _loop: asyncio.AbstractEventLoop | None = None,\n        ) -> asyncio.Protocol:\n            return config.http_protocol_class(  # type: ignore[call-arg]\n                config=config,\n                server_state=self.server_state,\n                app_state=self.lifespan.state,\n                _loop=_loop,\n            )\n\n        loop = asyncio.get_running_loop()\n\n        listeners: Sequence[socket.SocketType]\n        if sockets is not None:  # pragma: full coverage\n            # Explicitly passed a list of open sockets.\n            # We use this when the server is run from a Gunicorn worker.\n\n            def _share_socket(\n                sock: socket.SocketType,\n            ) -> socket.SocketType:  # pragma py-not-win32\n                # Windows requires the socket be explicitly shared across\n                # multiple workers (processes).\n                from socket import fromshare  # type: ignore[attr-defined]\n\n                sock_data = sock.share(os.getpid())  # type: ignore[attr-defined]\n                return fromshare(sock_data)\n\n            self.servers: list[asyncio.base_events.Server] = []\n            for sock in sockets:\n                is_windows = platform.system() == \"Windows\"\n                if config.workers > 1 and is_windows:  # pragma: py-not-win32\n                    sock = _share_socket(sock)  # type: ignore[assignment]\n                server = await loop.create_server(create_protocol, sock=sock, ssl=config.ssl, backlog=config.backlog)\n                self.servers.append(server)\n            listeners = sockets\n\n        elif config.fd is not None:  # pragma: py-win32\n            # Use an existing socket, from a file descriptor.\n            sock = socket.fromfd(config.fd, socket.AF_UNIX, socket.SOCK_STREAM)\n            server = await loop.create_server(create_protocol, sock=sock, ssl=config.ssl, backlog=config.backlog)\n            assert server.sockets is not None  # mypy\n            listeners = server.sockets\n            self.servers = [server]\n\n        elif config.uds is not None:  # pragma: py-win32\n            # Create a socket using UNIX domain socket.\n            uds_perms = 0o666\n            if os.path.exists(config.uds):\n                uds_perms = os.stat(config.uds).st_mode  # pragma: full coverage\n            server = await loop.create_unix_server(\n                create_protocol, path=config.uds, ssl=config.ssl, backlog=config.backlog\n            )\n            os.chmod(config.uds, uds_perms)\n            assert server.sockets is not None  # mypy\n            listeners = server.sockets\n            self.servers = [server]\n\n        else:\n            # Standard case. Create a socket from a host/port pair.\n            try:\n                server = await loop.create_server(\n                    create_protocol,\n                    host=config.host,\n                    port=config.port,\n                    ssl=config.ssl,\n                    backlog=config.backlog,\n                )\n            except OSError as exc:\n                logger.error(exc)\n                await self.lifespan.shutdown()\n                sys.exit(1)\n\n            assert server.sockets is not None\n            listeners = server.sockets\n            self.servers = [server]\n\n        if sockets is None:\n            self._log_started_message(listeners)\n        else:\n            # We're most likely running multiple workers, so a message has already been\n            # logged by `config.bind_socket()`.\n            pass  # pragma: full coverage\n\n        self.started = True\n\n    def _log_started_message(self, listeners: Sequence[socket.SocketType]) -> None:\n        config = self.config\n\n        if config.fd is not None:  # pragma: py-win32\n            sock = listeners[0]\n            logger.info(\n                \"Uvicorn running on socket %s (Press CTRL+C to quit)\",\n                sock.getsockname(),\n            )\n\n        elif config.uds is not None:  # pragma: py-win32\n            logger.info(\"Uvicorn running on unix socket %s (Press CTRL+C to quit)\", config.uds)\n\n        else:\n            addr_format = \"%s://%s:%d\"\n            host = \"0.0.0.0\" if config.host is None else config.host\n            if \":\" in host:\n                # It's an IPv6 address.\n                addr_format = \"%s://[%s]:%d\"\n\n            port = config.port\n            if port == 0:\n                port = listeners[0].getsockname()[1]\n\n            protocol_name = \"https\" if config.ssl else \"http\"\n            message = f\"Uvicorn running on {addr_format} (Press CTRL+C to quit)\"\n            color_message = \"Uvicorn running on \" + click.style(addr_format, bold=True) + \" (Press CTRL+C to quit)\"\n            logger.info(\n                message,\n                protocol_name,\n                host,\n                port,\n                extra={\"color_message\": color_message},\n            )\n\n    async def main_loop(self) -> None:\n        counter = 0\n        should_exit = await self.on_tick(counter)\n        while not should_exit:\n            counter += 1\n            counter = counter % 864000\n            await asyncio.sleep(0.1)\n            should_exit = await self.on_tick(counter)\n\n    async def on_tick(self, counter: int) -> bool:\n        # Update the default headers, once per second.\n        if counter % 10 == 0:\n            current_time = time.time()\n            current_date = formatdate(current_time, usegmt=True).encode()\n\n            if self.config.date_header:\n                date_header = [(b\"date\", current_date)]\n            else:\n                date_header = []\n\n            self.server_state.default_headers = date_header + self.config.encoded_headers\n\n            # Callback to `callback_notify` once every `timeout_notify` seconds.\n            if self.config.callback_notify is not None:\n                if current_time - self.last_notified > self.config.timeout_notify:  # pragma: full coverage\n                    self.last_notified = current_time\n                    await self.config.callback_notify()\n\n        # Determine if we should exit.\n        if self.should_exit:\n            return True\n\n        max_requests = self.config.limit_max_requests\n        if max_requests is not None and self.server_state.total_requests >= max_requests:\n            logger.warning(f\"Maximum request limit of {max_requests} exceeded. Terminating process.\")\n            return True\n\n        return False\n\n    async def shutdown(self, sockets: list[socket.socket] | None = None) -> None:\n        logger.info(\"Shutting down\")\n\n        # Stop accepting new connections.\n        for server in self.servers:\n            server.close()\n        for sock in sockets or []:\n            sock.close()  # pragma: full coverage\n\n        # Request shutdown on all existing connections.\n        for connection in list(self.server_state.connections):\n            connection.shutdown()\n        await asyncio.sleep(0.1)\n\n        # When 3.10 is not supported anymore, use `async with asyncio.timeout(...):`.\n        try:\n            await asyncio.wait_for(\n                self._wait_tasks_to_complete(),\n                timeout=self.config.timeout_graceful_shutdown,\n            )\n        except asyncio.TimeoutError:\n            logger.error(\n                \"Cancel %s running task(s), timeout graceful shutdown exceeded\",\n                len(self.server_state.tasks),\n            )\n            for t in self.server_state.tasks:\n                t.cancel(msg=\"Task cancelled, timeout graceful shutdown exceeded\")\n\n        # Send the lifespan shutdown event, and wait for application shutdown.\n        if not self.force_exit:\n            await self.lifespan.shutdown()\n\n    async def _wait_tasks_to_complete(self) -> None:\n        # Wait for existing connections to finish sending responses.\n        if self.server_state.connections and not self.force_exit:\n            msg = \"Waiting for connections to close. (CTRL+C to force quit)\"\n            logger.info(msg)\n            while self.server_state.connections and not self.force_exit:\n                await asyncio.sleep(0.1)\n\n        # Wait for existing tasks to complete.\n        if self.server_state.tasks and not self.force_exit:\n            msg = \"Waiting for background tasks to complete. (CTRL+C to force quit)\"\n            logger.info(msg)\n            while self.server_state.tasks and not self.force_exit:\n                await asyncio.sleep(0.1)\n\n        for server in self.servers:\n            await server.wait_closed()\n\n    @contextlib.contextmanager\n    def capture_signals(self) -> Generator[None, None, None]:\n        # Signals can only be listened to from the main thread.\n        if threading.current_thread() is not threading.main_thread():\n            yield\n            return\n        # always use signal.signal, even if loop.add_signal_handler is available\n        # this allows to restore previous signal handlers later on\n        original_handlers = {sig: signal.signal(sig, self.handle_exit) for sig in HANDLED_SIGNALS}\n        try:\n            yield\n        finally:\n            for sig, handler in original_handlers.items():\n                signal.signal(sig, handler)\n        # If we did gracefully shut down due to a signal, try to\n        # trigger the expected behaviour now; multiple signals would be\n        # done LIFO, see https://stackoverflow.com/questions/48434964\n        for captured_signal in reversed(self._captured_signals):\n            signal.raise_signal(captured_signal)\n\n    def handle_exit(self, sig: int, frame: FrameType | None) -> None:\n        self._captured_signals.append(sig)\n        if self.should_exit and sig == signal.SIGINT:\n            self.force_exit = True  # pragma: full coverage\n        else:\n            self.should_exit = True\n", 338], "/usr/local/lib/python3.11/site-packages/redis/commands/core.py": ["# from __future__ import annotations\n\nimport datetime\nimport hashlib\nimport warnings\nfrom enum import Enum\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterator,\n    Awaitable,\n    Callable,\n    Dict,\n    Iterable,\n    Iterator,\n    List,\n    Literal,\n    Mapping,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Union,\n)\n\nfrom redis.exceptions import ConnectionError, DataError, NoScriptError, RedisError\nfrom redis.typing import (\n    AbsExpiryT,\n    AnyKeyT,\n    BitfieldOffsetT,\n    ChannelT,\n    CommandsProtocol,\n    ConsumerT,\n    EncodableT,\n    ExpiryT,\n    FieldT,\n    GroupT,\n    KeysT,\n    KeyT,\n    Number,\n    PatternT,\n    ResponseT,\n    ScriptTextT,\n    StreamIdT,\n    TimeoutSecT,\n    ZScoreBoundT,\n)\nfrom redis.utils import (\n    deprecated_function,\n    extract_expire_flags,\n)\n\nfrom .helpers import list_or_args\n\nif TYPE_CHECKING:\n    import redis.asyncio.client\n    import redis.client\n\n\nclass ACLCommands(CommandsProtocol):\n    \"\"\"\n    Redis Access Control List (ACL) commands.\n    see: https://redis.io/topics/acl\n    \"\"\"\n\n    def acl_cat(self, category: Optional[str] = None, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns a list of categories or commands within a category.\n\n        If ``category`` is not supplied, returns a list of all categories.\n        If ``category`` is supplied, returns a list of all commands within\n        that category.\n\n        For more information see https://redis.io/commands/acl-cat\n        \"\"\"\n        pieces: list[EncodableT] = [category] if category else []\n        return self.execute_command(\"ACL CAT\", *pieces, **kwargs)\n\n    def acl_dryrun(self, username, *args, **kwargs):\n        \"\"\"\n        Simulate the execution of a given command by a given ``username``.\n\n        For more information see https://redis.io/commands/acl-dryrun\n        \"\"\"\n        return self.execute_command(\"ACL DRYRUN\", username, *args, **kwargs)\n\n    def acl_deluser(self, *username: str, **kwargs) -> ResponseT:\n        \"\"\"\n        Delete the ACL for the specified ``username``\\\\s\n\n        For more information see https://redis.io/commands/acl-deluser\n        \"\"\"\n        return self.execute_command(\"ACL DELUSER\", *username, **kwargs)\n\n    def acl_genpass(self, bits: Optional[int] = None, **kwargs) -> ResponseT:\n        \"\"\"Generate a random password value.\n        If ``bits`` is supplied then use this number of bits, rounded to\n        the next multiple of 4.\n        See: https://redis.io/commands/acl-genpass\n        \"\"\"\n        pieces = []\n        if bits is not None:\n            try:\n                b = int(bits)\n                if b < 0 or b > 4096:\n                    raise ValueError\n                pieces.append(b)\n            except ValueError:\n                raise DataError(\n                    \"genpass optionally accepts a bits argument, between 0 and 4096.\"\n                )\n        return self.execute_command(\"ACL GENPASS\", *pieces, **kwargs)\n\n    def acl_getuser(self, username: str, **kwargs) -> ResponseT:\n        \"\"\"\n        Get the ACL details for the specified ``username``.\n\n        If ``username`` does not exist, return None\n\n        For more information see https://redis.io/commands/acl-getuser\n        \"\"\"\n        return self.execute_command(\"ACL GETUSER\", username, **kwargs)\n\n    def acl_help(self, **kwargs) -> ResponseT:\n        \"\"\"The ACL HELP command returns helpful text describing\n        the different subcommands.\n\n        For more information see https://redis.io/commands/acl-help\n        \"\"\"\n        return self.execute_command(\"ACL HELP\", **kwargs)\n\n    def acl_list(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Return a list of all ACLs on the server\n\n        For more information see https://redis.io/commands/acl-list\n        \"\"\"\n        return self.execute_command(\"ACL LIST\", **kwargs)\n\n    def acl_log(self, count: Optional[int] = None, **kwargs) -> ResponseT:\n        \"\"\"\n        Get ACL logs as a list.\n        :param int count: Get logs[0:count].\n        :rtype: List.\n\n        For more information see https://redis.io/commands/acl-log\n        \"\"\"\n        args = []\n        if count is not None:\n            if not isinstance(count, int):\n                raise DataError(\"ACL LOG count must be an integer\")\n            args.append(count)\n\n        return self.execute_command(\"ACL LOG\", *args, **kwargs)\n\n    def acl_log_reset(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Reset ACL logs.\n        :rtype: Boolean.\n\n        For more information see https://redis.io/commands/acl-log\n        \"\"\"\n        args = [b\"RESET\"]\n        return self.execute_command(\"ACL LOG\", *args, **kwargs)\n\n    def acl_load(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Load ACL rules from the configured ``aclfile``.\n\n        Note that the server must be configured with the ``aclfile``\n        directive to be able to load ACL rules from an aclfile.\n\n        For more information see https://redis.io/commands/acl-load\n        \"\"\"\n        return self.execute_command(\"ACL LOAD\", **kwargs)\n\n    def acl_save(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Save ACL rules to the configured ``aclfile``.\n\n        Note that the server must be configured with the ``aclfile``\n        directive to be able to save ACL rules to an aclfile.\n\n        For more information see https://redis.io/commands/acl-save\n        \"\"\"\n        return self.execute_command(\"ACL SAVE\", **kwargs)\n\n    def acl_setuser(\n        self,\n        username: str,\n        enabled: bool = False,\n        nopass: bool = False,\n        passwords: Optional[Union[str, Iterable[str]]] = None,\n        hashed_passwords: Optional[Union[str, Iterable[str]]] = None,\n        categories: Optional[Iterable[str]] = None,\n        commands: Optional[Iterable[str]] = None,\n        keys: Optional[Iterable[KeyT]] = None,\n        channels: Optional[Iterable[ChannelT]] = None,\n        selectors: Optional[Iterable[Tuple[str, KeyT]]] = None,\n        reset: bool = False,\n        reset_keys: bool = False,\n        reset_channels: bool = False,\n        reset_passwords: bool = False,\n        **kwargs,\n    ) -> ResponseT:\n        \"\"\"\n        Create or update an ACL user.\n\n        Create or update the ACL for `username`. If the user already exists,\n        the existing ACL is completely overwritten and replaced with the\n        specified values.\n\n        For more information, see https://redis.io/commands/acl-setuser\n\n        Args:\n            username: The name of the user whose ACL is to be created or updated.\n            enabled: Indicates whether the user should be allowed to authenticate.\n                     Defaults to `False`.\n            nopass: Indicates whether the user can authenticate without a password.\n                    This cannot be `True` if `passwords` are also specified.\n            passwords: A list of plain text passwords to add to or remove from the user.\n                       Each password must be prefixed with a '+' to add or a '-' to\n                       remove. For convenience, a single prefixed string can be used\n                       when adding or removing a single password.\n            hashed_passwords: A list of SHA-256 hashed passwords to add to or remove\n                              from the user. Each hashed password must be prefixed with\n                              a '+' to add or a '-' to remove. For convenience, a single\n                              prefixed string can be used when adding or removing a\n                              single password.\n            categories: A list of strings representing category permissions. Each string\n                        must be prefixed with either a '+' to add the category\n                        permission or a '-' to remove the category permission.\n            commands: A list of strings representing command permissions. Each string\n                      must be prefixed with either a '+' to add the command permission\n                      or a '-' to remove the command permission.\n            keys: A list of key patterns to grant the user access to. Key patterns allow\n                  ``'*'`` to support wildcard matching. For example, ``'*'`` grants\n                  access to all keys while ``'cache:*'`` grants access to all keys that\n                  are prefixed with ``cache:``.\n                  `keys` should not be prefixed with a ``'~'``.\n            reset: Indicates whether the user should be fully reset prior to applying\n                   the new ACL. Setting this to `True` will remove all existing\n                   passwords, flags, and privileges from the user and then apply the\n                   specified rules. If `False`, the user's existing passwords, flags,\n                   and privileges will be kept and any new specified rules will be\n                   applied on top.\n            reset_keys: Indicates whether the user's key permissions should be reset\n                        prior to applying any new key permissions specified in `keys`.\n                        If `False`, the user's existing key permissions will be kept and\n                        any new specified key permissions will be applied on top.\n            reset_channels: Indicates whether the user's channel permissions should be\n                            reset prior to applying any new channel permissions\n                            specified in `channels`. If `False`, the user's existing\n                            channel permissions will be kept and any new specified\n                            channel permissions will be applied on top.\n            reset_passwords: Indicates whether to remove all existing passwords and the\n                             `nopass` flag from the user prior to applying any new\n                             passwords specified in `passwords` or `hashed_passwords`.\n                             If `False`, the user's existing passwords and `nopass`\n                             status will be kept and any new specified passwords or\n                             hashed passwords will be applied on top.\n        \"\"\"\n        encoder = self.get_encoder()\n        pieces: List[EncodableT] = [username]\n\n        if reset:\n            pieces.append(b\"reset\")\n\n        if reset_keys:\n            pieces.append(b\"resetkeys\")\n\n        if reset_channels:\n            pieces.append(b\"resetchannels\")\n\n        if reset_passwords:\n            pieces.append(b\"resetpass\")\n\n        if enabled:\n            pieces.append(b\"on\")\n        else:\n            pieces.append(b\"off\")\n\n        if (passwords or hashed_passwords) and nopass:\n            raise DataError(\n                \"Cannot set 'nopass' and supply 'passwords' or 'hashed_passwords'\"\n            )\n\n        if passwords:\n            # as most users will have only one password, allow remove_passwords\n            # to be specified as a simple string or a list\n            passwords = list_or_args(passwords, [])\n            for i, password in enumerate(passwords):\n                password = encoder.encode(password)\n                if password.startswith(b\"+\"):\n                    pieces.append(b\">%s\" % password[1:])\n                elif password.startswith(b\"-\"):\n                    pieces.append(b\"<%s\" % password[1:])\n                else:\n                    raise DataError(\n                        f\"Password {i} must be prefixed with a \"\n                        f'\"+\" to add or a \"-\" to remove'\n                    )\n\n        if hashed_passwords:\n            # as most users will have only one password, allow remove_passwords\n            # to be specified as a simple string or a list\n            hashed_passwords = list_or_args(hashed_passwords, [])\n            for i, hashed_password in enumerate(hashed_passwords):\n                hashed_password = encoder.encode(hashed_password)\n                if hashed_password.startswith(b\"+\"):\n                    pieces.append(b\"#%s\" % hashed_password[1:])\n                elif hashed_password.startswith(b\"-\"):\n                    pieces.append(b\"!%s\" % hashed_password[1:])\n                else:\n                    raise DataError(\n                        f\"Hashed password {i} must be prefixed with a \"\n                        f'\"+\" to add or a \"-\" to remove'\n                    )\n\n        if nopass:\n            pieces.append(b\"nopass\")\n\n        if categories:\n            for category in categories:\n                category = encoder.encode(category)\n                # categories can be prefixed with one of (+@, +, -@, -)\n                if category.startswith(b\"+@\"):\n                    pieces.append(category)\n                elif category.startswith(b\"+\"):\n                    pieces.append(b\"+@%s\" % category[1:])\n                elif category.startswith(b\"-@\"):\n                    pieces.append(category)\n                elif category.startswith(b\"-\"):\n                    pieces.append(b\"-@%s\" % category[1:])\n                else:\n                    raise DataError(\n                        f'Category \"{encoder.decode(category, force=True)}\" '\n                        'must be prefixed with \"+\" or \"-\"'\n                    )\n        if commands:\n            for cmd in commands:\n                cmd = encoder.encode(cmd)\n                if not cmd.startswith(b\"+\") and not cmd.startswith(b\"-\"):\n                    raise DataError(\n                        f'Command \"{encoder.decode(cmd, force=True)}\" '\n                        'must be prefixed with \"+\" or \"-\"'\n                    )\n                pieces.append(cmd)\n\n        if keys:\n            for key in keys:\n                key = encoder.encode(key)\n                if not key.startswith(b\"%\") and not key.startswith(b\"~\"):\n                    key = b\"~%s\" % key\n                pieces.append(key)\n\n        if channels:\n            for channel in channels:\n                channel = encoder.encode(channel)\n                pieces.append(b\"&%s\" % channel)\n\n        if selectors:\n            for cmd, key in selectors:\n                cmd = encoder.encode(cmd)\n                if not cmd.startswith(b\"+\") and not cmd.startswith(b\"-\"):\n                    raise DataError(\n                        f'Command \"{encoder.decode(cmd, force=True)}\" '\n                        'must be prefixed with \"+\" or \"-\"'\n                    )\n\n                key = encoder.encode(key)\n                if not key.startswith(b\"%\") and not key.startswith(b\"~\"):\n                    key = b\"~%s\" % key\n\n                pieces.append(b\"(%s %s)\" % (cmd, key))\n\n        return self.execute_command(\"ACL SETUSER\", *pieces, **kwargs)\n\n    def acl_users(self, **kwargs) -> ResponseT:\n        \"\"\"Returns a list of all registered users on the server.\n\n        For more information see https://redis.io/commands/acl-users\n        \"\"\"\n        return self.execute_command(\"ACL USERS\", **kwargs)\n\n    def acl_whoami(self, **kwargs) -> ResponseT:\n        \"\"\"Get the username for the current connection\n\n        For more information see https://redis.io/commands/acl-whoami\n        \"\"\"\n        return self.execute_command(\"ACL WHOAMI\", **kwargs)\n\n\nAsyncACLCommands = ACLCommands\n\n\nclass ManagementCommands(CommandsProtocol):\n    \"\"\"\n    Redis management commands\n    \"\"\"\n\n    def auth(self, password: str, username: Optional[str] = None, **kwargs):\n        \"\"\"\n        Authenticates the user. If you do not pass username, Redis will try to\n        authenticate for the \"default\" user. If you do pass username, it will\n        authenticate for the given user.\n        For more information see https://redis.io/commands/auth\n        \"\"\"\n        pieces = []\n        if username is not None:\n            pieces.append(username)\n        pieces.append(password)\n        return self.execute_command(\"AUTH\", *pieces, **kwargs)\n\n    def bgrewriteaof(self, **kwargs):\n        \"\"\"Tell the Redis server to rewrite the AOF file from data in memory.\n\n        For more information see https://redis.io/commands/bgrewriteaof\n        \"\"\"\n        return self.execute_command(\"BGREWRITEAOF\", **kwargs)\n\n    def bgsave(self, schedule: bool = True, **kwargs) -> ResponseT:\n        \"\"\"\n        Tell the Redis server to save its data to disk.  Unlike save(),\n        this method is asynchronous and returns immediately.\n\n        For more information see https://redis.io/commands/bgsave\n        \"\"\"\n        pieces = []\n        if schedule:\n            pieces.append(\"SCHEDULE\")\n        return self.execute_command(\"BGSAVE\", *pieces, **kwargs)\n\n    def role(self) -> ResponseT:\n        \"\"\"\n        Provide information on the role of a Redis instance in\n        the context of replication, by returning if the instance\n        is currently a master, slave, or sentinel.\n\n        For more information see https://redis.io/commands/role\n        \"\"\"\n        return self.execute_command(\"ROLE\")\n\n    def client_kill(self, address: str, **kwargs) -> ResponseT:\n        \"\"\"Disconnects the client at ``address`` (ip:port)\n\n        For more information see https://redis.io/commands/client-kill\n        \"\"\"\n        return self.execute_command(\"CLIENT KILL\", address, **kwargs)\n\n    def client_kill_filter(\n        self,\n        _id: Optional[str] = None,\n        _type: Optional[str] = None,\n        addr: Optional[str] = None,\n        skipme: Optional[bool] = None,\n        laddr: Optional[bool] = None,\n        user: Optional[str] = None,\n        maxage: Optional[int] = None,\n        **kwargs,\n    ) -> ResponseT:\n        \"\"\"\n        Disconnects client(s) using a variety of filter options\n        :param _id: Kills a client by its unique ID field\n        :param _type: Kills a client by type where type is one of 'normal',\n        'master', 'slave' or 'pubsub'\n        :param addr: Kills a client by its 'address:port'\n        :param skipme: If True, then the client calling the command\n        will not get killed even if it is identified by one of the filter\n        options. If skipme is not provided, the server defaults to skipme=True\n        :param laddr: Kills a client by its 'local (bind) address:port'\n        :param user: Kills a client for a specific user name\n        :param maxage: Kills clients that are older than the specified age in seconds\n        \"\"\"\n        args = []\n        if _type is not None:\n            client_types = (\"normal\", \"master\", \"slave\", \"pubsub\")\n            if str(_type).lower() not in client_types:\n                raise DataError(f\"CLIENT KILL type must be one of {client_types!r}\")\n            args.extend((b\"TYPE\", _type))\n        if skipme is not None:\n            if not isinstance(skipme, bool):\n                raise DataError(\"CLIENT KILL skipme must be a bool\")\n            if skipme:\n                args.extend((b\"SKIPME\", b\"YES\"))\n            else:\n                args.extend((b\"SKIPME\", b\"NO\"))\n        if _id is not None:\n            args.extend((b\"ID\", _id))\n        if addr is not None:\n            args.extend((b\"ADDR\", addr))\n        if laddr is not None:\n            args.extend((b\"LADDR\", laddr))\n        if user is not None:\n            args.extend((b\"USER\", user))\n        if maxage is not None:\n            args.extend((b\"MAXAGE\", maxage))\n        if not args:\n            raise DataError(\n                \"CLIENT KILL <filter> <value> ... ... <filter> \"\n                \"<value> must specify at least one filter\"\n            )\n        return self.execute_command(\"CLIENT KILL\", *args, **kwargs)\n\n    def client_info(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns information and statistics about the current\n        client connection.\n\n        For more information see https://redis.io/commands/client-info\n        \"\"\"\n        return self.execute_command(\"CLIENT INFO\", **kwargs)\n\n    def client_list(\n        self, _type: Optional[str] = None, client_id: List[EncodableT] = [], **kwargs\n    ) -> ResponseT:\n        \"\"\"\n        Returns a list of currently connected clients.\n        If type of client specified, only that type will be returned.\n\n        :param _type: optional. one of the client types (normal, master,\n         replica, pubsub)\n        :param client_id: optional. a list of client ids\n\n        For more information see https://redis.io/commands/client-list\n        \"\"\"\n        args = []\n        if _type is not None:\n            client_types = (\"normal\", \"master\", \"replica\", \"pubsub\")\n            if str(_type).lower() not in client_types:\n                raise DataError(f\"CLIENT LIST _type must be one of {client_types!r}\")\n            args.append(b\"TYPE\")\n            args.append(_type)\n        if not isinstance(client_id, list):\n            raise DataError(\"client_id must be a list\")\n        if client_id:\n            args.append(b\"ID\")\n            args += client_id\n        return self.execute_command(\"CLIENT LIST\", *args, **kwargs)\n\n    def client_getname(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns the current connection name\n\n        For more information see https://redis.io/commands/client-getname\n        \"\"\"\n        return self.execute_command(\"CLIENT GETNAME\", **kwargs)\n\n    def client_getredir(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns the ID (an integer) of the client to whom we are\n        redirecting tracking notifications.\n\n        see: https://redis.io/commands/client-getredir\n        \"\"\"\n        return self.execute_command(\"CLIENT GETREDIR\", **kwargs)\n\n    def client_reply(\n        self, reply: Union[Literal[\"ON\"], Literal[\"OFF\"], Literal[\"SKIP\"]], **kwargs\n    ) -> ResponseT:\n        \"\"\"\n        Enable and disable redis server replies.\n\n        ``reply`` Must be ON OFF or SKIP,\n        ON - The default most with server replies to commands\n        OFF - Disable server responses to commands\n        SKIP - Skip the response of the immediately following command.\n\n        Note: When setting OFF or SKIP replies, you will need a client object\n        with a timeout specified in seconds, and will need to catch the\n        TimeoutError.\n        The test_client_reply unit test illustrates this, and\n        conftest.py has a client with a timeout.\n\n        See https://redis.io/commands/client-reply\n        \"\"\"\n        replies = [\"ON\", \"OFF\", \"SKIP\"]\n        if reply not in replies:\n            raise DataError(f\"CLIENT REPLY must be one of {replies!r}\")\n        return self.execute_command(\"CLIENT REPLY\", reply, **kwargs)\n\n    def client_id(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns the current connection id\n\n        For more information see https://redis.io/commands/client-id\n        \"\"\"\n        return self.execute_command(\"CLIENT ID\", **kwargs)\n\n    def client_tracking_on(\n        self,\n        clientid: Optional[int] = None,\n        prefix: Sequence[KeyT] = [],\n        bcast: bool = False,\n        optin: bool = False,\n        optout: bool = False,\n        noloop: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Turn on the tracking mode.\n        For more information about the options look at client_tracking func.\n\n        See https://redis.io/commands/client-tracking\n        \"\"\"\n        return self.client_tracking(\n            True, clientid, prefix, bcast, optin, optout, noloop\n        )\n\n    def client_tracking_off(\n        self,\n        clientid: Optional[int] = None,\n        prefix: Sequence[KeyT] = [],\n        bcast: bool = False,\n        optin: bool = False,\n        optout: bool = False,\n        noloop: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Turn off the tracking mode.\n        For more information about the options look at client_tracking func.\n\n        See https://redis.io/commands/client-tracking\n        \"\"\"\n        return self.client_tracking(\n            False, clientid, prefix, bcast, optin, optout, noloop\n        )\n\n    def client_tracking(\n        self,\n        on: bool = True,\n        clientid: Optional[int] = None,\n        prefix: Sequence[KeyT] = [],\n        bcast: bool = False,\n        optin: bool = False,\n        optout: bool = False,\n        noloop: bool = False,\n        **kwargs,\n    ) -> ResponseT:\n        \"\"\"\n        Enables the tracking feature of the Redis server, that is used\n        for server assisted client side caching.\n\n        ``on`` indicate for tracking on or tracking off. The dafualt is on.\n\n        ``clientid`` send invalidation messages to the connection with\n        the specified ID.\n\n        ``bcast`` enable tracking in broadcasting mode. In this mode\n        invalidation messages are reported for all the prefixes\n        specified, regardless of the keys requested by the connection.\n\n        ``optin``  when broadcasting is NOT active, normally don't track\n        keys in read only commands, unless they are called immediately\n        after a CLIENT CACHING yes command.\n\n        ``optout`` when broadcasting is NOT active, normally track keys in\n        read only commands, unless they are called immediately after a\n        CLIENT CACHING no command.\n\n        ``noloop`` don't send notifications about keys modified by this\n        connection itself.\n\n        ``prefix``  for broadcasting, register a given key prefix, so that\n        notifications will be provided only for keys starting with this string.\n\n        See https://redis.io/commands/client-tracking\n        \"\"\"\n\n        if len(prefix) != 0 and bcast is False:\n            raise DataError(\"Prefix can only be used with bcast\")\n\n        pieces = [\"ON\"] if on else [\"OFF\"]\n        if clientid is not None:\n            pieces.extend([\"REDIRECT\", clientid])\n        for p in prefix:\n            pieces.extend([\"PREFIX\", p])\n        if bcast:\n            pieces.append(\"BCAST\")\n        if optin:\n            pieces.append(\"OPTIN\")\n        if optout:\n            pieces.append(\"OPTOUT\")\n        if noloop:\n            pieces.append(\"NOLOOP\")\n\n        return self.execute_command(\"CLIENT TRACKING\", *pieces)\n\n    def client_trackinginfo(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns the information about the current client connection's\n        use of the server assisted client side cache.\n\n        See https://redis.io/commands/client-trackinginfo\n        \"\"\"\n        return self.execute_command(\"CLIENT TRACKINGINFO\", **kwargs)\n\n    def client_setname(self, name: str, **kwargs) -> ResponseT:\n        \"\"\"\n        Sets the current connection name\n\n        For more information see https://redis.io/commands/client-setname\n\n        .. note::\n           This method sets client name only for **current** connection.\n\n           If you want to set a common name for all connections managed\n           by this client, use ``client_name`` constructor argument.\n        \"\"\"\n        return self.execute_command(\"CLIENT SETNAME\", name, **kwargs)\n\n    def client_setinfo(self, attr: str, value: str, **kwargs) -> ResponseT:\n        \"\"\"\n        Sets the current connection library name or version\n        For mor information see https://redis.io/commands/client-setinfo\n        \"\"\"\n        return self.execute_command(\"CLIENT SETINFO\", attr, value, **kwargs)\n\n    def client_unblock(\n        self, client_id: int, error: bool = False, **kwargs\n    ) -> ResponseT:\n        \"\"\"\n        Unblocks a connection by its client id.\n        If ``error`` is True, unblocks the client with a special error message.\n        If ``error`` is False (default), the client is unblocked using the\n        regular timeout mechanism.\n\n        For more information see https://redis.io/commands/client-unblock\n        \"\"\"\n        args = [\"CLIENT UNBLOCK\", int(client_id)]\n        if error:\n            args.append(b\"ERROR\")\n        return self.execute_command(*args, **kwargs)\n\n    def client_pause(self, timeout: int, all: bool = True, **kwargs) -> ResponseT:\n        \"\"\"\n        Suspend all the Redis clients for the specified amount of time.\n\n\n        For more information see https://redis.io/commands/client-pause\n\n        Args:\n            timeout: milliseconds to pause clients\n            all: If true (default) all client commands are blocked.\n                 otherwise, clients are only blocked if they attempt to execute\n                 a write command.\n\n        For the WRITE mode, some commands have special behavior:\n\n        * EVAL/EVALSHA: Will block client for all scripts.\n        * PUBLISH: Will block client.\n        * PFCOUNT: Will block client.\n        * WAIT: Acknowledgments will be delayed, so this command will\n            appear blocked.\n        \"\"\"\n        args = [\"CLIENT PAUSE\", str(timeout)]\n        if not isinstance(timeout, int):\n            raise DataError(\"CLIENT PAUSE timeout must be an integer\")\n        if not all:\n            args.append(\"WRITE\")\n        return self.execute_command(*args, **kwargs)\n\n    def client_unpause(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Unpause all redis clients\n\n        For more information see https://redis.io/commands/client-unpause\n        \"\"\"\n        return self.execute_command(\"CLIENT UNPAUSE\", **kwargs)\n\n    def client_no_evict(self, mode: str) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Sets the client eviction mode for the current connection.\n\n        For more information see https://redis.io/commands/client-no-evict\n        \"\"\"\n        return self.execute_command(\"CLIENT NO-EVICT\", mode)\n\n    def client_no_touch(self, mode: str) -> Union[Awaitable[str], str]:\n        \"\"\"\n        # The command controls whether commands sent by the client will alter\n        # the LRU/LFU of the keys they access.\n        # When turned on, the current client will not change LFU/LRU stats,\n        # unless it sends the TOUCH command.\n\n        For more information see https://redis.io/commands/client-no-touch\n        \"\"\"\n        return self.execute_command(\"CLIENT NO-TOUCH\", mode)\n\n    def command(self, **kwargs):\n        \"\"\"\n        Returns dict reply of details about all Redis commands.\n\n        For more information see https://redis.io/commands/command\n        \"\"\"\n        return self.execute_command(\"COMMAND\", **kwargs)\n\n    def command_info(self, **kwargs) -> None:\n        raise NotImplementedError(\n            \"COMMAND INFO is intentionally not implemented in the client.\"\n        )\n\n    def command_count(self, **kwargs) -> ResponseT:\n        return self.execute_command(\"COMMAND COUNT\", **kwargs)\n\n    def command_list(\n        self,\n        module: Optional[str] = None,\n        category: Optional[str] = None,\n        pattern: Optional[str] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Return an array of the server's command names.\n        You can use one of the following filters:\n        ``module``: get the commands that belong to the module\n        ``category``: get the commands in the ACL category\n        ``pattern``: get the commands that match the given pattern\n\n        For more information see https://redis.io/commands/command-list/\n        \"\"\"\n        pieces = []\n        if module is not None:\n            pieces.extend([\"MODULE\", module])\n        if category is not None:\n            pieces.extend([\"ACLCAT\", category])\n        if pattern is not None:\n            pieces.extend([\"PATTERN\", pattern])\n\n        if pieces:\n            pieces.insert(0, \"FILTERBY\")\n\n        return self.execute_command(\"COMMAND LIST\", *pieces)\n\n    def command_getkeysandflags(self, *args: List[str]) -> List[Union[str, List[str]]]:\n        \"\"\"\n        Returns array of keys from a full Redis command and their usage flags.\n\n        For more information see https://redis.io/commands/command-getkeysandflags\n        \"\"\"\n        return self.execute_command(\"COMMAND GETKEYSANDFLAGS\", *args)\n\n    def command_docs(self, *args):\n        \"\"\"\n        This function throws a NotImplementedError since it is intentionally\n        not supported.\n        \"\"\"\n        raise NotImplementedError(\n            \"COMMAND DOCS is intentionally not implemented in the client.\"\n        )\n\n    def config_get(\n        self, pattern: PatternT = \"*\", *args: List[PatternT], **kwargs\n    ) -> ResponseT:\n        \"\"\"\n        Return a dictionary of configuration based on the ``pattern``\n\n        For more information see https://redis.io/commands/config-get\n        \"\"\"\n        return self.execute_command(\"CONFIG GET\", pattern, *args, **kwargs)\n\n    def config_set(\n        self,\n        name: KeyT,\n        value: EncodableT,\n        *args: List[Union[KeyT, EncodableT]],\n        **kwargs,\n    ) -> ResponseT:\n        \"\"\"Set config item ``name`` with ``value``\n\n        For more information see https://redis.io/commands/config-set\n        \"\"\"\n        return self.execute_command(\"CONFIG SET\", name, value, *args, **kwargs)\n\n    def config_resetstat(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Reset runtime statistics\n\n        For more information see https://redis.io/commands/config-resetstat\n        \"\"\"\n        return self.execute_command(\"CONFIG RESETSTAT\", **kwargs)\n\n    def config_rewrite(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Rewrite config file with the minimal change to reflect running config.\n\n        For more information see https://redis.io/commands/config-rewrite\n        \"\"\"\n        return self.execute_command(\"CONFIG REWRITE\", **kwargs)\n\n    def dbsize(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns the number of keys in the current database\n\n        For more information see https://redis.io/commands/dbsize\n        \"\"\"\n        return self.execute_command(\"DBSIZE\", **kwargs)\n\n    def debug_object(self, key: KeyT, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns version specific meta information about a given key\n\n        For more information see https://redis.io/commands/debug-object\n        \"\"\"\n        return self.execute_command(\"DEBUG OBJECT\", key, **kwargs)\n\n    def debug_segfault(self, **kwargs) -> None:\n        raise NotImplementedError(\n            \"\"\"\n            DEBUG SEGFAULT is intentionally not implemented in the client.\n\n            For more information see https://redis.io/commands/debug-segfault\n            \"\"\"\n        )\n\n    def echo(self, value: EncodableT, **kwargs) -> ResponseT:\n        \"\"\"\n        Echo the string back from the server\n\n        For more information see https://redis.io/commands/echo\n        \"\"\"\n        return self.execute_command(\"ECHO\", value, **kwargs)\n\n    def flushall(self, asynchronous: bool = False, **kwargs) -> ResponseT:\n        \"\"\"\n        Delete all keys in all databases on the current host.\n\n        ``asynchronous`` indicates whether the operation is\n        executed asynchronously by the server.\n\n        For more information see https://redis.io/commands/flushall\n        \"\"\"\n        args = []\n        if asynchronous:\n            args.append(b\"ASYNC\")\n        return self.execute_command(\"FLUSHALL\", *args, **kwargs)\n\n    def flushdb(self, asynchronous: bool = False, **kwargs) -> ResponseT:\n        \"\"\"\n        Delete all keys in the current database.\n\n        ``asynchronous`` indicates whether the operation is\n        executed asynchronously by the server.\n\n        For more information see https://redis.io/commands/flushdb\n        \"\"\"\n        args = []\n        if asynchronous:\n            args.append(b\"ASYNC\")\n        return self.execute_command(\"FLUSHDB\", *args, **kwargs)\n\n    def sync(self) -> ResponseT:\n        \"\"\"\n        Initiates a replication stream from the master.\n\n        For more information see https://redis.io/commands/sync\n        \"\"\"\n        from redis.client import NEVER_DECODE\n\n        options = {}\n        options[NEVER_DECODE] = []\n        return self.execute_command(\"SYNC\", **options)\n\n    def psync(self, replicationid: str, offset: int):\n        \"\"\"\n        Initiates a replication stream from the master.\n        Newer version for `sync`.\n\n        For more information see https://redis.io/commands/sync\n        \"\"\"\n        from redis.client import NEVER_DECODE\n\n        options = {}\n        options[NEVER_DECODE] = []\n        return self.execute_command(\"PSYNC\", replicationid, offset, **options)\n\n    def swapdb(self, first: int, second: int, **kwargs) -> ResponseT:\n        \"\"\"\n        Swap two databases\n\n        For more information see https://redis.io/commands/swapdb\n        \"\"\"\n        return self.execute_command(\"SWAPDB\", first, second, **kwargs)\n\n    def select(self, index: int, **kwargs) -> ResponseT:\n        \"\"\"Select the Redis logical database at index.\n\n        See: https://redis.io/commands/select\n        \"\"\"\n        return self.execute_command(\"SELECT\", index, **kwargs)\n\n    def info(\n        self, section: Optional[str] = None, *args: List[str], **kwargs\n    ) -> ResponseT:\n        \"\"\"\n        Returns a dictionary containing information about the Redis server\n\n        The ``section`` option can be used to select a specific section\n        of information\n\n        The section option is not supported by older versions of Redis Server,\n        and will generate ResponseError\n\n        For more information see https://redis.io/commands/info\n        \"\"\"\n        if section is None:\n            return self.execute_command(\"INFO\", **kwargs)\n        else:\n            return self.execute_command(\"INFO\", section, *args, **kwargs)\n\n    def lastsave(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Return a Python datetime object representing the last time the\n        Redis database was saved to disk\n\n        For more information see https://redis.io/commands/lastsave\n        \"\"\"\n        return self.execute_command(\"LASTSAVE\", **kwargs)\n\n    def latency_doctor(self):\n        \"\"\"Raise a NotImplementedError, as the client will not support LATENCY DOCTOR.\n        This funcion is best used within the redis-cli.\n\n        For more information see https://redis.io/commands/latency-doctor\n        \"\"\"\n        raise NotImplementedError(\n            \"\"\"\n            LATENCY DOCTOR is intentionally not implemented in the client.\n\n            For more information see https://redis.io/commands/latency-doctor\n            \"\"\"\n        )\n\n    def latency_graph(self):\n        \"\"\"Raise a NotImplementedError, as the client will not support LATENCY GRAPH.\n        This funcion is best used within the redis-cli.\n\n        For more information see https://redis.io/commands/latency-graph.\n        \"\"\"\n        raise NotImplementedError(\n            \"\"\"\n            LATENCY GRAPH is intentionally not implemented in the client.\n\n            For more information see https://redis.io/commands/latency-graph\n            \"\"\"\n        )\n\n    def lolwut(self, *version_numbers: Union[str, float], **kwargs) -> ResponseT:\n        \"\"\"\n        Get the Redis version and a piece of generative computer art\n\n        See: https://redis.io/commands/lolwut\n        \"\"\"\n        if version_numbers:\n            return self.execute_command(\"LOLWUT VERSION\", *version_numbers, **kwargs)\n        else:\n            return self.execute_command(\"LOLWUT\", **kwargs)\n\n    def reset(self) -> ResponseT:\n        \"\"\"Perform a full reset on the connection's server side contenxt.\n\n        See: https://redis.io/commands/reset\n        \"\"\"\n        return self.execute_command(\"RESET\")\n\n    def migrate(\n        self,\n        host: str,\n        port: int,\n        keys: KeysT,\n        destination_db: int,\n        timeout: int,\n        copy: bool = False,\n        replace: bool = False,\n        auth: Optional[str] = None,\n        **kwargs,\n    ) -> ResponseT:\n        \"\"\"\n        Migrate 1 or more keys from the current Redis server to a different\n        server specified by the ``host``, ``port`` and ``destination_db``.\n\n        The ``timeout``, specified in milliseconds, indicates the maximum\n        time the connection between the two servers can be idle before the\n        command is interrupted.\n\n        If ``copy`` is True, the specified ``keys`` are NOT deleted from\n        the source server.\n\n        If ``replace`` is True, this operation will overwrite the keys\n        on the destination server if they exist.\n\n        If ``auth`` is specified, authenticate to the destination server with\n        the password provided.\n\n        For more information see https://redis.io/commands/migrate\n        \"\"\"\n        keys = list_or_args(keys, [])\n        if not keys:\n            raise DataError(\"MIGRATE requires at least one key\")\n        pieces = []\n        if copy:\n            pieces.append(b\"COPY\")\n        if replace:\n            pieces.append(b\"REPLACE\")\n        if auth:\n            pieces.append(b\"AUTH\")\n            pieces.append(auth)\n        pieces.append(b\"KEYS\")\n        pieces.extend(keys)\n        return self.execute_command(\n            \"MIGRATE\", host, port, \"\", destination_db, timeout, *pieces, **kwargs\n        )\n\n    def object(self, infotype: str, key: KeyT, **kwargs) -> ResponseT:\n        \"\"\"\n        Return the encoding, idletime, or refcount about the key\n        \"\"\"\n        return self.execute_command(\n            \"OBJECT\", infotype, key, infotype=infotype, **kwargs\n        )\n\n    def memory_doctor(self, **kwargs) -> None:\n        raise NotImplementedError(\n            \"\"\"\n            MEMORY DOCTOR is intentionally not implemented in the client.\n\n            For more information see https://redis.io/commands/memory-doctor\n            \"\"\"\n        )\n\n    def memory_help(self, **kwargs) -> None:\n        raise NotImplementedError(\n            \"\"\"\n            MEMORY HELP is intentionally not implemented in the client.\n\n            For more information see https://redis.io/commands/memory-help\n            \"\"\"\n        )\n\n    def memory_stats(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Return a dictionary of memory stats\n\n        For more information see https://redis.io/commands/memory-stats\n        \"\"\"\n        return self.execute_command(\"MEMORY STATS\", **kwargs)\n\n    def memory_malloc_stats(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Return an internal statistics report from the memory allocator.\n\n        See: https://redis.io/commands/memory-malloc-stats\n        \"\"\"\n        return self.execute_command(\"MEMORY MALLOC-STATS\", **kwargs)\n\n    def memory_usage(\n        self, key: KeyT, samples: Optional[int] = None, **kwargs\n    ) -> ResponseT:\n        \"\"\"\n        Return the total memory usage for key, its value and associated\n        administrative overheads.\n\n        For nested data structures, ``samples`` is the number of elements to\n        sample. If left unspecified, the server's default is 5. Use 0 to sample\n        all elements.\n\n        For more information see https://redis.io/commands/memory-usage\n        \"\"\"\n        args = []\n        if isinstance(samples, int):\n            args.extend([b\"SAMPLES\", samples])\n        return self.execute_command(\"MEMORY USAGE\", key, *args, **kwargs)\n\n    def memory_purge(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Attempts to purge dirty pages for reclamation by allocator\n\n        For more information see https://redis.io/commands/memory-purge\n        \"\"\"\n        return self.execute_command(\"MEMORY PURGE\", **kwargs)\n\n    def latency_histogram(self, *args):\n        \"\"\"\n        This function throws a NotImplementedError since it is intentionally\n        not supported.\n        \"\"\"\n        raise NotImplementedError(\n            \"LATENCY HISTOGRAM is intentionally not implemented in the client.\"\n        )\n\n    def latency_history(self, event: str) -> ResponseT:\n        \"\"\"\n        Returns the raw data of the ``event``'s latency spikes time series.\n\n        For more information see https://redis.io/commands/latency-history\n        \"\"\"\n        return self.execute_command(\"LATENCY HISTORY\", event)\n\n    def latency_latest(self) -> ResponseT:\n        \"\"\"\n        Reports the latest latency events logged.\n\n        For more information see https://redis.io/commands/latency-latest\n        \"\"\"\n        return self.execute_command(\"LATENCY LATEST\")\n\n    def latency_reset(self, *events: str) -> ResponseT:\n        \"\"\"\n        Resets the latency spikes time series of all, or only some, events.\n\n        For more information see https://redis.io/commands/latency-reset\n        \"\"\"\n        return self.execute_command(\"LATENCY RESET\", *events)\n\n    def ping(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Ping the Redis server\n\n        For more information see https://redis.io/commands/ping\n        \"\"\"\n        return self.execute_command(\"PING\", **kwargs)\n\n    def quit(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Ask the server to close the connection.\n\n        For more information see https://redis.io/commands/quit\n        \"\"\"\n        return self.execute_command(\"QUIT\", **kwargs)\n\n    def replicaof(self, *args, **kwargs) -> ResponseT:\n        \"\"\"\n        Update the replication settings of a redis replica, on the fly.\n\n        Examples of valid arguments include:\n\n        NO ONE (set no replication)\n        host port (set to the host and port of a redis server)\n\n        For more information see  https://redis.io/commands/replicaof\n        \"\"\"\n        return self.execute_command(\"REPLICAOF\", *args, **kwargs)\n\n    def save(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Tell the Redis server to save its data to disk,\n        blocking until the save is complete\n\n        For more information see https://redis.io/commands/save\n        \"\"\"\n        return self.execute_command(\"SAVE\", **kwargs)\n\n    def shutdown(\n        self,\n        save: bool = False,\n        nosave: bool = False,\n        now: bool = False,\n        force: bool = False,\n        abort: bool = False,\n        **kwargs,\n    ) -> None:\n        \"\"\"Shutdown the Redis server.  If Redis has persistence configured,\n        data will be flushed before shutdown.\n        It is possible to specify modifiers to alter the behavior of the command:\n        ``save`` will force a DB saving operation even if no save points are configured.\n        ``nosave`` will prevent a DB saving operation even if one or more save points\n        are configured.\n        ``now`` skips waiting for lagging replicas, i.e. it bypasses the first step in\n        the shutdown sequence.\n        ``force`` ignores any errors that would normally prevent the server from exiting\n        ``abort`` cancels an ongoing shutdown and cannot be combined with other flags.\n\n        For more information see https://redis.io/commands/shutdown\n        \"\"\"\n        if save and nosave:\n            raise DataError(\"SHUTDOWN save and nosave cannot both be set\")\n        args = [\"SHUTDOWN\"]\n        if save:\n            args.append(\"SAVE\")\n        if nosave:\n            args.append(\"NOSAVE\")\n        if now:\n            args.append(\"NOW\")\n        if force:\n            args.append(\"FORCE\")\n        if abort:\n            args.append(\"ABORT\")\n        try:\n            self.execute_command(*args, **kwargs)\n        except ConnectionError:\n            # a ConnectionError here is expected\n            return\n        raise RedisError(\"SHUTDOWN seems to have failed.\")\n\n    def slaveof(\n        self, host: Optional[str] = None, port: Optional[int] = None, **kwargs\n    ) -> ResponseT:\n        \"\"\"\n        Set the server to be a replicated slave of the instance identified\n        by the ``host`` and ``port``. If called without arguments, the\n        instance is promoted to a master instead.\n\n        For more information see https://redis.io/commands/slaveof\n        \"\"\"\n        if host is None and port is None:\n            return self.execute_command(\"SLAVEOF\", b\"NO\", b\"ONE\", **kwargs)\n        return self.execute_command(\"SLAVEOF\", host, port, **kwargs)\n\n    def slowlog_get(self, num: Optional[int] = None, **kwargs) -> ResponseT:\n        \"\"\"\n        Get the entries from the slowlog. If ``num`` is specified, get the\n        most recent ``num`` items.\n\n        For more information see https://redis.io/commands/slowlog-get\n        \"\"\"\n        from redis.client import NEVER_DECODE\n\n        args = [\"SLOWLOG GET\"]\n        if num is not None:\n            args.append(num)\n        decode_responses = self.get_connection_kwargs().get(\"decode_responses\", False)\n        if decode_responses is True:\n            kwargs[NEVER_DECODE] = []\n        return self.execute_command(*args, **kwargs)\n\n    def slowlog_len(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Get the number of items in the slowlog\n\n        For more information see https://redis.io/commands/slowlog-len\n        \"\"\"\n        return self.execute_command(\"SLOWLOG LEN\", **kwargs)\n\n    def slowlog_reset(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Remove all items in the slowlog\n\n        For more information see https://redis.io/commands/slowlog-reset\n        \"\"\"\n        return self.execute_command(\"SLOWLOG RESET\", **kwargs)\n\n    def time(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns the server time as a 2-item tuple of ints:\n        (seconds since epoch, microseconds into this second).\n\n        For more information see https://redis.io/commands/time\n        \"\"\"\n        return self.execute_command(\"TIME\", **kwargs)\n\n    def wait(self, num_replicas: int, timeout: int, **kwargs) -> ResponseT:\n        \"\"\"\n        Redis synchronous replication\n        That returns the number of replicas that processed the query when\n        we finally have at least ``num_replicas``, or when the ``timeout`` was\n        reached.\n\n        For more information see https://redis.io/commands/wait\n        \"\"\"\n        return self.execute_command(\"WAIT\", num_replicas, timeout, **kwargs)\n\n    def waitaof(\n        self, num_local: int, num_replicas: int, timeout: int, **kwargs\n    ) -> ResponseT:\n        \"\"\"\n        This command blocks the current client until all previous write\n        commands by that client are acknowledged as having been fsynced\n        to the AOF of the local Redis and/or at least the specified number\n        of replicas.\n\n        For more information see https://redis.io/commands/waitaof\n        \"\"\"\n        return self.execute_command(\n            \"WAITAOF\", num_local, num_replicas, timeout, **kwargs\n        )\n\n    def hello(self):\n        \"\"\"\n        This function throws a NotImplementedError since it is intentionally\n        not supported.\n        \"\"\"\n        raise NotImplementedError(\n            \"HELLO is intentionally not implemented in the client.\"\n        )\n\n    def failover(self):\n        \"\"\"\n        This function throws a NotImplementedError since it is intentionally\n        not supported.\n        \"\"\"\n        raise NotImplementedError(\n            \"FAILOVER is intentionally not implemented in the client.\"\n        )\n\n\nclass AsyncManagementCommands(ManagementCommands):\n    async def command_info(self, **kwargs) -> None:\n        return super().command_info(**kwargs)\n\n    async def debug_segfault(self, **kwargs) -> None:\n        return super().debug_segfault(**kwargs)\n\n    async def memory_doctor(self, **kwargs) -> None:\n        return super().memory_doctor(**kwargs)\n\n    async def memory_help(self, **kwargs) -> None:\n        return super().memory_help(**kwargs)\n\n    async def shutdown(\n        self,\n        save: bool = False,\n        nosave: bool = False,\n        now: bool = False,\n        force: bool = False,\n        abort: bool = False,\n        **kwargs,\n    ) -> None:\n        \"\"\"Shutdown the Redis server.  If Redis has persistence configured,\n        data will be flushed before shutdown.  If the \"save\" option is set,\n        a data flush will be attempted even if there is no persistence\n        configured.  If the \"nosave\" option is set, no data flush will be\n        attempted.  The \"save\" and \"nosave\" options cannot both be set.\n\n        For more information see https://redis.io/commands/shutdown\n        \"\"\"\n        if save and nosave:\n            raise DataError(\"SHUTDOWN save and nosave cannot both be set\")\n        args = [\"SHUTDOWN\"]\n        if save:\n            args.append(\"SAVE\")\n        if nosave:\n            args.append(\"NOSAVE\")\n        if now:\n            args.append(\"NOW\")\n        if force:\n            args.append(\"FORCE\")\n        if abort:\n            args.append(\"ABORT\")\n        try:\n            await self.execute_command(*args, **kwargs)\n        except ConnectionError:\n            # a ConnectionError here is expected\n            return\n        raise RedisError(\"SHUTDOWN seems to have failed.\")\n\n\nclass BitFieldOperation:\n    \"\"\"\n    Command builder for BITFIELD commands.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: Union[\"redis.client.Redis\", \"redis.asyncio.client.Redis\"],\n        key: str,\n        default_overflow: Optional[str] = None,\n    ):\n        self.client = client\n        self.key = key\n        self._default_overflow = default_overflow\n        # for typing purposes, run the following in constructor and in reset()\n        self.operations: list[tuple[EncodableT, ...]] = []\n        self._last_overflow = \"WRAP\"\n        self.reset()\n\n    def reset(self):\n        \"\"\"\n        Reset the state of the instance to when it was constructed\n        \"\"\"\n        self.operations = []\n        self._last_overflow = \"WRAP\"\n        self.overflow(self._default_overflow or self._last_overflow)\n\n    def overflow(self, overflow: str):\n        \"\"\"\n        Update the overflow algorithm of successive INCRBY operations\n        :param overflow: Overflow algorithm, one of WRAP, SAT, FAIL. See the\n            Redis docs for descriptions of these algorithmsself.\n        :returns: a :py:class:`BitFieldOperation` instance.\n        \"\"\"\n        overflow = overflow.upper()\n        if overflow != self._last_overflow:\n            self._last_overflow = overflow\n            self.operations.append((\"OVERFLOW\", overflow))\n        return self\n\n    def incrby(\n        self,\n        fmt: str,\n        offset: BitfieldOffsetT,\n        increment: int,\n        overflow: Optional[str] = None,\n    ):\n        \"\"\"\n        Increment a bitfield by a given amount.\n        :param fmt: format-string for the bitfield being updated, e.g. 'u8'\n            for an unsigned 8-bit integer.\n        :param offset: offset (in number of bits). If prefixed with a\n            '#', this is an offset multiplier, e.g. given the arguments\n            fmt='u8', offset='#2', the offset will be 16.\n        :param int increment: value to increment the bitfield by.\n        :param str overflow: overflow algorithm. Defaults to WRAP, but other\n            acceptable values are SAT and FAIL. See the Redis docs for\n            descriptions of these algorithms.\n        :returns: a :py:class:`BitFieldOperation` instance.\n        \"\"\"\n        if overflow is not None:\n            self.overflow(overflow)\n\n        self.operations.append((\"INCRBY\", fmt, offset, increment))\n        return self\n\n    def get(self, fmt: str, offset: BitfieldOffsetT):\n        \"\"\"\n        Get the value of a given bitfield.\n        :param fmt: format-string for the bitfield being read, e.g. 'u8' for\n            an unsigned 8-bit integer.\n        :param offset: offset (in number of bits). If prefixed with a\n            '#', this is an offset multiplier, e.g. given the arguments\n            fmt='u8', offset='#2', the offset will be 16.\n        :returns: a :py:class:`BitFieldOperation` instance.\n        \"\"\"\n        self.operations.append((\"GET\", fmt, offset))\n        return self\n\n    def set(self, fmt: str, offset: BitfieldOffsetT, value: int):\n        \"\"\"\n        Set the value of a given bitfield.\n        :param fmt: format-string for the bitfield being read, e.g. 'u8' for\n            an unsigned 8-bit integer.\n        :param offset: offset (in number of bits). If prefixed with a\n            '#', this is an offset multiplier, e.g. given the arguments\n            fmt='u8', offset='#2', the offset will be 16.\n        :param int value: value to set at the given position.\n        :returns: a :py:class:`BitFieldOperation` instance.\n        \"\"\"\n        self.operations.append((\"SET\", fmt, offset, value))\n        return self\n\n    @property\n    def command(self):\n        cmd = [\"BITFIELD\", self.key]\n        for ops in self.operations:\n            cmd.extend(ops)\n        return cmd\n\n    def execute(self) -> ResponseT:\n        \"\"\"\n        Execute the operation(s) in a single BITFIELD command. The return value\n        is a list of values corresponding to each operation. If the client\n        used to create this instance was a pipeline, the list of values\n        will be present within the pipeline's execute.\n        \"\"\"\n        command = self.command\n        self.reset()\n        return self.client.execute_command(*command)\n\n\nclass BasicKeyCommands(CommandsProtocol):\n    \"\"\"\n    Redis basic key-based commands\n    \"\"\"\n\n    def append(self, key: KeyT, value: EncodableT) -> ResponseT:\n        \"\"\"\n        Appends the string ``value`` to the value at ``key``. If ``key``\n        doesn't already exist, create it with a value of ``value``.\n        Returns the new length of the value at ``key``.\n\n        For more information see https://redis.io/commands/append\n        \"\"\"\n        return self.execute_command(\"APPEND\", key, value)\n\n    def bitcount(\n        self,\n        key: KeyT,\n        start: Optional[int] = None,\n        end: Optional[int] = None,\n        mode: Optional[str] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Returns the count of set bits in the value of ``key``.  Optional\n        ``start`` and ``end`` parameters indicate which bytes to consider\n\n        For more information see https://redis.io/commands/bitcount\n        \"\"\"\n        params = [key]\n        if start is not None and end is not None:\n            params.append(start)\n            params.append(end)\n        elif (start is not None and end is None) or (end is not None and start is None):\n            raise DataError(\"Both start and end must be specified\")\n        if mode is not None:\n            params.append(mode)\n        return self.execute_command(\"BITCOUNT\", *params, keys=[key])\n\n    def bitfield(\n        self: Union[\"redis.client.Redis\", \"redis.asyncio.client.Redis\"],\n        key: KeyT,\n        default_overflow: Optional[str] = None,\n    ) -> BitFieldOperation:\n        \"\"\"\n        Return a BitFieldOperation instance to conveniently construct one or\n        more bitfield operations on ``key``.\n\n        For more information see https://redis.io/commands/bitfield\n        \"\"\"\n        return BitFieldOperation(self, key, default_overflow=default_overflow)\n\n    def bitfield_ro(\n        self: Union[\"redis.client.Redis\", \"redis.asyncio.client.Redis\"],\n        key: KeyT,\n        encoding: str,\n        offset: BitfieldOffsetT,\n        items: Optional[list] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Return an array of the specified bitfield values\n        where the first value is found using ``encoding`` and ``offset``\n        parameters and remaining values are result of corresponding\n        encoding/offset pairs in optional list ``items``\n        Read-only variant of the BITFIELD command.\n\n        For more information see https://redis.io/commands/bitfield_ro\n        \"\"\"\n        params = [key, \"GET\", encoding, offset]\n\n        items = items or []\n        for encoding, offset in items:\n            params.extend([\"GET\", encoding, offset])\n        return self.execute_command(\"BITFIELD_RO\", *params, keys=[key])\n\n    def bitop(self, operation: str, dest: KeyT, *keys: KeyT) -> ResponseT:\n        \"\"\"\n        Perform a bitwise operation using ``operation`` between ``keys`` and\n        store the result in ``dest``.\n\n        For more information see https://redis.io/commands/bitop\n        \"\"\"\n        return self.execute_command(\"BITOP\", operation, dest, *keys)\n\n    def bitpos(\n        self,\n        key: KeyT,\n        bit: int,\n        start: Optional[int] = None,\n        end: Optional[int] = None,\n        mode: Optional[str] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Return the position of the first bit set to 1 or 0 in a string.\n        ``start`` and ``end`` defines search range. The range is interpreted\n        as a range of bytes and not a range of bits, so start=0 and end=2\n        means to look at the first three bytes.\n\n        For more information see https://redis.io/commands/bitpos\n        \"\"\"\n        if bit not in (0, 1):\n            raise DataError(\"bit must be 0 or 1\")\n        params = [key, bit]\n\n        start is not None and params.append(start)\n\n        if start is not None and end is not None:\n            params.append(end)\n        elif start is None and end is not None:\n            raise DataError(\"start argument is not set, when end is specified\")\n\n        if mode is not None:\n            params.append(mode)\n        return self.execute_command(\"BITPOS\", *params, keys=[key])\n\n    def copy(\n        self,\n        source: str,\n        destination: str,\n        destination_db: Optional[str] = None,\n        replace: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Copy the value stored in the ``source`` key to the ``destination`` key.\n\n        ``destination_db`` an alternative destination database. By default,\n        the ``destination`` key is created in the source Redis database.\n\n        ``replace`` whether the ``destination`` key should be removed before\n        copying the value to it. By default, the value is not copied if\n        the ``destination`` key already exists.\n\n        For more information see https://redis.io/commands/copy\n        \"\"\"\n        params = [source, destination]\n        if destination_db is not None:\n            params.extend([\"DB\", destination_db])\n        if replace:\n            params.append(\"REPLACE\")\n        return self.execute_command(\"COPY\", *params)\n\n    def decrby(self, name: KeyT, amount: int = 1) -> ResponseT:\n        \"\"\"\n        Decrements the value of ``key`` by ``amount``.  If no key exists,\n        the value will be initialized as 0 - ``amount``\n\n        For more information see https://redis.io/commands/decrby\n        \"\"\"\n        return self.execute_command(\"DECRBY\", name, amount)\n\n    decr = decrby\n\n    def delete(self, *names: KeyT) -> ResponseT:\n        \"\"\"\n        Delete one or more keys specified by ``names``\n        \"\"\"\n        return self.execute_command(\"DEL\", *names)\n\n    def __delitem__(self, name: KeyT):\n        self.delete(name)\n\n    def dump(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Return a serialized version of the value stored at the specified key.\n        If key does not exist a nil bulk reply is returned.\n\n        For more information see https://redis.io/commands/dump\n        \"\"\"\n        from redis.client import NEVER_DECODE\n\n        options = {}\n        options[NEVER_DECODE] = []\n        return self.execute_command(\"DUMP\", name, **options)\n\n    def exists(self, *names: KeyT) -> ResponseT:\n        \"\"\"\n        Returns the number of ``names`` that exist\n\n        For more information see https://redis.io/commands/exists\n        \"\"\"\n        return self.execute_command(\"EXISTS\", *names, keys=names)\n\n    __contains__ = exists\n\n    def expire(\n        self,\n        name: KeyT,\n        time: ExpiryT,\n        nx: bool = False,\n        xx: bool = False,\n        gt: bool = False,\n        lt: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Set an expire flag on key ``name`` for ``time`` seconds with given\n        ``option``. ``time`` can be represented by an integer or a Python timedelta\n        object.\n\n        Valid options are:\n            NX -> Set expiry only when the key has no expiry\n            XX -> Set expiry only when the key has an existing expiry\n            GT -> Set expiry only when the new expiry is greater than current one\n            LT -> Set expiry only when the new expiry is less than current one\n\n        For more information see https://redis.io/commands/expire\n        \"\"\"\n        if isinstance(time, datetime.timedelta):\n            time = int(time.total_seconds())\n\n        exp_option = list()\n        if nx:\n            exp_option.append(\"NX\")\n        if xx:\n            exp_option.append(\"XX\")\n        if gt:\n            exp_option.append(\"GT\")\n        if lt:\n            exp_option.append(\"LT\")\n\n        return self.execute_command(\"EXPIRE\", name, time, *exp_option)\n\n    def expireat(\n        self,\n        name: KeyT,\n        when: AbsExpiryT,\n        nx: bool = False,\n        xx: bool = False,\n        gt: bool = False,\n        lt: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Set an expire flag on key ``name`` with given ``option``. ``when``\n        can be represented as an integer indicating unix time or a Python\n        datetime object.\n\n        Valid options are:\n            -> NX -- Set expiry only when the key has no expiry\n            -> XX -- Set expiry only when the key has an existing expiry\n            -> GT -- Set expiry only when the new expiry is greater than current one\n            -> LT -- Set expiry only when the new expiry is less than current one\n\n        For more information see https://redis.io/commands/expireat\n        \"\"\"\n        if isinstance(when, datetime.datetime):\n            when = int(when.timestamp())\n\n        exp_option = list()\n        if nx:\n            exp_option.append(\"NX\")\n        if xx:\n            exp_option.append(\"XX\")\n        if gt:\n            exp_option.append(\"GT\")\n        if lt:\n            exp_option.append(\"LT\")\n\n        return self.execute_command(\"EXPIREAT\", name, when, *exp_option)\n\n    def expiretime(self, key: str) -> int:\n        \"\"\"\n        Returns the absolute Unix timestamp (since January 1, 1970) in seconds\n        at which the given key will expire.\n\n        For more information see https://redis.io/commands/expiretime\n        \"\"\"\n        return self.execute_command(\"EXPIRETIME\", key)\n\n    def get(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Return the value at key ``name``, or None if the key doesn't exist\n\n        For more information see https://redis.io/commands/get\n        \"\"\"\n        return self.execute_command(\"GET\", name, keys=[name])\n\n    def getdel(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Get the value at key ``name`` and delete the key. This command\n        is similar to GET, except for the fact that it also deletes\n        the key on success (if and only if the key's value type\n        is a string).\n\n        For more information see https://redis.io/commands/getdel\n        \"\"\"\n        return self.execute_command(\"GETDEL\", name)\n\n    def getex(\n        self,\n        name: KeyT,\n        ex: Optional[ExpiryT] = None,\n        px: Optional[ExpiryT] = None,\n        exat: Optional[AbsExpiryT] = None,\n        pxat: Optional[AbsExpiryT] = None,\n        persist: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Get the value of key and optionally set its expiration.\n        GETEX is similar to GET, but is a write command with\n        additional options. All time parameters can be given as\n        datetime.timedelta or integers.\n\n        ``ex`` sets an expire flag on key ``name`` for ``ex`` seconds.\n\n        ``px`` sets an expire flag on key ``name`` for ``px`` milliseconds.\n\n        ``exat`` sets an expire flag on key ``name`` for ``ex`` seconds,\n        specified in unix time.\n\n        ``pxat`` sets an expire flag on key ``name`` for ``ex`` milliseconds,\n        specified in unix time.\n\n        ``persist`` remove the time to live associated with ``name``.\n\n        For more information see https://redis.io/commands/getex\n        \"\"\"\n        opset = {ex, px, exat, pxat}\n        if len(opset) > 2 or len(opset) > 1 and persist:\n            raise DataError(\n                \"``ex``, ``px``, ``exat``, ``pxat``, \"\n                \"and ``persist`` are mutually exclusive.\"\n            )\n\n        exp_options: list[EncodableT] = extract_expire_flags(ex, px, exat, pxat)\n\n        if persist:\n            exp_options.append(\"PERSIST\")\n\n        return self.execute_command(\"GETEX\", name, *exp_options)\n\n    def __getitem__(self, name: KeyT):\n        \"\"\"\n        Return the value at key ``name``, raises a KeyError if the key\n        doesn't exist.\n        \"\"\"\n        value = self.get(name)\n        if value is not None:\n            return value\n        raise KeyError(name)\n\n    def getbit(self, name: KeyT, offset: int) -> ResponseT:\n        \"\"\"\n        Returns an integer indicating the value of ``offset`` in ``name``\n\n        For more information see https://redis.io/commands/getbit\n        \"\"\"\n        return self.execute_command(\"GETBIT\", name, offset, keys=[name])\n\n    def getrange(self, key: KeyT, start: int, end: int) -> ResponseT:\n        \"\"\"\n        Returns the substring of the string value stored at ``key``,\n        determined by the offsets ``start`` and ``end`` (both are inclusive)\n\n        For more information see https://redis.io/commands/getrange\n        \"\"\"\n        return self.execute_command(\"GETRANGE\", key, start, end, keys=[key])\n\n    def getset(self, name: KeyT, value: EncodableT) -> ResponseT:\n        \"\"\"\n        Sets the value at key ``name`` to ``value``\n        and returns the old value at key ``name`` atomically.\n\n        As per Redis 6.2, GETSET is considered deprecated.\n        Please use SET with GET parameter in new code.\n\n        For more information see https://redis.io/commands/getset\n        \"\"\"\n        return self.execute_command(\"GETSET\", name, value)\n\n    def incrby(self, name: KeyT, amount: int = 1) -> ResponseT:\n        \"\"\"\n        Increments the value of ``key`` by ``amount``.  If no key exists,\n        the value will be initialized as ``amount``\n\n        For more information see https://redis.io/commands/incrby\n        \"\"\"\n        return self.execute_command(\"INCRBY\", name, amount)\n\n    incr = incrby\n\n    def incrbyfloat(self, name: KeyT, amount: float = 1.0) -> ResponseT:\n        \"\"\"\n        Increments the value at key ``name`` by floating ``amount``.\n        If no key exists, the value will be initialized as ``amount``\n\n        For more information see https://redis.io/commands/incrbyfloat\n        \"\"\"\n        return self.execute_command(\"INCRBYFLOAT\", name, amount)\n\n    def keys(self, pattern: PatternT = \"*\", **kwargs) -> ResponseT:\n        \"\"\"\n        Returns a list of keys matching ``pattern``\n\n        For more information see https://redis.io/commands/keys\n        \"\"\"\n        return self.execute_command(\"KEYS\", pattern, **kwargs)\n\n    def lmove(\n        self, first_list: str, second_list: str, src: str = \"LEFT\", dest: str = \"RIGHT\"\n    ) -> ResponseT:\n        \"\"\"\n        Atomically returns and removes the first/last element of a list,\n        pushing it as the first/last element on the destination list.\n        Returns the element being popped and pushed.\n\n        For more information see https://redis.io/commands/lmove\n        \"\"\"\n        params = [first_list, second_list, src, dest]\n        return self.execute_command(\"LMOVE\", *params)\n\n    def blmove(\n        self,\n        first_list: str,\n        second_list: str,\n        timeout: int,\n        src: str = \"LEFT\",\n        dest: str = \"RIGHT\",\n    ) -> ResponseT:\n        \"\"\"\n        Blocking version of lmove.\n\n        For more information see https://redis.io/commands/blmove\n        \"\"\"\n        params = [first_list, second_list, src, dest, timeout]\n        return self.execute_command(\"BLMOVE\", *params)\n\n    def mget(self, keys: KeysT, *args: EncodableT) -> ResponseT:\n        \"\"\"\n        Returns a list of values ordered identically to ``keys``\n\n        For more information see https://redis.io/commands/mget\n        \"\"\"\n        from redis.client import EMPTY_RESPONSE\n\n        args = list_or_args(keys, args)\n        options = {}\n        if not args:\n            options[EMPTY_RESPONSE] = []\n        options[\"keys\"] = args\n        return self.execute_command(\"MGET\", *args, **options)\n\n    def mset(self, mapping: Mapping[AnyKeyT, EncodableT]) -> ResponseT:\n        \"\"\"\n        Sets key/values based on a mapping. Mapping is a dictionary of\n        key/value pairs. Both keys and values should be strings or types that\n        can be cast to a string via str().\n\n        For more information see https://redis.io/commands/mset\n        \"\"\"\n        items = []\n        for pair in mapping.items():\n            items.extend(pair)\n        return self.execute_command(\"MSET\", *items)\n\n    def msetnx(self, mapping: Mapping[AnyKeyT, EncodableT]) -> ResponseT:\n        \"\"\"\n        Sets key/values based on a mapping if none of the keys are already set.\n        Mapping is a dictionary of key/value pairs. Both keys and values\n        should be strings or types that can be cast to a string via str().\n        Returns a boolean indicating if the operation was successful.\n\n        For more information see https://redis.io/commands/msetnx\n        \"\"\"\n        items = []\n        for pair in mapping.items():\n            items.extend(pair)\n        return self.execute_command(\"MSETNX\", *items)\n\n    def move(self, name: KeyT, db: int) -> ResponseT:\n        \"\"\"\n        Moves the key ``name`` to a different Redis database ``db``\n\n        For more information see https://redis.io/commands/move\n        \"\"\"\n        return self.execute_command(\"MOVE\", name, db)\n\n    def persist(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Removes an expiration on ``name``\n\n        For more information see https://redis.io/commands/persist\n        \"\"\"\n        return self.execute_command(\"PERSIST\", name)\n\n    def pexpire(\n        self,\n        name: KeyT,\n        time: ExpiryT,\n        nx: bool = False,\n        xx: bool = False,\n        gt: bool = False,\n        lt: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Set an expire flag on key ``name`` for ``time`` milliseconds\n        with given ``option``. ``time`` can be represented by an\n        integer or a Python timedelta object.\n\n        Valid options are:\n            NX -> Set expiry only when the key has no expiry\n            XX -> Set expiry only when the key has an existing expiry\n            GT -> Set expiry only when the new expiry is greater than current one\n            LT -> Set expiry only when the new expiry is less than current one\n\n        For more information see https://redis.io/commands/pexpire\n        \"\"\"\n        if isinstance(time, datetime.timedelta):\n            time = int(time.total_seconds() * 1000)\n\n        exp_option = list()\n        if nx:\n            exp_option.append(\"NX\")\n        if xx:\n            exp_option.append(\"XX\")\n        if gt:\n            exp_option.append(\"GT\")\n        if lt:\n            exp_option.append(\"LT\")\n        return self.execute_command(\"PEXPIRE\", name, time, *exp_option)\n\n    def pexpireat(\n        self,\n        name: KeyT,\n        when: AbsExpiryT,\n        nx: bool = False,\n        xx: bool = False,\n        gt: bool = False,\n        lt: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Set an expire flag on key ``name`` with given ``option``. ``when``\n        can be represented as an integer representing unix time in\n        milliseconds (unix time * 1000) or a Python datetime object.\n\n        Valid options are:\n            NX -> Set expiry only when the key has no expiry\n            XX -> Set expiry only when the key has an existing expiry\n            GT -> Set expiry only when the new expiry is greater than current one\n            LT -> Set expiry only when the new expiry is less than current one\n\n        For more information see https://redis.io/commands/pexpireat\n        \"\"\"\n        if isinstance(when, datetime.datetime):\n            when = int(when.timestamp() * 1000)\n        exp_option = list()\n        if nx:\n            exp_option.append(\"NX\")\n        if xx:\n            exp_option.append(\"XX\")\n        if gt:\n            exp_option.append(\"GT\")\n        if lt:\n            exp_option.append(\"LT\")\n        return self.execute_command(\"PEXPIREAT\", name, when, *exp_option)\n\n    def pexpiretime(self, key: str) -> int:\n        \"\"\"\n        Returns the absolute Unix timestamp (since January 1, 1970) in milliseconds\n        at which the given key will expire.\n\n        For more information see https://redis.io/commands/pexpiretime\n        \"\"\"\n        return self.execute_command(\"PEXPIRETIME\", key)\n\n    def psetex(self, name: KeyT, time_ms: ExpiryT, value: EncodableT):\n        \"\"\"\n        Set the value of key ``name`` to ``value`` that expires in ``time_ms``\n        milliseconds. ``time_ms`` can be represented by an integer or a Python\n        timedelta object\n\n        For more information see https://redis.io/commands/psetex\n        \"\"\"\n        if isinstance(time_ms, datetime.timedelta):\n            time_ms = int(time_ms.total_seconds() * 1000)\n        return self.execute_command(\"PSETEX\", name, time_ms, value)\n\n    def pttl(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Returns the number of milliseconds until the key ``name`` will expire\n\n        For more information see https://redis.io/commands/pttl\n        \"\"\"\n        return self.execute_command(\"PTTL\", name)\n\n    def hrandfield(\n        self, key: str, count: Optional[int] = None, withvalues: bool = False\n    ) -> ResponseT:\n        \"\"\"\n        Return a random field from the hash value stored at key.\n\n        count: if the argument is positive, return an array of distinct fields.\n        If called with a negative count, the behavior changes and the command\n        is allowed to return the same field multiple times. In this case,\n        the number of returned fields is the absolute value of the\n        specified count.\n        withvalues: The optional WITHVALUES modifier changes the reply so it\n        includes the respective values of the randomly selected hash fields.\n\n        For more information see https://redis.io/commands/hrandfield\n        \"\"\"\n        params = []\n        if count is not None:\n            params.append(count)\n        if withvalues:\n            params.append(\"WITHVALUES\")\n\n        return self.execute_command(\"HRANDFIELD\", key, *params)\n\n    def randomkey(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns the name of a random key\n\n        For more information see https://redis.io/commands/randomkey\n        \"\"\"\n        return self.execute_command(\"RANDOMKEY\", **kwargs)\n\n    def rename(self, src: KeyT, dst: KeyT) -> ResponseT:\n        \"\"\"\n        Rename key ``src`` to ``dst``\n\n        For more information see https://redis.io/commands/rename\n        \"\"\"\n        return self.execute_command(\"RENAME\", src, dst)\n\n    def renamenx(self, src: KeyT, dst: KeyT):\n        \"\"\"\n        Rename key ``src`` to ``dst`` if ``dst`` doesn't already exist\n\n        For more information see https://redis.io/commands/renamenx\n        \"\"\"\n        return self.execute_command(\"RENAMENX\", src, dst)\n\n    def restore(\n        self,\n        name: KeyT,\n        ttl: float,\n        value: EncodableT,\n        replace: bool = False,\n        absttl: bool = False,\n        idletime: Optional[int] = None,\n        frequency: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Create a key using the provided serialized value, previously obtained\n        using DUMP.\n\n        ``replace`` allows an existing key on ``name`` to be overridden. If\n        it's not specified an error is raised on collision.\n\n        ``absttl`` if True, specified ``ttl`` should represent an absolute Unix\n        timestamp in milliseconds in which the key will expire. (Redis 5.0 or\n        greater).\n\n        ``idletime`` Used for eviction, this is the number of seconds the\n        key must be idle, prior to execution.\n\n        ``frequency`` Used for eviction, this is the frequency counter of\n        the object stored at the key, prior to execution.\n\n        For more information see https://redis.io/commands/restore\n        \"\"\"\n        params = [name, ttl, value]\n        if replace:\n            params.append(\"REPLACE\")\n        if absttl:\n            params.append(\"ABSTTL\")\n        if idletime is not None:\n            params.append(\"IDLETIME\")\n            try:\n                params.append(int(idletime))\n            except ValueError:\n                raise DataError(\"idletimemust be an integer\")\n\n        if frequency is not None:\n            params.append(\"FREQ\")\n            try:\n                params.append(int(frequency))\n            except ValueError:\n                raise DataError(\"frequency must be an integer\")\n\n        return self.execute_command(\"RESTORE\", *params)\n\n    def set(\n        self,\n        name: KeyT,\n        value: EncodableT,\n        ex: Optional[ExpiryT] = None,\n        px: Optional[ExpiryT] = None,\n        nx: bool = False,\n        xx: bool = False,\n        keepttl: bool = False,\n        get: bool = False,\n        exat: Optional[AbsExpiryT] = None,\n        pxat: Optional[AbsExpiryT] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Set the value at key ``name`` to ``value``\n\n        ``ex`` sets an expire flag on key ``name`` for ``ex`` seconds.\n\n        ``px`` sets an expire flag on key ``name`` for ``px`` milliseconds.\n\n        ``nx`` if set to True, set the value at key ``name`` to ``value`` only\n            if it does not exist.\n\n        ``xx`` if set to True, set the value at key ``name`` to ``value`` only\n            if it already exists.\n\n        ``keepttl`` if True, retain the time to live associated with the key.\n            (Available since Redis 6.0)\n\n        ``get`` if True, set the value at key ``name`` to ``value`` and return\n            the old value stored at key, or None if the key did not exist.\n            (Available since Redis 6.2)\n\n        ``exat`` sets an expire flag on key ``name`` for ``ex`` seconds,\n            specified in unix time.\n\n        ``pxat`` sets an expire flag on key ``name`` for ``ex`` milliseconds,\n            specified in unix time.\n\n        For more information see https://redis.io/commands/set\n        \"\"\"\n        opset = {ex, px, exat, pxat}\n        if len(opset) > 2 or len(opset) > 1 and keepttl:\n            raise DataError(\n                \"``ex``, ``px``, ``exat``, ``pxat``, \"\n                \"and ``keepttl`` are mutually exclusive.\"\n            )\n\n        if nx and xx:\n            raise DataError(\"``nx`` and ``xx`` are mutually exclusive.\")\n\n        pieces: list[EncodableT] = [name, value]\n        options = {}\n\n        pieces.extend(extract_expire_flags(ex, px, exat, pxat))\n\n        if keepttl:\n            pieces.append(\"KEEPTTL\")\n\n        if nx:\n            pieces.append(\"NX\")\n        if xx:\n            pieces.append(\"XX\")\n\n        if get:\n            pieces.append(\"GET\")\n            options[\"get\"] = True\n\n        return self.execute_command(\"SET\", *pieces, **options)\n\n    def __setitem__(self, name: KeyT, value: EncodableT):\n        self.set(name, value)\n\n    def setbit(self, name: KeyT, offset: int, value: int) -> ResponseT:\n        \"\"\"\n        Flag the ``offset`` in ``name`` as ``value``. Returns an integer\n        indicating the previous value of ``offset``.\n\n        For more information see https://redis.io/commands/setbit\n        \"\"\"\n        value = value and 1 or 0\n        return self.execute_command(\"SETBIT\", name, offset, value)\n\n    def setex(self, name: KeyT, time: ExpiryT, value: EncodableT) -> ResponseT:\n        \"\"\"\n        Set the value of key ``name`` to ``value`` that expires in ``time``\n        seconds. ``time`` can be represented by an integer or a Python\n        timedelta object.\n\n        For more information see https://redis.io/commands/setex\n        \"\"\"\n        if isinstance(time, datetime.timedelta):\n            time = int(time.total_seconds())\n        return self.execute_command(\"SETEX\", name, time, value)\n\n    def setnx(self, name: KeyT, value: EncodableT) -> ResponseT:\n        \"\"\"\n        Set the value of key ``name`` to ``value`` if key doesn't exist\n\n        For more information see https://redis.io/commands/setnx\n        \"\"\"\n        return self.execute_command(\"SETNX\", name, value)\n\n    def setrange(self, name: KeyT, offset: int, value: EncodableT) -> ResponseT:\n        \"\"\"\n        Overwrite bytes in the value of ``name`` starting at ``offset`` with\n        ``value``. If ``offset`` plus the length of ``value`` exceeds the\n        length of the original value, the new value will be larger than before.\n        If ``offset`` exceeds the length of the original value, null bytes\n        will be used to pad between the end of the previous value and the start\n        of what's being injected.\n\n        Returns the length of the new string.\n\n        For more information see https://redis.io/commands/setrange\n        \"\"\"\n        return self.execute_command(\"SETRANGE\", name, offset, value)\n\n    def stralgo(\n        self,\n        algo: Literal[\"LCS\"],\n        value1: KeyT,\n        value2: KeyT,\n        specific_argument: Union[Literal[\"strings\"], Literal[\"keys\"]] = \"strings\",\n        len: bool = False,\n        idx: bool = False,\n        minmatchlen: Optional[int] = None,\n        withmatchlen: bool = False,\n        **kwargs,\n    ) -> ResponseT:\n        \"\"\"\n        Implements complex algorithms that operate on strings.\n        Right now the only algorithm implemented is the LCS algorithm\n        (longest common substring). However new algorithms could be\n        implemented in the future.\n\n        ``algo`` Right now must be LCS\n        ``value1`` and ``value2`` Can be two strings or two keys\n        ``specific_argument`` Specifying if the arguments to the algorithm\n        will be keys or strings. strings is the default.\n        ``len`` Returns just the len of the match.\n        ``idx`` Returns the match positions in each string.\n        ``minmatchlen`` Restrict the list of matches to the ones of a given\n        minimal length. Can be provided only when ``idx`` set to True.\n        ``withmatchlen`` Returns the matches with the len of the match.\n        Can be provided only when ``idx`` set to True.\n\n        For more information see https://redis.io/commands/stralgo\n        \"\"\"\n        # check validity\n        supported_algo = [\"LCS\"]\n        if algo not in supported_algo:\n            supported_algos_str = \", \".join(supported_algo)\n            raise DataError(f\"The supported algorithms are: {supported_algos_str}\")\n        if specific_argument not in [\"keys\", \"strings\"]:\n            raise DataError(\"specific_argument can be only keys or strings\")\n        if len and idx:\n            raise DataError(\"len and idx cannot be provided together.\")\n\n        pieces: list[EncodableT] = [algo, specific_argument.upper(), value1, value2]\n        if len:\n            pieces.append(b\"LEN\")\n        if idx:\n            pieces.append(b\"IDX\")\n        try:\n            int(minmatchlen)\n            pieces.extend([b\"MINMATCHLEN\", minmatchlen])\n        except TypeError:\n            pass\n        if withmatchlen:\n            pieces.append(b\"WITHMATCHLEN\")\n\n        return self.execute_command(\n            \"STRALGO\",\n            *pieces,\n            len=len,\n            idx=idx,\n            minmatchlen=minmatchlen,\n            withmatchlen=withmatchlen,\n            **kwargs,\n        )\n\n    def strlen(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Return the number of bytes stored in the value of ``name``\n\n        For more information see https://redis.io/commands/strlen\n        \"\"\"\n        return self.execute_command(\"STRLEN\", name, keys=[name])\n\n    def substr(self, name: KeyT, start: int, end: int = -1) -> ResponseT:\n        \"\"\"\n        Return a substring of the string at key ``name``. ``start`` and ``end``\n        are 0-based integers specifying the portion of the string to return.\n        \"\"\"\n        return self.execute_command(\"SUBSTR\", name, start, end, keys=[name])\n\n    def touch(self, *args: KeyT) -> ResponseT:\n        \"\"\"\n        Alters the last access time of a key(s) ``*args``. A key is ignored\n        if it does not exist.\n\n        For more information see https://redis.io/commands/touch\n        \"\"\"\n        return self.execute_command(\"TOUCH\", *args)\n\n    def ttl(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Returns the number of seconds until the key ``name`` will expire\n\n        For more information see https://redis.io/commands/ttl\n        \"\"\"\n        return self.execute_command(\"TTL\", name)\n\n    def type(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Returns the type of key ``name``\n\n        For more information see https://redis.io/commands/type\n        \"\"\"\n        return self.execute_command(\"TYPE\", name, keys=[name])\n\n    def watch(self, *names: KeyT) -> None:\n        \"\"\"\n        Watches the values at keys ``names``, or None if the key doesn't exist\n\n        For more information see https://redis.io/commands/watch\n        \"\"\"\n        warnings.warn(DeprecationWarning(\"Call WATCH from a Pipeline object\"))\n\n    def unwatch(self) -> None:\n        \"\"\"\n        Unwatches all previously watched keys for a transaction\n\n        For more information see https://redis.io/commands/unwatch\n        \"\"\"\n        warnings.warn(DeprecationWarning(\"Call UNWATCH from a Pipeline object\"))\n\n    def unlink(self, *names: KeyT) -> ResponseT:\n        \"\"\"\n        Unlink one or more keys specified by ``names``\n\n        For more information see https://redis.io/commands/unlink\n        \"\"\"\n        return self.execute_command(\"UNLINK\", *names)\n\n    def lcs(\n        self,\n        key1: str,\n        key2: str,\n        len: Optional[bool] = False,\n        idx: Optional[bool] = False,\n        minmatchlen: Optional[int] = 0,\n        withmatchlen: Optional[bool] = False,\n    ) -> Union[str, int, list]:\n        \"\"\"\n        Find the longest common subsequence between ``key1`` and ``key2``.\n        If ``len`` is true the length of the match will will be returned.\n        If ``idx`` is true the match position in each strings will be returned.\n        ``minmatchlen`` restrict the list of matches to the ones of\n        the given ``minmatchlen``.\n        If ``withmatchlen`` the length of the match also will be returned.\n        For more information see https://redis.io/commands/lcs\n        \"\"\"\n        pieces = [key1, key2]\n        if len:\n            pieces.append(\"LEN\")\n        if idx:\n            pieces.append(\"IDX\")\n        if minmatchlen != 0:\n            pieces.extend([\"MINMATCHLEN\", minmatchlen])\n        if withmatchlen:\n            pieces.append(\"WITHMATCHLEN\")\n        return self.execute_command(\"LCS\", *pieces, keys=[key1, key2])\n\n\nclass AsyncBasicKeyCommands(BasicKeyCommands):\n    def __delitem__(self, name: KeyT):\n        raise TypeError(\"Async Redis client does not support class deletion\")\n\n    def __contains__(self, name: KeyT):\n        raise TypeError(\"Async Redis client does not support class inclusion\")\n\n    def __getitem__(self, name: KeyT):\n        raise TypeError(\"Async Redis client does not support class retrieval\")\n\n    def __setitem__(self, name: KeyT, value: EncodableT):\n        raise TypeError(\"Async Redis client does not support class assignment\")\n\n    async def watch(self, *names: KeyT) -> None:\n        return super().watch(*names)\n\n    async def unwatch(self) -> None:\n        return super().unwatch()\n\n\nclass ListCommands(CommandsProtocol):\n    \"\"\"\n    Redis commands for List data type.\n    see: https://redis.io/topics/data-types#lists\n    \"\"\"\n\n    def blpop(\n        self, keys: List, timeout: Optional[Number] = 0\n    ) -> Union[Awaitable[list], list]:\n        \"\"\"\n        LPOP a value off of the first non-empty list\n        named in the ``keys`` list.\n\n        If none of the lists in ``keys`` has a value to LPOP, then block\n        for ``timeout`` seconds, or until a value gets pushed on to one\n        of the lists.\n\n        If timeout is 0, then block indefinitely.\n\n        For more information see https://redis.io/commands/blpop\n        \"\"\"\n        if timeout is None:\n            timeout = 0\n        keys = list_or_args(keys, None)\n        keys.append(timeout)\n        return self.execute_command(\"BLPOP\", *keys)\n\n    def brpop(\n        self, keys: List, timeout: Optional[Number] = 0\n    ) -> Union[Awaitable[list], list]:\n        \"\"\"\n        RPOP a value off of the first non-empty list\n        named in the ``keys`` list.\n\n        If none of the lists in ``keys`` has a value to RPOP, then block\n        for ``timeout`` seconds, or until a value gets pushed on to one\n        of the lists.\n\n        If timeout is 0, then block indefinitely.\n\n        For more information see https://redis.io/commands/brpop\n        \"\"\"\n        if timeout is None:\n            timeout = 0\n        keys = list_or_args(keys, None)\n        keys.append(timeout)\n        return self.execute_command(\"BRPOP\", *keys)\n\n    def brpoplpush(\n        self, src: str, dst: str, timeout: Optional[Number] = 0\n    ) -> Union[Awaitable[Optional[str]], Optional[str]]:\n        \"\"\"\n        Pop a value off the tail of ``src``, push it on the head of ``dst``\n        and then return it.\n\n        This command blocks until a value is in ``src`` or until ``timeout``\n        seconds elapse, whichever is first. A ``timeout`` value of 0 blocks\n        forever.\n\n        For more information see https://redis.io/commands/brpoplpush\n        \"\"\"\n        if timeout is None:\n            timeout = 0\n        return self.execute_command(\"BRPOPLPUSH\", src, dst, timeout)\n\n    def blmpop(\n        self,\n        timeout: float,\n        numkeys: int,\n        *args: List[str],\n        direction: str,\n        count: Optional[int] = 1,\n    ) -> Optional[list]:\n        \"\"\"\n        Pop ``count`` values (default 1) from first non-empty in the list\n        of provided key names.\n\n        When all lists are empty this command blocks the connection until another\n        client pushes to it or until the timeout, timeout of 0 blocks indefinitely\n\n        For more information see https://redis.io/commands/blmpop\n        \"\"\"\n        args = [timeout, numkeys, *args, direction, \"COUNT\", count]\n\n        return self.execute_command(\"BLMPOP\", *args)\n\n    def lmpop(\n        self,\n        num_keys: int,\n        *args: List[str],\n        direction: str,\n        count: Optional[int] = 1,\n    ) -> Union[Awaitable[list], list]:\n        \"\"\"\n        Pop ``count`` values (default 1) first non-empty list key from the list\n        of args provided key names.\n\n        For more information see https://redis.io/commands/lmpop\n        \"\"\"\n        args = [num_keys] + list(args) + [direction]\n        if count != 1:\n            args.extend([\"COUNT\", count])\n\n        return self.execute_command(\"LMPOP\", *args)\n\n    def lindex(\n        self, name: str, index: int\n    ) -> Union[Awaitable[Optional[str]], Optional[str]]:\n        \"\"\"\n        Return the item from list ``name`` at position ``index``\n\n        Negative indexes are supported and will return an item at the\n        end of the list\n\n        For more information see https://redis.io/commands/lindex\n        \"\"\"\n        return self.execute_command(\"LINDEX\", name, index, keys=[name])\n\n    def linsert(\n        self, name: str, where: str, refvalue: str, value: str\n    ) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Insert ``value`` in list ``name`` either immediately before or after\n        [``where``] ``refvalue``\n\n        Returns the new length of the list on success or -1 if ``refvalue``\n        is not in the list.\n\n        For more information see https://redis.io/commands/linsert\n        \"\"\"\n        return self.execute_command(\"LINSERT\", name, where, refvalue, value)\n\n    def llen(self, name: str) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Return the length of the list ``name``\n\n        For more information see https://redis.io/commands/llen\n        \"\"\"\n        return self.execute_command(\"LLEN\", name, keys=[name])\n\n    def lpop(\n        self,\n        name: str,\n        count: Optional[int] = None,\n    ) -> Union[Awaitable[Union[str, List, None]], Union[str, List, None]]:\n        \"\"\"\n        Removes and returns the first elements of the list ``name``.\n\n        By default, the command pops a single element from the beginning of\n        the list. When provided with the optional ``count`` argument, the reply\n        will consist of up to count elements, depending on the list's length.\n\n        For more information see https://redis.io/commands/lpop\n        \"\"\"\n        if count is not None:\n            return self.execute_command(\"LPOP\", name, count)\n        else:\n            return self.execute_command(\"LPOP\", name)\n\n    def lpush(self, name: str, *values: FieldT) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Push ``values`` onto the head of the list ``name``\n\n        For more information see https://redis.io/commands/lpush\n        \"\"\"\n        return self.execute_command(\"LPUSH\", name, *values)\n\n    def lpushx(self, name: str, *values: FieldT) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Push ``value`` onto the head of the list ``name`` if ``name`` exists\n\n        For more information see https://redis.io/commands/lpushx\n        \"\"\"\n        return self.execute_command(\"LPUSHX\", name, *values)\n\n    def lrange(self, name: str, start: int, end: int) -> Union[Awaitable[list], list]:\n        \"\"\"\n        Return a slice of the list ``name`` between\n        position ``start`` and ``end``\n\n        ``start`` and ``end`` can be negative numbers just like\n        Python slicing notation\n\n        For more information see https://redis.io/commands/lrange\n        \"\"\"\n        return self.execute_command(\"LRANGE\", name, start, end, keys=[name])\n\n    def lrem(self, name: str, count: int, value: str) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Remove the first ``count`` occurrences of elements equal to ``value``\n        from the list stored at ``name``.\n\n        The count argument influences the operation in the following ways:\n            count > 0: Remove elements equal to value moving from head to tail.\n            count < 0: Remove elements equal to value moving from tail to head.\n            count = 0: Remove all elements equal to value.\n\n            For more information see https://redis.io/commands/lrem\n        \"\"\"\n        return self.execute_command(\"LREM\", name, count, value)\n\n    def lset(self, name: str, index: int, value: str) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Set element at ``index`` of list ``name`` to ``value``\n\n        For more information see https://redis.io/commands/lset\n        \"\"\"\n        return self.execute_command(\"LSET\", name, index, value)\n\n    def ltrim(self, name: str, start: int, end: int) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Trim the list ``name``, removing all values not within the slice\n        between ``start`` and ``end``\n\n        ``start`` and ``end`` can be negative numbers just like\n        Python slicing notation\n\n        For more information see https://redis.io/commands/ltrim\n        \"\"\"\n        return self.execute_command(\"LTRIM\", name, start, end)\n\n    def rpop(\n        self,\n        name: str,\n        count: Optional[int] = None,\n    ) -> Union[Awaitable[Union[str, List, None]], Union[str, List, None]]:\n        \"\"\"\n        Removes and returns the last elements of the list ``name``.\n\n        By default, the command pops a single element from the end of the list.\n        When provided with the optional ``count`` argument, the reply will\n        consist of up to count elements, depending on the list's length.\n\n        For more information see https://redis.io/commands/rpop\n        \"\"\"\n        if count is not None:\n            return self.execute_command(\"RPOP\", name, count)\n        else:\n            return self.execute_command(\"RPOP\", name)\n\n    def rpoplpush(self, src: str, dst: str) -> Union[Awaitable[str], str]:\n        \"\"\"\n        RPOP a value off of the ``src`` list and atomically LPUSH it\n        on to the ``dst`` list.  Returns the value.\n\n        For more information see https://redis.io/commands/rpoplpush\n        \"\"\"\n        return self.execute_command(\"RPOPLPUSH\", src, dst)\n\n    def rpush(self, name: str, *values: FieldT) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Push ``values`` onto the tail of the list ``name``\n\n        For more information see https://redis.io/commands/rpush\n        \"\"\"\n        return self.execute_command(\"RPUSH\", name, *values)\n\n    def rpushx(self, name: str, *values: str) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Push ``value`` onto the tail of the list ``name`` if ``name`` exists\n\n        For more information see https://redis.io/commands/rpushx\n        \"\"\"\n        return self.execute_command(\"RPUSHX\", name, *values)\n\n    def lpos(\n        self,\n        name: str,\n        value: str,\n        rank: Optional[int] = None,\n        count: Optional[int] = None,\n        maxlen: Optional[int] = None,\n    ) -> Union[str, List, None]:\n        \"\"\"\n        Get position of ``value`` within the list ``name``\n\n         If specified, ``rank`` indicates the \"rank\" of the first element to\n         return in case there are multiple copies of ``value`` in the list.\n         By default, LPOS returns the position of the first occurrence of\n         ``value`` in the list. When ``rank`` 2, LPOS returns the position of\n         the second ``value`` in the list. If ``rank`` is negative, LPOS\n         searches the list in reverse. For example, -1 would return the\n         position of the last occurrence of ``value`` and -2 would return the\n         position of the next to last occurrence of ``value``.\n\n         If specified, ``count`` indicates that LPOS should return a list of\n         up to ``count`` positions. A ``count`` of 2 would return a list of\n         up to 2 positions. A ``count`` of 0 returns a list of all positions\n         matching ``value``. When ``count`` is specified and but ``value``\n         does not exist in the list, an empty list is returned.\n\n         If specified, ``maxlen`` indicates the maximum number of list\n         elements to scan. A ``maxlen`` of 1000 will only return the\n         position(s) of items within the first 1000 entries in the list.\n         A ``maxlen`` of 0 (the default) will scan the entire list.\n\n         For more information see https://redis.io/commands/lpos\n        \"\"\"\n        pieces: list[EncodableT] = [name, value]\n        if rank is not None:\n            pieces.extend([\"RANK\", rank])\n\n        if count is not None:\n            pieces.extend([\"COUNT\", count])\n\n        if maxlen is not None:\n            pieces.extend([\"MAXLEN\", maxlen])\n\n        return self.execute_command(\"LPOS\", *pieces, keys=[name])\n\n    def sort(\n        self,\n        name: str,\n        start: Optional[int] = None,\n        num: Optional[int] = None,\n        by: Optional[str] = None,\n        get: Optional[List[str]] = None,\n        desc: bool = False,\n        alpha: bool = False,\n        store: Optional[str] = None,\n        groups: Optional[bool] = False,\n    ) -> Union[List, int]:\n        \"\"\"\n        Sort and return the list, set or sorted set at ``name``.\n\n        ``start`` and ``num`` allow for paging through the sorted data\n\n        ``by`` allows using an external key to weight and sort the items.\n            Use an \"*\" to indicate where in the key the item value is located\n\n        ``get`` allows for returning items from external keys rather than the\n            sorted data itself.  Use an \"*\" to indicate where in the key\n            the item value is located\n\n        ``desc`` allows for reversing the sort\n\n        ``alpha`` allows for sorting lexicographically rather than numerically\n\n        ``store`` allows for storing the result of the sort into\n            the key ``store``\n\n        ``groups`` if set to True and if ``get`` contains at least two\n            elements, sort will return a list of tuples, each containing the\n            values fetched from the arguments to ``get``.\n\n        For more information see https://redis.io/commands/sort\n        \"\"\"\n        if (start is not None and num is None) or (num is not None and start is None):\n            raise DataError(\"``start`` and ``num`` must both be specified\")\n\n        pieces: list[EncodableT] = [name]\n        if by is not None:\n            pieces.extend([b\"BY\", by])\n        if start is not None and num is not None:\n            pieces.extend([b\"LIMIT\", start, num])\n        if get is not None:\n            # If get is a string assume we want to get a single value.\n            # Otherwise assume it's an interable and we want to get multiple\n            # values. We can't just iterate blindly because strings are\n            # iterable.\n            if isinstance(get, (bytes, str)):\n                pieces.extend([b\"GET\", get])\n            else:\n                for g in get:\n                    pieces.extend([b\"GET\", g])\n        if desc:\n            pieces.append(b\"DESC\")\n        if alpha:\n            pieces.append(b\"ALPHA\")\n        if store is not None:\n            pieces.extend([b\"STORE\", store])\n        if groups:\n            if not get or isinstance(get, (bytes, str)) or len(get) < 2:\n                raise DataError(\n                    'when using \"groups\" the \"get\" argument '\n                    \"must be specified and contain at least \"\n                    \"two keys\"\n                )\n\n        options = {\"groups\": len(get) if groups else None}\n        options[\"keys\"] = [name]\n        return self.execute_command(\"SORT\", *pieces, **options)\n\n    def sort_ro(\n        self,\n        key: str,\n        start: Optional[int] = None,\n        num: Optional[int] = None,\n        by: Optional[str] = None,\n        get: Optional[List[str]] = None,\n        desc: bool = False,\n        alpha: bool = False,\n    ) -> list:\n        \"\"\"\n        Returns the elements contained in the list, set or sorted set at key.\n        (read-only variant of the SORT command)\n\n        ``start`` and ``num`` allow for paging through the sorted data\n\n        ``by`` allows using an external key to weight and sort the items.\n            Use an \"*\" to indicate where in the key the item value is located\n\n        ``get`` allows for returning items from external keys rather than the\n            sorted data itself.  Use an \"*\" to indicate where in the key\n            the item value is located\n\n        ``desc`` allows for reversing the sort\n\n        ``alpha`` allows for sorting lexicographically rather than numerically\n\n        For more information see https://redis.io/commands/sort_ro\n        \"\"\"\n        return self.sort(\n            key, start=start, num=num, by=by, get=get, desc=desc, alpha=alpha\n        )\n\n\nAsyncListCommands = ListCommands\n\n\nclass ScanCommands(CommandsProtocol):\n    \"\"\"\n    Redis SCAN commands.\n    see: https://redis.io/commands/scan\n    \"\"\"\n\n    def scan(\n        self,\n        cursor: int = 0,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n        _type: Optional[str] = None,\n        **kwargs,\n    ) -> ResponseT:\n        \"\"\"\n        Incrementally return lists of key names. Also return a cursor\n        indicating the scan position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` provides a hint to Redis about the number of keys to\n            return per batch.\n\n        ``_type`` filters the returned values by a particular Redis type.\n            Stock Redis instances allow for the following types:\n            HASH, LIST, SET, STREAM, STRING, ZSET\n            Additionally, Redis modules can expose other types as well.\n\n        For more information see https://redis.io/commands/scan\n        \"\"\"\n        pieces: list[EncodableT] = [cursor]\n        if match is not None:\n            pieces.extend([b\"MATCH\", match])\n        if count is not None:\n            pieces.extend([b\"COUNT\", count])\n        if _type is not None:\n            pieces.extend([b\"TYPE\", _type])\n        return self.execute_command(\"SCAN\", *pieces, **kwargs)\n\n    def scan_iter(\n        self,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n        _type: Optional[str] = None,\n        **kwargs,\n    ) -> Iterator:\n        \"\"\"\n        Make an iterator using the SCAN command so that the client doesn't\n        need to remember the cursor position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` provides a hint to Redis about the number of keys to\n            return per batch.\n\n        ``_type`` filters the returned values by a particular Redis type.\n            Stock Redis instances allow for the following types:\n            HASH, LIST, SET, STREAM, STRING, ZSET\n            Additionally, Redis modules can expose other types as well.\n        \"\"\"\n        cursor = \"0\"\n        while cursor != 0:\n            cursor, data = self.scan(\n                cursor=cursor, match=match, count=count, _type=_type, **kwargs\n            )\n            yield from data\n\n    def sscan(\n        self,\n        name: KeyT,\n        cursor: int = 0,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Incrementally return lists of elements in a set. Also return a cursor\n        indicating the scan position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` allows for hint the minimum number of returns\n\n        For more information see https://redis.io/commands/sscan\n        \"\"\"\n        pieces: list[EncodableT] = [name, cursor]\n        if match is not None:\n            pieces.extend([b\"MATCH\", match])\n        if count is not None:\n            pieces.extend([b\"COUNT\", count])\n        return self.execute_command(\"SSCAN\", *pieces)\n\n    def sscan_iter(\n        self,\n        name: KeyT,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n    ) -> Iterator:\n        \"\"\"\n        Make an iterator using the SSCAN command so that the client doesn't\n        need to remember the cursor position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` allows for hint the minimum number of returns\n        \"\"\"\n        cursor = \"0\"\n        while cursor != 0:\n            cursor, data = self.sscan(name, cursor=cursor, match=match, count=count)\n            yield from data\n\n    def hscan(\n        self,\n        name: KeyT,\n        cursor: int = 0,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n        no_values: Union[bool, None] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Incrementally return key/value slices in a hash. Also return a cursor\n        indicating the scan position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` allows for hint the minimum number of returns\n\n        ``no_values`` indicates to return only the keys, without values.\n\n        For more information see https://redis.io/commands/hscan\n        \"\"\"\n        pieces: list[EncodableT] = [name, cursor]\n        if match is not None:\n            pieces.extend([b\"MATCH\", match])\n        if count is not None:\n            pieces.extend([b\"COUNT\", count])\n        if no_values is not None:\n            pieces.extend([b\"NOVALUES\"])\n        return self.execute_command(\"HSCAN\", *pieces, no_values=no_values)\n\n    def hscan_iter(\n        self,\n        name: str,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n        no_values: Union[bool, None] = None,\n    ) -> Iterator:\n        \"\"\"\n        Make an iterator using the HSCAN command so that the client doesn't\n        need to remember the cursor position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` allows for hint the minimum number of returns\n\n        ``no_values`` indicates to return only the keys, without values\n        \"\"\"\n        cursor = \"0\"\n        while cursor != 0:\n            cursor, data = self.hscan(\n                name, cursor=cursor, match=match, count=count, no_values=no_values\n            )\n            if no_values:\n                yield from data\n            else:\n                yield from data.items()\n\n    def zscan(\n        self,\n        name: KeyT,\n        cursor: int = 0,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n        score_cast_func: Union[type, Callable] = float,\n    ) -> ResponseT:\n        \"\"\"\n        Incrementally return lists of elements in a sorted set. Also return a\n        cursor indicating the scan position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` allows for hint the minimum number of returns\n\n        ``score_cast_func`` a callable used to cast the score return value\n\n        For more information see https://redis.io/commands/zscan\n        \"\"\"\n        pieces = [name, cursor]\n        if match is not None:\n            pieces.extend([b\"MATCH\", match])\n        if count is not None:\n            pieces.extend([b\"COUNT\", count])\n        options = {\"score_cast_func\": score_cast_func}\n        return self.execute_command(\"ZSCAN\", *pieces, **options)\n\n    def zscan_iter(\n        self,\n        name: KeyT,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n        score_cast_func: Union[type, Callable] = float,\n    ) -> Iterator:\n        \"\"\"\n        Make an iterator using the ZSCAN command so that the client doesn't\n        need to remember the cursor position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` allows for hint the minimum number of returns\n\n        ``score_cast_func`` a callable used to cast the score return value\n        \"\"\"\n        cursor = \"0\"\n        while cursor != 0:\n            cursor, data = self.zscan(\n                name,\n                cursor=cursor,\n                match=match,\n                count=count,\n                score_cast_func=score_cast_func,\n            )\n            yield from data\n\n\nclass AsyncScanCommands(ScanCommands):\n    async def scan_iter(\n        self,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n        _type: Optional[str] = None,\n        **kwargs,\n    ) -> AsyncIterator:\n        \"\"\"\n        Make an iterator using the SCAN command so that the client doesn't\n        need to remember the cursor position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` provides a hint to Redis about the number of keys to\n            return per batch.\n\n        ``_type`` filters the returned values by a particular Redis type.\n            Stock Redis instances allow for the following types:\n            HASH, LIST, SET, STREAM, STRING, ZSET\n            Additionally, Redis modules can expose other types as well.\n        \"\"\"\n        cursor = \"0\"\n        while cursor != 0:\n            cursor, data = await self.scan(\n                cursor=cursor, match=match, count=count, _type=_type, **kwargs\n            )\n            for d in data:\n                yield d\n\n    async def sscan_iter(\n        self,\n        name: KeyT,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n    ) -> AsyncIterator:\n        \"\"\"\n        Make an iterator using the SSCAN command so that the client doesn't\n        need to remember the cursor position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` allows for hint the minimum number of returns\n        \"\"\"\n        cursor = \"0\"\n        while cursor != 0:\n            cursor, data = await self.sscan(\n                name, cursor=cursor, match=match, count=count\n            )\n            for d in data:\n                yield d\n\n    async def hscan_iter(\n        self,\n        name: str,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n        no_values: Union[bool, None] = None,\n    ) -> AsyncIterator:\n        \"\"\"\n        Make an iterator using the HSCAN command so that the client doesn't\n        need to remember the cursor position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` allows for hint the minimum number of returns\n\n        ``no_values`` indicates to return only the keys, without values\n        \"\"\"\n        cursor = \"0\"\n        while cursor != 0:\n            cursor, data = await self.hscan(\n                name, cursor=cursor, match=match, count=count, no_values=no_values\n            )\n            if no_values:\n                for it in data:\n                    yield it\n            else:\n                for it in data.items():\n                    yield it\n\n    async def zscan_iter(\n        self,\n        name: KeyT,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n        score_cast_func: Union[type, Callable] = float,\n    ) -> AsyncIterator:\n        \"\"\"\n        Make an iterator using the ZSCAN command so that the client doesn't\n        need to remember the cursor position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` allows for hint the minimum number of returns\n\n        ``score_cast_func`` a callable used to cast the score return value\n        \"\"\"\n        cursor = \"0\"\n        while cursor != 0:\n            cursor, data = await self.zscan(\n                name,\n                cursor=cursor,\n                match=match,\n                count=count,\n                score_cast_func=score_cast_func,\n            )\n            for d in data:\n                yield d\n\n\nclass SetCommands(CommandsProtocol):\n    \"\"\"\n    Redis commands for Set data type.\n    see: https://redis.io/topics/data-types#sets\n    \"\"\"\n\n    def sadd(self, name: KeyT, *values: FieldT) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Add ``value(s)`` to set ``name``\n\n        For more information see https://redis.io/commands/sadd\n        \"\"\"\n        return self.execute_command(\"SADD\", name, *values)\n\n    def scard(self, name: KeyT) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Return the number of elements in set ``name``\n\n        For more information see https://redis.io/commands/scard\n        \"\"\"\n        return self.execute_command(\"SCARD\", name, keys=[name])\n\n    def sdiff(self, keys: List, *args: List) -> Union[Awaitable[list], list]:\n        \"\"\"\n        Return the difference of sets specified by ``keys``\n\n        For more information see https://redis.io/commands/sdiff\n        \"\"\"\n        args = list_or_args(keys, args)\n        return self.execute_command(\"SDIFF\", *args, keys=args)\n\n    def sdiffstore(\n        self, dest: str, keys: List, *args: List\n    ) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Store the difference of sets specified by ``keys`` into a new\n        set named ``dest``.  Returns the number of keys in the new set.\n\n        For more information see https://redis.io/commands/sdiffstore\n        \"\"\"\n        args = list_or_args(keys, args)\n        return self.execute_command(\"SDIFFSTORE\", dest, *args)\n\n    def sinter(self, keys: List, *args: List) -> Union[Awaitable[list], list]:\n        \"\"\"\n        Return the intersection of sets specified by ``keys``\n\n        For more information see https://redis.io/commands/sinter\n        \"\"\"\n        args = list_or_args(keys, args)\n        return self.execute_command(\"SINTER\", *args, keys=args)\n\n    def sintercard(\n        self, numkeys: int, keys: List[KeyT], limit: int = 0\n    ) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Return the cardinality of the intersect of multiple sets specified by ``keys``.\n\n        When LIMIT provided (defaults to 0 and means unlimited), if the intersection\n        cardinality reaches limit partway through the computation, the algorithm will\n        exit and yield limit as the cardinality\n\n        For more information see https://redis.io/commands/sintercard\n        \"\"\"\n        args = [numkeys, *keys, \"LIMIT\", limit]\n        return self.execute_command(\"SINTERCARD\", *args, keys=keys)\n\n    def sinterstore(\n        self, dest: KeyT, keys: List, *args: List\n    ) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Store the intersection of sets specified by ``keys`` into a new\n        set named ``dest``.  Returns the number of keys in the new set.\n\n        For more information see https://redis.io/commands/sinterstore\n        \"\"\"\n        args = list_or_args(keys, args)\n        return self.execute_command(\"SINTERSTORE\", dest, *args)\n\n    def sismember(\n        self, name: KeyT, value: str\n    ) -> Union[Awaitable[Union[Literal[0], Literal[1]]], Union[Literal[0], Literal[1]]]:\n        \"\"\"\n        Return whether ``value`` is a member of set ``name``:\n        - 1 if the value is a member of the set.\n        - 0 if the value is not a member of the set or if key does not exist.\n\n        For more information see https://redis.io/commands/sismember\n        \"\"\"\n        return self.execute_command(\"SISMEMBER\", name, value, keys=[name])\n\n    def smembers(self, name: KeyT) -> Union[Awaitable[Set], Set]:\n        \"\"\"\n        Return all members of the set ``name``\n\n        For more information see https://redis.io/commands/smembers\n        \"\"\"\n        return self.execute_command(\"SMEMBERS\", name, keys=[name])\n\n    def smismember(\n        self, name: KeyT, values: List, *args: List\n    ) -> Union[\n        Awaitable[List[Union[Literal[0], Literal[1]]]],\n        List[Union[Literal[0], Literal[1]]],\n    ]:\n        \"\"\"\n        Return whether each value in ``values`` is a member of the set ``name``\n        as a list of ``int`` in the order of ``values``:\n        - 1 if the value is a member of the set.\n        - 0 if the value is not a member of the set or if key does not exist.\n\n        For more information see https://redis.io/commands/smismember\n        \"\"\"\n        args = list_or_args(values, args)\n        return self.execute_command(\"SMISMEMBER\", name, *args, keys=[name])\n\n    def smove(self, src: KeyT, dst: KeyT, value: str) -> Union[Awaitable[bool], bool]:\n        \"\"\"\n        Move ``value`` from set ``src`` to set ``dst`` atomically\n\n        For more information see https://redis.io/commands/smove\n        \"\"\"\n        return self.execute_command(\"SMOVE\", src, dst, value)\n\n    def spop(self, name: KeyT, count: Optional[int] = None) -> Union[str, List, None]:\n        \"\"\"\n        Remove and return a random member of set ``name``\n\n        For more information see https://redis.io/commands/spop\n        \"\"\"\n        args = (count is not None) and [count] or []\n        return self.execute_command(\"SPOP\", name, *args)\n\n    def srandmember(\n        self, name: KeyT, number: Optional[int] = None\n    ) -> Union[str, List, None]:\n        \"\"\"\n        If ``number`` is None, returns a random member of set ``name``.\n\n        If ``number`` is supplied, returns a list of ``number`` random\n        members of set ``name``. Note this is only available when running\n        Redis 2.6+.\n\n        For more information see https://redis.io/commands/srandmember\n        \"\"\"\n        args = (number is not None) and [number] or []\n        return self.execute_command(\"SRANDMEMBER\", name, *args)\n\n    def srem(self, name: KeyT, *values: FieldT) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Remove ``values`` from set ``name``\n\n        For more information see https://redis.io/commands/srem\n        \"\"\"\n        return self.execute_command(\"SREM\", name, *values)\n\n    def sunion(self, keys: List, *args: List) -> Union[Awaitable[List], List]:\n        \"\"\"\n        Return the union of sets specified by ``keys``\n\n        For more information see https://redis.io/commands/sunion\n        \"\"\"\n        args = list_or_args(keys, args)\n        return self.execute_command(\"SUNION\", *args, keys=args)\n\n    def sunionstore(\n        self, dest: KeyT, keys: List, *args: List\n    ) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Store the union of sets specified by ``keys`` into a new\n        set named ``dest``.  Returns the number of keys in the new set.\n\n        For more information see https://redis.io/commands/sunionstore\n        \"\"\"\n        args = list_or_args(keys, args)\n        return self.execute_command(\"SUNIONSTORE\", dest, *args)\n\n\nAsyncSetCommands = SetCommands\n\n\nclass StreamCommands(CommandsProtocol):\n    \"\"\"\n    Redis commands for Stream data type.\n    see: https://redis.io/topics/streams-intro\n    \"\"\"\n\n    def xack(self, name: KeyT, groupname: GroupT, *ids: StreamIdT) -> ResponseT:\n        \"\"\"\n        Acknowledges the successful processing of one or more messages.\n\n        Args:\n            name: name of the stream.\n            groupname: name of the consumer group.\n            *ids: message ids to acknowledge.\n\n        For more information see https://redis.io/commands/xack\n        \"\"\"\n        return self.execute_command(\"XACK\", name, groupname, *ids)\n\n    def xackdel(\n        self,\n        name: KeyT,\n        groupname: GroupT,\n        *ids: StreamIdT,\n        ref_policy: Literal[\"KEEPREF\", \"DELREF\", \"ACKED\"] = \"KEEPREF\",\n    ) -> ResponseT:\n        \"\"\"\n        Combines the functionality of XACK and XDEL. Acknowledges the specified\n        message IDs in the given consumer group and simultaneously attempts to\n        delete the corresponding entries from the stream.\n        \"\"\"\n        if not ids:\n            raise DataError(\"XACKDEL requires at least one message ID\")\n\n        if ref_policy not in {\"KEEPREF\", \"DELREF\", \"ACKED\"}:\n            raise DataError(\"XACKDEL ref_policy must be one of: KEEPREF, DELREF, ACKED\")\n\n        pieces = [name, groupname, ref_policy, \"IDS\", len(ids)]\n        pieces.extend(ids)\n        return self.execute_command(\"XACKDEL\", *pieces)\n\n    def xadd(\n        self,\n        name: KeyT,\n        fields: Dict[FieldT, EncodableT],\n        id: StreamIdT = \"*\",\n        maxlen: Optional[int] = None,\n        approximate: bool = True,\n        nomkstream: bool = False,\n        minid: Union[StreamIdT, None] = None,\n        limit: Optional[int] = None,\n        ref_policy: Optional[Literal[\"KEEPREF\", \"DELREF\", \"ACKED\"]] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Add to a stream.\n        name: name of the stream\n        fields: dict of field/value pairs to insert into the stream\n        id: Location to insert this record. By default it is appended.\n        maxlen: truncate old stream members beyond this size.\n        Can't be specified with minid.\n        approximate: actual stream length may be slightly more than maxlen\n        nomkstream: When set to true, do not make a stream\n        minid: the minimum id in the stream to query.\n        Can't be specified with maxlen.\n        limit: specifies the maximum number of entries to retrieve\n        ref_policy: optional reference policy for consumer groups when trimming:\n            - KEEPREF (default): When trimming, preserves references in consumer groups' PEL\n            - DELREF: When trimming, removes all references from consumer groups' PEL\n            - ACKED: When trimming, only removes entries acknowledged by all consumer groups\n\n        For more information see https://redis.io/commands/xadd\n        \"\"\"\n        pieces: list[EncodableT] = []\n        if maxlen is not None and minid is not None:\n            raise DataError(\"Only one of ```maxlen``` or ```minid``` may be specified\")\n\n        if ref_policy is not None and ref_policy not in {\"KEEPREF\", \"DELREF\", \"ACKED\"}:\n            raise DataError(\"XADD ref_policy must be one of: KEEPREF, DELREF, ACKED\")\n\n        if maxlen is not None:\n            if not isinstance(maxlen, int) or maxlen < 0:\n                raise DataError(\"XADD maxlen must be non-negative integer\")\n            pieces.append(b\"MAXLEN\")\n            if approximate:\n                pieces.append(b\"~\")\n            pieces.append(str(maxlen))\n        if minid is not None:\n            pieces.append(b\"MINID\")\n            if approximate:\n                pieces.append(b\"~\")\n            pieces.append(minid)\n        if limit is not None:\n            pieces.extend([b\"LIMIT\", limit])\n        if nomkstream:\n            pieces.append(b\"NOMKSTREAM\")\n        if ref_policy is not None:\n            pieces.append(ref_policy)\n        pieces.append(id)\n        if not isinstance(fields, dict) or len(fields) == 0:\n            raise DataError(\"XADD fields must be a non-empty dict\")\n        for pair in fields.items():\n            pieces.extend(pair)\n        return self.execute_command(\"XADD\", name, *pieces)\n\n    def xautoclaim(\n        self,\n        name: KeyT,\n        groupname: GroupT,\n        consumername: ConsumerT,\n        min_idle_time: int,\n        start_id: StreamIdT = \"0-0\",\n        count: Optional[int] = None,\n        justid: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Transfers ownership of pending stream entries that match the specified\n        criteria. Conceptually, equivalent to calling XPENDING and then XCLAIM,\n        but provides a more straightforward way to deal with message delivery\n        failures via SCAN-like semantics.\n        name: name of the stream.\n        groupname: name of the consumer group.\n        consumername: name of a consumer that claims the message.\n        min_idle_time: filter messages that were idle less than this amount of\n        milliseconds.\n        start_id: filter messages with equal or greater ID.\n        count: optional integer, upper limit of the number of entries that the\n        command attempts to claim. Set to 100 by default.\n        justid: optional boolean, false by default. Return just an array of IDs\n        of messages successfully claimed, without returning the actual message\n\n        For more information see https://redis.io/commands/xautoclaim\n        \"\"\"\n        try:\n            if int(min_idle_time) < 0:\n                raise DataError(\n                    \"XAUTOCLAIM min_idle_time must be a nonnegative integer\"\n                )\n        except TypeError:\n            pass\n\n        kwargs = {}\n        pieces = [name, groupname, consumername, min_idle_time, start_id]\n\n        try:\n            if int(count) < 0:\n                raise DataError(\"XPENDING count must be a integer >= 0\")\n            pieces.extend([b\"COUNT\", count])\n        except TypeError:\n            pass\n        if justid:\n            pieces.append(b\"JUSTID\")\n            kwargs[\"parse_justid\"] = True\n\n        return self.execute_command(\"XAUTOCLAIM\", *pieces, **kwargs)\n\n    def xclaim(\n        self,\n        name: KeyT,\n        groupname: GroupT,\n        consumername: ConsumerT,\n        min_idle_time: int,\n        message_ids: Union[List[StreamIdT], Tuple[StreamIdT]],\n        idle: Optional[int] = None,\n        time: Optional[int] = None,\n        retrycount: Optional[int] = None,\n        force: bool = False,\n        justid: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Changes the ownership of a pending message.\n\n        name: name of the stream.\n\n        groupname: name of the consumer group.\n\n        consumername: name of a consumer that claims the message.\n\n        min_idle_time: filter messages that were idle less than this amount of\n        milliseconds\n\n        message_ids: non-empty list or tuple of message IDs to claim\n\n        idle: optional. Set the idle time (last time it was delivered) of the\n        message in ms\n\n        time: optional integer. This is the same as idle but instead of a\n        relative amount of milliseconds, it sets the idle time to a specific\n        Unix time (in milliseconds).\n\n        retrycount: optional integer. set the retry counter to the specified\n        value. This counter is incremented every time a message is delivered\n        again.\n\n        force: optional boolean, false by default. Creates the pending message\n        entry in the PEL even if certain specified IDs are not already in the\n        PEL assigned to a different client.\n\n        justid: optional boolean, false by default. Return just an array of IDs\n        of messages successfully claimed, without returning the actual message\n\n        For more information see https://redis.io/commands/xclaim\n        \"\"\"\n        if not isinstance(min_idle_time, int) or min_idle_time < 0:\n            raise DataError(\"XCLAIM min_idle_time must be a non negative integer\")\n        if not isinstance(message_ids, (list, tuple)) or not message_ids:\n            raise DataError(\n                \"XCLAIM message_ids must be a non empty list or \"\n                \"tuple of message IDs to claim\"\n            )\n\n        kwargs = {}\n        pieces: list[EncodableT] = [name, groupname, consumername, str(min_idle_time)]\n        pieces.extend(list(message_ids))\n\n        if idle is not None:\n            if not isinstance(idle, int):\n                raise DataError(\"XCLAIM idle must be an integer\")\n            pieces.extend((b\"IDLE\", str(idle)))\n        if time is not None:\n            if not isinstance(time, int):\n                raise DataError(\"XCLAIM time must be an integer\")\n            pieces.extend((b\"TIME\", str(time)))\n        if retrycount is not None:\n            if not isinstance(retrycount, int):\n                raise DataError(\"XCLAIM retrycount must be an integer\")\n            pieces.extend((b\"RETRYCOUNT\", str(retrycount)))\n\n        if force:\n            if not isinstance(force, bool):\n                raise DataError(\"XCLAIM force must be a boolean\")\n            pieces.append(b\"FORCE\")\n        if justid:\n            if not isinstance(justid, bool):\n                raise DataError(\"XCLAIM justid must be a boolean\")\n            pieces.append(b\"JUSTID\")\n            kwargs[\"parse_justid\"] = True\n        return self.execute_command(\"XCLAIM\", *pieces, **kwargs)\n\n    def xdel(self, name: KeyT, *ids: StreamIdT) -> ResponseT:\n        \"\"\"\n        Deletes one or more messages from a stream.\n\n        Args:\n            name: name of the stream.\n            *ids: message ids to delete.\n\n        For more information see https://redis.io/commands/xdel\n        \"\"\"\n        return self.execute_command(\"XDEL\", name, *ids)\n\n    def xdelex(\n        self,\n        name: KeyT,\n        *ids: StreamIdT,\n        ref_policy: Literal[\"KEEPREF\", \"DELREF\", \"ACKED\"] = \"KEEPREF\",\n    ) -> ResponseT:\n        \"\"\"\n        Extended version of XDEL that provides more control over how message entries\n        are deleted concerning consumer groups.\n        \"\"\"\n        if not ids:\n            raise DataError(\"XDELEX requires at least one message ID\")\n\n        if ref_policy not in {\"KEEPREF\", \"DELREF\", \"ACKED\"}:\n            raise DataError(\"XDELEX ref_policy must be one of: KEEPREF, DELREF, ACKED\")\n\n        pieces = [name, ref_policy, \"IDS\", len(ids)]\n        pieces.extend(ids)\n        return self.execute_command(\"XDELEX\", *pieces)\n\n    def xgroup_create(\n        self,\n        name: KeyT,\n        groupname: GroupT,\n        id: StreamIdT = \"$\",\n        mkstream: bool = False,\n        entries_read: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Create a new consumer group associated with a stream.\n        name: name of the stream.\n        groupname: name of the consumer group.\n        id: ID of the last item in the stream to consider already delivered.\n\n        For more information see https://redis.io/commands/xgroup-create\n        \"\"\"\n        pieces: list[EncodableT] = [\"XGROUP CREATE\", name, groupname, id]\n        if mkstream:\n            pieces.append(b\"MKSTREAM\")\n        if entries_read is not None:\n            pieces.extend([\"ENTRIESREAD\", entries_read])\n\n        return self.execute_command(*pieces)\n\n    def xgroup_delconsumer(\n        self, name: KeyT, groupname: GroupT, consumername: ConsumerT\n    ) -> ResponseT:\n        \"\"\"\n        Remove a specific consumer from a consumer group.\n        Returns the number of pending messages that the consumer had before it\n        was deleted.\n        name: name of the stream.\n        groupname: name of the consumer group.\n        consumername: name of consumer to delete\n\n        For more information see https://redis.io/commands/xgroup-delconsumer\n        \"\"\"\n        return self.execute_command(\"XGROUP DELCONSUMER\", name, groupname, consumername)\n\n    def xgroup_destroy(self, name: KeyT, groupname: GroupT) -> ResponseT:\n        \"\"\"\n        Destroy a consumer group.\n        name: name of the stream.\n        groupname: name of the consumer group.\n\n        For more information see https://redis.io/commands/xgroup-destroy\n        \"\"\"\n        return self.execute_command(\"XGROUP DESTROY\", name, groupname)\n\n    def xgroup_createconsumer(\n        self, name: KeyT, groupname: GroupT, consumername: ConsumerT\n    ) -> ResponseT:\n        \"\"\"\n        Consumers in a consumer group are auto-created every time a new\n        consumer name is mentioned by some command.\n        They can be explicitly created by using this command.\n        name: name of the stream.\n        groupname: name of the consumer group.\n        consumername: name of consumer to create.\n\n        See: https://redis.io/commands/xgroup-createconsumer\n        \"\"\"\n        return self.execute_command(\n            \"XGROUP CREATECONSUMER\", name, groupname, consumername\n        )\n\n    def xgroup_setid(\n        self,\n        name: KeyT,\n        groupname: GroupT,\n        id: StreamIdT,\n        entries_read: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Set the consumer group last delivered ID to something else.\n        name: name of the stream.\n        groupname: name of the consumer group.\n        id: ID of the last item in the stream to consider already delivered.\n\n        For more information see https://redis.io/commands/xgroup-setid\n        \"\"\"\n        pieces = [name, groupname, id]\n        if entries_read is not None:\n            pieces.extend([\"ENTRIESREAD\", entries_read])\n        return self.execute_command(\"XGROUP SETID\", *pieces)\n\n    def xinfo_consumers(self, name: KeyT, groupname: GroupT) -> ResponseT:\n        \"\"\"\n        Returns general information about the consumers in the group.\n        name: name of the stream.\n        groupname: name of the consumer group.\n\n        For more information see https://redis.io/commands/xinfo-consumers\n        \"\"\"\n        return self.execute_command(\"XINFO CONSUMERS\", name, groupname)\n\n    def xinfo_groups(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Returns general information about the consumer groups of the stream.\n        name: name of the stream.\n\n        For more information see https://redis.io/commands/xinfo-groups\n        \"\"\"\n        return self.execute_command(\"XINFO GROUPS\", name)\n\n    def xinfo_stream(self, name: KeyT, full: bool = False) -> ResponseT:\n        \"\"\"\n        Returns general information about the stream.\n        name: name of the stream.\n        full: optional boolean, false by default. Return full summary\n\n        For more information see https://redis.io/commands/xinfo-stream\n        \"\"\"\n        pieces = [name]\n        options = {}\n        if full:\n            pieces.append(b\"FULL\")\n            options = {\"full\": full}\n        return self.execute_command(\"XINFO STREAM\", *pieces, **options)\n\n    def xlen(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Returns the number of elements in a given stream.\n\n        For more information see https://redis.io/commands/xlen\n        \"\"\"\n        return self.execute_command(\"XLEN\", name, keys=[name])\n\n    def xpending(self, name: KeyT, groupname: GroupT) -> ResponseT:\n        \"\"\"\n        Returns information about pending messages of a group.\n        name: name of the stream.\n        groupname: name of the consumer group.\n\n        For more information see https://redis.io/commands/xpending\n        \"\"\"\n        return self.execute_command(\"XPENDING\", name, groupname, keys=[name])\n\n    def xpending_range(\n        self,\n        name: KeyT,\n        groupname: GroupT,\n        min: StreamIdT,\n        max: StreamIdT,\n        count: int,\n        consumername: Union[ConsumerT, None] = None,\n        idle: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Returns information about pending messages, in a range.\n\n        name: name of the stream.\n        groupname: name of the consumer group.\n        idle: available from  version 6.2. filter entries by their\n        idle-time, given in milliseconds (optional).\n        min: minimum stream ID.\n        max: maximum stream ID.\n        count: number of messages to return\n        consumername: name of a consumer to filter by (optional).\n        \"\"\"\n        if {min, max, count} == {None}:\n            if idle is not None or consumername is not None:\n                raise DataError(\n                    \"if XPENDING is provided with idle time\"\n                    \" or consumername, it must be provided\"\n                    \" with min, max and count parameters\"\n                )\n            return self.xpending(name, groupname)\n\n        pieces = [name, groupname]\n        if min is None or max is None or count is None:\n            raise DataError(\n                \"XPENDING must be provided with min, max \"\n                \"and count parameters, or none of them.\"\n            )\n        # idle\n        try:\n            if int(idle) < 0:\n                raise DataError(\"XPENDING idle must be a integer >= 0\")\n            pieces.extend([\"IDLE\", idle])\n        except TypeError:\n            pass\n        # count\n        try:\n            if int(count) < 0:\n                raise DataError(\"XPENDING count must be a integer >= 0\")\n            pieces.extend([min, max, count])\n        except TypeError:\n            pass\n        # consumername\n        if consumername:\n            pieces.append(consumername)\n\n        return self.execute_command(\"XPENDING\", *pieces, parse_detail=True)\n\n    def xrange(\n        self,\n        name: KeyT,\n        min: StreamIdT = \"-\",\n        max: StreamIdT = \"+\",\n        count: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Read stream values within an interval.\n\n        name: name of the stream.\n\n        start: first stream ID. defaults to '-',\n               meaning the earliest available.\n\n        finish: last stream ID. defaults to '+',\n                meaning the latest available.\n\n        count: if set, only return this many items, beginning with the\n               earliest available.\n\n        For more information see https://redis.io/commands/xrange\n        \"\"\"\n        pieces = [min, max]\n        if count is not None:\n            if not isinstance(count, int) or count < 1:\n                raise DataError(\"XRANGE count must be a positive integer\")\n            pieces.append(b\"COUNT\")\n            pieces.append(str(count))\n\n        return self.execute_command(\"XRANGE\", name, *pieces, keys=[name])\n\n    def xread(\n        self,\n        streams: Dict[KeyT, StreamIdT],\n        count: Optional[int] = None,\n        block: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Block and monitor multiple streams for new data.\n\n        streams: a dict of stream names to stream IDs, where\n                   IDs indicate the last ID already seen.\n\n        count: if set, only return this many items, beginning with the\n               earliest available.\n\n        block: number of milliseconds to wait, if nothing already present.\n\n        For more information see https://redis.io/commands/xread\n        \"\"\"\n        pieces = []\n        if block is not None:\n            if not isinstance(block, int) or block < 0:\n                raise DataError(\"XREAD block must be a non-negative integer\")\n            pieces.append(b\"BLOCK\")\n            pieces.append(str(block))\n        if count is not None:\n            if not isinstance(count, int) or count < 1:\n                raise DataError(\"XREAD count must be a positive integer\")\n            pieces.append(b\"COUNT\")\n            pieces.append(str(count))\n        if not isinstance(streams, dict) or len(streams) == 0:\n            raise DataError(\"XREAD streams must be a non empty dict\")\n        pieces.append(b\"STREAMS\")\n        keys, values = zip(*streams.items())\n        pieces.extend(keys)\n        pieces.extend(values)\n        return self.execute_command(\"XREAD\", *pieces, keys=keys)\n\n    def xreadgroup(\n        self,\n        groupname: str,\n        consumername: str,\n        streams: Dict[KeyT, StreamIdT],\n        count: Optional[int] = None,\n        block: Optional[int] = None,\n        noack: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Read from a stream via a consumer group.\n\n        groupname: name of the consumer group.\n\n        consumername: name of the requesting consumer.\n\n        streams: a dict of stream names to stream IDs, where\n               IDs indicate the last ID already seen.\n\n        count: if set, only return this many items, beginning with the\n               earliest available.\n\n        block: number of milliseconds to wait, if nothing already present.\n        noack: do not add messages to the PEL\n\n        For more information see https://redis.io/commands/xreadgroup\n        \"\"\"\n        pieces: list[EncodableT] = [b\"GROUP\", groupname, consumername]\n        if count is not None:\n            if not isinstance(count, int) or count < 1:\n                raise DataError(\"XREADGROUP count must be a positive integer\")\n            pieces.append(b\"COUNT\")\n            pieces.append(str(count))\n        if block is not None:\n            if not isinstance(block, int) or block < 0:\n                raise DataError(\"XREADGROUP block must be a non-negative integer\")\n            pieces.append(b\"BLOCK\")\n            pieces.append(str(block))\n        if noack:\n            pieces.append(b\"NOACK\")\n        if not isinstance(streams, dict) or len(streams) == 0:\n            raise DataError(\"XREADGROUP streams must be a non empty dict\")\n        pieces.append(b\"STREAMS\")\n        pieces.extend(streams.keys())\n        pieces.extend(streams.values())\n        return self.execute_command(\"XREADGROUP\", *pieces)\n\n    def xrevrange(\n        self,\n        name: KeyT,\n        max: StreamIdT = \"+\",\n        min: StreamIdT = \"-\",\n        count: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Read stream values within an interval, in reverse order.\n\n        name: name of the stream\n\n        start: first stream ID. defaults to '+',\n               meaning the latest available.\n\n        finish: last stream ID. defaults to '-',\n                meaning the earliest available.\n\n        count: if set, only return this many items, beginning with the\n               latest available.\n\n        For more information see https://redis.io/commands/xrevrange\n        \"\"\"\n        pieces: list[EncodableT] = [max, min]\n        if count is not None:\n            if not isinstance(count, int) or count < 1:\n                raise DataError(\"XREVRANGE count must be a positive integer\")\n            pieces.append(b\"COUNT\")\n            pieces.append(str(count))\n\n        return self.execute_command(\"XREVRANGE\", name, *pieces, keys=[name])\n\n    def xtrim(\n        self,\n        name: KeyT,\n        maxlen: Optional[int] = None,\n        approximate: bool = True,\n        minid: Union[StreamIdT, None] = None,\n        limit: Optional[int] = None,\n        ref_policy: Optional[Literal[\"KEEPREF\", \"DELREF\", \"ACKED\"]] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Trims old messages from a stream.\n        name: name of the stream.\n        maxlen: truncate old stream messages beyond this size\n        Can't be specified with minid.\n        approximate: actual stream length may be slightly more than maxlen\n        minid: the minimum id in the stream to query\n        Can't be specified with maxlen.\n        limit: specifies the maximum number of entries to retrieve\n        ref_policy: optional reference policy for consumer groups:\n            - KEEPREF (default): Trims entries but preserves references in consumer groups' PEL\n            - DELREF: Trims entries and removes all references from consumer groups' PEL\n            - ACKED: Only trims entries that were read and acknowledged by all consumer groups\n\n        For more information see https://redis.io/commands/xtrim\n        \"\"\"\n        pieces: list[EncodableT] = []\n        if maxlen is not None and minid is not None:\n            raise DataError(\"Only one of ``maxlen`` or ``minid`` may be specified\")\n\n        if maxlen is None and minid is None:\n            raise DataError(\"One of ``maxlen`` or ``minid`` must be specified\")\n\n        if ref_policy is not None and ref_policy not in {\"KEEPREF\", \"DELREF\", \"ACKED\"}:\n            raise DataError(\"XTRIM ref_policy must be one of: KEEPREF, DELREF, ACKED\")\n\n        if maxlen is not None:\n            pieces.append(b\"MAXLEN\")\n        if minid is not None:\n            pieces.append(b\"MINID\")\n        if approximate:\n            pieces.append(b\"~\")\n        if maxlen is not None:\n            pieces.append(maxlen)\n        if minid is not None:\n            pieces.append(minid)\n        if limit is not None:\n            pieces.append(b\"LIMIT\")\n            pieces.append(limit)\n        if ref_policy is not None:\n            pieces.append(ref_policy)\n\n        return self.execute_command(\"XTRIM\", name, *pieces)\n\n\nAsyncStreamCommands = StreamCommands\n\n\nclass SortedSetCommands(CommandsProtocol):\n    \"\"\"\n    Redis commands for Sorted Sets data type.\n    see: https://redis.io/topics/data-types-intro#redis-sorted-sets\n    \"\"\"\n\n    def zadd(\n        self,\n        name: KeyT,\n        mapping: Mapping[AnyKeyT, EncodableT],\n        nx: bool = False,\n        xx: bool = False,\n        ch: bool = False,\n        incr: bool = False,\n        gt: bool = False,\n        lt: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Set any number of element-name, score pairs to the key ``name``. Pairs\n        are specified as a dict of element-names keys to score values.\n\n        ``nx`` forces ZADD to only create new elements and not to update\n        scores for elements that already exist.\n\n        ``xx`` forces ZADD to only update scores of elements that already\n        exist. New elements will not be added.\n\n        ``ch`` modifies the return value to be the numbers of elements changed.\n        Changed elements include new elements that were added and elements\n        whose scores changed.\n\n        ``incr`` modifies ZADD to behave like ZINCRBY. In this mode only a\n        single element/score pair can be specified and the score is the amount\n        the existing score will be incremented by. When using this mode the\n        return value of ZADD will be the new score of the element.\n\n        ``LT`` Only update existing elements if the new score is less than\n        the current score. This flag doesn't prevent adding new elements.\n\n        ``GT`` Only update existing elements if the new score is greater than\n        the current score. This flag doesn't prevent adding new elements.\n\n        The return value of ZADD varies based on the mode specified. With no\n        options, ZADD returns the number of new elements added to the sorted\n        set.\n\n        ``NX``, ``LT``, and ``GT`` are mutually exclusive options.\n\n        See: https://redis.io/commands/ZADD\n        \"\"\"\n        if not mapping:\n            raise DataError(\"ZADD requires at least one element/score pair\")\n        if nx and xx:\n            raise DataError(\"ZADD allows either 'nx' or 'xx', not both\")\n        if gt and lt:\n            raise DataError(\"ZADD allows either 'gt' or 'lt', not both\")\n        if incr and len(mapping) != 1:\n            raise DataError(\n                \"ZADD option 'incr' only works when passing a single element/score pair\"\n            )\n        if nx and (gt or lt):\n            raise DataError(\"Only one of 'nx', 'lt', or 'gr' may be defined.\")\n\n        pieces: list[EncodableT] = []\n        options = {}\n        if nx:\n            pieces.append(b\"NX\")\n        if xx:\n            pieces.append(b\"XX\")\n        if ch:\n            pieces.append(b\"CH\")\n        if incr:\n            pieces.append(b\"INCR\")\n            options[\"as_score\"] = True\n        if gt:\n            pieces.append(b\"GT\")\n        if lt:\n            pieces.append(b\"LT\")\n        for pair in mapping.items():\n            pieces.append(pair[1])\n            pieces.append(pair[0])\n        return self.execute_command(\"ZADD\", name, *pieces, **options)\n\n    def zcard(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Return the number of elements in the sorted set ``name``\n\n        For more information see https://redis.io/commands/zcard\n        \"\"\"\n        return self.execute_command(\"ZCARD\", name, keys=[name])\n\n    def zcount(self, name: KeyT, min: ZScoreBoundT, max: ZScoreBoundT) -> ResponseT:\n        \"\"\"\n        Returns the number of elements in the sorted set at key ``name`` with\n        a score between ``min`` and ``max``.\n\n        For more information see https://redis.io/commands/zcount\n        \"\"\"\n        return self.execute_command(\"ZCOUNT\", name, min, max, keys=[name])\n\n    def zdiff(self, keys: KeysT, withscores: bool = False) -> ResponseT:\n        \"\"\"\n        Returns the difference between the first and all successive input\n        sorted sets provided in ``keys``.\n\n        For more information see https://redis.io/commands/zdiff\n        \"\"\"\n        pieces = [len(keys), *keys]\n        if withscores:\n            pieces.append(\"WITHSCORES\")\n        return self.execute_command(\"ZDIFF\", *pieces, keys=keys)\n\n    def zdiffstore(self, dest: KeyT, keys: KeysT) -> ResponseT:\n        \"\"\"\n        Computes the difference between the first and all successive input\n        sorted sets provided in ``keys`` and stores the result in ``dest``.\n\n        For more information see https://redis.io/commands/zdiffstore\n        \"\"\"\n        pieces = [len(keys), *keys]\n        return self.execute_command(\"ZDIFFSTORE\", dest, *pieces)\n\n    def zincrby(self, name: KeyT, amount: float, value: EncodableT) -> ResponseT:\n        \"\"\"\n        Increment the score of ``value`` in sorted set ``name`` by ``amount``\n\n        For more information see https://redis.io/commands/zincrby\n        \"\"\"\n        return self.execute_command(\"ZINCRBY\", name, amount, value)\n\n    def zinter(\n        self, keys: KeysT, aggregate: Optional[str] = None, withscores: bool = False\n    ) -> ResponseT:\n        \"\"\"\n        Return the intersect of multiple sorted sets specified by ``keys``.\n        With the ``aggregate`` option, it is possible to specify how the\n        results of the union are aggregated. This option defaults to SUM,\n        where the score of an element is summed across the inputs where it\n        exists. When this option is set to either MIN or MAX, the resulting\n        set will contain the minimum or maximum score of an element across\n        the inputs where it exists.\n\n        For more information see https://redis.io/commands/zinter\n        \"\"\"\n        return self._zaggregate(\"ZINTER\", None, keys, aggregate, withscores=withscores)\n\n    def zinterstore(\n        self,\n        dest: KeyT,\n        keys: Union[Sequence[KeyT], Mapping[AnyKeyT, float]],\n        aggregate: Optional[str] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Intersect multiple sorted sets specified by ``keys`` into a new\n        sorted set, ``dest``. Scores in the destination will be aggregated\n        based on the ``aggregate``. This option defaults to SUM, where the\n        score of an element is summed across the inputs where it exists.\n        When this option is set to either MIN or MAX, the resulting set will\n        contain the minimum or maximum score of an element across the inputs\n        where it exists.\n\n        For more information see https://redis.io/commands/zinterstore\n        \"\"\"\n        return self._zaggregate(\"ZINTERSTORE\", dest, keys, aggregate)\n\n    def zintercard(\n        self, numkeys: int, keys: List[str], limit: int = 0\n    ) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Return the cardinality of the intersect of multiple sorted sets\n        specified by ``keys``.\n        When LIMIT provided (defaults to 0 and means unlimited), if the intersection\n        cardinality reaches limit partway through the computation, the algorithm will\n        exit and yield limit as the cardinality\n\n        For more information see https://redis.io/commands/zintercard\n        \"\"\"\n        args = [numkeys, *keys, \"LIMIT\", limit]\n        return self.execute_command(\"ZINTERCARD\", *args, keys=keys)\n\n    def zlexcount(self, name, min, max):\n        \"\"\"\n        Return the number of items in the sorted set ``name`` between the\n        lexicographical range ``min`` and ``max``.\n\n        For more information see https://redis.io/commands/zlexcount\n        \"\"\"\n        return self.execute_command(\"ZLEXCOUNT\", name, min, max, keys=[name])\n\n    def zpopmax(self, name: KeyT, count: Optional[int] = None) -> ResponseT:\n        \"\"\"\n        Remove and return up to ``count`` members with the highest scores\n        from the sorted set ``name``.\n\n        For more information see https://redis.io/commands/zpopmax\n        \"\"\"\n        args = (count is not None) and [count] or []\n        options = {\"withscores\": True}\n        return self.execute_command(\"ZPOPMAX\", name, *args, **options)\n\n    def zpopmin(self, name: KeyT, count: Optional[int] = None) -> ResponseT:\n        \"\"\"\n        Remove and return up to ``count`` members with the lowest scores\n        from the sorted set ``name``.\n\n        For more information see https://redis.io/commands/zpopmin\n        \"\"\"\n        args = (count is not None) and [count] or []\n        options = {\"withscores\": True}\n        return self.execute_command(\"ZPOPMIN\", name, *args, **options)\n\n    def zrandmember(\n        self, key: KeyT, count: Optional[int] = None, withscores: bool = False\n    ) -> ResponseT:\n        \"\"\"\n        Return a random element from the sorted set value stored at key.\n\n        ``count`` if the argument is positive, return an array of distinct\n        fields. If called with a negative count, the behavior changes and\n        the command is allowed to return the same field multiple times.\n        In this case, the number of returned fields is the absolute value\n        of the specified count.\n\n        ``withscores`` The optional WITHSCORES modifier changes the reply so it\n        includes the respective scores of the randomly selected elements from\n        the sorted set.\n\n        For more information see https://redis.io/commands/zrandmember\n        \"\"\"\n        params = []\n        if count is not None:\n            params.append(count)\n        if withscores:\n            params.append(\"WITHSCORES\")\n\n        return self.execute_command(\"ZRANDMEMBER\", key, *params)\n\n    def bzpopmax(self, keys: KeysT, timeout: TimeoutSecT = 0) -> ResponseT:\n        \"\"\"\n        ZPOPMAX a value off of the first non-empty sorted set\n        named in the ``keys`` list.\n\n        If none of the sorted sets in ``keys`` has a value to ZPOPMAX,\n        then block for ``timeout`` seconds, or until a member gets added\n        to one of the sorted sets.\n\n        If timeout is 0, then block indefinitely.\n\n        For more information see https://redis.io/commands/bzpopmax\n        \"\"\"\n        if timeout is None:\n            timeout = 0\n        keys = list_or_args(keys, None)\n        keys.append(timeout)\n        return self.execute_command(\"BZPOPMAX\", *keys)\n\n    def bzpopmin(self, keys: KeysT, timeout: TimeoutSecT = 0) -> ResponseT:\n        \"\"\"\n        ZPOPMIN a value off of the first non-empty sorted set\n        named in the ``keys`` list.\n\n        If none of the sorted sets in ``keys`` has a value to ZPOPMIN,\n        then block for ``timeout`` seconds, or until a member gets added\n        to one of the sorted sets.\n\n        If timeout is 0, then block indefinitely.\n\n        For more information see https://redis.io/commands/bzpopmin\n        \"\"\"\n        if timeout is None:\n            timeout = 0\n        keys: list[EncodableT] = list_or_args(keys, None)\n        keys.append(timeout)\n        return self.execute_command(\"BZPOPMIN\", *keys)\n\n    def zmpop(\n        self,\n        num_keys: int,\n        keys: List[str],\n        min: Optional[bool] = False,\n        max: Optional[bool] = False,\n        count: Optional[int] = 1,\n    ) -> Union[Awaitable[list], list]:\n        \"\"\"\n        Pop ``count`` values (default 1) off of the first non-empty sorted set\n        named in the ``keys`` list.\n        For more information see https://redis.io/commands/zmpop\n        \"\"\"\n        args = [num_keys] + keys\n        if (min and max) or (not min and not max):\n            raise DataError\n        elif min:\n            args.append(\"MIN\")\n        else:\n            args.append(\"MAX\")\n        if count != 1:\n            args.extend([\"COUNT\", count])\n\n        return self.execute_command(\"ZMPOP\", *args)\n\n    def bzmpop(\n        self,\n        timeout: float,\n        numkeys: int,\n        keys: List[str],\n        min: Optional[bool] = False,\n        max: Optional[bool] = False,\n        count: Optional[int] = 1,\n    ) -> Optional[list]:\n        \"\"\"\n        Pop ``count`` values (default 1) off of the first non-empty sorted set\n        named in the ``keys`` list.\n\n        If none of the sorted sets in ``keys`` has a value to pop,\n        then block for ``timeout`` seconds, or until a member gets added\n        to one of the sorted sets.\n\n        If timeout is 0, then block indefinitely.\n\n        For more information see https://redis.io/commands/bzmpop\n        \"\"\"\n        args = [timeout, numkeys, *keys]\n        if (min and max) or (not min and not max):\n            raise DataError(\"Either min or max, but not both must be set\")\n        elif min:\n            args.append(\"MIN\")\n        else:\n            args.append(\"MAX\")\n        args.extend([\"COUNT\", count])\n\n        return self.execute_command(\"BZMPOP\", *args)\n\n    def _zrange(\n        self,\n        command,\n        dest: Union[KeyT, None],\n        name: KeyT,\n        start: int,\n        end: int,\n        desc: bool = False,\n        byscore: bool = False,\n        bylex: bool = False,\n        withscores: bool = False,\n        score_cast_func: Union[type, Callable, None] = float,\n        offset: Optional[int] = None,\n        num: Optional[int] = None,\n    ) -> ResponseT:\n        if byscore and bylex:\n            raise DataError(\"``byscore`` and ``bylex`` can not be specified together.\")\n        if (offset is not None and num is None) or (num is not None and offset is None):\n            raise DataError(\"``offset`` and ``num`` must both be specified.\")\n        if bylex and withscores:\n            raise DataError(\n                \"``withscores`` not supported in combination with ``bylex``.\"\n            )\n        pieces = [command]\n        if dest:\n            pieces.append(dest)\n        pieces.extend([name, start, end])\n        if byscore:\n            pieces.append(\"BYSCORE\")\n        if bylex:\n            pieces.append(\"BYLEX\")\n        if desc:\n            pieces.append(\"REV\")\n        if offset is not None and num is not None:\n            pieces.extend([\"LIMIT\", offset, num])\n        if withscores:\n            pieces.append(\"WITHSCORES\")\n        options = {\"withscores\": withscores, \"score_cast_func\": score_cast_func}\n        options[\"keys\"] = [name]\n        return self.execute_command(*pieces, **options)\n\n    def zrange(\n        self,\n        name: KeyT,\n        start: int,\n        end: int,\n        desc: bool = False,\n        withscores: bool = False,\n        score_cast_func: Union[type, Callable] = float,\n        byscore: bool = False,\n        bylex: bool = False,\n        offset: Optional[int] = None,\n        num: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Return a range of values from sorted set ``name`` between\n        ``start`` and ``end`` sorted in ascending order.\n\n        ``start`` and ``end`` can be negative, indicating the end of the range.\n\n        ``desc`` a boolean indicating whether to sort the results in reversed\n        order.\n\n        ``withscores`` indicates to return the scores along with the values.\n        The return type is a list of (value, score) pairs.\n\n        ``score_cast_func`` a callable used to cast the score return value.\n\n        ``byscore`` when set to True, returns the range of elements from the\n        sorted set having scores equal or between ``start`` and ``end``.\n\n        ``bylex`` when set to True, returns the range of elements from the\n        sorted set between the ``start`` and ``end`` lexicographical closed\n        range intervals.\n        Valid ``start`` and ``end`` must start with ( or [, in order to specify\n        whether the range interval is exclusive or inclusive, respectively.\n\n        ``offset`` and ``num`` are specified, then return a slice of the range.\n        Can't be provided when using ``bylex``.\n\n        For more information see https://redis.io/commands/zrange\n        \"\"\"\n        # Need to support ``desc`` also when using old redis version\n        # because it was supported in 3.5.3 (of redis-py)\n        if not byscore and not bylex and (offset is None and num is None) and desc:\n            return self.zrevrange(name, start, end, withscores, score_cast_func)\n\n        return self._zrange(\n            \"ZRANGE\",\n            None,\n            name,\n            start,\n            end,\n            desc,\n            byscore,\n            bylex,\n            withscores,\n            score_cast_func,\n            offset,\n            num,\n        )\n\n    def zrevrange(\n        self,\n        name: KeyT,\n        start: int,\n        end: int,\n        withscores: bool = False,\n        score_cast_func: Union[type, Callable] = float,\n    ) -> ResponseT:\n        \"\"\"\n        Return a range of values from sorted set ``name`` between\n        ``start`` and ``end`` sorted in descending order.\n\n        ``start`` and ``end`` can be negative, indicating the end of the range.\n\n        ``withscores`` indicates to return the scores along with the values\n        The return type is a list of (value, score) pairs\n\n        ``score_cast_func`` a callable used to cast the score return value\n\n        For more information see https://redis.io/commands/zrevrange\n        \"\"\"\n        pieces = [\"ZREVRANGE\", name, start, end]\n        if withscores:\n            pieces.append(b\"WITHSCORES\")\n        options = {\"withscores\": withscores, \"score_cast_func\": score_cast_func}\n        options[\"keys\"] = name\n        return self.execute_command(*pieces, **options)\n\n    def zrangestore(\n        self,\n        dest: KeyT,\n        name: KeyT,\n        start: int,\n        end: int,\n        byscore: bool = False,\n        bylex: bool = False,\n        desc: bool = False,\n        offset: Optional[int] = None,\n        num: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Stores in ``dest`` the result of a range of values from sorted set\n        ``name`` between ``start`` and ``end`` sorted in ascending order.\n\n        ``start`` and ``end`` can be negative, indicating the end of the range.\n\n        ``byscore`` when set to True, returns the range of elements from the\n        sorted set having scores equal or between ``start`` and ``end``.\n\n        ``bylex`` when set to True, returns the range of elements from the\n        sorted set between the ``start`` and ``end`` lexicographical closed\n        range intervals.\n        Valid ``start`` and ``end`` must start with ( or [, in order to specify\n        whether the range interval is exclusive or inclusive, respectively.\n\n        ``desc`` a boolean indicating whether to sort the results in reversed\n        order.\n\n        ``offset`` and ``num`` are specified, then return a slice of the range.\n        Can't be provided when using ``bylex``.\n\n        For more information see https://redis.io/commands/zrangestore\n        \"\"\"\n        return self._zrange(\n            \"ZRANGESTORE\",\n            dest,\n            name,\n            start,\n            end,\n            desc,\n            byscore,\n            bylex,\n            False,\n            None,\n            offset,\n            num,\n        )\n\n    def zrangebylex(\n        self,\n        name: KeyT,\n        min: EncodableT,\n        max: EncodableT,\n        start: Optional[int] = None,\n        num: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Return the lexicographical range of values from sorted set ``name``\n        between ``min`` and ``max``.\n\n        If ``start`` and ``num`` are specified, then return a slice of the\n        range.\n\n        For more information see https://redis.io/commands/zrangebylex\n        \"\"\"\n        if (start is not None and num is None) or (num is not None and start is None):\n            raise DataError(\"``start`` and ``num`` must both be specified\")\n        pieces = [\"ZRANGEBYLEX\", name, min, max]\n        if start is not None and num is not None:\n            pieces.extend([b\"LIMIT\", start, num])\n        return self.execute_command(*pieces, keys=[name])\n\n    def zrevrangebylex(\n        self,\n        name: KeyT,\n        max: EncodableT,\n        min: EncodableT,\n        start: Optional[int] = None,\n        num: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Return the reversed lexicographical range of values from sorted set\n        ``name`` between ``max`` and ``min``.\n\n        If ``start`` and ``num`` are specified, then return a slice of the\n        range.\n\n        For more information see https://redis.io/commands/zrevrangebylex\n        \"\"\"\n        if (start is not None and num is None) or (num is not None and start is None):\n            raise DataError(\"``start`` and ``num`` must both be specified\")\n        pieces = [\"ZREVRANGEBYLEX\", name, max, min]\n        if start is not None and num is not None:\n            pieces.extend([\"LIMIT\", start, num])\n        return self.execute_command(*pieces, keys=[name])\n\n    def zrangebyscore(\n        self,\n        name: KeyT,\n        min: ZScoreBoundT,\n        max: ZScoreBoundT,\n        start: Optional[int] = None,\n        num: Optional[int] = None,\n        withscores: bool = False,\n        score_cast_func: Union[type, Callable] = float,\n    ) -> ResponseT:\n        \"\"\"\n        Return a range of values from the sorted set ``name`` with scores\n        between ``min`` and ``max``.\n\n        If ``start`` and ``num`` are specified, then return a slice\n        of the range.\n\n        ``withscores`` indicates to return the scores along with the values.\n        The return type is a list of (value, score) pairs\n\n        `score_cast_func`` a callable used to cast the score return value\n\n        For more information see https://redis.io/commands/zrangebyscore\n        \"\"\"\n        if (start is not None and num is None) or (num is not None and start is None):\n            raise DataError(\"``start`` and ``num`` must both be specified\")\n        pieces = [\"ZRANGEBYSCORE\", name, min, max]\n        if start is not None and num is not None:\n            pieces.extend([\"LIMIT\", start, num])\n        if withscores:\n            pieces.append(\"WITHSCORES\")\n        options = {\"withscores\": withscores, \"score_cast_func\": score_cast_func}\n        options[\"keys\"] = [name]\n        return self.execute_command(*pieces, **options)\n\n    def zrevrangebyscore(\n        self,\n        name: KeyT,\n        max: ZScoreBoundT,\n        min: ZScoreBoundT,\n        start: Optional[int] = None,\n        num: Optional[int] = None,\n        withscores: bool = False,\n        score_cast_func: Union[type, Callable] = float,\n    ):\n        \"\"\"\n        Return a range of values from the sorted set ``name`` with scores\n        between ``min`` and ``max`` in descending order.\n\n        If ``start`` and ``num`` are specified, then return a slice\n        of the range.\n\n        ``withscores`` indicates to return the scores along with the values.\n        The return type is a list of (value, score) pairs\n\n        ``score_cast_func`` a callable used to cast the score return value\n\n        For more information see https://redis.io/commands/zrevrangebyscore\n        \"\"\"\n        if (start is not None and num is None) or (num is not None and start is None):\n            raise DataError(\"``start`` and ``num`` must both be specified\")\n        pieces = [\"ZREVRANGEBYSCORE\", name, max, min]\n        if start is not None and num is not None:\n            pieces.extend([\"LIMIT\", start, num])\n        if withscores:\n            pieces.append(\"WITHSCORES\")\n        options = {\"withscores\": withscores, \"score_cast_func\": score_cast_func}\n        options[\"keys\"] = [name]\n        return self.execute_command(*pieces, **options)\n\n    def zrank(\n        self,\n        name: KeyT,\n        value: EncodableT,\n        withscore: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Returns a 0-based value indicating the rank of ``value`` in sorted set\n        ``name``.\n        The optional WITHSCORE argument supplements the command's\n        reply with the score of the element returned.\n\n        For more information see https://redis.io/commands/zrank\n        \"\"\"\n        if withscore:\n            return self.execute_command(\"ZRANK\", name, value, \"WITHSCORE\", keys=[name])\n        return self.execute_command(\"ZRANK\", name, value, keys=[name])\n\n    def zrem(self, name: KeyT, *values: FieldT) -> ResponseT:\n        \"\"\"\n        Remove member ``values`` from sorted set ``name``\n\n        For more information see https://redis.io/commands/zrem\n        \"\"\"\n        return self.execute_command(\"ZREM\", name, *values)\n\n    def zremrangebylex(self, name: KeyT, min: EncodableT, max: EncodableT) -> ResponseT:\n        \"\"\"\n        Remove all elements in the sorted set ``name`` between the\n        lexicographical range specified by ``min`` and ``max``.\n\n        Returns the number of elements removed.\n\n        For more information see https://redis.io/commands/zremrangebylex\n        \"\"\"\n        return self.execute_command(\"ZREMRANGEBYLEX\", name, min, max)\n\n    def zremrangebyrank(self, name: KeyT, min: int, max: int) -> ResponseT:\n        \"\"\"\n        Remove all elements in the sorted set ``name`` with ranks between\n        ``min`` and ``max``. Values are 0-based, ordered from smallest score\n        to largest. Values can be negative indicating the highest scores.\n        Returns the number of elements removed\n\n        For more information see https://redis.io/commands/zremrangebyrank\n        \"\"\"\n        return self.execute_command(\"ZREMRANGEBYRANK\", name, min, max)\n\n    def zremrangebyscore(\n        self, name: KeyT, min: ZScoreBoundT, max: ZScoreBoundT\n    ) -> ResponseT:\n        \"\"\"\n        Remove all elements in the sorted set ``name`` with scores\n        between ``min`` and ``max``. Returns the number of elements removed.\n\n        For more information see https://redis.io/commands/zremrangebyscore\n        \"\"\"\n        return self.execute_command(\"ZREMRANGEBYSCORE\", name, min, max)\n\n    def zrevrank(\n        self,\n        name: KeyT,\n        value: EncodableT,\n        withscore: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Returns a 0-based value indicating the descending rank of\n        ``value`` in sorted set ``name``.\n        The optional ``withscore`` argument supplements the command's\n        reply with the score of the element returned.\n\n        For more information see https://redis.io/commands/zrevrank\n        \"\"\"\n        if withscore:\n            return self.execute_command(\n                \"ZREVRANK\", name, value, \"WITHSCORE\", keys=[name]\n            )\n        return self.execute_command(\"ZREVRANK\", name, value, keys=[name])\n\n    def zscore(self, name: KeyT, value: EncodableT) -> ResponseT:\n        \"\"\"\n        Return the score of element ``value`` in sorted set ``name``\n\n        For more information see https://redis.io/commands/zscore\n        \"\"\"\n        return self.execute_command(\"ZSCORE\", name, value, keys=[name])\n\n    def zunion(\n        self,\n        keys: Union[Sequence[KeyT], Mapping[AnyKeyT, float]],\n        aggregate: Optional[str] = None,\n        withscores: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Return the union of multiple sorted sets specified by ``keys``.\n        ``keys`` can be provided as dictionary of keys and their weights.\n        Scores will be aggregated based on the ``aggregate``, or SUM if\n        none is provided.\n\n        For more information see https://redis.io/commands/zunion\n        \"\"\"\n        return self._zaggregate(\"ZUNION\", None, keys, aggregate, withscores=withscores)\n\n    def zunionstore(\n        self,\n        dest: KeyT,\n        keys: Union[Sequence[KeyT], Mapping[AnyKeyT, float]],\n        aggregate: Optional[str] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Union multiple sorted sets specified by ``keys`` into\n        a new sorted set, ``dest``. Scores in the destination will be\n        aggregated based on the ``aggregate``, or SUM if none is provided.\n\n        For more information see https://redis.io/commands/zunionstore\n        \"\"\"\n        return self._zaggregate(\"ZUNIONSTORE\", dest, keys, aggregate)\n\n    def zmscore(self, key: KeyT, members: List[str]) -> ResponseT:\n        \"\"\"\n        Returns the scores associated with the specified members\n        in the sorted set stored at key.\n        ``members`` should be a list of the member name.\n        Return type is a list of score.\n        If the member does not exist, a None will be returned\n        in corresponding position.\n\n        For more information see https://redis.io/commands/zmscore\n        \"\"\"\n        if not members:\n            raise DataError(\"ZMSCORE members must be a non-empty list\")\n        pieces = [key] + members\n        return self.execute_command(\"ZMSCORE\", *pieces, keys=[key])\n\n    def _zaggregate(\n        self,\n        command: str,\n        dest: Union[KeyT, None],\n        keys: Union[Sequence[KeyT], Mapping[AnyKeyT, float]],\n        aggregate: Optional[str] = None,\n        **options,\n    ) -> ResponseT:\n        pieces: list[EncodableT] = [command]\n        if dest is not None:\n            pieces.append(dest)\n        pieces.append(len(keys))\n        if isinstance(keys, dict):\n            keys, weights = keys.keys(), keys.values()\n        else:\n            weights = None\n        pieces.extend(keys)\n        if weights:\n            pieces.append(b\"WEIGHTS\")\n            pieces.extend(weights)\n        if aggregate:\n            if aggregate.upper() in [\"SUM\", \"MIN\", \"MAX\"]:\n                pieces.append(b\"AGGREGATE\")\n                pieces.append(aggregate)\n            else:\n                raise DataError(\"aggregate can be sum, min or max.\")\n        if options.get(\"withscores\", False):\n            pieces.append(b\"WITHSCORES\")\n        options[\"keys\"] = keys\n        return self.execute_command(*pieces, **options)\n\n\nAsyncSortedSetCommands = SortedSetCommands\n\n\nclass HyperlogCommands(CommandsProtocol):\n    \"\"\"\n    Redis commands of HyperLogLogs data type.\n    see: https://redis.io/topics/data-types-intro#hyperloglogs\n    \"\"\"\n\n    def pfadd(self, name: KeyT, *values: FieldT) -> ResponseT:\n        \"\"\"\n        Adds the specified elements to the specified HyperLogLog.\n\n        For more information see https://redis.io/commands/pfadd\n        \"\"\"\n        return self.execute_command(\"PFADD\", name, *values)\n\n    def pfcount(self, *sources: KeyT) -> ResponseT:\n        \"\"\"\n        Return the approximated cardinality of\n        the set observed by the HyperLogLog at key(s).\n\n        For more information see https://redis.io/commands/pfcount\n        \"\"\"\n        return self.execute_command(\"PFCOUNT\", *sources)\n\n    def pfmerge(self, dest: KeyT, *sources: KeyT) -> ResponseT:\n        \"\"\"\n        Merge N different HyperLogLogs into a single one.\n\n        For more information see https://redis.io/commands/pfmerge\n        \"\"\"\n        return self.execute_command(\"PFMERGE\", dest, *sources)\n\n\nAsyncHyperlogCommands = HyperlogCommands\n\n\nclass HashDataPersistOptions(Enum):\n    # set the value for each provided key to each\n    # provided value only if all do not already exist.\n    FNX = \"FNX\"\n\n    # set the value for each provided key to each\n    # provided value only if all already exist.\n    FXX = \"FXX\"\n\n\nclass HashCommands(CommandsProtocol):\n    \"\"\"\n    Redis commands for Hash data type.\n    see: https://redis.io/topics/data-types-intro#redis-hashes\n    \"\"\"\n\n    def hdel(self, name: str, *keys: str) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Delete ``keys`` from hash ``name``\n\n        For more information see https://redis.io/commands/hdel\n        \"\"\"\n        return self.execute_command(\"HDEL\", name, *keys)\n\n    def hexists(self, name: str, key: str) -> Union[Awaitable[bool], bool]:\n        \"\"\"\n        Returns a boolean indicating if ``key`` exists within hash ``name``\n\n        For more information see https://redis.io/commands/hexists\n        \"\"\"\n        return self.execute_command(\"HEXISTS\", name, key, keys=[name])\n\n    def hget(\n        self, name: str, key: str\n    ) -> Union[Awaitable[Optional[str]], Optional[str]]:\n        \"\"\"\n        Return the value of ``key`` within the hash ``name``\n\n        For more information see https://redis.io/commands/hget\n        \"\"\"\n        return self.execute_command(\"HGET\", name, key, keys=[name])\n\n    def hgetall(self, name: str) -> Union[Awaitable[dict], dict]:\n        \"\"\"\n        Return a Python dict of the hash's name/value pairs\n\n        For more information see https://redis.io/commands/hgetall\n        \"\"\"\n        return self.execute_command(\"HGETALL\", name, keys=[name])\n\n    def hgetdel(\n        self, name: str, *keys: str\n    ) -> Union[\n        Awaitable[Optional[List[Union[str, bytes]]]], Optional[List[Union[str, bytes]]]\n    ]:\n        \"\"\"\n        Return the value of ``key`` within the hash ``name`` and\n        delete the field in the hash.\n        This command is similar to HGET, except for the fact that it also deletes\n        the key on success from the hash with the provided ```name```.\n\n        Available since Redis 8.0\n        For more information see https://redis.io/commands/hgetdel\n        \"\"\"\n        if len(keys) == 0:\n            raise DataError(\"'hgetdel' should have at least one key provided\")\n\n        return self.execute_command(\"HGETDEL\", name, \"FIELDS\", len(keys), *keys)\n\n    def hgetex(\n        self,\n        name: KeyT,\n        *keys: str,\n        ex: Optional[ExpiryT] = None,\n        px: Optional[ExpiryT] = None,\n        exat: Optional[AbsExpiryT] = None,\n        pxat: Optional[AbsExpiryT] = None,\n        persist: bool = False,\n    ) -> Union[\n        Awaitable[Optional[List[Union[str, bytes]]]], Optional[List[Union[str, bytes]]]\n    ]:\n        \"\"\"\n        Return the values of ``key`` and ``keys`` within the hash ``name``\n        and optionally set their expiration.\n\n        ``ex`` sets an expire flag on ``kyes`` for ``ex`` seconds.\n\n        ``px`` sets an expire flag on ``keys`` for ``px`` milliseconds.\n\n        ``exat`` sets an expire flag on ``keys`` for ``ex`` seconds,\n        specified in unix time.\n\n        ``pxat`` sets an expire flag on ``keys`` for ``ex`` milliseconds,\n        specified in unix time.\n\n        ``persist`` remove the time to live associated with the ``keys``.\n\n        Available since Redis 8.0\n        For more information see https://redis.io/commands/hgetex\n        \"\"\"\n        if not keys:\n            raise DataError(\"'hgetex' should have at least one key provided\")\n\n        opset = {ex, px, exat, pxat}\n        if len(opset) > 2 or len(opset) > 1 and persist:\n            raise DataError(\n                \"``ex``, ``px``, ``exat``, ``pxat``, \"\n                \"and ``persist`` are mutually exclusive.\"\n            )\n\n        exp_options: list[EncodableT] = extract_expire_flags(ex, px, exat, pxat)\n\n        if persist:\n            exp_options.append(\"PERSIST\")\n\n        return self.execute_command(\n            \"HGETEX\",\n            name,\n            *exp_options,\n            \"FIELDS\",\n            len(keys),\n            *keys,\n        )\n\n    def hincrby(\n        self, name: str, key: str, amount: int = 1\n    ) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Increment the value of ``key`` in hash ``name`` by ``amount``\n\n        For more information see https://redis.io/commands/hincrby\n        \"\"\"\n        return self.execute_command(\"HINCRBY\", name, key, amount)\n\n    def hincrbyfloat(\n        self, name: str, key: str, amount: float = 1.0\n    ) -> Union[Awaitable[float], float]:\n        \"\"\"\n        Increment the value of ``key`` in hash ``name`` by floating ``amount``\n\n        For more information see https://redis.io/commands/hincrbyfloat\n        \"\"\"\n        return self.execute_command(\"HINCRBYFLOAT\", name, key, amount)\n\n    def hkeys(self, name: str) -> Union[Awaitable[List], List]:\n        \"\"\"\n        Return the list of keys within hash ``name``\n\n        For more information see https://redis.io/commands/hkeys\n        \"\"\"\n        return self.execute_command(\"HKEYS\", name, keys=[name])\n\n    def hlen(self, name: str) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Return the number of elements in hash ``name``\n\n        For more information see https://redis.io/commands/hlen\n        \"\"\"\n        return self.execute_command(\"HLEN\", name, keys=[name])\n\n    def hset(\n        self,\n        name: str,\n        key: Optional[str] = None,\n        value: Optional[str] = None,\n        mapping: Optional[dict] = None,\n        items: Optional[list] = None,\n    ) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Set ``key`` to ``value`` within hash ``name``,\n        ``mapping`` accepts a dict of key/value pairs that will be\n        added to hash ``name``.\n        ``items`` accepts a list of key/value pairs that will be\n        added to hash ``name``.\n        Returns the number of fields that were added.\n\n        For more information see https://redis.io/commands/hset\n        \"\"\"\n\n        if key is None and not mapping and not items:\n            raise DataError(\"'hset' with no key value pairs\")\n\n        pieces = []\n        if items:\n            pieces.extend(items)\n        if key is not None:\n            pieces.extend((key, value))\n        if mapping:\n            for pair in mapping.items():\n                pieces.extend(pair)\n\n        return self.execute_command(\"HSET\", name, *pieces)\n\n    def hsetex(\n        self,\n        name: str,\n        key: Optional[str] = None,\n        value: Optional[str] = None,\n        mapping: Optional[dict] = None,\n        items: Optional[list] = None,\n        ex: Optional[ExpiryT] = None,\n        px: Optional[ExpiryT] = None,\n        exat: Optional[AbsExpiryT] = None,\n        pxat: Optional[AbsExpiryT] = None,\n        data_persist_option: Optional[HashDataPersistOptions] = None,\n        keepttl: bool = False,\n    ) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Set ``key`` to ``value`` within hash ``name``\n\n        ``mapping`` accepts a dict of key/value pairs that will be\n        added to hash ``name``.\n\n        ``items`` accepts a list of key/value pairs that will be\n        added to hash ``name``.\n\n        ``ex`` sets an expire flag on ``keys`` for ``ex`` seconds.\n\n        ``px`` sets an expire flag on ``keys`` for ``px`` milliseconds.\n\n        ``exat`` sets an expire flag on ``keys`` for ``ex`` seconds,\n            specified in unix time.\n\n        ``pxat`` sets an expire flag on ``keys`` for ``ex`` milliseconds,\n            specified in unix time.\n\n        ``data_persist_option`` can be set to ``FNX`` or ``FXX`` to control the\n            behavior of the command.\n            ``FNX`` will set the value for each provided key to each\n                provided value only if all do not already exist.\n            ``FXX`` will set the value for each provided key to each\n                provided value only if all already exist.\n\n        ``keepttl`` if True, retain the time to live associated with the keys.\n\n        Returns the number of fields that were added.\n\n        Available since Redis 8.0\n        For more information see https://redis.io/commands/hsetex\n        \"\"\"\n        if key is None and not mapping and not items:\n            raise DataError(\"'hsetex' with no key value pairs\")\n\n        if items and len(items) % 2 != 0:\n            raise DataError(\n                \"'hsetex' with odd number of items. \"\n                \"'items' must contain a list of key/value pairs.\"\n            )\n\n        opset = {ex, px, exat, pxat}\n        if len(opset) > 2 or len(opset) > 1 and keepttl:\n            raise DataError(\n                \"``ex``, ``px``, ``exat``, ``pxat``, \"\n                \"and ``keepttl`` are mutually exclusive.\"\n            )\n\n        exp_options: list[EncodableT] = extract_expire_flags(ex, px, exat, pxat)\n        if data_persist_option:\n            exp_options.append(data_persist_option.value)\n\n        if keepttl:\n            exp_options.append(\"KEEPTTL\")\n\n        pieces = []\n        if items:\n            pieces.extend(items)\n        if key is not None:\n            pieces.extend((key, value))\n        if mapping:\n            for pair in mapping.items():\n                pieces.extend(pair)\n\n        return self.execute_command(\n            \"HSETEX\", name, *exp_options, \"FIELDS\", int(len(pieces) / 2), *pieces\n        )\n\n    def hsetnx(self, name: str, key: str, value: str) -> Union[Awaitable[bool], bool]:\n        \"\"\"\n        Set ``key`` to ``value`` within hash ``name`` if ``key`` does not\n        exist.  Returns 1 if HSETNX created a field, otherwise 0.\n\n        For more information see https://redis.io/commands/hsetnx\n        \"\"\"\n        return self.execute_command(\"HSETNX\", name, key, value)\n\n    @deprecated_function(\n        version=\"4.0.0\",\n        reason=\"Use 'hset' instead.\",\n        name=\"hmset\",\n    )\n    def hmset(self, name: str, mapping: dict) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Set key to value within hash ``name`` for each corresponding\n        key and value from the ``mapping`` dict.\n\n        For more information see https://redis.io/commands/hmset\n        \"\"\"\n        if not mapping:\n            raise DataError(\"'hmset' with 'mapping' of length 0\")\n        items = []\n        for pair in mapping.items():\n            items.extend(pair)\n        return self.execute_command(\"HMSET\", name, *items)\n\n    def hmget(self, name: str, keys: List, *args: List) -> Union[Awaitable[List], List]:\n        \"\"\"\n        Returns a list of values ordered identically to ``keys``\n\n        For more information see https://redis.io/commands/hmget\n        \"\"\"\n        args = list_or_args(keys, args)\n        return self.execute_command(\"HMGET\", name, *args, keys=[name])\n\n    def hvals(self, name: str) -> Union[Awaitable[List], List]:\n        \"\"\"\n        Return the list of values within hash ``name``\n\n        For more information see https://redis.io/commands/hvals\n        \"\"\"\n        return self.execute_command(\"HVALS\", name, keys=[name])\n\n    def hstrlen(self, name: str, key: str) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Return the number of bytes stored in the value of ``key``\n        within hash ``name``\n\n        For more information see https://redis.io/commands/hstrlen\n        \"\"\"\n        return self.execute_command(\"HSTRLEN\", name, key, keys=[name])\n\n    def hexpire(\n        self,\n        name: KeyT,\n        seconds: ExpiryT,\n        *fields: str,\n        nx: bool = False,\n        xx: bool = False,\n        gt: bool = False,\n        lt: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Sets or updates the expiration time for fields within a hash key, using relative\n        time in seconds.\n\n        If a field already has an expiration time, the behavior of the update can be\n        controlled using the `nx`, `xx`, `gt`, and `lt` parameters.\n\n        The return value provides detailed information about the outcome for each field.\n\n        For more information, see https://redis.io/commands/hexpire\n\n        Args:\n            name: The name of the hash key.\n            seconds: Expiration time in seconds, relative. Can be an integer, or a\n                     Python `timedelta` object.\n            fields: List of fields within the hash to apply the expiration time to.\n            nx: Set expiry only when the field has no expiry.\n            xx: Set expiry only when the field has an existing expiry.\n            gt: Set expiry only when the new expiry is greater than the current one.\n            lt: Set expiry only when the new expiry is less than the current one.\n\n        Returns:\n            Returns a list which contains for each field in the request:\n                - `-2` if the field does not exist, or if the key does not exist.\n                - `0` if the specified NX | XX | GT | LT condition was not met.\n                - `1` if the expiration time was set or updated.\n                - `2` if the field was deleted because the specified expiration time is\n                  in the past.\n        \"\"\"\n        conditions = [nx, xx, gt, lt]\n        if sum(conditions) > 1:\n            raise ValueError(\"Only one of 'nx', 'xx', 'gt', 'lt' can be specified.\")\n\n        if isinstance(seconds, datetime.timedelta):\n            seconds = int(seconds.total_seconds())\n\n        options = []\n        if nx:\n            options.append(\"NX\")\n        if xx:\n            options.append(\"XX\")\n        if gt:\n            options.append(\"GT\")\n        if lt:\n            options.append(\"LT\")\n\n        return self.execute_command(\n            \"HEXPIRE\", name, seconds, *options, \"FIELDS\", len(fields), *fields\n        )\n\n    def hpexpire(\n        self,\n        name: KeyT,\n        milliseconds: ExpiryT,\n        *fields: str,\n        nx: bool = False,\n        xx: bool = False,\n        gt: bool = False,\n        lt: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Sets or updates the expiration time for fields within a hash key, using relative\n        time in milliseconds.\n\n        If a field already has an expiration time, the behavior of the update can be\n        controlled using the `nx`, `xx`, `gt`, and `lt` parameters.\n\n        The return value provides detailed information about the outcome for each field.\n\n        For more information, see https://redis.io/commands/hpexpire\n\n        Args:\n            name: The name of the hash key.\n            milliseconds: Expiration time in milliseconds, relative. Can be an integer,\n                          or a Python `timedelta` object.\n            fields: List of fields within the hash to apply the expiration time to.\n            nx: Set expiry only when the field has no expiry.\n            xx: Set expiry only when the field has an existing expiry.\n            gt: Set expiry only when the new expiry is greater than the current one.\n            lt: Set expiry only when the new expiry is less than the current one.\n\n        Returns:\n            Returns a list which contains for each field in the request:\n                - `-2` if the field does not exist, or if the key does not exist.\n                - `0` if the specified NX | XX | GT | LT condition was not met.\n                - `1` if the expiration time was set or updated.\n                - `2` if the field was deleted because the specified expiration time is\n                  in the past.\n        \"\"\"\n        conditions = [nx, xx, gt, lt]\n        if sum(conditions) > 1:\n            raise ValueError(\"Only one of 'nx', 'xx', 'gt', 'lt' can be specified.\")\n\n        if isinstance(milliseconds, datetime.timedelta):\n            milliseconds = int(milliseconds.total_seconds() * 1000)\n\n        options = []\n        if nx:\n            options.append(\"NX\")\n        if xx:\n            options.append(\"XX\")\n        if gt:\n            options.append(\"GT\")\n        if lt:\n            options.append(\"LT\")\n\n        return self.execute_command(\n            \"HPEXPIRE\", name, milliseconds, *options, \"FIELDS\", len(fields), *fields\n        )\n\n    def hexpireat(\n        self,\n        name: KeyT,\n        unix_time_seconds: AbsExpiryT,\n        *fields: str,\n        nx: bool = False,\n        xx: bool = False,\n        gt: bool = False,\n        lt: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Sets or updates the expiration time for fields within a hash key, using an\n        absolute Unix timestamp in seconds.\n\n        If a field already has an expiration time, the behavior of the update can be\n        controlled using the `nx`, `xx`, `gt`, and `lt` parameters.\n\n        The return value provides detailed information about the outcome for each field.\n\n        For more information, see https://redis.io/commands/hexpireat\n\n        Args:\n            name: The name of the hash key.\n            unix_time_seconds: Expiration time as Unix timestamp in seconds. Can be an\n                               integer or a Python `datetime` object.\n            fields: List of fields within the hash to apply the expiration time to.\n            nx: Set expiry only when the field has no expiry.\n            xx: Set expiry only when the field has an existing expiration time.\n            gt: Set expiry only when the new expiry is greater than the current one.\n            lt: Set expiry only when the new expiry is less than the current one.\n\n        Returns:\n            Returns a list which contains for each field in the request:\n                - `-2` if the field does not exist, or if the key does not exist.\n                - `0` if the specified NX | XX | GT | LT condition was not met.\n                - `1` if the expiration time was set or updated.\n                - `2` if the field was deleted because the specified expiration time is\n                  in the past.\n        \"\"\"\n        conditions = [nx, xx, gt, lt]\n        if sum(conditions) > 1:\n            raise ValueError(\"Only one of 'nx', 'xx', 'gt', 'lt' can be specified.\")\n\n        if isinstance(unix_time_seconds, datetime.datetime):\n            unix_time_seconds = int(unix_time_seconds.timestamp())\n\n        options = []\n        if nx:\n            options.append(\"NX\")\n        if xx:\n            options.append(\"XX\")\n        if gt:\n            options.append(\"GT\")\n        if lt:\n            options.append(\"LT\")\n\n        return self.execute_command(\n            \"HEXPIREAT\",\n            name,\n            unix_time_seconds,\n            *options,\n            \"FIELDS\",\n            len(fields),\n            *fields,\n        )\n\n    def hpexpireat(\n        self,\n        name: KeyT,\n        unix_time_milliseconds: AbsExpiryT,\n        *fields: str,\n        nx: bool = False,\n        xx: bool = False,\n        gt: bool = False,\n        lt: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Sets or updates the expiration time for fields within a hash key, using an\n        absolute Unix timestamp in milliseconds.\n\n        If a field already has an expiration time, the behavior of the update can be\n        controlled using the `nx`, `xx`, `gt`, and `lt` parameters.\n\n        The return value provides detailed information about the outcome for each field.\n\n        For more information, see https://redis.io/commands/hpexpireat\n\n        Args:\n            name: The name of the hash key.\n            unix_time_milliseconds: Expiration time as Unix timestamp in milliseconds.\n                                    Can be an integer or a Python `datetime` object.\n            fields: List of fields within the hash to apply the expiry.\n            nx: Set expiry only when the field has no expiry.\n            xx: Set expiry only when the field has an existing expiry.\n            gt: Set expiry only when the new expiry is greater than the current one.\n            lt: Set expiry only when the new expiry is less than the current one.\n\n        Returns:\n            Returns a list which contains for each field in the request:\n                - `-2` if the field does not exist, or if the key does not exist.\n                - `0` if the specified NX | XX | GT | LT condition was not met.\n                - `1` if the expiration time was set or updated.\n                - `2` if the field was deleted because the specified expiration time is\n                  in the past.\n        \"\"\"\n        conditions = [nx, xx, gt, lt]\n        if sum(conditions) > 1:\n            raise ValueError(\"Only one of 'nx', 'xx', 'gt', 'lt' can be specified.\")\n\n        if isinstance(unix_time_milliseconds, datetime.datetime):\n            unix_time_milliseconds = int(unix_time_milliseconds.timestamp() * 1000)\n\n        options = []\n        if nx:\n            options.append(\"NX\")\n        if xx:\n            options.append(\"XX\")\n        if gt:\n            options.append(\"GT\")\n        if lt:\n            options.append(\"LT\")\n\n        return self.execute_command(\n            \"HPEXPIREAT\",\n            name,\n            unix_time_milliseconds,\n            *options,\n            \"FIELDS\",\n            len(fields),\n            *fields,\n        )\n\n    def hpersist(self, name: KeyT, *fields: str) -> ResponseT:\n        \"\"\"\n        Removes the expiration time for each specified field in a hash.\n\n        For more information, see https://redis.io/commands/hpersist\n\n        Args:\n            name: The name of the hash key.\n            fields: A list of fields within the hash from which to remove the\n                    expiration time.\n\n        Returns:\n            Returns a list which contains for each field in the request:\n                - `-2` if the field does not exist, or if the key does not exist.\n                - `-1` if the field exists but has no associated expiration time.\n                - `1` if the expiration time was successfully removed from the field.\n        \"\"\"\n        return self.execute_command(\"HPERSIST\", name, \"FIELDS\", len(fields), *fields)\n\n    def hexpiretime(self, key: KeyT, *fields: str) -> ResponseT:\n        \"\"\"\n        Returns the expiration times of hash fields as Unix timestamps in seconds.\n\n        For more information, see https://redis.io/commands/hexpiretime\n\n        Args:\n            key: The hash key.\n            fields: A list of fields within the hash for which to get the expiration\n                    time.\n\n        Returns:\n            Returns a list which contains for each field in the request:\n                - `-2` if the field does not exist, or if the key does not exist.\n                - `-1` if the field exists but has no associated expire time.\n                - A positive integer representing the expiration Unix timestamp in\n                  seconds, if the field has an associated expiration time.\n        \"\"\"\n        return self.execute_command(\n            \"HEXPIRETIME\", key, \"FIELDS\", len(fields), *fields, keys=[key]\n        )\n\n    def hpexpiretime(self, key: KeyT, *fields: str) -> ResponseT:\n        \"\"\"\n        Returns the expiration times of hash fields as Unix timestamps in milliseconds.\n\n        For more information, see https://redis.io/commands/hpexpiretime\n\n        Args:\n            key: The hash key.\n            fields: A list of fields within the hash for which to get the expiration\n                    time.\n\n        Returns:\n            Returns a list which contains for each field in the request:\n                - `-2` if the field does not exist, or if the key does not exist.\n                - `-1` if the field exists but has no associated expire time.\n                - A positive integer representing the expiration Unix timestamp in\n                  milliseconds, if the field has an associated expiration time.\n        \"\"\"\n        return self.execute_command(\n            \"HPEXPIRETIME\", key, \"FIELDS\", len(fields), *fields, keys=[key]\n        )\n\n    def httl(self, key: KeyT, *fields: str) -> ResponseT:\n        \"\"\"\n        Returns the TTL (Time To Live) in seconds for each specified field within a hash\n        key.\n\n        For more information, see https://redis.io/commands/httl\n\n        Args:\n            key: The hash key.\n            fields: A list of fields within the hash for which to get the TTL.\n\n        Returns:\n            Returns a list which contains for each field in the request:\n                - `-2` if the field does not exist, or if the key does not exist.\n                - `-1` if the field exists but has no associated expire time.\n                - A positive integer representing the TTL in seconds if the field has\n                  an associated expiration time.\n        \"\"\"\n        return self.execute_command(\n            \"HTTL\", key, \"FIELDS\", len(fields), *fields, keys=[key]\n        )\n\n    def hpttl(self, key: KeyT, *fields: str) -> ResponseT:\n        \"\"\"\n        Returns the TTL (Time To Live) in milliseconds for each specified field within a\n        hash key.\n\n        For more information, see https://redis.io/commands/hpttl\n\n        Args:\n            key: The hash key.\n            fields: A list of fields within the hash for which to get the TTL.\n\n        Returns:\n            Returns a list which contains for each field in the request:\n                - `-2` if the field does not exist, or if the key does not exist.\n                - `-1` if the field exists but has no associated expire time.\n                - A positive integer representing the TTL in milliseconds if the field\n                  has an associated expiration time.\n        \"\"\"\n        return self.execute_command(\n            \"HPTTL\", key, \"FIELDS\", len(fields), *fields, keys=[key]\n        )\n\n\nAsyncHashCommands = HashCommands\n\n\nclass Script:\n    \"\"\"\n    An executable Lua script object returned by ``register_script``\n    \"\"\"\n\n    def __init__(self, registered_client: \"redis.client.Redis\", script: ScriptTextT):\n        self.registered_client = registered_client\n        self.script = script\n        # Precalculate and store the SHA1 hex digest of the script.\n\n        if isinstance(script, str):\n            # We need the encoding from the client in order to generate an\n            # accurate byte representation of the script\n            encoder = self.get_encoder()\n            script = encoder.encode(script)\n        self.sha = hashlib.sha1(script).hexdigest()\n\n    def __call__(\n        self,\n        keys: Union[Sequence[KeyT], None] = None,\n        args: Union[Iterable[EncodableT], None] = None,\n        client: Union[\"redis.client.Redis\", None] = None,\n    ):\n        \"\"\"Execute the script, passing any required ``args``\"\"\"\n        keys = keys or []\n        args = args or []\n        if client is None:\n            client = self.registered_client\n        args = tuple(keys) + tuple(args)\n        # make sure the Redis server knows about the script\n        from redis.client import Pipeline\n\n        if isinstance(client, Pipeline):\n            # Make sure the pipeline can register the script before executing.\n            client.scripts.add(self)\n        try:\n            return client.evalsha(self.sha, len(keys), *args)\n        except NoScriptError:\n            # Maybe the client is pointed to a different server than the client\n            # that created this instance?\n            # Overwrite the sha just in case there was a discrepancy.\n            self.sha = client.script_load(self.script)\n            return client.evalsha(self.sha, len(keys), *args)\n\n    def get_encoder(self):\n        \"\"\"Get the encoder to encode string scripts into bytes.\"\"\"\n        try:\n            return self.registered_client.get_encoder()\n        except AttributeError:\n            # DEPRECATED\n            # In version <=4.1.2, this was the code we used to get the encoder.\n            # However, after 4.1.2 we added support for scripting in clustered\n            # redis. ClusteredRedis doesn't have a `.connection_pool` attribute\n            # so we changed the Script class to use\n            # `self.registered_client.get_encoder` (see above).\n            # However, that is technically a breaking change, as consumers who\n            # use Scripts directly might inject a `registered_client` that\n            # doesn't have a `.get_encoder` field. This try/except prevents us\n            # from breaking backward-compatibility. Ideally, it would be\n            # removed in the next major release.\n            return self.registered_client.connection_pool.get_encoder()\n\n\nclass AsyncScript:\n    \"\"\"\n    An executable Lua script object returned by ``register_script``\n    \"\"\"\n\n    def __init__(\n        self,\n        registered_client: \"redis.asyncio.client.Redis\",\n        script: ScriptTextT,\n    ):\n        self.registered_client = registered_client\n        self.script = script\n        # Precalculate and store the SHA1 hex digest of the script.\n\n        if isinstance(script, str):\n            # We need the encoding from the client in order to generate an\n            # accurate byte representation of the script\n            try:\n                encoder = registered_client.connection_pool.get_encoder()\n            except AttributeError:\n                # Cluster\n                encoder = registered_client.get_encoder()\n            script = encoder.encode(script)\n        self.sha = hashlib.sha1(script).hexdigest()\n\n    async def __call__(\n        self,\n        keys: Union[Sequence[KeyT], None] = None,\n        args: Union[Iterable[EncodableT], None] = None,\n        client: Union[\"redis.asyncio.client.Redis\", None] = None,\n    ):\n        \"\"\"Execute the script, passing any required ``args``\"\"\"\n        keys = keys or []\n        args = args or []\n        if client is None:\n            client = self.registered_client\n        args = tuple(keys) + tuple(args)\n        # make sure the Redis server knows about the script\n        from redis.asyncio.client import Pipeline\n\n        if isinstance(client, Pipeline):\n            # Make sure the pipeline can register the script before executing.\n            client.scripts.add(self)\n        try:\n            return await client.evalsha(self.sha, len(keys), *args)\n        except NoScriptError:\n            # Maybe the client is pointed to a different server than the client\n            # that created this instance?\n            # Overwrite the sha just in case there was a discrepancy.\n            self.sha = await client.script_load(self.script)\n            return await client.evalsha(self.sha, len(keys), *args)\n\n\nclass PubSubCommands(CommandsProtocol):\n    \"\"\"\n    Redis PubSub commands.\n    see https://redis.io/topics/pubsub\n    \"\"\"\n\n    def publish(self, channel: ChannelT, message: EncodableT, **kwargs) -> ResponseT:\n        \"\"\"\n        Publish ``message`` on ``channel``.\n        Returns the number of subscribers the message was delivered to.\n\n        For more information see https://redis.io/commands/publish\n        \"\"\"\n        return self.execute_command(\"PUBLISH\", channel, message, **kwargs)\n\n    def spublish(self, shard_channel: ChannelT, message: EncodableT) -> ResponseT:\n        \"\"\"\n        Posts a message to the given shard channel.\n        Returns the number of clients that received the message\n\n        For more information see https://redis.io/commands/spublish\n        \"\"\"\n        return self.execute_command(\"SPUBLISH\", shard_channel, message)\n\n    def pubsub_channels(self, pattern: PatternT = \"*\", **kwargs) -> ResponseT:\n        \"\"\"\n        Return a list of channels that have at least one subscriber\n\n        For more information see https://redis.io/commands/pubsub-channels\n        \"\"\"\n        return self.execute_command(\"PUBSUB CHANNELS\", pattern, **kwargs)\n\n    def pubsub_shardchannels(self, pattern: PatternT = \"*\", **kwargs) -> ResponseT:\n        \"\"\"\n        Return a list of shard_channels that have at least one subscriber\n\n        For more information see https://redis.io/commands/pubsub-shardchannels\n        \"\"\"\n        return self.execute_command(\"PUBSUB SHARDCHANNELS\", pattern, **kwargs)\n\n    def pubsub_numpat(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns the number of subscriptions to patterns\n\n        For more information see https://redis.io/commands/pubsub-numpat\n        \"\"\"\n        return self.execute_command(\"PUBSUB NUMPAT\", **kwargs)\n\n    def pubsub_numsub(self, *args: ChannelT, **kwargs) -> ResponseT:\n        \"\"\"\n        Return a list of (channel, number of subscribers) tuples\n        for each channel given in ``*args``\n\n        For more information see https://redis.io/commands/pubsub-numsub\n        \"\"\"\n        return self.execute_command(\"PUBSUB NUMSUB\", *args, **kwargs)\n\n    def pubsub_shardnumsub(self, *args: ChannelT, **kwargs) -> ResponseT:\n        \"\"\"\n        Return a list of (shard_channel, number of subscribers) tuples\n        for each channel given in ``*args``\n\n        For more information see https://redis.io/commands/pubsub-shardnumsub\n        \"\"\"\n        return self.execute_command(\"PUBSUB SHARDNUMSUB\", *args, **kwargs)\n\n\nAsyncPubSubCommands = PubSubCommands\n\n\nclass ScriptCommands(CommandsProtocol):\n    \"\"\"\n    Redis Lua script commands. see:\n    https://redis.io/ebook/part-3-next-steps/chapter-11-scripting-redis-with-lua/\n    \"\"\"\n\n    def _eval(\n        self, command: str, script: str, numkeys: int, *keys_and_args: str\n    ) -> Union[Awaitable[str], str]:\n        return self.execute_command(command, script, numkeys, *keys_and_args)\n\n    def eval(\n        self, script: str, numkeys: int, *keys_and_args: str\n    ) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Execute the Lua ``script``, specifying the ``numkeys`` the script\n        will touch and the key names and argument values in ``keys_and_args``.\n        Returns the result of the script.\n\n        In practice, use the object returned by ``register_script``. This\n        function exists purely for Redis API completion.\n\n        For more information see  https://redis.io/commands/eval\n        \"\"\"\n        return self._eval(\"EVAL\", script, numkeys, *keys_and_args)\n\n    def eval_ro(\n        self, script: str, numkeys: int, *keys_and_args: str\n    ) -> Union[Awaitable[str], str]:\n        \"\"\"\n        The read-only variant of the EVAL command\n\n        Execute the read-only Lua ``script`` specifying the ``numkeys`` the script\n        will touch and the key names and argument values in ``keys_and_args``.\n        Returns the result of the script.\n\n        For more information see  https://redis.io/commands/eval_ro\n        \"\"\"\n        return self._eval(\"EVAL_RO\", script, numkeys, *keys_and_args)\n\n    def _evalsha(\n        self, command: str, sha: str, numkeys: int, *keys_and_args: list\n    ) -> Union[Awaitable[str], str]:\n        return self.execute_command(command, sha, numkeys, *keys_and_args)\n\n    def evalsha(\n        self, sha: str, numkeys: int, *keys_and_args: str\n    ) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Use the ``sha`` to execute a Lua script already registered via EVAL\n        or SCRIPT LOAD. Specify the ``numkeys`` the script will touch and the\n        key names and argument values in ``keys_and_args``. Returns the result\n        of the script.\n\n        In practice, use the object returned by ``register_script``. This\n        function exists purely for Redis API completion.\n\n        For more information see  https://redis.io/commands/evalsha\n        \"\"\"\n        return self._evalsha(\"EVALSHA\", sha, numkeys, *keys_and_args)\n\n    def evalsha_ro(\n        self, sha: str, numkeys: int, *keys_and_args: str\n    ) -> Union[Awaitable[str], str]:\n        \"\"\"\n        The read-only variant of the EVALSHA command\n\n        Use the ``sha`` to execute a read-only Lua script already registered via EVAL\n        or SCRIPT LOAD. Specify the ``numkeys`` the script will touch and the\n        key names and argument values in ``keys_and_args``. Returns the result\n        of the script.\n\n        For more information see  https://redis.io/commands/evalsha_ro\n        \"\"\"\n        return self._evalsha(\"EVALSHA_RO\", sha, numkeys, *keys_and_args)\n\n    def script_exists(self, *args: str) -> ResponseT:\n        \"\"\"\n        Check if a script exists in the script cache by specifying the SHAs of\n        each script as ``args``. Returns a list of boolean values indicating if\n        if each already script exists in the cache_data.\n\n        For more information see  https://redis.io/commands/script-exists\n        \"\"\"\n        return self.execute_command(\"SCRIPT EXISTS\", *args)\n\n    def script_debug(self, *args) -> None:\n        raise NotImplementedError(\n            \"SCRIPT DEBUG is intentionally not implemented in the client.\"\n        )\n\n    def script_flush(\n        self, sync_type: Union[Literal[\"SYNC\"], Literal[\"ASYNC\"]] = None\n    ) -> ResponseT:\n        \"\"\"Flush all scripts from the script cache_data.\n\n        ``sync_type`` is by default SYNC (synchronous) but it can also be\n                      ASYNC.\n\n        For more information see  https://redis.io/commands/script-flush\n        \"\"\"\n\n        # Redis pre 6 had no sync_type.\n        if sync_type not in [\"SYNC\", \"ASYNC\", None]:\n            raise DataError(\n                \"SCRIPT FLUSH defaults to SYNC in redis > 6.2, or \"\n                \"accepts SYNC/ASYNC. For older versions, \"\n                \"of redis leave as None.\"\n            )\n        if sync_type is None:\n            pieces = []\n        else:\n            pieces = [sync_type]\n        return self.execute_command(\"SCRIPT FLUSH\", *pieces)\n\n    def script_kill(self) -> ResponseT:\n        \"\"\"\n        Kill the currently executing Lua script\n\n        For more information see https://redis.io/commands/script-kill\n        \"\"\"\n        return self.execute_command(\"SCRIPT KILL\")\n\n    def script_load(self, script: ScriptTextT) -> ResponseT:\n        \"\"\"\n        Load a Lua ``script`` into the script cache_data. Returns the SHA.\n\n        For more information see https://redis.io/commands/script-load\n        \"\"\"\n        return self.execute_command(\"SCRIPT LOAD\", script)\n\n    def register_script(self: \"redis.client.Redis\", script: ScriptTextT) -> Script:\n        \"\"\"\n        Register a Lua ``script`` specifying the ``keys`` it will touch.\n        Returns a Script object that is callable and hides the complexity of\n        deal with scripts, keys, and shas. This is the preferred way to work\n        with Lua scripts.\n        \"\"\"\n        return Script(self, script)\n\n\nclass AsyncScriptCommands(ScriptCommands):\n    async def script_debug(self, *args) -> None:\n        return super().script_debug()\n\n    def register_script(\n        self: \"redis.asyncio.client.Redis\",\n        script: ScriptTextT,\n    ) -> AsyncScript:\n        \"\"\"\n        Register a Lua ``script`` specifying the ``keys`` it will touch.\n        Returns a Script object that is callable and hides the complexity of\n        deal with scripts, keys, and shas. This is the preferred way to work\n        with Lua scripts.\n        \"\"\"\n        return AsyncScript(self, script)\n\n\nclass GeoCommands(CommandsProtocol):\n    \"\"\"\n    Redis Geospatial commands.\n    see: https://redis.com/redis-best-practices/indexing-patterns/geospatial/\n    \"\"\"\n\n    def geoadd(\n        self,\n        name: KeyT,\n        values: Sequence[EncodableT],\n        nx: bool = False,\n        xx: bool = False,\n        ch: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Add the specified geospatial items to the specified key identified\n        by the ``name`` argument. The Geospatial items are given as ordered\n        members of the ``values`` argument, each item or place is formed by\n        the triad longitude, latitude and name.\n\n        Note: You can use ZREM to remove elements.\n\n        ``nx`` forces ZADD to only create new elements and not to update\n        scores for elements that already exist.\n\n        ``xx`` forces ZADD to only update scores of elements that already\n        exist. New elements will not be added.\n\n        ``ch`` modifies the return value to be the numbers of elements changed.\n        Changed elements include new elements that were added and elements\n        whose scores changed.\n\n        For more information see https://redis.io/commands/geoadd\n        \"\"\"\n        if nx and xx:\n            raise DataError(\"GEOADD allows either 'nx' or 'xx', not both\")\n        if len(values) % 3 != 0:\n            raise DataError(\"GEOADD requires places with lon, lat and name values\")\n        pieces = [name]\n        if nx:\n            pieces.append(\"NX\")\n        if xx:\n            pieces.append(\"XX\")\n        if ch:\n            pieces.append(\"CH\")\n        pieces.extend(values)\n        return self.execute_command(\"GEOADD\", *pieces)\n\n    def geodist(\n        self, name: KeyT, place1: FieldT, place2: FieldT, unit: Optional[str] = None\n    ) -> ResponseT:\n        \"\"\"\n        Return the distance between ``place1`` and ``place2`` members of the\n        ``name`` key.\n        The units must be one of the following : m, km mi, ft. By default\n        meters are used.\n\n        For more information see https://redis.io/commands/geodist\n        \"\"\"\n        pieces: list[EncodableT] = [name, place1, place2]\n        if unit and unit not in (\"m\", \"km\", \"mi\", \"ft\"):\n            raise DataError(\"GEODIST invalid unit\")\n        elif unit:\n            pieces.append(unit)\n        return self.execute_command(\"GEODIST\", *pieces, keys=[name])\n\n    def geohash(self, name: KeyT, *values: FieldT) -> ResponseT:\n        \"\"\"\n        Return the geo hash string for each item of ``values`` members of\n        the specified key identified by the ``name`` argument.\n\n        For more information see https://redis.io/commands/geohash\n        \"\"\"\n        return self.execute_command(\"GEOHASH\", name, *values, keys=[name])\n\n    def geopos(self, name: KeyT, *values: FieldT) -> ResponseT:\n        \"\"\"\n        Return the positions of each item of ``values`` as members of\n        the specified key identified by the ``name`` argument. Each position\n        is represented by the pairs lon and lat.\n\n        For more information see https://redis.io/commands/geopos\n        \"\"\"\n        return self.execute_command(\"GEOPOS\", name, *values, keys=[name])\n\n    def georadius(\n        self,\n        name: KeyT,\n        longitude: float,\n        latitude: float,\n        radius: float,\n        unit: Optional[str] = None,\n        withdist: bool = False,\n        withcoord: bool = False,\n        withhash: bool = False,\n        count: Optional[int] = None,\n        sort: Optional[str] = None,\n        store: Optional[KeyT] = None,\n        store_dist: Optional[KeyT] = None,\n        any: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Return the members of the specified key identified by the\n        ``name`` argument which are within the borders of the area specified\n        with the ``latitude`` and ``longitude`` location and the maximum\n        distance from the center specified by the ``radius`` value.\n\n        The units must be one of the following : m, km mi, ft. By default\n\n        ``withdist`` indicates to return the distances of each place.\n\n        ``withcoord`` indicates to return the latitude and longitude of\n        each place.\n\n        ``withhash`` indicates to return the geohash string of each place.\n\n        ``count`` indicates to return the number of elements up to N.\n\n        ``sort`` indicates to return the places in a sorted way, ASC for\n        nearest to fairest and DESC for fairest to nearest.\n\n        ``store`` indicates to save the places names in a sorted set named\n        with a specific key, each element of the destination sorted set is\n        populated with the score got from the original geo sorted set.\n\n        ``store_dist`` indicates to save the places names in a sorted set\n        named with a specific key, instead of ``store`` the sorted set\n        destination score is set with the distance.\n\n        For more information see https://redis.io/commands/georadius\n        \"\"\"\n        return self._georadiusgeneric(\n            \"GEORADIUS\",\n            name,\n            longitude,\n            latitude,\n            radius,\n            unit=unit,\n            withdist=withdist,\n            withcoord=withcoord,\n            withhash=withhash,\n            count=count,\n            sort=sort,\n            store=store,\n            store_dist=store_dist,\n            any=any,\n        )\n\n    def georadiusbymember(\n        self,\n        name: KeyT,\n        member: FieldT,\n        radius: float,\n        unit: Optional[str] = None,\n        withdist: bool = False,\n        withcoord: bool = False,\n        withhash: bool = False,\n        count: Optional[int] = None,\n        sort: Optional[str] = None,\n        store: Union[KeyT, None] = None,\n        store_dist: Union[KeyT, None] = None,\n        any: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        This command is exactly like ``georadius`` with the sole difference\n        that instead of taking, as the center of the area to query, a longitude\n        and latitude value, it takes the name of a member already existing\n        inside the geospatial index represented by the sorted set.\n\n        For more information see https://redis.io/commands/georadiusbymember\n        \"\"\"\n        return self._georadiusgeneric(\n            \"GEORADIUSBYMEMBER\",\n            name,\n            member,\n            radius,\n            unit=unit,\n            withdist=withdist,\n            withcoord=withcoord,\n            withhash=withhash,\n            count=count,\n            sort=sort,\n            store=store,\n            store_dist=store_dist,\n            any=any,\n        )\n\n    def _georadiusgeneric(\n        self, command: str, *args: EncodableT, **kwargs: Union[EncodableT, None]\n    ) -> ResponseT:\n        pieces = list(args)\n        if kwargs[\"unit\"] and kwargs[\"unit\"] not in (\"m\", \"km\", \"mi\", \"ft\"):\n            raise DataError(\"GEORADIUS invalid unit\")\n        elif kwargs[\"unit\"]:\n            pieces.append(kwargs[\"unit\"])\n        else:\n            pieces.append(\"m\")\n\n        if kwargs[\"any\"] and kwargs[\"count\"] is None:\n            raise DataError(\"``any`` can't be provided without ``count``\")\n\n        for arg_name, byte_repr in (\n            (\"withdist\", \"WITHDIST\"),\n            (\"withcoord\", \"WITHCOORD\"),\n            (\"withhash\", \"WITHHASH\"),\n        ):\n            if kwargs[arg_name]:\n                pieces.append(byte_repr)\n\n        if kwargs[\"count\"] is not None:\n            pieces.extend([\"COUNT\", kwargs[\"count\"]])\n            if kwargs[\"any\"]:\n                pieces.append(\"ANY\")\n\n        if kwargs[\"sort\"]:\n            if kwargs[\"sort\"] == \"ASC\":\n                pieces.append(\"ASC\")\n            elif kwargs[\"sort\"] == \"DESC\":\n                pieces.append(\"DESC\")\n            else:\n                raise DataError(\"GEORADIUS invalid sort\")\n\n        if kwargs[\"store\"] and kwargs[\"store_dist\"]:\n            raise DataError(\"GEORADIUS store and store_dist cant be set together\")\n\n        if kwargs[\"store\"]:\n            pieces.extend([b\"STORE\", kwargs[\"store\"]])\n\n        if kwargs[\"store_dist\"]:\n            pieces.extend([b\"STOREDIST\", kwargs[\"store_dist\"]])\n\n        return self.execute_command(command, *pieces, **kwargs)\n\n    def geosearch(\n        self,\n        name: KeyT,\n        member: Union[FieldT, None] = None,\n        longitude: Union[float, None] = None,\n        latitude: Union[float, None] = None,\n        unit: str = \"m\",\n        radius: Union[float, None] = None,\n        width: Union[float, None] = None,\n        height: Union[float, None] = None,\n        sort: Optional[str] = None,\n        count: Optional[int] = None,\n        any: bool = False,\n        withcoord: bool = False,\n        withdist: bool = False,\n        withhash: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Return the members of specified key identified by the\n        ``name`` argument, which are within the borders of the\n        area specified by a given shape. This command extends the\n        GEORADIUS command, so in addition to searching within circular\n        areas, it supports searching within rectangular areas.\n\n        This command should be used in place of the deprecated\n        GEORADIUS and GEORADIUSBYMEMBER commands.\n\n        ``member`` Use the position of the given existing\n         member in the sorted set. Can't be given with ``longitude``\n         and ``latitude``.\n\n        ``longitude`` and ``latitude`` Use the position given by\n        this coordinates. Can't be given with ``member``\n        ``radius`` Similar to GEORADIUS, search inside circular\n        area according the given radius. Can't be given with\n        ``height`` and ``width``.\n        ``height`` and ``width`` Search inside an axis-aligned\n        rectangle, determined by the given height and width.\n        Can't be given with ``radius``\n\n        ``unit`` must be one of the following : m, km, mi, ft.\n        `m` for meters (the default value), `km` for kilometers,\n        `mi` for miles and `ft` for feet.\n\n        ``sort`` indicates to return the places in a sorted way,\n        ASC for nearest to furthest and DESC for furthest to nearest.\n\n        ``count`` limit the results to the first count matching items.\n\n        ``any`` is set to True, the command will return as soon as\n        enough matches are found. Can't be provided without ``count``\n\n        ``withdist`` indicates to return the distances of each place.\n        ``withcoord`` indicates to return the latitude and longitude of\n        each place.\n\n        ``withhash`` indicates to return the geohash string of each place.\n\n        For more information see https://redis.io/commands/geosearch\n        \"\"\"\n\n        return self._geosearchgeneric(\n            \"GEOSEARCH\",\n            name,\n            member=member,\n            longitude=longitude,\n            latitude=latitude,\n            unit=unit,\n            radius=radius,\n            width=width,\n            height=height,\n            sort=sort,\n            count=count,\n            any=any,\n            withcoord=withcoord,\n            withdist=withdist,\n            withhash=withhash,\n            store=None,\n            store_dist=None,\n        )\n\n    def geosearchstore(\n        self,\n        dest: KeyT,\n        name: KeyT,\n        member: Optional[FieldT] = None,\n        longitude: Optional[float] = None,\n        latitude: Optional[float] = None,\n        unit: str = \"m\",\n        radius: Optional[float] = None,\n        width: Optional[float] = None,\n        height: Optional[float] = None,\n        sort: Optional[str] = None,\n        count: Optional[int] = None,\n        any: bool = False,\n        storedist: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        This command is like GEOSEARCH, but stores the result in\n        ``dest``. By default, it stores the results in the destination\n        sorted set with their geospatial information.\n        if ``store_dist`` set to True, the command will stores the\n        items in a sorted set populated with their distance from the\n        center of the circle or box, as a floating-point number.\n\n        For more information see https://redis.io/commands/geosearchstore\n        \"\"\"\n        return self._geosearchgeneric(\n            \"GEOSEARCHSTORE\",\n            dest,\n            name,\n            member=member,\n            longitude=longitude,\n            latitude=latitude,\n            unit=unit,\n            radius=radius,\n            width=width,\n            height=height,\n            sort=sort,\n            count=count,\n            any=any,\n            withcoord=None,\n            withdist=None,\n            withhash=None,\n            store=None,\n            store_dist=storedist,\n        )\n\n    def _geosearchgeneric(\n        self, command: str, *args: EncodableT, **kwargs: Union[EncodableT, None]\n    ) -> ResponseT:\n        pieces = list(args)\n\n        # FROMMEMBER or FROMLONLAT\n        if kwargs[\"member\"] is None:\n            if kwargs[\"longitude\"] is None or kwargs[\"latitude\"] is None:\n                raise DataError(\"GEOSEARCH must have member or longitude and latitude\")\n        if kwargs[\"member\"]:\n            if kwargs[\"longitude\"] or kwargs[\"latitude\"]:\n                raise DataError(\n                    \"GEOSEARCH member and longitude or latitude cant be set together\"\n                )\n            pieces.extend([b\"FROMMEMBER\", kwargs[\"member\"]])\n        if kwargs[\"longitude\"] is not None and kwargs[\"latitude\"] is not None:\n            pieces.extend([b\"FROMLONLAT\", kwargs[\"longitude\"], kwargs[\"latitude\"]])\n\n        # BYRADIUS or BYBOX\n        if kwargs[\"radius\"] is None:\n            if kwargs[\"width\"] is None or kwargs[\"height\"] is None:\n                raise DataError(\"GEOSEARCH must have radius or width and height\")\n        if kwargs[\"unit\"] is None:\n            raise DataError(\"GEOSEARCH must have unit\")\n        if kwargs[\"unit\"].lower() not in (\"m\", \"km\", \"mi\", \"ft\"):\n            raise DataError(\"GEOSEARCH invalid unit\")\n        if kwargs[\"radius\"]:\n            if kwargs[\"width\"] or kwargs[\"height\"]:\n                raise DataError(\n                    \"GEOSEARCH radius and width or height cant be set together\"\n                )\n            pieces.extend([b\"BYRADIUS\", kwargs[\"radius\"], kwargs[\"unit\"]])\n        if kwargs[\"width\"] and kwargs[\"height\"]:\n            pieces.extend([b\"BYBOX\", kwargs[\"width\"], kwargs[\"height\"], kwargs[\"unit\"]])\n\n        # sort\n        if kwargs[\"sort\"]:\n            if kwargs[\"sort\"].upper() == \"ASC\":\n                pieces.append(b\"ASC\")\n            elif kwargs[\"sort\"].upper() == \"DESC\":\n                pieces.append(b\"DESC\")\n            else:\n                raise DataError(\"GEOSEARCH invalid sort\")\n\n        # count any\n        if kwargs[\"count\"]:\n            pieces.extend([b\"COUNT\", kwargs[\"count\"]])\n            if kwargs[\"any\"]:\n                pieces.append(b\"ANY\")\n        elif kwargs[\"any\"]:\n            raise DataError(\"GEOSEARCH ``any`` can't be provided without count\")\n\n        # other properties\n        for arg_name, byte_repr in (\n            (\"withdist\", b\"WITHDIST\"),\n            (\"withcoord\", b\"WITHCOORD\"),\n            (\"withhash\", b\"WITHHASH\"),\n            (\"store_dist\", b\"STOREDIST\"),\n        ):\n            if kwargs[arg_name]:\n                pieces.append(byte_repr)\n\n        kwargs[\"keys\"] = [args[0] if command == \"GEOSEARCH\" else args[1]]\n\n        return self.execute_command(command, *pieces, **kwargs)\n\n\nAsyncGeoCommands = GeoCommands\n\n\nclass ModuleCommands(CommandsProtocol):\n    \"\"\"\n    Redis Module commands.\n    see: https://redis.io/topics/modules-intro\n    \"\"\"\n\n    def module_load(self, path, *args) -> ResponseT:\n        \"\"\"\n        Loads the module from ``path``.\n        Passes all ``*args`` to the module, during loading.\n        Raises ``ModuleError`` if a module is not found at ``path``.\n\n        For more information see https://redis.io/commands/module-load\n        \"\"\"\n        return self.execute_command(\"MODULE LOAD\", path, *args)\n\n    def module_loadex(\n        self,\n        path: str,\n        options: Optional[List[str]] = None,\n        args: Optional[List[str]] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Loads a module from a dynamic library at runtime with configuration directives.\n\n        For more information see https://redis.io/commands/module-loadex\n        \"\"\"\n        pieces = []\n        if options is not None:\n            pieces.append(\"CONFIG\")\n            pieces.extend(options)\n        if args is not None:\n            pieces.append(\"ARGS\")\n            pieces.extend(args)\n\n        return self.execute_command(\"MODULE LOADEX\", path, *pieces)\n\n    def module_unload(self, name) -> ResponseT:\n        \"\"\"\n        Unloads the module ``name``.\n        Raises ``ModuleError`` if ``name`` is not in loaded modules.\n\n        For more information see https://redis.io/commands/module-unload\n        \"\"\"\n        return self.execute_command(\"MODULE UNLOAD\", name)\n\n    def module_list(self) -> ResponseT:\n        \"\"\"\n        Returns a list of dictionaries containing the name and version of\n        all loaded modules.\n\n        For more information see https://redis.io/commands/module-list\n        \"\"\"\n        return self.execute_command(\"MODULE LIST\")\n\n    def command_info(self) -> None:\n        raise NotImplementedError(\n            \"COMMAND INFO is intentionally not implemented in the client.\"\n        )\n\n    def command_count(self) -> ResponseT:\n        return self.execute_command(\"COMMAND COUNT\")\n\n    def command_getkeys(self, *args) -> ResponseT:\n        return self.execute_command(\"COMMAND GETKEYS\", *args)\n\n    def command(self) -> ResponseT:\n        return self.execute_command(\"COMMAND\")\n\n\nclass AsyncModuleCommands(ModuleCommands):\n    async def command_info(self) -> None:\n        return super().command_info()\n\n\nclass ClusterCommands(CommandsProtocol):\n    \"\"\"\n    Class for Redis Cluster commands\n    \"\"\"\n\n    def cluster(self, cluster_arg, *args, **kwargs) -> ResponseT:\n        return self.execute_command(f\"CLUSTER {cluster_arg.upper()}\", *args, **kwargs)\n\n    def readwrite(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Disables read queries for a connection to a Redis Cluster slave node.\n\n        For more information see https://redis.io/commands/readwrite\n        \"\"\"\n        return self.execute_command(\"READWRITE\", **kwargs)\n\n    def readonly(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Enables read queries for a connection to a Redis Cluster replica node.\n\n        For more information see https://redis.io/commands/readonly\n        \"\"\"\n        return self.execute_command(\"READONLY\", **kwargs)\n\n\nAsyncClusterCommands = ClusterCommands\n\n\nclass FunctionCommands:\n    \"\"\"\n    Redis Function commands\n    \"\"\"\n\n    def function_load(\n        self, code: str, replace: Optional[bool] = False\n    ) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Load a library to Redis.\n        :param code: the source code (must start with\n        Shebang statement that provides a metadata about the library)\n        :param replace: changes the behavior to overwrite the existing library\n        with the new contents.\n        Return the library name that was loaded.\n\n        For more information see https://redis.io/commands/function-load\n        \"\"\"\n        pieces = [\"REPLACE\"] if replace else []\n        pieces.append(code)\n        return self.execute_command(\"FUNCTION LOAD\", *pieces)\n\n    def function_delete(self, library: str) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Delete the library called ``library`` and all its functions.\n\n        For more information see https://redis.io/commands/function-delete\n        \"\"\"\n        return self.execute_command(\"FUNCTION DELETE\", library)\n\n    def function_flush(self, mode: str = \"SYNC\") -> Union[Awaitable[str], str]:\n        \"\"\"\n        Deletes all the libraries.\n\n        For more information see https://redis.io/commands/function-flush\n        \"\"\"\n        return self.execute_command(\"FUNCTION FLUSH\", mode)\n\n    def function_list(\n        self, library: Optional[str] = \"*\", withcode: Optional[bool] = False\n    ) -> Union[Awaitable[List], List]:\n        \"\"\"\n        Return information about the functions and libraries.\n\n        Args:\n\n            library: specify a pattern for matching library names\n            withcode: cause the server to include the libraries source implementation\n                in the reply\n        \"\"\"\n        args = [\"LIBRARYNAME\", library]\n        if withcode:\n            args.append(\"WITHCODE\")\n        return self.execute_command(\"FUNCTION LIST\", *args)\n\n    def _fcall(\n        self, command: str, function, numkeys: int, *keys_and_args: Any\n    ) -> Union[Awaitable[str], str]:\n        return self.execute_command(command, function, numkeys, *keys_and_args)\n\n    def fcall(\n        self, function, numkeys: int, *keys_and_args: Any\n    ) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Invoke a function.\n\n        For more information see https://redis.io/commands/fcall\n        \"\"\"\n        return self._fcall(\"FCALL\", function, numkeys, *keys_and_args)\n\n    def fcall_ro(\n        self, function, numkeys: int, *keys_and_args: Any\n    ) -> Union[Awaitable[str], str]:\n        \"\"\"\n        This is a read-only variant of the FCALL command that cannot\n        execute commands that modify data.\n\n        For more information see https://redis.io/commands/fcall_ro\n        \"\"\"\n        return self._fcall(\"FCALL_RO\", function, numkeys, *keys_and_args)\n\n    def function_dump(self) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Return the serialized payload of loaded libraries.\n\n        For more information see https://redis.io/commands/function-dump\n        \"\"\"\n        from redis.client import NEVER_DECODE\n\n        options = {}\n        options[NEVER_DECODE] = []\n\n        return self.execute_command(\"FUNCTION DUMP\", **options)\n\n    def function_restore(\n        self, payload: str, policy: Optional[str] = \"APPEND\"\n    ) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Restore libraries from the serialized ``payload``.\n        You can use the optional policy argument to provide a policy\n        for handling existing libraries.\n\n        For more information see https://redis.io/commands/function-restore\n        \"\"\"\n        return self.execute_command(\"FUNCTION RESTORE\", payload, policy)\n\n    def function_kill(self) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Kill a function that is currently executing.\n\n        For more information see https://redis.io/commands/function-kill\n        \"\"\"\n        return self.execute_command(\"FUNCTION KILL\")\n\n    def function_stats(self) -> Union[Awaitable[List], List]:\n        \"\"\"\n        Return information about the function that's currently running\n        and information about the available execution engines.\n\n        For more information see https://redis.io/commands/function-stats\n        \"\"\"\n        return self.execute_command(\"FUNCTION STATS\")\n\n\nAsyncFunctionCommands = FunctionCommands\n\n\nclass DataAccessCommands(\n    BasicKeyCommands,\n    HyperlogCommands,\n    HashCommands,\n    GeoCommands,\n    ListCommands,\n    ScanCommands,\n    SetCommands,\n    StreamCommands,\n    SortedSetCommands,\n):\n    \"\"\"\n    A class containing all of the implemented data access redis commands.\n    This class is to be used as a mixin for synchronous Redis clients.\n    \"\"\"\n\n\nclass AsyncDataAccessCommands(\n    AsyncBasicKeyCommands,\n    AsyncHyperlogCommands,\n    AsyncHashCommands,\n    AsyncGeoCommands,\n    AsyncListCommands,\n    AsyncScanCommands,\n    AsyncSetCommands,\n    AsyncStreamCommands,\n    AsyncSortedSetCommands,\n):\n    \"\"\"\n    A class containing all of the implemented data access redis commands.\n    This class is to be used as a mixin for asynchronous Redis clients.\n    \"\"\"\n\n\nclass CoreCommands(\n    ACLCommands,\n    ClusterCommands,\n    DataAccessCommands,\n    ManagementCommands,\n    ModuleCommands,\n    PubSubCommands,\n    ScriptCommands,\n    FunctionCommands,\n):\n    \"\"\"\n    A class containing all of the implemented redis commands. This class is\n    to be used as a mixin for synchronous Redis clients.\n    \"\"\"\n\n\nclass AsyncCoreCommands(\n    AsyncACLCommands,\n    AsyncClusterCommands,\n    AsyncDataAccessCommands,\n    AsyncManagementCommands,\n    AsyncModuleCommands,\n    AsyncPubSubCommands,\n    AsyncScriptCommands,\n    AsyncFunctionCommands,\n):\n    \"\"\"\n    A class containing all of the implemented redis commands. This class is\n    to be used as a mixin for asynchronous Redis clients.\n    \"\"\"\n", 6734], "/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py": ["import asyncio\nimport copy\nimport inspect\nimport re\nimport warnings\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterator,\n    Awaitable,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Mapping,\n    MutableMapping,\n    Optional,\n    Protocol,\n    Set,\n    Tuple,\n    Type,\n    TypedDict,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom redis._parsers.helpers import (\n    _RedisCallbacks,\n    _RedisCallbacksRESP2,\n    _RedisCallbacksRESP3,\n    bool_ok,\n)\nfrom redis.asyncio.connection import (\n    Connection,\n    ConnectionPool,\n    SSLConnection,\n    UnixDomainSocketConnection,\n)\nfrom redis.asyncio.lock import Lock\nfrom redis.asyncio.retry import Retry\nfrom redis.backoff import ExponentialWithJitterBackoff\nfrom redis.client import (\n    EMPTY_RESPONSE,\n    NEVER_DECODE,\n    AbstractRedis,\n    CaseInsensitiveDict,\n)\nfrom redis.commands import (\n    AsyncCoreCommands,\n    AsyncRedisModuleCommands,\n    AsyncSentinelCommands,\n    list_or_args,\n)\nfrom redis.credentials import CredentialProvider\nfrom redis.event import (\n    AfterPooledConnectionsInstantiationEvent,\n    AfterPubSubConnectionInstantiationEvent,\n    AfterSingleConnectionInstantiationEvent,\n    ClientType,\n    EventDispatcher,\n)\nfrom redis.exceptions import (\n    ConnectionError,\n    ExecAbortError,\n    PubSubError,\n    RedisError,\n    ResponseError,\n    WatchError,\n)\nfrom redis.typing import ChannelT, EncodableT, KeyT\nfrom redis.utils import (\n    SSL_AVAILABLE,\n    _set_info_logger,\n    deprecated_args,\n    deprecated_function,\n    get_lib_version,\n    safe_str,\n    str_if_bytes,\n    truncate_text,\n)\n\nif TYPE_CHECKING and SSL_AVAILABLE:\n    from ssl import TLSVersion, VerifyMode\nelse:\n    TLSVersion = None\n    VerifyMode = None\n\nPubSubHandler = Callable[[Dict[str, str]], Awaitable[None]]\n_KeyT = TypeVar(\"_KeyT\", bound=KeyT)\n_ArgT = TypeVar(\"_ArgT\", KeyT, EncodableT)\n_RedisT = TypeVar(\"_RedisT\", bound=\"Redis\")\n_NormalizeKeysT = TypeVar(\"_NormalizeKeysT\", bound=Mapping[ChannelT, object])\nif TYPE_CHECKING:\n    from redis.commands.core import Script\n\n\nclass ResponseCallbackProtocol(Protocol):\n    def __call__(self, response: Any, **kwargs): ...\n\n\nclass AsyncResponseCallbackProtocol(Protocol):\n    async def __call__(self, response: Any, **kwargs): ...\n\n\nResponseCallbackT = Union[ResponseCallbackProtocol, AsyncResponseCallbackProtocol]\n\n\nclass Redis(\n    AbstractRedis, AsyncRedisModuleCommands, AsyncCoreCommands, AsyncSentinelCommands\n):\n    \"\"\"\n    Implementation of the Redis protocol.\n\n    This abstract class provides a Python interface to all Redis commands\n    and an implementation of the Redis protocol.\n\n    Pipelines derive from this, implementing how\n    the commands are sent and received to the Redis server. Based on\n    configuration, an instance will either use a ConnectionPool, or\n    Connection object to talk to redis.\n    \"\"\"\n\n    response_callbacks: MutableMapping[Union[str, bytes], ResponseCallbackT]\n\n    @classmethod\n    def from_url(\n        cls,\n        url: str,\n        single_connection_client: bool = False,\n        auto_close_connection_pool: Optional[bool] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Return a Redis client object configured from the given URL\n\n        For example::\n\n            redis://[[username]:[password]]@localhost:6379/0\n            rediss://[[username]:[password]]@localhost:6379/0\n            unix://[username@]/path/to/socket.sock?db=0[&password=password]\n\n        Three URL schemes are supported:\n\n        - `redis://` creates a TCP socket connection. See more at:\n          <https://www.iana.org/assignments/uri-schemes/prov/redis>\n        - `rediss://` creates a SSL wrapped TCP socket connection. See more at:\n          <https://www.iana.org/assignments/uri-schemes/prov/rediss>\n        - ``unix://``: creates a Unix Domain Socket connection.\n\n        The username, password, hostname, path and all querystring values\n        are passed through urllib.parse.unquote in order to replace any\n        percent-encoded values with their corresponding characters.\n\n        There are several ways to specify a database number. The first value\n        found will be used:\n\n        1. A ``db`` querystring option, e.g. redis://localhost?db=0\n\n        2. If using the redis:// or rediss:// schemes, the path argument\n               of the url, e.g. redis://localhost/0\n\n        3. A ``db`` keyword argument to this function.\n\n        If none of these options are specified, the default db=0 is used.\n\n        All querystring options are cast to their appropriate Python types.\n        Boolean arguments can be specified with string values \"True\"/\"False\"\n        or \"Yes\"/\"No\". Values that cannot be properly cast cause a\n        ``ValueError`` to be raised. Once parsed, the querystring arguments\n        and keyword arguments are passed to the ``ConnectionPool``'s\n        class initializer. In the case of conflicting arguments, querystring\n        arguments always win.\n\n        \"\"\"\n        connection_pool = ConnectionPool.from_url(url, **kwargs)\n        client = cls(\n            connection_pool=connection_pool,\n            single_connection_client=single_connection_client,\n        )\n        if auto_close_connection_pool is not None:\n            warnings.warn(\n                DeprecationWarning(\n                    '\"auto_close_connection_pool\" is deprecated '\n                    \"since version 5.0.1. \"\n                    \"Please create a ConnectionPool explicitly and \"\n                    \"provide to the Redis() constructor instead.\"\n                )\n            )\n        else:\n            auto_close_connection_pool = True\n        client.auto_close_connection_pool = auto_close_connection_pool\n        return client\n\n    @classmethod\n    def from_pool(\n        cls: Type[\"Redis\"],\n        connection_pool: ConnectionPool,\n    ) -> \"Redis\":\n        \"\"\"\n        Return a Redis client from the given connection pool.\n        The Redis client will take ownership of the connection pool and\n        close it when the Redis client is closed.\n        \"\"\"\n        client = cls(\n            connection_pool=connection_pool,\n        )\n        client.auto_close_connection_pool = True\n        return client\n\n    @deprecated_args(\n        args_to_warn=[\"retry_on_timeout\"],\n        reason=\"TimeoutError is included by default.\",\n        version=\"6.0.0\",\n    )\n    def __init__(\n        self,\n        *,\n        host: str = \"localhost\",\n        port: int = 6379,\n        db: Union[str, int] = 0,\n        password: Optional[str] = None,\n        socket_timeout: Optional[float] = None,\n        socket_connect_timeout: Optional[float] = None,\n        socket_keepalive: Optional[bool] = None,\n        socket_keepalive_options: Optional[Mapping[int, Union[int, bytes]]] = None,\n        connection_pool: Optional[ConnectionPool] = None,\n        unix_socket_path: Optional[str] = None,\n        encoding: str = \"utf-8\",\n        encoding_errors: str = \"strict\",\n        decode_responses: bool = False,\n        retry_on_timeout: bool = False,\n        retry: Retry = Retry(\n            backoff=ExponentialWithJitterBackoff(base=1, cap=10), retries=3\n        ),\n        retry_on_error: Optional[list] = None,\n        ssl: bool = False,\n        ssl_keyfile: Optional[str] = None,\n        ssl_certfile: Optional[str] = None,\n        ssl_cert_reqs: Union[str, VerifyMode] = \"required\",\n        ssl_ca_certs: Optional[str] = None,\n        ssl_ca_data: Optional[str] = None,\n        ssl_check_hostname: bool = True,\n        ssl_min_version: Optional[TLSVersion] = None,\n        ssl_ciphers: Optional[str] = None,\n        max_connections: Optional[int] = None,\n        single_connection_client: bool = False,\n        health_check_interval: int = 0,\n        client_name: Optional[str] = None,\n        lib_name: Optional[str] = \"redis-py\",\n        lib_version: Optional[str] = get_lib_version(),\n        username: Optional[str] = None,\n        auto_close_connection_pool: Optional[bool] = None,\n        redis_connect_func=None,\n        credential_provider: Optional[CredentialProvider] = None,\n        protocol: Optional[int] = 2,\n        event_dispatcher: Optional[EventDispatcher] = None,\n    ):\n        \"\"\"\n        Initialize a new Redis client.\n\n        To specify a retry policy for specific errors, you have two options:\n\n        1. Set the `retry_on_error` to a list of the error/s to retry on, and\n        you can also set `retry` to a valid `Retry` object(in case the default\n        one is not appropriate) - with this approach the retries will be triggered\n        on the default errors specified in the Retry object enriched with the\n        errors specified in `retry_on_error`.\n\n        2. Define a `Retry` object with configured 'supported_errors' and set\n        it to the `retry` parameter - with this approach you completely redefine\n        the errors on which retries will happen.\n\n        `retry_on_timeout` is deprecated - please include the TimeoutError\n        either in the Retry object or in the `retry_on_error` list.\n\n        When 'connection_pool' is provided - the retry configuration of the\n        provided pool will be used.\n        \"\"\"\n        kwargs: Dict[str, Any]\n        if event_dispatcher is None:\n            self._event_dispatcher = EventDispatcher()\n        else:\n            self._event_dispatcher = event_dispatcher\n        # auto_close_connection_pool only has an effect if connection_pool is\n        # None. It is assumed that if connection_pool is not None, the user\n        # wants to manage the connection pool themselves.\n        if auto_close_connection_pool is not None:\n            warnings.warn(\n                DeprecationWarning(\n                    '\"auto_close_connection_pool\" is deprecated '\n                    \"since version 5.0.1. \"\n                    \"Please create a ConnectionPool explicitly and \"\n                    \"provide to the Redis() constructor instead.\"\n                )\n            )\n        else:\n            auto_close_connection_pool = True\n\n        if not connection_pool:\n            # Create internal connection pool, expected to be closed by Redis instance\n            if not retry_on_error:\n                retry_on_error = []\n            kwargs = {\n                \"db\": db,\n                \"username\": username,\n                \"password\": password,\n                \"credential_provider\": credential_provider,\n                \"socket_timeout\": socket_timeout,\n                \"encoding\": encoding,\n                \"encoding_errors\": encoding_errors,\n                \"decode_responses\": decode_responses,\n                \"retry_on_error\": retry_on_error,\n                \"retry\": copy.deepcopy(retry),\n                \"max_connections\": max_connections,\n                \"health_check_interval\": health_check_interval,\n                \"client_name\": client_name,\n                \"lib_name\": lib_name,\n                \"lib_version\": lib_version,\n                \"redis_connect_func\": redis_connect_func,\n                \"protocol\": protocol,\n            }\n            # based on input, setup appropriate connection args\n            if unix_socket_path is not None:\n                kwargs.update(\n                    {\n                        \"path\": unix_socket_path,\n                        \"connection_class\": UnixDomainSocketConnection,\n                    }\n                )\n            else:\n                # TCP specific options\n                kwargs.update(\n                    {\n                        \"host\": host,\n                        \"port\": port,\n                        \"socket_connect_timeout\": socket_connect_timeout,\n                        \"socket_keepalive\": socket_keepalive,\n                        \"socket_keepalive_options\": socket_keepalive_options,\n                    }\n                )\n\n                if ssl:\n                    kwargs.update(\n                        {\n                            \"connection_class\": SSLConnection,\n                            \"ssl_keyfile\": ssl_keyfile,\n                            \"ssl_certfile\": ssl_certfile,\n                            \"ssl_cert_reqs\": ssl_cert_reqs,\n                            \"ssl_ca_certs\": ssl_ca_certs,\n                            \"ssl_ca_data\": ssl_ca_data,\n                            \"ssl_check_hostname\": ssl_check_hostname,\n                            \"ssl_min_version\": ssl_min_version,\n                            \"ssl_ciphers\": ssl_ciphers,\n                        }\n                    )\n            # This arg only used if no pool is passed in\n            self.auto_close_connection_pool = auto_close_connection_pool\n            connection_pool = ConnectionPool(**kwargs)\n            self._event_dispatcher.dispatch(\n                AfterPooledConnectionsInstantiationEvent(\n                    [connection_pool], ClientType.ASYNC, credential_provider\n                )\n            )\n        else:\n            # If a pool is passed in, do not close it\n            self.auto_close_connection_pool = False\n            self._event_dispatcher.dispatch(\n                AfterPooledConnectionsInstantiationEvent(\n                    [connection_pool], ClientType.ASYNC, credential_provider\n                )\n            )\n\n        self.connection_pool = connection_pool\n        self.single_connection_client = single_connection_client\n        self.connection: Optional[Connection] = None\n\n        self.response_callbacks = CaseInsensitiveDict(_RedisCallbacks)\n\n        if self.connection_pool.connection_kwargs.get(\"protocol\") in [\"3\", 3]:\n            self.response_callbacks.update(_RedisCallbacksRESP3)\n        else:\n            self.response_callbacks.update(_RedisCallbacksRESP2)\n\n        # If using a single connection client, we need to lock creation-of and use-of\n        # the client in order to avoid race conditions such as using asyncio.gather\n        # on a set of redis commands\n        self._single_conn_lock = asyncio.Lock()\n\n    def __repr__(self):\n        return (\n            f\"<{self.__class__.__module__}.{self.__class__.__name__}\"\n            f\"({self.connection_pool!r})>\"\n        )\n\n    def __await__(self):\n        return self.initialize().__await__()\n\n    async def initialize(self: _RedisT) -> _RedisT:\n        if self.single_connection_client:\n            async with self._single_conn_lock:\n                if self.connection is None:\n                    self.connection = await self.connection_pool.get_connection()\n\n            self._event_dispatcher.dispatch(\n                AfterSingleConnectionInstantiationEvent(\n                    self.connection, ClientType.ASYNC, self._single_conn_lock\n                )\n            )\n        return self\n\n    def set_response_callback(self, command: str, callback: ResponseCallbackT):\n        \"\"\"Set a custom Response Callback\"\"\"\n        self.response_callbacks[command] = callback\n\n    def get_encoder(self):\n        \"\"\"Get the connection pool's encoder\"\"\"\n        return self.connection_pool.get_encoder()\n\n    def get_connection_kwargs(self):\n        \"\"\"Get the connection's key-word arguments\"\"\"\n        return self.connection_pool.connection_kwargs\n\n    def get_retry(self) -> Optional[Retry]:\n        return self.get_connection_kwargs().get(\"retry\")\n\n    def set_retry(self, retry: Retry) -> None:\n        self.get_connection_kwargs().update({\"retry\": retry})\n        self.connection_pool.set_retry(retry)\n\n    def load_external_module(self, funcname, func):\n        \"\"\"\n        This function can be used to add externally defined redis modules,\n        and their namespaces to the redis client.\n\n        funcname - A string containing the name of the function to create\n        func - The function, being added to this class.\n\n        ex: Assume that one has a custom redis module named foomod that\n        creates command named 'foo.dothing' and 'foo.anotherthing' in redis.\n        To load function functions into this namespace:\n\n        from redis import Redis\n        from foomodule import F\n        r = Redis()\n        r.load_external_module(\"foo\", F)\n        r.foo().dothing('your', 'arguments')\n\n        For a concrete example see the reimport of the redisjson module in\n        tests/test_connection.py::test_loading_external_modules\n        \"\"\"\n        setattr(self, funcname, func)\n\n    def pipeline(\n        self, transaction: bool = True, shard_hint: Optional[str] = None\n    ) -> \"Pipeline\":\n        \"\"\"\n        Return a new pipeline object that can queue multiple commands for\n        later execution. ``transaction`` indicates whether all commands\n        should be executed atomically. Apart from making a group of operations\n        atomic, pipelines are useful for reducing the back-and-forth overhead\n        between the client and server.\n        \"\"\"\n        return Pipeline(\n            self.connection_pool, self.response_callbacks, transaction, shard_hint\n        )\n\n    async def transaction(\n        self,\n        func: Callable[[\"Pipeline\"], Union[Any, Awaitable[Any]]],\n        *watches: KeyT,\n        shard_hint: Optional[str] = None,\n        value_from_callable: bool = False,\n        watch_delay: Optional[float] = None,\n    ):\n        \"\"\"\n        Convenience method for executing the callable `func` as a transaction\n        while watching all keys specified in `watches`. The 'func' callable\n        should expect a single argument which is a Pipeline object.\n        \"\"\"\n        pipe: Pipeline\n        async with self.pipeline(True, shard_hint) as pipe:\n            while True:\n                try:\n                    if watches:\n                        await pipe.watch(*watches)\n                    func_value = func(pipe)\n                    if inspect.isawaitable(func_value):\n                        func_value = await func_value\n                    exec_value = await pipe.execute()\n                    return func_value if value_from_callable else exec_value\n                except WatchError:\n                    if watch_delay is not None and watch_delay > 0:\n                        await asyncio.sleep(watch_delay)\n                    continue\n\n    def lock(\n        self,\n        name: KeyT,\n        timeout: Optional[float] = None,\n        sleep: float = 0.1,\n        blocking: bool = True,\n        blocking_timeout: Optional[float] = None,\n        lock_class: Optional[Type[Lock]] = None,\n        thread_local: bool = True,\n        raise_on_release_error: bool = True,\n    ) -> Lock:\n        \"\"\"\n        Return a new Lock object using key ``name`` that mimics\n        the behavior of threading.Lock.\n\n        If specified, ``timeout`` indicates a maximum life for the lock.\n        By default, it will remain locked until release() is called.\n\n        ``sleep`` indicates the amount of time to sleep per loop iteration\n        when the lock is in blocking mode and another client is currently\n        holding the lock.\n\n        ``blocking`` indicates whether calling ``acquire`` should block until\n        the lock has been acquired or to fail immediately, causing ``acquire``\n        to return False and the lock not being acquired. Defaults to True.\n        Note this value can be overridden by passing a ``blocking``\n        argument to ``acquire``.\n\n        ``blocking_timeout`` indicates the maximum amount of time in seconds to\n        spend trying to acquire the lock. A value of ``None`` indicates\n        continue trying forever. ``blocking_timeout`` can be specified as a\n        float or integer, both representing the number of seconds to wait.\n\n        ``lock_class`` forces the specified lock implementation. Note that as\n        of redis-py 3.0, the only lock class we implement is ``Lock`` (which is\n        a Lua-based lock). So, it's unlikely you'll need this parameter, unless\n        you have created your own custom lock class.\n\n        ``thread_local`` indicates whether the lock token is placed in\n        thread-local storage. By default, the token is placed in thread local\n        storage so that a thread only sees its token, not a token set by\n        another thread. Consider the following timeline:\n\n            time: 0, thread-1 acquires `my-lock`, with a timeout of 5 seconds.\n                     thread-1 sets the token to \"abc\"\n            time: 1, thread-2 blocks trying to acquire `my-lock` using the\n                     Lock instance.\n            time: 5, thread-1 has not yet completed. redis expires the lock\n                     key.\n            time: 5, thread-2 acquired `my-lock` now that it's available.\n                     thread-2 sets the token to \"xyz\"\n            time: 6, thread-1 finishes its work and calls release(). if the\n                     token is *not* stored in thread local storage, then\n                     thread-1 would see the token value as \"xyz\" and would be\n                     able to successfully release the thread-2's lock.\n\n        ``raise_on_release_error`` indicates whether to raise an exception when\n        the lock is no longer owned when exiting the context manager. By default,\n        this is True, meaning an exception will be raised. If False, the warning\n        will be logged and the exception will be suppressed.\n\n        In some use cases it's necessary to disable thread local storage. For\n        example, if you have code where one thread acquires a lock and passes\n        that lock instance to a worker thread to release later. If thread\n        local storage isn't disabled in this case, the worker thread won't see\n        the token set by the thread that acquired the lock. Our assumption\n        is that these cases aren't common and as such default to using\n        thread local storage.\"\"\"\n        if lock_class is None:\n            lock_class = Lock\n        return lock_class(\n            self,\n            name,\n            timeout=timeout,\n            sleep=sleep,\n            blocking=blocking,\n            blocking_timeout=blocking_timeout,\n            thread_local=thread_local,\n            raise_on_release_error=raise_on_release_error,\n        )\n\n    def pubsub(self, **kwargs) -> \"PubSub\":\n        \"\"\"\n        Return a Publish/Subscribe object. With this object, you can\n        subscribe to channels and listen for messages that get published to\n        them.\n        \"\"\"\n        return PubSub(\n            self.connection_pool, event_dispatcher=self._event_dispatcher, **kwargs\n        )\n\n    def monitor(self) -> \"Monitor\":\n        return Monitor(self.connection_pool)\n\n    def client(self) -> \"Redis\":\n        return self.__class__(\n            connection_pool=self.connection_pool, single_connection_client=True\n        )\n\n    async def __aenter__(self: _RedisT) -> _RedisT:\n        return await self.initialize()\n\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        await self.aclose()\n\n    _DEL_MESSAGE = \"Unclosed Redis client\"\n\n    # passing _warnings and _grl as argument default since they may be gone\n    # by the time __del__ is called at shutdown\n    def __del__(\n        self,\n        _warn: Any = warnings.warn,\n        _grl: Any = asyncio.get_running_loop,\n    ) -> None:\n        if hasattr(self, \"connection\") and (self.connection is not None):\n            _warn(f\"Unclosed client session {self!r}\", ResourceWarning, source=self)\n            try:\n                context = {\"client\": self, \"message\": self._DEL_MESSAGE}\n                _grl().call_exception_handler(context)\n            except RuntimeError:\n                pass\n            self.connection._close()\n\n    async def aclose(self, close_connection_pool: Optional[bool] = None) -> None:\n        \"\"\"\n        Closes Redis client connection\n\n        Args:\n            close_connection_pool:\n                decides whether to close the connection pool used by this Redis client,\n                overriding Redis.auto_close_connection_pool.\n                By default, let Redis.auto_close_connection_pool decide\n                whether to close the connection pool.\n        \"\"\"\n        conn = self.connection\n        if conn:\n            self.connection = None\n            await self.connection_pool.release(conn)\n        if close_connection_pool or (\n            close_connection_pool is None and self.auto_close_connection_pool\n        ):\n            await self.connection_pool.disconnect()\n\n    @deprecated_function(version=\"5.0.1\", reason=\"Use aclose() instead\", name=\"close\")\n    async def close(self, close_connection_pool: Optional[bool] = None) -> None:\n        \"\"\"\n        Alias for aclose(), for backwards compatibility\n        \"\"\"\n        await self.aclose(close_connection_pool)\n\n    async def _send_command_parse_response(self, conn, command_name, *args, **options):\n        \"\"\"\n        Send a command and parse the response\n        \"\"\"\n        await conn.send_command(*args)\n        return await self.parse_response(conn, command_name, **options)\n\n    async def _close_connection(self, conn: Connection):\n        \"\"\"\n        Close the connection before retrying.\n\n        The supported exceptions are already checked in the\n        retry object so we don't need to do it here.\n\n        After we disconnect the connection, it will try to reconnect and\n        do a health check as part of the send_command logic(on connection level).\n        \"\"\"\n        await conn.disconnect()\n\n    # COMMAND EXECUTION AND PROTOCOL PARSING\n    async def execute_command(self, *args, **options):\n        \"\"\"Execute a command and return a parsed response\"\"\"\n        await self.initialize()\n        pool = self.connection_pool\n        command_name = args[0]\n        conn = self.connection or await pool.get_connection()\n\n        if self.single_connection_client:\n            await self._single_conn_lock.acquire()\n        try:\n            return await conn.retry.call_with_retry(\n                lambda: self._send_command_parse_response(\n                    conn, command_name, *args, **options\n                ),\n                lambda _: self._close_connection(conn),\n            )\n        finally:\n            if self.single_connection_client:\n                self._single_conn_lock.release()\n            if not self.connection:\n                await pool.release(conn)\n\n    async def parse_response(\n        self, connection: Connection, command_name: Union[str, bytes], **options\n    ):\n        \"\"\"Parses a response from the Redis server\"\"\"\n        try:\n            if NEVER_DECODE in options:\n                response = await connection.read_response(disable_decoding=True)\n                options.pop(NEVER_DECODE)\n            else:\n                response = await connection.read_response()\n        except ResponseError:\n            if EMPTY_RESPONSE in options:\n                return options[EMPTY_RESPONSE]\n            raise\n\n        if EMPTY_RESPONSE in options:\n            options.pop(EMPTY_RESPONSE)\n\n        # Remove keys entry, it needs only for cache.\n        options.pop(\"keys\", None)\n\n        if command_name in self.response_callbacks:\n            # Mypy bug: https://github.com/python/mypy/issues/10977\n            command_name = cast(str, command_name)\n            retval = self.response_callbacks[command_name](response, **options)\n            return await retval if inspect.isawaitable(retval) else retval\n        return response\n\n\nStrictRedis = Redis\n\n\nclass MonitorCommandInfo(TypedDict):\n    time: float\n    db: int\n    client_address: str\n    client_port: str\n    client_type: str\n    command: str\n\n\nclass Monitor:\n    \"\"\"\n    Monitor is useful for handling the MONITOR command to the redis server.\n    next_command() method returns one command from monitor\n    listen() method yields commands from monitor.\n    \"\"\"\n\n    monitor_re = re.compile(r\"\\[(\\d+) (.*?)\\] (.*)\")\n    command_re = re.compile(r'\"(.*?)(?<!\\\\)\"')\n\n    def __init__(self, connection_pool: ConnectionPool):\n        self.connection_pool = connection_pool\n        self.connection: Optional[Connection] = None\n\n    async def connect(self):\n        if self.connection is None:\n            self.connection = await self.connection_pool.get_connection()\n\n    async def __aenter__(self):\n        await self.connect()\n        await self.connection.send_command(\"MONITOR\")\n        # check that monitor returns 'OK', but don't return it to user\n        response = await self.connection.read_response()\n        if not bool_ok(response):\n            raise RedisError(f\"MONITOR failed: {response}\")\n        return self\n\n    async def __aexit__(self, *args):\n        await self.connection.disconnect()\n        await self.connection_pool.release(self.connection)\n\n    async def next_command(self) -> MonitorCommandInfo:\n        \"\"\"Parse the response from a monitor command\"\"\"\n        await self.connect()\n        response = await self.connection.read_response()\n        if isinstance(response, bytes):\n            response = self.connection.encoder.decode(response, force=True)\n        command_time, command_data = response.split(\" \", 1)\n        m = self.monitor_re.match(command_data)\n        db_id, client_info, command = m.groups()\n        command = \" \".join(self.command_re.findall(command))\n        # Redis escapes double quotes because each piece of the command\n        # string is surrounded by double quotes. We don't have that\n        # requirement so remove the escaping and leave the quote.\n        command = command.replace('\\\\\"', '\"')\n\n        if client_info == \"lua\":\n            client_address = \"lua\"\n            client_port = \"\"\n            client_type = \"lua\"\n        elif client_info.startswith(\"unix\"):\n            client_address = \"unix\"\n            client_port = client_info[5:]\n            client_type = \"unix\"\n        else:\n            # use rsplit as ipv6 addresses contain colons\n            client_address, client_port = client_info.rsplit(\":\", 1)\n            client_type = \"tcp\"\n        return {\n            \"time\": float(command_time),\n            \"db\": int(db_id),\n            \"client_address\": client_address,\n            \"client_port\": client_port,\n            \"client_type\": client_type,\n            \"command\": command,\n        }\n\n    async def listen(self) -> AsyncIterator[MonitorCommandInfo]:\n        \"\"\"Listen for commands coming to the server.\"\"\"\n        while True:\n            yield await self.next_command()\n\n\nclass PubSub:\n    \"\"\"\n    PubSub provides publish, subscribe and listen support to Redis channels.\n\n    After subscribing to one or more channels, the listen() method will block\n    until a message arrives on one of the subscribed channels. That message\n    will be returned and it's safe to start listening again.\n    \"\"\"\n\n    PUBLISH_MESSAGE_TYPES = (\"message\", \"pmessage\")\n    UNSUBSCRIBE_MESSAGE_TYPES = (\"unsubscribe\", \"punsubscribe\")\n    HEALTH_CHECK_MESSAGE = \"redis-py-health-check\"\n\n    def __init__(\n        self,\n        connection_pool: ConnectionPool,\n        shard_hint: Optional[str] = None,\n        ignore_subscribe_messages: bool = False,\n        encoder=None,\n        push_handler_func: Optional[Callable] = None,\n        event_dispatcher: Optional[\"EventDispatcher\"] = None,\n    ):\n        if event_dispatcher is None:\n            self._event_dispatcher = EventDispatcher()\n        else:\n            self._event_dispatcher = event_dispatcher\n        self.connection_pool = connection_pool\n        self.shard_hint = shard_hint\n        self.ignore_subscribe_messages = ignore_subscribe_messages\n        self.connection = None\n        # we need to know the encoding options for this connection in order\n        # to lookup channel and pattern names for callback handlers.\n        self.encoder = encoder\n        self.push_handler_func = push_handler_func\n        if self.encoder is None:\n            self.encoder = self.connection_pool.get_encoder()\n        if self.encoder.decode_responses:\n            self.health_check_response = [\n                [\"pong\", self.HEALTH_CHECK_MESSAGE],\n                self.HEALTH_CHECK_MESSAGE,\n            ]\n        else:\n            self.health_check_response = [\n                [b\"pong\", self.encoder.encode(self.HEALTH_CHECK_MESSAGE)],\n                self.encoder.encode(self.HEALTH_CHECK_MESSAGE),\n            ]\n        if self.push_handler_func is None:\n            _set_info_logger()\n        self.channels = {}\n        self.pending_unsubscribe_channels = set()\n        self.patterns = {}\n        self.pending_unsubscribe_patterns = set()\n        self._lock = asyncio.Lock()\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        await self.aclose()\n\n    def __del__(self):\n        if self.connection:\n            self.connection.deregister_connect_callback(self.on_connect)\n\n    async def aclose(self):\n        # In case a connection property does not yet exist\n        # (due to a crash earlier in the Redis() constructor), return\n        # immediately as there is nothing to clean-up.\n        if not hasattr(self, \"connection\"):\n            return\n        async with self._lock:\n            if self.connection:\n                await self.connection.disconnect()\n                self.connection.deregister_connect_callback(self.on_connect)\n                await self.connection_pool.release(self.connection)\n                self.connection = None\n            self.channels = {}\n            self.pending_unsubscribe_channels = set()\n            self.patterns = {}\n            self.pending_unsubscribe_patterns = set()\n\n    @deprecated_function(version=\"5.0.1\", reason=\"Use aclose() instead\", name=\"close\")\n    async def close(self) -> None:\n        \"\"\"Alias for aclose(), for backwards compatibility\"\"\"\n        await self.aclose()\n\n    @deprecated_function(version=\"5.0.1\", reason=\"Use aclose() instead\", name=\"reset\")\n    async def reset(self) -> None:\n        \"\"\"Alias for aclose(), for backwards compatibility\"\"\"\n        await self.aclose()\n\n    async def on_connect(self, connection: Connection):\n        \"\"\"Re-subscribe to any channels and patterns previously subscribed to\"\"\"\n        # NOTE: for python3, we can't pass bytestrings as keyword arguments\n        # so we need to decode channel/pattern names back to unicode strings\n        # before passing them to [p]subscribe.\n        self.pending_unsubscribe_channels.clear()\n        self.pending_unsubscribe_patterns.clear()\n        if self.channels:\n            channels = {}\n            for k, v in self.channels.items():\n                channels[self.encoder.decode(k, force=True)] = v\n            await self.subscribe(**channels)\n        if self.patterns:\n            patterns = {}\n            for k, v in self.patterns.items():\n                patterns[self.encoder.decode(k, force=True)] = v\n            await self.psubscribe(**patterns)\n\n    @property\n    def subscribed(self):\n        \"\"\"Indicates if there are subscriptions to any channels or patterns\"\"\"\n        return bool(self.channels or self.patterns)\n\n    async def execute_command(self, *args: EncodableT):\n        \"\"\"Execute a publish/subscribe command\"\"\"\n\n        # NOTE: don't parse the response in this function -- it could pull a\n        # legitimate message off the stack if the connection is already\n        # subscribed to one or more channels\n\n        await self.connect()\n        connection = self.connection\n        kwargs = {\"check_health\": not self.subscribed}\n        await self._execute(connection, connection.send_command, *args, **kwargs)\n\n    async def connect(self):\n        \"\"\"\n        Ensure that the PubSub is connected\n        \"\"\"\n        if self.connection is None:\n            self.connection = await self.connection_pool.get_connection()\n            # register a callback that re-subscribes to any channels we\n            # were listening to when we were disconnected\n            self.connection.register_connect_callback(self.on_connect)\n        else:\n            await self.connection.connect()\n        if self.push_handler_func is not None:\n            self.connection._parser.set_pubsub_push_handler(self.push_handler_func)\n\n        self._event_dispatcher.dispatch(\n            AfterPubSubConnectionInstantiationEvent(\n                self.connection, self.connection_pool, ClientType.ASYNC, self._lock\n            )\n        )\n\n    async def _reconnect(self, conn):\n        \"\"\"\n        Try to reconnect\n        \"\"\"\n        await conn.disconnect()\n        await conn.connect()\n\n    async def _execute(self, conn, command, *args, **kwargs):\n        \"\"\"\n        Connect manually upon disconnection. If the Redis server is down,\n        this will fail and raise a ConnectionError as desired.\n        After reconnection, the ``on_connect`` callback should have been\n        called by the # connection to resubscribe us to any channels and\n        patterns we were previously listening to\n        \"\"\"\n        return await conn.retry.call_with_retry(\n            lambda: command(*args, **kwargs),\n            lambda _: self._reconnect(conn),\n        )\n\n    async def parse_response(self, block: bool = True, timeout: float = 0):\n        \"\"\"Parse the response from a publish/subscribe command\"\"\"\n        conn = self.connection\n        if conn is None:\n            raise RuntimeError(\n                \"pubsub connection not set: \"\n                \"did you forget to call subscribe() or psubscribe()?\"\n            )\n\n        await self.check_health()\n\n        if not conn.is_connected:\n            await conn.connect()\n\n        read_timeout = None if block else timeout\n        response = await self._execute(\n            conn,\n            conn.read_response,\n            timeout=read_timeout,\n            disconnect_on_error=False,\n            push_request=True,\n        )\n\n        if conn.health_check_interval and response in self.health_check_response:\n            # ignore the health check message as user might not expect it\n            return None\n        return response\n\n    async def check_health(self):\n        conn = self.connection\n        if conn is None:\n            raise RuntimeError(\n                \"pubsub connection not set: \"\n                \"did you forget to call subscribe() or psubscribe()?\"\n            )\n\n        if (\n            conn.health_check_interval\n            and asyncio.get_running_loop().time() > conn.next_health_check\n        ):\n            await conn.send_command(\n                \"PING\", self.HEALTH_CHECK_MESSAGE, check_health=False\n            )\n\n    def _normalize_keys(self, data: _NormalizeKeysT) -> _NormalizeKeysT:\n        \"\"\"\n        normalize channel/pattern names to be either bytes or strings\n        based on whether responses are automatically decoded. this saves us\n        from coercing the value for each message coming in.\n        \"\"\"\n        encode = self.encoder.encode\n        decode = self.encoder.decode\n        return {decode(encode(k)): v for k, v in data.items()}  # type: ignore[return-value]  # noqa: E501\n\n    async def psubscribe(self, *args: ChannelT, **kwargs: PubSubHandler):\n        \"\"\"\n        Subscribe to channel patterns. Patterns supplied as keyword arguments\n        expect a pattern name as the key and a callable as the value. A\n        pattern's callable will be invoked automatically when a message is\n        received on that pattern rather than producing a message via\n        ``listen()``.\n        \"\"\"\n        parsed_args = list_or_args((args[0],), args[1:]) if args else args\n        new_patterns: Dict[ChannelT, PubSubHandler] = dict.fromkeys(parsed_args)\n        # Mypy bug: https://github.com/python/mypy/issues/10970\n        new_patterns.update(kwargs)  # type: ignore[arg-type]\n        ret_val = await self.execute_command(\"PSUBSCRIBE\", *new_patterns.keys())\n        # update the patterns dict AFTER we send the command. we don't want to\n        # subscribe twice to these patterns, once for the command and again\n        # for the reconnection.\n        new_patterns = self._normalize_keys(new_patterns)\n        self.patterns.update(new_patterns)\n        self.pending_unsubscribe_patterns.difference_update(new_patterns)\n        return ret_val\n\n    def punsubscribe(self, *args: ChannelT) -> Awaitable:\n        \"\"\"\n        Unsubscribe from the supplied patterns. If empty, unsubscribe from\n        all patterns.\n        \"\"\"\n        patterns: Iterable[ChannelT]\n        if args:\n            parsed_args = list_or_args((args[0],), args[1:])\n            patterns = self._normalize_keys(dict.fromkeys(parsed_args)).keys()\n        else:\n            parsed_args = []\n            patterns = self.patterns\n        self.pending_unsubscribe_patterns.update(patterns)\n        return self.execute_command(\"PUNSUBSCRIBE\", *parsed_args)\n\n    async def subscribe(self, *args: ChannelT, **kwargs: Callable):\n        \"\"\"\n        Subscribe to channels. Channels supplied as keyword arguments expect\n        a channel name as the key and a callable as the value. A channel's\n        callable will be invoked automatically when a message is received on\n        that channel rather than producing a message via ``listen()`` or\n        ``get_message()``.\n        \"\"\"\n        parsed_args = list_or_args((args[0],), args[1:]) if args else ()\n        new_channels = dict.fromkeys(parsed_args)\n        # Mypy bug: https://github.com/python/mypy/issues/10970\n        new_channels.update(kwargs)  # type: ignore[arg-type]\n        ret_val = await self.execute_command(\"SUBSCRIBE\", *new_channels.keys())\n        # update the channels dict AFTER we send the command. we don't want to\n        # subscribe twice to these channels, once for the command and again\n        # for the reconnection.\n        new_channels = self._normalize_keys(new_channels)\n        self.channels.update(new_channels)\n        self.pending_unsubscribe_channels.difference_update(new_channels)\n        return ret_val\n\n    def unsubscribe(self, *args) -> Awaitable:\n        \"\"\"\n        Unsubscribe from the supplied channels. If empty, unsubscribe from\n        all channels\n        \"\"\"\n        if args:\n            parsed_args = list_or_args(args[0], args[1:])\n            channels = self._normalize_keys(dict.fromkeys(parsed_args))\n        else:\n            parsed_args = []\n            channels = self.channels\n        self.pending_unsubscribe_channels.update(channels)\n        return self.execute_command(\"UNSUBSCRIBE\", *parsed_args)\n\n    async def listen(self) -> AsyncIterator:\n        \"\"\"Listen for messages on channels this client has been subscribed to\"\"\"\n        while self.subscribed:\n            response = await self.handle_message(await self.parse_response(block=True))\n            if response is not None:\n                yield response\n\n    async def get_message(\n        self, ignore_subscribe_messages: bool = False, timeout: Optional[float] = 0.0\n    ):\n        \"\"\"\n        Get the next message if one is available, otherwise None.\n\n        If timeout is specified, the system will wait for `timeout` seconds\n        before returning. Timeout should be specified as a floating point\n        number or None to wait indefinitely.\n        \"\"\"\n        response = await self.parse_response(block=(timeout is None), timeout=timeout)\n        if response:\n            return await self.handle_message(response, ignore_subscribe_messages)\n        return None\n\n    def ping(self, message=None) -> Awaitable:\n        \"\"\"\n        Ping the Redis server\n        \"\"\"\n        args = [\"PING\", message] if message is not None else [\"PING\"]\n        return self.execute_command(*args)\n\n    async def handle_message(self, response, ignore_subscribe_messages=False):\n        \"\"\"\n        Parses a pub/sub message. If the channel or pattern was subscribed to\n        with a message handler, the handler is invoked instead of a parsed\n        message being returned.\n        \"\"\"\n        if response is None:\n            return None\n        if isinstance(response, bytes):\n            response = [b\"pong\", response] if response != b\"PONG\" else [b\"pong\", b\"\"]\n        message_type = str_if_bytes(response[0])\n        if message_type == \"pmessage\":\n            message = {\n                \"type\": message_type,\n                \"pattern\": response[1],\n                \"channel\": response[2],\n                \"data\": response[3],\n            }\n        elif message_type == \"pong\":\n            message = {\n                \"type\": message_type,\n                \"pattern\": None,\n                \"channel\": None,\n                \"data\": response[1],\n            }\n        else:\n            message = {\n                \"type\": message_type,\n                \"pattern\": None,\n                \"channel\": response[1],\n                \"data\": response[2],\n            }\n\n        # if this is an unsubscribe message, remove it from memory\n        if message_type in self.UNSUBSCRIBE_MESSAGE_TYPES:\n            if message_type == \"punsubscribe\":\n                pattern = response[1]\n                if pattern in self.pending_unsubscribe_patterns:\n                    self.pending_unsubscribe_patterns.remove(pattern)\n                    self.patterns.pop(pattern, None)\n            else:\n                channel = response[1]\n                if channel in self.pending_unsubscribe_channels:\n                    self.pending_unsubscribe_channels.remove(channel)\n                    self.channels.pop(channel, None)\n\n        if message_type in self.PUBLISH_MESSAGE_TYPES:\n            # if there's a message handler, invoke it\n            if message_type == \"pmessage\":\n                handler = self.patterns.get(message[\"pattern\"], None)\n            else:\n                handler = self.channels.get(message[\"channel\"], None)\n            if handler:\n                if inspect.iscoroutinefunction(handler):\n                    await handler(message)\n                else:\n                    handler(message)\n                return None\n        elif message_type != \"pong\":\n            # this is a subscribe/unsubscribe message. ignore if we don't\n            # want them\n            if ignore_subscribe_messages or self.ignore_subscribe_messages:\n                return None\n\n        return message\n\n    async def run(\n        self,\n        *,\n        exception_handler: Optional[\"PSWorkerThreadExcHandlerT\"] = None,\n        poll_timeout: float = 1.0,\n    ) -> None:\n        \"\"\"Process pub/sub messages using registered callbacks.\n\n        This is the equivalent of :py:meth:`redis.PubSub.run_in_thread` in\n        redis-py, but it is a coroutine. To launch it as a separate task, use\n        ``asyncio.create_task``:\n\n            >>> task = asyncio.create_task(pubsub.run())\n\n        To shut it down, use asyncio cancellation:\n\n            >>> task.cancel()\n            >>> await task\n        \"\"\"\n        for channel, handler in self.channels.items():\n            if handler is None:\n                raise PubSubError(f\"Channel: '{channel}' has no handler registered\")\n        for pattern, handler in self.patterns.items():\n            if handler is None:\n                raise PubSubError(f\"Pattern: '{pattern}' has no handler registered\")\n\n        await self.connect()\n        while True:\n            try:\n                await self.get_message(\n                    ignore_subscribe_messages=True, timeout=poll_timeout\n                )\n            except asyncio.CancelledError:\n                raise\n            except BaseException as e:\n                if exception_handler is None:\n                    raise\n                res = exception_handler(e, self)\n                if inspect.isawaitable(res):\n                    await res\n            # Ensure that other tasks on the event loop get a chance to run\n            # if we didn't have to block for I/O anywhere.\n            await asyncio.sleep(0)\n\n\nclass PubsubWorkerExceptionHandler(Protocol):\n    def __call__(self, e: BaseException, pubsub: PubSub): ...\n\n\nclass AsyncPubsubWorkerExceptionHandler(Protocol):\n    async def __call__(self, e: BaseException, pubsub: PubSub): ...\n\n\nPSWorkerThreadExcHandlerT = Union[\n    PubsubWorkerExceptionHandler, AsyncPubsubWorkerExceptionHandler\n]\n\n\nCommandT = Tuple[Tuple[Union[str, bytes], ...], Mapping[str, Any]]\nCommandStackT = List[CommandT]\n\n\nclass Pipeline(Redis):  # lgtm [py/init-calls-subclass]\n    \"\"\"\n    Pipelines provide a way to transmit multiple commands to the Redis server\n    in one transmission.  This is convenient for batch processing, such as\n    saving all the values in a list to Redis.\n\n    All commands executed within a pipeline(when running in transactional mode,\n    which is the default behavior) are wrapped with MULTI and EXEC\n    calls. This guarantees all commands executed in the pipeline will be\n    executed atomically.\n\n    Any command raising an exception does *not* halt the execution of\n    subsequent commands in the pipeline. Instead, the exception is caught\n    and its instance is placed into the response list returned by execute().\n    Code iterating over the response list should be able to deal with an\n    instance of an exception as a potential value. In general, these will be\n    ResponseError exceptions, such as those raised when issuing a command\n    on a key of a different datatype.\n    \"\"\"\n\n    UNWATCH_COMMANDS = {\"DISCARD\", \"EXEC\", \"UNWATCH\"}\n\n    def __init__(\n        self,\n        connection_pool: ConnectionPool,\n        response_callbacks: MutableMapping[Union[str, bytes], ResponseCallbackT],\n        transaction: bool,\n        shard_hint: Optional[str],\n    ):\n        self.connection_pool = connection_pool\n        self.connection = None\n        self.response_callbacks = response_callbacks\n        self.is_transaction = transaction\n        self.shard_hint = shard_hint\n        self.watching = False\n        self.command_stack: CommandStackT = []\n        self.scripts: Set[Script] = set()\n        self.explicit_transaction = False\n\n    async def __aenter__(self: _RedisT) -> _RedisT:\n        return self\n\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        await self.reset()\n\n    def __await__(self):\n        return self._async_self().__await__()\n\n    _DEL_MESSAGE = \"Unclosed Pipeline client\"\n\n    def __len__(self):\n        return len(self.command_stack)\n\n    def __bool__(self):\n        \"\"\"Pipeline instances should always evaluate to True\"\"\"\n        return True\n\n    async def _async_self(self):\n        return self\n\n    async def reset(self):\n        self.command_stack = []\n        self.scripts = set()\n        # make sure to reset the connection state in the event that we were\n        # watching something\n        if self.watching and self.connection:\n            try:\n                # call this manually since our unwatch or\n                # immediate_execute_command methods can call reset()\n                await self.connection.send_command(\"UNWATCH\")\n                await self.connection.read_response()\n            except ConnectionError:\n                # disconnect will also remove any previous WATCHes\n                if self.connection:\n                    await self.connection.disconnect()\n        # clean up the other instance attributes\n        self.watching = False\n        self.explicit_transaction = False\n        # we can safely return the connection to the pool here since we're\n        # sure we're no longer WATCHing anything\n        if self.connection:\n            await self.connection_pool.release(self.connection)\n            self.connection = None\n\n    async def aclose(self) -> None:\n        \"\"\"Alias for reset(), a standard method name for cleanup\"\"\"\n        await self.reset()\n\n    def multi(self):\n        \"\"\"\n        Start a transactional block of the pipeline after WATCH commands\n        are issued. End the transactional block with `execute`.\n        \"\"\"\n        if self.explicit_transaction:\n            raise RedisError(\"Cannot issue nested calls to MULTI\")\n        if self.command_stack:\n            raise RedisError(\n                \"Commands without an initial WATCH have already been issued\"\n            )\n        self.explicit_transaction = True\n\n    def execute_command(\n        self, *args, **kwargs\n    ) -> Union[\"Pipeline\", Awaitable[\"Pipeline\"]]:\n        if (self.watching or args[0] == \"WATCH\") and not self.explicit_transaction:\n            return self.immediate_execute_command(*args, **kwargs)\n        return self.pipeline_execute_command(*args, **kwargs)\n\n    async def _disconnect_reset_raise_on_watching(\n        self,\n        conn: Connection,\n        error: Exception,\n    ):\n        \"\"\"\n        Close the connection reset watching state and\n        raise an exception if we were watching.\n\n        The supported exceptions are already checked in the\n        retry object so we don't need to do it here.\n\n        After we disconnect the connection, it will try to reconnect and\n        do a health check as part of the send_command logic(on connection level).\n        \"\"\"\n        await conn.disconnect()\n        # if we were already watching a variable, the watch is no longer\n        # valid since this connection has died. raise a WatchError, which\n        # indicates the user should retry this transaction.\n        if self.watching:\n            await self.reset()\n            raise WatchError(\n                f\"A {type(error).__name__} occurred while watching one or more keys\"\n            )\n\n    async def immediate_execute_command(self, *args, **options):\n        \"\"\"\n        Execute a command immediately, but don't auto-retry on the supported\n        errors for retry if we're already WATCHing a variable.\n        Used when issuing WATCH or subsequent commands retrieving their values but before\n        MULTI is called.\n        \"\"\"\n        command_name = args[0]\n        conn = self.connection\n        # if this is the first call, we need a connection\n        if not conn:\n            conn = await self.connection_pool.get_connection()\n            self.connection = conn\n\n        return await conn.retry.call_with_retry(\n            lambda: self._send_command_parse_response(\n                conn, command_name, *args, **options\n            ),\n            lambda error: self._disconnect_reset_raise_on_watching(conn, error),\n        )\n\n    def pipeline_execute_command(self, *args, **options):\n        \"\"\"\n        Stage a command to be executed when execute() is next called\n\n        Returns the current Pipeline object back so commands can be\n        chained together, such as:\n\n        pipe = pipe.set('foo', 'bar').incr('baz').decr('bang')\n\n        At some other point, you can then run: pipe.execute(),\n        which will execute all commands queued in the pipe.\n        \"\"\"\n        self.command_stack.append((args, options))\n        return self\n\n    async def _execute_transaction(  # noqa: C901\n        self, connection: Connection, commands: CommandStackT, raise_on_error\n    ):\n        pre: CommandT = ((\"MULTI\",), {})\n        post: CommandT = ((\"EXEC\",), {})\n        cmds = (pre, *commands, post)\n        all_cmds = connection.pack_commands(\n            args for args, options in cmds if EMPTY_RESPONSE not in options\n        )\n        await connection.send_packed_command(all_cmds)\n        errors = []\n\n        # parse off the response for MULTI\n        # NOTE: we need to handle ResponseErrors here and continue\n        # so that we read all the additional command messages from\n        # the socket\n        try:\n            await self.parse_response(connection, \"_\")\n        except ResponseError as err:\n            errors.append((0, err))\n\n        # and all the other commands\n        for i, command in enumerate(commands):\n            if EMPTY_RESPONSE in command[1]:\n                errors.append((i, command[1][EMPTY_RESPONSE]))\n            else:\n                try:\n                    await self.parse_response(connection, \"_\")\n                except ResponseError as err:\n                    self.annotate_exception(err, i + 1, command[0])\n                    errors.append((i, err))\n\n        # parse the EXEC.\n        try:\n            response = await self.parse_response(connection, \"_\")\n        except ExecAbortError as err:\n            if errors:\n                raise errors[0][1] from err\n            raise\n\n        # EXEC clears any watched keys\n        self.watching = False\n\n        if response is None:\n            raise WatchError(\"Watched variable changed.\") from None\n\n        # put any parse errors into the response\n        for i, e in errors:\n            response.insert(i, e)\n\n        if len(response) != len(commands):\n            if self.connection:\n                await self.connection.disconnect()\n            raise ResponseError(\n                \"Wrong number of response items from pipeline execution\"\n            ) from None\n\n        # find any errors in the response and raise if necessary\n        if raise_on_error:\n            self.raise_first_error(commands, response)\n\n        # We have to run response callbacks manually\n        data = []\n        for r, cmd in zip(response, commands):\n            if not isinstance(r, Exception):\n                args, options = cmd\n                command_name = args[0]\n\n                # Remove keys entry, it needs only for cache.\n                options.pop(\"keys\", None)\n\n                if command_name in self.response_callbacks:\n                    r = self.response_callbacks[command_name](r, **options)\n                    if inspect.isawaitable(r):\n                        r = await r\n            data.append(r)\n        return data\n\n    async def _execute_pipeline(\n        self, connection: Connection, commands: CommandStackT, raise_on_error: bool\n    ):\n        # build up all commands into a single request to increase network perf\n        all_cmds = connection.pack_commands([args for args, _ in commands])\n        await connection.send_packed_command(all_cmds)\n\n        response = []\n        for args, options in commands:\n            try:\n                response.append(\n                    await self.parse_response(connection, args[0], **options)\n                )\n            except ResponseError as e:\n                response.append(e)\n\n        if raise_on_error:\n            self.raise_first_error(commands, response)\n        return response\n\n    def raise_first_error(self, commands: CommandStackT, response: Iterable[Any]):\n        for i, r in enumerate(response):\n            if isinstance(r, ResponseError):\n                self.annotate_exception(r, i + 1, commands[i][0])\n                raise r\n\n    def annotate_exception(\n        self, exception: Exception, number: int, command: Iterable[object]\n    ) -> None:\n        cmd = \" \".join(map(safe_str, command))\n        msg = (\n            f\"Command # {number} ({truncate_text(cmd)}) \"\n            \"of pipeline caused error: {exception.args}\"\n        )\n        exception.args = (msg,) + exception.args[1:]\n\n    async def parse_response(\n        self, connection: Connection, command_name: Union[str, bytes], **options\n    ):\n        result = await super().parse_response(connection, command_name, **options)\n        if command_name in self.UNWATCH_COMMANDS:\n            self.watching = False\n        elif command_name == \"WATCH\":\n            self.watching = True\n        return result\n\n    async def load_scripts(self):\n        # make sure all scripts that are about to be run on this pipeline exist\n        scripts = list(self.scripts)\n        immediate = self.immediate_execute_command\n        shas = [s.sha for s in scripts]\n        # we can't use the normal script_* methods because they would just\n        # get buffered in the pipeline.\n        exists = await immediate(\"SCRIPT EXISTS\", *shas)\n        if not all(exists):\n            for s, exist in zip(scripts, exists):\n                if not exist:\n                    s.sha = await immediate(\"SCRIPT LOAD\", s.script)\n\n    async def _disconnect_raise_on_watching(self, conn: Connection, error: Exception):\n        \"\"\"\n        Close the connection, raise an exception if we were watching.\n\n        The supported exceptions are already checked in the\n        retry object so we don't need to do it here.\n\n        After we disconnect the connection, it will try to reconnect and\n        do a health check as part of the send_command logic(on connection level).\n        \"\"\"\n        await conn.disconnect()\n        # if we were watching a variable, the watch is no longer valid\n        # since this connection has died. raise a WatchError, which\n        # indicates the user should retry this transaction.\n        if self.watching:\n            raise WatchError(\n                f\"A {type(error).__name__} occurred while watching one or more keys\"\n            )\n\n    async def execute(self, raise_on_error: bool = True) -> List[Any]:\n        \"\"\"Execute all the commands in the current pipeline\"\"\"\n        stack = self.command_stack\n        if not stack and not self.watching:\n            return []\n        if self.scripts:\n            await self.load_scripts()\n        if self.is_transaction or self.explicit_transaction:\n            execute = self._execute_transaction\n        else:\n            execute = self._execute_pipeline\n\n        conn = self.connection\n        if not conn:\n            conn = await self.connection_pool.get_connection()\n            # assign to self.connection so reset() releases the connection\n            # back to the pool after we're done\n            self.connection = conn\n        conn = cast(Connection, conn)\n\n        try:\n            return await conn.retry.call_with_retry(\n                lambda: execute(conn, stack, raise_on_error),\n                lambda error: self._disconnect_raise_on_watching(conn, error),\n            )\n        finally:\n            await self.reset()\n\n    async def discard(self):\n        \"\"\"Flushes all previously queued commands\n        See: https://redis.io/commands/DISCARD\n        \"\"\"\n        await self.execute_command(\"DISCARD\")\n\n    async def watch(self, *names: KeyT):\n        \"\"\"Watches the values at keys ``names``\"\"\"\n        if self.explicit_transaction:\n            raise RedisError(\"Cannot issue a WATCH after a MULTI\")\n        return await self.execute_command(\"WATCH\", *names)\n\n    async def unwatch(self):\n        \"\"\"Unwatches all previously specified keys\"\"\"\n        return self.watching and await self.execute_command(\"UNWATCH\") or True\n", 1618], "/usr/local/lib/python3.11/site-packages/redis/utils.py": ["import datetime\nimport logging\nimport textwrap\nfrom collections.abc import Callable\nfrom contextlib import contextmanager\nfrom functools import wraps\nfrom typing import Any, Dict, List, Mapping, Optional, TypeVar, Union\n\nfrom redis.exceptions import DataError\nfrom redis.typing import AbsExpiryT, EncodableT, ExpiryT\n\ntry:\n    import hiredis  # noqa\n\n    # Only support Hiredis >= 3.0:\n    hiredis_version = hiredis.__version__.split(\".\")\n    HIREDIS_AVAILABLE = int(hiredis_version[0]) > 3 or (\n        int(hiredis_version[0]) == 3 and int(hiredis_version[1]) >= 2\n    )\n    if not HIREDIS_AVAILABLE:\n        raise ImportError(\"hiredis package should be >= 3.2.0\")\nexcept ImportError:\n    HIREDIS_AVAILABLE = False\n\ntry:\n    import ssl  # noqa\n\n    SSL_AVAILABLE = True\nexcept ImportError:\n    SSL_AVAILABLE = False\n\ntry:\n    import cryptography  # noqa\n\n    CRYPTOGRAPHY_AVAILABLE = True\nexcept ImportError:\n    CRYPTOGRAPHY_AVAILABLE = False\n\nfrom importlib import metadata\n\n\ndef from_url(url, **kwargs):\n    \"\"\"\n    Returns an active Redis client generated from the given database URL.\n\n    Will attempt to extract the database id from the path url fragment, if\n    none is provided.\n    \"\"\"\n    from redis.client import Redis\n\n    return Redis.from_url(url, **kwargs)\n\n\n@contextmanager\ndef pipeline(redis_obj):\n    p = redis_obj.pipeline()\n    yield p\n    p.execute()\n\n\ndef str_if_bytes(value: Union[str, bytes]) -> str:\n    return (\n        value.decode(\"utf-8\", errors=\"replace\") if isinstance(value, bytes) else value\n    )\n\n\ndef safe_str(value):\n    return str(str_if_bytes(value))\n\n\ndef dict_merge(*dicts: Mapping[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Merge all provided dicts into 1 dict.\n    *dicts : `dict`\n        dictionaries to merge\n    \"\"\"\n    merged = {}\n\n    for d in dicts:\n        merged.update(d)\n\n    return merged\n\n\ndef list_keys_to_dict(key_list, callback):\n    return dict.fromkeys(key_list, callback)\n\n\ndef merge_result(command, res):\n    \"\"\"\n    Merge all items in `res` into a list.\n\n    This command is used when sending a command to multiple nodes\n    and the result from each node should be merged into a single list.\n\n    res : 'dict'\n    \"\"\"\n    result = set()\n\n    for v in res.values():\n        for value in v:\n            result.add(value)\n\n    return list(result)\n\n\ndef warn_deprecated(name, reason=\"\", version=\"\", stacklevel=2):\n    import warnings\n\n    msg = f\"Call to deprecated {name}.\"\n    if reason:\n        msg += f\" ({reason})\"\n    if version:\n        msg += f\" -- Deprecated since version {version}.\"\n    warnings.warn(msg, category=DeprecationWarning, stacklevel=stacklevel)\n\n\ndef deprecated_function(reason=\"\", version=\"\", name=None):\n    \"\"\"\n    Decorator to mark a function as deprecated.\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            warn_deprecated(name or func.__name__, reason, version, stacklevel=3)\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n\n\ndef warn_deprecated_arg_usage(\n    arg_name: Union[list, str],\n    function_name: str,\n    reason: str = \"\",\n    version: str = \"\",\n    stacklevel: int = 2,\n):\n    import warnings\n\n    msg = (\n        f\"Call to '{function_name}' function with deprecated\"\n        f\" usage of input argument/s '{arg_name}'.\"\n    )\n    if reason:\n        msg += f\" ({reason})\"\n    if version:\n        msg += f\" -- Deprecated since version {version}.\"\n    warnings.warn(msg, category=DeprecationWarning, stacklevel=stacklevel)\n\n\nC = TypeVar(\"C\", bound=Callable)\n\n\ndef deprecated_args(\n    args_to_warn: list = [\"*\"],\n    allowed_args: list = [],\n    reason: str = \"\",\n    version: str = \"\",\n) -> Callable[[C], C]:\n    \"\"\"\n    Decorator to mark specified args of a function as deprecated.\n    If '*' is in args_to_warn, all arguments will be marked as deprecated.\n    \"\"\"\n\n    def decorator(func: C) -> C:\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            # Get function argument names\n            arg_names = func.__code__.co_varnames[: func.__code__.co_argcount]\n\n            provided_args = dict(zip(arg_names, args))\n            provided_args.update(kwargs)\n\n            provided_args.pop(\"self\", None)\n            for allowed_arg in allowed_args:\n                provided_args.pop(allowed_arg, None)\n\n            for arg in args_to_warn:\n                if arg == \"*\" and len(provided_args) > 0:\n                    warn_deprecated_arg_usage(\n                        list(provided_args.keys()),\n                        func.__name__,\n                        reason,\n                        version,\n                        stacklevel=3,\n                    )\n                elif arg in provided_args:\n                    warn_deprecated_arg_usage(\n                        arg, func.__name__, reason, version, stacklevel=3\n                    )\n\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n\n\ndef _set_info_logger():\n    \"\"\"\n    Set up a logger that log info logs to stdout.\n    (This is used by the default push response handler)\n    \"\"\"\n    if \"push_response\" not in logging.root.manager.loggerDict.keys():\n        logger = logging.getLogger(\"push_response\")\n        logger.setLevel(logging.INFO)\n        handler = logging.StreamHandler()\n        handler.setLevel(logging.INFO)\n        logger.addHandler(handler)\n\n\ndef get_lib_version():\n    try:\n        libver = metadata.version(\"redis\")\n    except metadata.PackageNotFoundError:\n        libver = \"99.99.99\"\n    return libver\n\n\ndef format_error_message(host_error: str, exception: BaseException) -> str:\n    if not exception.args:\n        return f\"Error connecting to {host_error}.\"\n    elif len(exception.args) == 1:\n        return f\"Error {exception.args[0]} connecting to {host_error}.\"\n    else:\n        return (\n            f\"Error {exception.args[0]} connecting to {host_error}. \"\n            f\"{exception.args[1]}.\"\n        )\n\n\ndef compare_versions(version1: str, version2: str) -> int:\n    \"\"\"\n    Compare two versions.\n\n    :return: -1 if version1 > version2\n             0 if both versions are equal\n             1 if version1 < version2\n    \"\"\"\n\n    num_versions1 = list(map(int, version1.split(\".\")))\n    num_versions2 = list(map(int, version2.split(\".\")))\n\n    if len(num_versions1) > len(num_versions2):\n        diff = len(num_versions1) - len(num_versions2)\n        for _ in range(diff):\n            num_versions2.append(0)\n    elif len(num_versions1) < len(num_versions2):\n        diff = len(num_versions2) - len(num_versions1)\n        for _ in range(diff):\n            num_versions1.append(0)\n\n    for i, ver in enumerate(num_versions1):\n        if num_versions1[i] > num_versions2[i]:\n            return -1\n        elif num_versions1[i] < num_versions2[i]:\n            return 1\n\n    return 0\n\n\ndef ensure_string(key):\n    if isinstance(key, bytes):\n        return key.decode(\"utf-8\")\n    elif isinstance(key, str):\n        return key\n    else:\n        raise TypeError(\"Key must be either a string or bytes\")\n\n\ndef extract_expire_flags(\n    ex: Optional[ExpiryT] = None,\n    px: Optional[ExpiryT] = None,\n    exat: Optional[AbsExpiryT] = None,\n    pxat: Optional[AbsExpiryT] = None,\n) -> List[EncodableT]:\n    exp_options: list[EncodableT] = []\n    if ex is not None:\n        exp_options.append(\"EX\")\n        if isinstance(ex, datetime.timedelta):\n            exp_options.append(int(ex.total_seconds()))\n        elif isinstance(ex, int):\n            exp_options.append(ex)\n        elif isinstance(ex, str) and ex.isdigit():\n            exp_options.append(int(ex))\n        else:\n            raise DataError(\"ex must be datetime.timedelta or int\")\n    elif px is not None:\n        exp_options.append(\"PX\")\n        if isinstance(px, datetime.timedelta):\n            exp_options.append(int(px.total_seconds() * 1000))\n        elif isinstance(px, int):\n            exp_options.append(px)\n        else:\n            raise DataError(\"px must be datetime.timedelta or int\")\n    elif exat is not None:\n        if isinstance(exat, datetime.datetime):\n            exat = int(exat.timestamp())\n        exp_options.extend([\"EXAT\", exat])\n    elif pxat is not None:\n        if isinstance(pxat, datetime.datetime):\n            pxat = int(pxat.timestamp() * 1000)\n        exp_options.extend([\"PXAT\", pxat])\n\n    return exp_options\n\n\ndef truncate_text(txt, max_length=100):\n    return textwrap.shorten(\n        text=txt, width=max_length, placeholder=\"...\", break_long_words=True\n    )\n", 314], "/usr/local/lib/python3.11/asyncio/locks.py": ["\"\"\"Synchronization primitives.\"\"\"\n\n__all__ = ('Lock', 'Event', 'Condition', 'Semaphore',\n           'BoundedSemaphore', 'Barrier')\n\nimport collections\nimport enum\n\nfrom . import exceptions\nfrom . import mixins\nfrom . import tasks\n\nclass _ContextManagerMixin:\n    async def __aenter__(self):\n        await self.acquire()\n        # We have no use for the \"as ...\"  clause in the with\n        # statement for locks.\n        return None\n\n    async def __aexit__(self, exc_type, exc, tb):\n        self.release()\n\n\nclass Lock(_ContextManagerMixin, mixins._LoopBoundMixin):\n    \"\"\"Primitive lock objects.\n\n    A primitive lock is a synchronization primitive that is not owned\n    by a particular coroutine when locked.  A primitive lock is in one\n    of two states, 'locked' or 'unlocked'.\n\n    It is created in the unlocked state.  It has two basic methods,\n    acquire() and release().  When the state is unlocked, acquire()\n    changes the state to locked and returns immediately.  When the\n    state is locked, acquire() blocks until a call to release() in\n    another coroutine changes it to unlocked, then the acquire() call\n    resets it to locked and returns.  The release() method should only\n    be called in the locked state; it changes the state to unlocked\n    and returns immediately.  If an attempt is made to release an\n    unlocked lock, a RuntimeError will be raised.\n\n    When more than one coroutine is blocked in acquire() waiting for\n    the state to turn to unlocked, only one coroutine proceeds when a\n    release() call resets the state to unlocked; first coroutine which\n    is blocked in acquire() is being processed.\n\n    acquire() is a coroutine and should be called with 'await'.\n\n    Locks also support the asynchronous context management protocol.\n    'async with lock' statement should be used.\n\n    Usage:\n\n        lock = Lock()\n        ...\n        await lock.acquire()\n        try:\n            ...\n        finally:\n            lock.release()\n\n    Context manager usage:\n\n        lock = Lock()\n        ...\n        async with lock:\n             ...\n\n    Lock objects can be tested for locking state:\n\n        if not lock.locked():\n           await lock.acquire()\n        else:\n           # lock is acquired\n           ...\n\n    \"\"\"\n\n    def __init__(self):\n        self._waiters = None\n        self._locked = False\n\n    def __repr__(self):\n        res = super().__repr__()\n        extra = 'locked' if self._locked else 'unlocked'\n        if self._waiters:\n            extra = f'{extra}, waiters:{len(self._waiters)}'\n        return f'<{res[1:-1]} [{extra}]>'\n\n    def locked(self):\n        \"\"\"Return True if lock is acquired.\"\"\"\n        return self._locked\n\n    async def acquire(self):\n        \"\"\"Acquire a lock.\n\n        This method blocks until the lock is unlocked, then sets it to\n        locked and returns True.\n        \"\"\"\n        if (not self._locked and (self._waiters is None or\n                all(w.cancelled() for w in self._waiters))):\n            self._locked = True\n            return True\n\n        if self._waiters is None:\n            self._waiters = collections.deque()\n        fut = self._get_loop().create_future()\n        self._waiters.append(fut)\n\n        # Finally block should be called before the CancelledError\n        # handling as we don't want CancelledError to call\n        # _wake_up_first() and attempt to wake up itself.\n        try:\n            try:\n                await fut\n            finally:\n                self._waiters.remove(fut)\n        except exceptions.CancelledError:\n            if not self._locked:\n                self._wake_up_first()\n            raise\n\n        self._locked = True\n        return True\n\n    def release(self):\n        \"\"\"Release a lock.\n\n        When the lock is locked, reset it to unlocked, and return.\n        If any other coroutines are blocked waiting for the lock to become\n        unlocked, allow exactly one of them to proceed.\n\n        When invoked on an unlocked lock, a RuntimeError is raised.\n\n        There is no return value.\n        \"\"\"\n        if self._locked:\n            self._locked = False\n            self._wake_up_first()\n        else:\n            raise RuntimeError('Lock is not acquired.')\n\n    def _wake_up_first(self):\n        \"\"\"Wake up the first waiter if it isn't done.\"\"\"\n        if not self._waiters:\n            return\n        try:\n            fut = next(iter(self._waiters))\n        except StopIteration:\n            return\n\n        # .done() necessarily means that a waiter will wake up later on and\n        # either take the lock, or, if it was cancelled and lock wasn't\n        # taken already, will hit this again and wake up a new waiter.\n        if not fut.done():\n            fut.set_result(True)\n\n\nclass Event(mixins._LoopBoundMixin):\n    \"\"\"Asynchronous equivalent to threading.Event.\n\n    Class implementing event objects. An event manages a flag that can be set\n    to true with the set() method and reset to false with the clear() method.\n    The wait() method blocks until the flag is true. The flag is initially\n    false.\n    \"\"\"\n\n    def __init__(self):\n        self._waiters = collections.deque()\n        self._value = False\n\n    def __repr__(self):\n        res = super().__repr__()\n        extra = 'set' if self._value else 'unset'\n        if self._waiters:\n            extra = f'{extra}, waiters:{len(self._waiters)}'\n        return f'<{res[1:-1]} [{extra}]>'\n\n    def is_set(self):\n        \"\"\"Return True if and only if the internal flag is true.\"\"\"\n        return self._value\n\n    def set(self):\n        \"\"\"Set the internal flag to true. All coroutines waiting for it to\n        become true are awakened. Coroutine that call wait() once the flag is\n        true will not block at all.\n        \"\"\"\n        if not self._value:\n            self._value = True\n\n            for fut in self._waiters:\n                if not fut.done():\n                    fut.set_result(True)\n\n    def clear(self):\n        \"\"\"Reset the internal flag to false. Subsequently, coroutines calling\n        wait() will block until set() is called to set the internal flag\n        to true again.\"\"\"\n        self._value = False\n\n    async def wait(self):\n        \"\"\"Block until the internal flag is true.\n\n        If the internal flag is true on entry, return True\n        immediately.  Otherwise, block until another coroutine calls\n        set() to set the flag to true, then return True.\n        \"\"\"\n        if self._value:\n            return True\n\n        fut = self._get_loop().create_future()\n        self._waiters.append(fut)\n        try:\n            await fut\n            return True\n        finally:\n            self._waiters.remove(fut)\n\n\nclass Condition(_ContextManagerMixin, mixins._LoopBoundMixin):\n    \"\"\"Asynchronous equivalent to threading.Condition.\n\n    This class implements condition variable objects. A condition variable\n    allows one or more coroutines to wait until they are notified by another\n    coroutine.\n\n    A new Lock object is created and used as the underlying lock.\n    \"\"\"\n\n    def __init__(self, lock=None):\n        if lock is None:\n            lock = Lock()\n\n        self._lock = lock\n        # Export the lock's locked(), acquire() and release() methods.\n        self.locked = lock.locked\n        self.acquire = lock.acquire\n        self.release = lock.release\n\n        self._waiters = collections.deque()\n\n    def __repr__(self):\n        res = super().__repr__()\n        extra = 'locked' if self.locked() else 'unlocked'\n        if self._waiters:\n            extra = f'{extra}, waiters:{len(self._waiters)}'\n        return f'<{res[1:-1]} [{extra}]>'\n\n    async def wait(self):\n        \"\"\"Wait until notified.\n\n        If the calling coroutine has not acquired the lock when this\n        method is called, a RuntimeError is raised.\n\n        This method releases the underlying lock, and then blocks\n        until it is awakened by a notify() or notify_all() call for\n        the same condition variable in another coroutine.  Once\n        awakened, it re-acquires the lock and returns True.\n        \"\"\"\n        if not self.locked():\n            raise RuntimeError('cannot wait on un-acquired lock')\n\n        self.release()\n        try:\n            fut = self._get_loop().create_future()\n            self._waiters.append(fut)\n            try:\n                await fut\n                return True\n            finally:\n                self._waiters.remove(fut)\n\n        finally:\n            # Must reacquire lock even if wait is cancelled\n            cancelled = False\n            while True:\n                try:\n                    await self.acquire()\n                    break\n                except exceptions.CancelledError:\n                    cancelled = True\n\n            if cancelled:\n                raise exceptions.CancelledError\n\n    async def wait_for(self, predicate):\n        \"\"\"Wait until a predicate becomes true.\n\n        The predicate should be a callable which result will be\n        interpreted as a boolean value.  The final predicate value is\n        the return value.\n        \"\"\"\n        result = predicate()\n        while not result:\n            await self.wait()\n            result = predicate()\n        return result\n\n    def notify(self, n=1):\n        \"\"\"By default, wake up one coroutine waiting on this condition, if any.\n        If the calling coroutine has not acquired the lock when this method\n        is called, a RuntimeError is raised.\n\n        This method wakes up at most n of the coroutines waiting for the\n        condition variable; it is a no-op if no coroutines are waiting.\n\n        Note: an awakened coroutine does not actually return from its\n        wait() call until it can reacquire the lock. Since notify() does\n        not release the lock, its caller should.\n        \"\"\"\n        if not self.locked():\n            raise RuntimeError('cannot notify on un-acquired lock')\n\n        idx = 0\n        for fut in self._waiters:\n            if idx >= n:\n                break\n\n            if not fut.done():\n                idx += 1\n                fut.set_result(False)\n\n    def notify_all(self):\n        \"\"\"Wake up all threads waiting on this condition. This method acts\n        like notify(), but wakes up all waiting threads instead of one. If the\n        calling thread has not acquired the lock when this method is called,\n        a RuntimeError is raised.\n        \"\"\"\n        self.notify(len(self._waiters))\n\n\nclass Semaphore(_ContextManagerMixin, mixins._LoopBoundMixin):\n    \"\"\"A Semaphore implementation.\n\n    A semaphore manages an internal counter which is decremented by each\n    acquire() call and incremented by each release() call. The counter\n    can never go below zero; when acquire() finds that it is zero, it blocks,\n    waiting until some other thread calls release().\n\n    Semaphores also support the context management protocol.\n\n    The optional argument gives the initial value for the internal\n    counter; it defaults to 1. If the value given is less than 0,\n    ValueError is raised.\n    \"\"\"\n\n    def __init__(self, value=1):\n        if value < 0:\n            raise ValueError(\"Semaphore initial value must be >= 0\")\n        self._waiters = None\n        self._value = value\n\n    def __repr__(self):\n        res = super().__repr__()\n        extra = 'locked' if self.locked() else f'unlocked, value:{self._value}'\n        if self._waiters:\n            extra = f'{extra}, waiters:{len(self._waiters)}'\n        return f'<{res[1:-1]} [{extra}]>'\n\n    def locked(self):\n        \"\"\"Returns True if semaphore cannot be acquired immediately.\"\"\"\n        return self._value == 0 or (\n            any(not w.cancelled() for w in (self._waiters or ())))\n\n    async def acquire(self):\n        \"\"\"Acquire a semaphore.\n\n        If the internal counter is larger than zero on entry,\n        decrement it by one and return True immediately.  If it is\n        zero on entry, block, waiting until some other coroutine has\n        called release() to make it larger than 0, and then return\n        True.\n        \"\"\"\n        if not self.locked():\n            self._value -= 1\n            return True\n\n        if self._waiters is None:\n            self._waiters = collections.deque()\n        fut = self._get_loop().create_future()\n        self._waiters.append(fut)\n\n        # Finally block should be called before the CancelledError\n        # handling as we don't want CancelledError to call\n        # _wake_up_first() and attempt to wake up itself.\n        try:\n            try:\n                await fut\n            finally:\n                self._waiters.remove(fut)\n        except exceptions.CancelledError:\n            if not fut.cancelled():\n                self._value += 1\n                self._wake_up_next()\n            raise\n\n        if self._value > 0:\n            self._wake_up_next()\n        return True\n\n    def release(self):\n        \"\"\"Release a semaphore, incrementing the internal counter by one.\n\n        When it was zero on entry and another coroutine is waiting for it to\n        become larger than zero again, wake up that coroutine.\n        \"\"\"\n        self._value += 1\n        self._wake_up_next()\n\n    def _wake_up_next(self):\n        \"\"\"Wake up the first waiter that isn't done.\"\"\"\n        if not self._waiters:\n            return\n\n        for fut in self._waiters:\n            if not fut.done():\n                self._value -= 1\n                fut.set_result(True)\n                return\n\n\nclass BoundedSemaphore(Semaphore):\n    \"\"\"A bounded semaphore implementation.\n\n    This raises ValueError in release() if it would increase the value\n    above the initial value.\n    \"\"\"\n\n    def __init__(self, value=1):\n        self._bound_value = value\n        super().__init__(value)\n\n    def release(self):\n        if self._value >= self._bound_value:\n            raise ValueError('BoundedSemaphore released too many times')\n        super().release()\n\n\n\nclass _BarrierState(enum.Enum):\n    FILLING = 'filling'\n    DRAINING = 'draining'\n    RESETTING = 'resetting'\n    BROKEN = 'broken'\n\n\nclass Barrier(mixins._LoopBoundMixin):\n    \"\"\"Asyncio equivalent to threading.Barrier\n\n    Implements a Barrier primitive.\n    Useful for synchronizing a fixed number of tasks at known synchronization\n    points. Tasks block on 'wait()' and are simultaneously awoken once they\n    have all made their call.\n    \"\"\"\n\n    def __init__(self, parties):\n        \"\"\"Create a barrier, initialised to 'parties' tasks.\"\"\"\n        if parties < 1:\n            raise ValueError('parties must be > 0')\n\n        self._cond = Condition() # notify all tasks when state changes\n\n        self._parties = parties\n        self._state = _BarrierState.FILLING\n        self._count = 0       # count tasks in Barrier\n\n    def __repr__(self):\n        res = super().__repr__()\n        extra = f'{self._state.value}'\n        if not self.broken:\n            extra += f', waiters:{self.n_waiting}/{self.parties}'\n        return f'<{res[1:-1]} [{extra}]>'\n\n    async def __aenter__(self):\n        # wait for the barrier reaches the parties number\n        # when start draining release and return index of waited task\n        return await self.wait()\n\n    async def __aexit__(self, *args):\n        pass\n\n    async def wait(self):\n        \"\"\"Wait for the barrier.\n\n        When the specified number of tasks have started waiting, they are all\n        simultaneously awoken.\n        Returns an unique and individual index number from 0 to 'parties-1'.\n        \"\"\"\n        async with self._cond:\n            await self._block() # Block while the barrier drains or resets.\n            try:\n                index = self._count\n                self._count += 1\n                if index + 1 == self._parties:\n                    # We release the barrier\n                    await self._release()\n                else:\n                    await self._wait()\n                return index\n            finally:\n                self._count -= 1\n                # Wake up any tasks waiting for barrier to drain.\n                self._exit()\n\n    async def _block(self):\n        # Block until the barrier is ready for us,\n        # or raise an exception if it is broken.\n        #\n        # It is draining or resetting, wait until done\n        # unless a CancelledError occurs\n        await self._cond.wait_for(\n            lambda: self._state not in (\n                _BarrierState.DRAINING, _BarrierState.RESETTING\n            )\n        )\n\n        # see if the barrier is in a broken state\n        if self._state is _BarrierState.BROKEN:\n            raise exceptions.BrokenBarrierError(\"Barrier aborted\")\n\n    async def _release(self):\n        # Release the tasks waiting in the barrier.\n\n        # Enter draining state.\n        # Next waiting tasks will be blocked until the end of draining.\n        self._state = _BarrierState.DRAINING\n        self._cond.notify_all()\n\n    async def _wait(self):\n        # Wait in the barrier until we are released. Raise an exception\n        # if the barrier is reset or broken.\n\n        # wait for end of filling\n        # unless a CancelledError occurs\n        await self._cond.wait_for(lambda: self._state is not _BarrierState.FILLING)\n\n        if self._state in (_BarrierState.BROKEN, _BarrierState.RESETTING):\n            raise exceptions.BrokenBarrierError(\"Abort or reset of barrier\")\n\n    def _exit(self):\n        # If we are the last tasks to exit the barrier, signal any tasks\n        # waiting for the barrier to drain.\n        if self._count == 0:\n            if self._state in (_BarrierState.RESETTING, _BarrierState.DRAINING):\n                self._state = _BarrierState.FILLING\n            self._cond.notify_all()\n\n    async def reset(self):\n        \"\"\"Reset the barrier to the initial state.\n\n        Any tasks currently waiting will get the BrokenBarrier exception\n        raised.\n        \"\"\"\n        async with self._cond:\n            if self._count > 0:\n                if self._state is not _BarrierState.RESETTING:\n                    #reset the barrier, waking up tasks\n                    self._state = _BarrierState.RESETTING\n            else:\n                self._state = _BarrierState.FILLING\n            self._cond.notify_all()\n\n    async def abort(self):\n        \"\"\"Place the barrier into a 'broken' state.\n\n        Useful in case of error.  Any currently waiting tasks and tasks\n        attempting to 'wait()' will have BrokenBarrierError raised.\n        \"\"\"\n        async with self._cond:\n            self._state = _BarrierState.BROKEN\n            self._cond.notify_all()\n\n    @property\n    def parties(self):\n        \"\"\"Return the number of tasks required to trip the barrier.\"\"\"\n        return self._parties\n\n    @property\n    def n_waiting(self):\n        \"\"\"Return the number of tasks currently waiting at the barrier.\"\"\"\n        if self._state is _BarrierState.FILLING:\n            return self._count\n        return 0\n\n    @property\n    def broken(self):\n        \"\"\"Return True if the barrier is in a broken state.\"\"\"\n        return self._state is _BarrierState.BROKEN\n", 587], "/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py": ["import asyncio\nimport copy\nimport enum\nimport inspect\nimport socket\nimport sys\nimport warnings\nimport weakref\nfrom abc import abstractmethod\nfrom itertools import chain\nfrom types import MappingProxyType\nfrom typing import (\n    Any,\n    Callable,\n    Iterable,\n    List,\n    Mapping,\n    Optional,\n    Protocol,\n    Set,\n    Tuple,\n    Type,\n    TypedDict,\n    TypeVar,\n    Union,\n)\nfrom urllib.parse import ParseResult, parse_qs, unquote, urlparse\n\nfrom ..utils import SSL_AVAILABLE\n\nif SSL_AVAILABLE:\n    import ssl\n    from ssl import SSLContext, TLSVersion\nelse:\n    ssl = None\n    TLSVersion = None\n    SSLContext = None\n\nfrom ..auth.token import TokenInterface\nfrom ..event import AsyncAfterConnectionReleasedEvent, EventDispatcher\nfrom ..utils import deprecated_args, format_error_message\n\n# the functionality is available in 3.11.x but has a major issue before\n# 3.11.3. See https://github.com/redis/redis-py/issues/2633\nif sys.version_info >= (3, 11, 3):\n    from asyncio import timeout as async_timeout\nelse:\n    from async_timeout import timeout as async_timeout\n\nfrom redis.asyncio.retry import Retry\nfrom redis.backoff import NoBackoff\nfrom redis.connection import DEFAULT_RESP_VERSION\nfrom redis.credentials import CredentialProvider, UsernamePasswordCredentialProvider\nfrom redis.exceptions import (\n    AuthenticationError,\n    AuthenticationWrongNumberOfArgsError,\n    ConnectionError,\n    DataError,\n    RedisError,\n    ResponseError,\n    TimeoutError,\n)\nfrom redis.typing import EncodableT\nfrom redis.utils import HIREDIS_AVAILABLE, get_lib_version, str_if_bytes\n\nfrom .._parsers import (\n    BaseParser,\n    Encoder,\n    _AsyncHiredisParser,\n    _AsyncRESP2Parser,\n    _AsyncRESP3Parser,\n)\n\nSYM_STAR = b\"*\"\nSYM_DOLLAR = b\"$\"\nSYM_CRLF = b\"\\r\\n\"\nSYM_LF = b\"\\n\"\nSYM_EMPTY = b\"\"\n\n\nclass _Sentinel(enum.Enum):\n    sentinel = object()\n\n\nSENTINEL = _Sentinel.sentinel\n\n\nDefaultParser: Type[Union[_AsyncRESP2Parser, _AsyncRESP3Parser, _AsyncHiredisParser]]\nif HIREDIS_AVAILABLE:\n    DefaultParser = _AsyncHiredisParser\nelse:\n    DefaultParser = _AsyncRESP2Parser\n\n\nclass ConnectCallbackProtocol(Protocol):\n    def __call__(self, connection: \"AbstractConnection\"): ...\n\n\nclass AsyncConnectCallbackProtocol(Protocol):\n    async def __call__(self, connection: \"AbstractConnection\"): ...\n\n\nConnectCallbackT = Union[ConnectCallbackProtocol, AsyncConnectCallbackProtocol]\n\n\nclass AbstractConnection:\n    \"\"\"Manages communication to and from a Redis server\"\"\"\n\n    __slots__ = (\n        \"db\",\n        \"username\",\n        \"client_name\",\n        \"lib_name\",\n        \"lib_version\",\n        \"credential_provider\",\n        \"password\",\n        \"socket_timeout\",\n        \"socket_connect_timeout\",\n        \"redis_connect_func\",\n        \"retry_on_timeout\",\n        \"retry_on_error\",\n        \"health_check_interval\",\n        \"next_health_check\",\n        \"last_active_at\",\n        \"encoder\",\n        \"ssl_context\",\n        \"protocol\",\n        \"_reader\",\n        \"_writer\",\n        \"_parser\",\n        \"_connect_callbacks\",\n        \"_buffer_cutoff\",\n        \"_lock\",\n        \"_socket_read_size\",\n        \"__dict__\",\n    )\n\n    def __init__(\n        self,\n        *,\n        db: Union[str, int] = 0,\n        password: Optional[str] = None,\n        socket_timeout: Optional[float] = None,\n        socket_connect_timeout: Optional[float] = None,\n        retry_on_timeout: bool = False,\n        retry_on_error: Union[list, _Sentinel] = SENTINEL,\n        encoding: str = \"utf-8\",\n        encoding_errors: str = \"strict\",\n        decode_responses: bool = False,\n        parser_class: Type[BaseParser] = DefaultParser,\n        socket_read_size: int = 65536,\n        health_check_interval: float = 0,\n        client_name: Optional[str] = None,\n        lib_name: Optional[str] = \"redis-py\",\n        lib_version: Optional[str] = get_lib_version(),\n        username: Optional[str] = None,\n        retry: Optional[Retry] = None,\n        redis_connect_func: Optional[ConnectCallbackT] = None,\n        encoder_class: Type[Encoder] = Encoder,\n        credential_provider: Optional[CredentialProvider] = None,\n        protocol: Optional[int] = 2,\n        event_dispatcher: Optional[EventDispatcher] = None,\n    ):\n        if (username or password) and credential_provider is not None:\n            raise DataError(\n                \"'username' and 'password' cannot be passed along with 'credential_\"\n                \"provider'. Please provide only one of the following arguments: \\n\"\n                \"1. 'password' and (optional) 'username'\\n\"\n                \"2. 'credential_provider'\"\n            )\n        if event_dispatcher is None:\n            self._event_dispatcher = EventDispatcher()\n        else:\n            self._event_dispatcher = event_dispatcher\n        self.db = db\n        self.client_name = client_name\n        self.lib_name = lib_name\n        self.lib_version = lib_version\n        self.credential_provider = credential_provider\n        self.password = password\n        self.username = username\n        self.socket_timeout = socket_timeout\n        if socket_connect_timeout is None:\n            socket_connect_timeout = socket_timeout\n        self.socket_connect_timeout = socket_connect_timeout\n        self.retry_on_timeout = retry_on_timeout\n        if retry_on_error is SENTINEL:\n            retry_on_error = []\n        if retry_on_timeout:\n            retry_on_error.append(TimeoutError)\n            retry_on_error.append(socket.timeout)\n            retry_on_error.append(asyncio.TimeoutError)\n        self.retry_on_error = retry_on_error\n        if retry or retry_on_error:\n            if not retry:\n                self.retry = Retry(NoBackoff(), 1)\n            else:\n                # deep-copy the Retry object as it is mutable\n                self.retry = copy.deepcopy(retry)\n            # Update the retry's supported errors with the specified errors\n            self.retry.update_supported_errors(retry_on_error)\n        else:\n            self.retry = Retry(NoBackoff(), 0)\n        self.health_check_interval = health_check_interval\n        self.next_health_check: float = -1\n        self.encoder = encoder_class(encoding, encoding_errors, decode_responses)\n        self.redis_connect_func = redis_connect_func\n        self._reader: Optional[asyncio.StreamReader] = None\n        self._writer: Optional[asyncio.StreamWriter] = None\n        self._socket_read_size = socket_read_size\n        self.set_parser(parser_class)\n        self._connect_callbacks: List[weakref.WeakMethod[ConnectCallbackT]] = []\n        self._buffer_cutoff = 6000\n        self._re_auth_token: Optional[TokenInterface] = None\n\n        try:\n            p = int(protocol)\n        except TypeError:\n            p = DEFAULT_RESP_VERSION\n        except ValueError:\n            raise ConnectionError(\"protocol must be an integer\")\n        finally:\n            if p < 2 or p > 3:\n                raise ConnectionError(\"protocol must be either 2 or 3\")\n            self.protocol = protocol\n\n    def __del__(self, _warnings: Any = warnings):\n        # For some reason, the individual streams don't get properly garbage\n        # collected and therefore produce no resource warnings.  We add one\n        # here, in the same style as those from the stdlib.\n        if getattr(self, \"_writer\", None):\n            _warnings.warn(\n                f\"unclosed Connection {self!r}\", ResourceWarning, source=self\n            )\n\n            try:\n                asyncio.get_running_loop()\n                self._close()\n            except RuntimeError:\n                # No actions been taken if pool already closed.\n                pass\n\n    def _close(self):\n        \"\"\"\n        Internal method to silently close the connection without waiting\n        \"\"\"\n        if self._writer:\n            self._writer.close()\n            self._writer = self._reader = None\n\n    def __repr__(self):\n        repr_args = \",\".join((f\"{k}={v}\" for k, v in self.repr_pieces()))\n        return f\"<{self.__class__.__module__}.{self.__class__.__name__}({repr_args})>\"\n\n    @abstractmethod\n    def repr_pieces(self):\n        pass\n\n    @property\n    def is_connected(self):\n        return self._reader is not None and self._writer is not None\n\n    def register_connect_callback(self, callback):\n        \"\"\"\n        Register a callback to be called when the connection is established either\n        initially or reconnected.  This allows listeners to issue commands that\n        are ephemeral to the connection, for example pub/sub subscription or\n        key tracking.  The callback must be a _method_ and will be kept as\n        a weak reference.\n        \"\"\"\n        wm = weakref.WeakMethod(callback)\n        if wm not in self._connect_callbacks:\n            self._connect_callbacks.append(wm)\n\n    def deregister_connect_callback(self, callback):\n        \"\"\"\n        De-register a previously registered callback.  It will no-longer receive\n        notifications on connection events.  Calling this is not required when the\n        listener goes away, since the callbacks are kept as weak methods.\n        \"\"\"\n        try:\n            self._connect_callbacks.remove(weakref.WeakMethod(callback))\n        except ValueError:\n            pass\n\n    def set_parser(self, parser_class: Type[BaseParser]) -> None:\n        \"\"\"\n        Creates a new instance of parser_class with socket size:\n        _socket_read_size and assigns it to the parser for the connection\n        :param parser_class: The required parser class\n        \"\"\"\n        self._parser = parser_class(socket_read_size=self._socket_read_size)\n\n    async def connect(self):\n        \"\"\"Connects to the Redis server if not already connected\"\"\"\n        await self.connect_check_health(check_health=True)\n\n    async def connect_check_health(\n        self, check_health: bool = True, retry_socket_connect: bool = True\n    ):\n        if self.is_connected:\n            return\n        try:\n            if retry_socket_connect:\n                await self.retry.call_with_retry(\n                    lambda: self._connect(), lambda error: self.disconnect()\n                )\n            else:\n                await self._connect()\n        except asyncio.CancelledError:\n            raise  # in 3.7 and earlier, this is an Exception, not BaseException\n        except (socket.timeout, asyncio.TimeoutError):\n            raise TimeoutError(\"Timeout connecting to server\")\n        except OSError as e:\n            raise ConnectionError(self._error_message(e))\n        except Exception as exc:\n            raise ConnectionError(exc) from exc\n\n        try:\n            if not self.redis_connect_func:\n                # Use the default on_connect function\n                await self.on_connect_check_health(check_health=check_health)\n            else:\n                # Use the passed function redis_connect_func\n                (\n                    await self.redis_connect_func(self)\n                    if asyncio.iscoroutinefunction(self.redis_connect_func)\n                    else self.redis_connect_func(self)\n                )\n        except RedisError:\n            # clean up after any error in on_connect\n            await self.disconnect()\n            raise\n\n        # run any user callbacks. right now the only internal callback\n        # is for pubsub channel/pattern resubscription\n        # first, remove any dead weakrefs\n        self._connect_callbacks = [ref for ref in self._connect_callbacks if ref()]\n        for ref in self._connect_callbacks:\n            callback = ref()\n            task = callback(self)\n            if task and inspect.isawaitable(task):\n                await task\n\n    @abstractmethod\n    async def _connect(self):\n        pass\n\n    @abstractmethod\n    def _host_error(self) -> str:\n        pass\n\n    def _error_message(self, exception: BaseException) -> str:\n        return format_error_message(self._host_error(), exception)\n\n    def get_protocol(self):\n        return self.protocol\n\n    async def on_connect(self) -> None:\n        \"\"\"Initialize the connection, authenticate and select a database\"\"\"\n        await self.on_connect_check_health(check_health=True)\n\n    async def on_connect_check_health(self, check_health: bool = True) -> None:\n        self._parser.on_connect(self)\n        parser = self._parser\n\n        auth_args = None\n        # if credential provider or username and/or password are set, authenticate\n        if self.credential_provider or (self.username or self.password):\n            cred_provider = (\n                self.credential_provider\n                or UsernamePasswordCredentialProvider(self.username, self.password)\n            )\n            auth_args = await cred_provider.get_credentials_async()\n\n            # if resp version is specified and we have auth args,\n            # we need to send them via HELLO\n        if auth_args and self.protocol not in [2, \"2\"]:\n            if isinstance(self._parser, _AsyncRESP2Parser):\n                self.set_parser(_AsyncRESP3Parser)\n                # update cluster exception classes\n                self._parser.EXCEPTION_CLASSES = parser.EXCEPTION_CLASSES\n                self._parser.on_connect(self)\n            if len(auth_args) == 1:\n                auth_args = [\"default\", auth_args[0]]\n            # avoid checking health here -- PING will fail if we try\n            # to check the health prior to the AUTH\n            await self.send_command(\n                \"HELLO\", self.protocol, \"AUTH\", *auth_args, check_health=False\n            )\n            response = await self.read_response()\n            if response.get(b\"proto\") != int(self.protocol) and response.get(\n                \"proto\"\n            ) != int(self.protocol):\n                raise ConnectionError(\"Invalid RESP version\")\n        # avoid checking health here -- PING will fail if we try\n        # to check the health prior to the AUTH\n        elif auth_args:\n            await self.send_command(\"AUTH\", *auth_args, check_health=False)\n\n            try:\n                auth_response = await self.read_response()\n            except AuthenticationWrongNumberOfArgsError:\n                # a username and password were specified but the Redis\n                # server seems to be < 6.0.0 which expects a single password\n                # arg. retry auth with just the password.\n                # https://github.com/andymccurdy/redis-py/issues/1274\n                await self.send_command(\"AUTH\", auth_args[-1], check_health=False)\n                auth_response = await self.read_response()\n\n            if str_if_bytes(auth_response) != \"OK\":\n                raise AuthenticationError(\"Invalid Username or Password\")\n\n        # if resp version is specified, switch to it\n        elif self.protocol not in [2, \"2\"]:\n            if isinstance(self._parser, _AsyncRESP2Parser):\n                self.set_parser(_AsyncRESP3Parser)\n                # update cluster exception classes\n                self._parser.EXCEPTION_CLASSES = parser.EXCEPTION_CLASSES\n                self._parser.on_connect(self)\n            await self.send_command(\"HELLO\", self.protocol, check_health=check_health)\n            response = await self.read_response()\n            # if response.get(b\"proto\") != self.protocol and response.get(\n            #     \"proto\"\n            # ) != self.protocol:\n            #     raise ConnectionError(\"Invalid RESP version\")\n\n        # if a client_name is given, set it\n        if self.client_name:\n            await self.send_command(\n                \"CLIENT\",\n                \"SETNAME\",\n                self.client_name,\n                check_health=check_health,\n            )\n            if str_if_bytes(await self.read_response()) != \"OK\":\n                raise ConnectionError(\"Error setting client name\")\n\n        # set the library name and version, pipeline for lower startup latency\n        if self.lib_name:\n            await self.send_command(\n                \"CLIENT\",\n                \"SETINFO\",\n                \"LIB-NAME\",\n                self.lib_name,\n                check_health=check_health,\n            )\n        if self.lib_version:\n            await self.send_command(\n                \"CLIENT\",\n                \"SETINFO\",\n                \"LIB-VER\",\n                self.lib_version,\n                check_health=check_health,\n            )\n        # if a database is specified, switch to it. Also pipeline this\n        if self.db:\n            await self.send_command(\"SELECT\", self.db, check_health=check_health)\n\n        # read responses from pipeline\n        for _ in (sent for sent in (self.lib_name, self.lib_version) if sent):\n            try:\n                await self.read_response()\n            except ResponseError:\n                pass\n\n        if self.db:\n            if str_if_bytes(await self.read_response()) != \"OK\":\n                raise ConnectionError(\"Invalid Database\")\n\n    async def disconnect(self, nowait: bool = False) -> None:\n        \"\"\"Disconnects from the Redis server\"\"\"\n        try:\n            async with async_timeout(self.socket_connect_timeout):\n                self._parser.on_disconnect()\n                if not self.is_connected:\n                    return\n                try:\n                    self._writer.close()  # type: ignore[union-attr]\n                    # wait for close to finish, except when handling errors and\n                    # forcefully disconnecting.\n                    if not nowait:\n                        await self._writer.wait_closed()  # type: ignore[union-attr]\n                except OSError:\n                    pass\n                finally:\n                    self._reader = None\n                    self._writer = None\n        except asyncio.TimeoutError:\n            raise TimeoutError(\n                f\"Timed out closing connection after {self.socket_connect_timeout}\"\n            ) from None\n\n    async def _send_ping(self):\n        \"\"\"Send PING, expect PONG in return\"\"\"\n        await self.send_command(\"PING\", check_health=False)\n        if str_if_bytes(await self.read_response()) != \"PONG\":\n            raise ConnectionError(\"Bad response from PING health check\")\n\n    async def _ping_failed(self, error):\n        \"\"\"Function to call when PING fails\"\"\"\n        await self.disconnect()\n\n    async def check_health(self):\n        \"\"\"Check the health of the connection with a PING/PONG\"\"\"\n        if (\n            self.health_check_interval\n            and asyncio.get_running_loop().time() > self.next_health_check\n        ):\n            await self.retry.call_with_retry(self._send_ping, self._ping_failed)\n\n    async def _send_packed_command(self, command: Iterable[bytes]) -> None:\n        self._writer.writelines(command)\n        await self._writer.drain()\n\n    async def send_packed_command(\n        self, command: Union[bytes, str, Iterable[bytes]], check_health: bool = True\n    ) -> None:\n        if not self.is_connected:\n            await self.connect_check_health(check_health=False)\n        if check_health:\n            await self.check_health()\n\n        try:\n            if isinstance(command, str):\n                command = command.encode()\n            if isinstance(command, bytes):\n                command = [command]\n            if self.socket_timeout:\n                await asyncio.wait_for(\n                    self._send_packed_command(command), self.socket_timeout\n                )\n            else:\n                self._writer.writelines(command)\n                await self._writer.drain()\n        except asyncio.TimeoutError:\n            await self.disconnect(nowait=True)\n            raise TimeoutError(\"Timeout writing to socket\") from None\n        except OSError as e:\n            await self.disconnect(nowait=True)\n            if len(e.args) == 1:\n                err_no, errmsg = \"UNKNOWN\", e.args[0]\n            else:\n                err_no = e.args[0]\n                errmsg = e.args[1]\n            raise ConnectionError(\n                f\"Error {err_no} while writing to socket. {errmsg}.\"\n            ) from e\n        except BaseException:\n            # BaseExceptions can be raised when a socket send operation is not\n            # finished, e.g. due to a timeout.  Ideally, a caller could then re-try\n            # to send un-sent data. However, the send_packed_command() API\n            # does not support it so there is no point in keeping the connection open.\n            await self.disconnect(nowait=True)\n            raise\n\n    async def send_command(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Pack and send a command to the Redis server\"\"\"\n        await self.send_packed_command(\n            self.pack_command(*args), check_health=kwargs.get(\"check_health\", True)\n        )\n\n    async def can_read_destructive(self):\n        \"\"\"Poll the socket to see if there's data that can be read.\"\"\"\n        try:\n            return await self._parser.can_read_destructive()\n        except OSError as e:\n            await self.disconnect(nowait=True)\n            host_error = self._host_error()\n            raise ConnectionError(f\"Error while reading from {host_error}: {e.args}\")\n\n    async def read_response(\n        self,\n        disable_decoding: bool = False,\n        timeout: Optional[float] = None,\n        *,\n        disconnect_on_error: bool = True,\n        push_request: Optional[bool] = False,\n    ):\n        \"\"\"Read the response from a previously sent command\"\"\"\n        read_timeout = timeout if timeout is not None else self.socket_timeout\n        host_error = self._host_error()\n        try:\n            if read_timeout is not None and self.protocol in [\"3\", 3]:\n                async with async_timeout(read_timeout):\n                    response = await self._parser.read_response(\n                        disable_decoding=disable_decoding, push_request=push_request\n                    )\n            elif read_timeout is not None:\n                async with async_timeout(read_timeout):\n                    response = await self._parser.read_response(\n                        disable_decoding=disable_decoding\n                    )\n            elif self.protocol in [\"3\", 3]:\n                response = await self._parser.read_response(\n                    disable_decoding=disable_decoding, push_request=push_request\n                )\n            else:\n                response = await self._parser.read_response(\n                    disable_decoding=disable_decoding\n                )\n        except asyncio.TimeoutError:\n            if timeout is not None:\n                # user requested timeout, return None. Operation can be retried\n                return None\n            # it was a self.socket_timeout error.\n            if disconnect_on_error:\n                await self.disconnect(nowait=True)\n            raise TimeoutError(f\"Timeout reading from {host_error}\")\n        except OSError as e:\n            if disconnect_on_error:\n                await self.disconnect(nowait=True)\n            raise ConnectionError(f\"Error while reading from {host_error} : {e.args}\")\n        except BaseException:\n            # Also by default close in case of BaseException.  A lot of code\n            # relies on this behaviour when doing Command/Response pairs.\n            # See #1128.\n            if disconnect_on_error:\n                await self.disconnect(nowait=True)\n            raise\n\n        if self.health_check_interval:\n            next_time = asyncio.get_running_loop().time() + self.health_check_interval\n            self.next_health_check = next_time\n\n        if isinstance(response, ResponseError):\n            raise response from None\n        return response\n\n    def pack_command(self, *args: EncodableT) -> List[bytes]:\n        \"\"\"Pack a series of arguments into the Redis protocol\"\"\"\n        output = []\n        # the client might have included 1 or more literal arguments in\n        # the command name, e.g., 'CONFIG GET'. The Redis server expects these\n        # arguments to be sent separately, so split the first argument\n        # manually. These arguments should be bytestrings so that they are\n        # not encoded.\n        assert not isinstance(args[0], float)\n        if isinstance(args[0], str):\n            args = tuple(args[0].encode().split()) + args[1:]\n        elif b\" \" in args[0]:\n            args = tuple(args[0].split()) + args[1:]\n\n        buff = SYM_EMPTY.join((SYM_STAR, str(len(args)).encode(), SYM_CRLF))\n\n        buffer_cutoff = self._buffer_cutoff\n        for arg in map(self.encoder.encode, args):\n            # to avoid large string mallocs, chunk the command into the\n            # output list if we're sending large values or memoryviews\n            arg_length = len(arg)\n            if (\n                len(buff) > buffer_cutoff\n                or arg_length > buffer_cutoff\n                or isinstance(arg, memoryview)\n            ):\n                buff = SYM_EMPTY.join(\n                    (buff, SYM_DOLLAR, str(arg_length).encode(), SYM_CRLF)\n                )\n                output.append(buff)\n                output.append(arg)\n                buff = SYM_CRLF\n            else:\n                buff = SYM_EMPTY.join(\n                    (\n                        buff,\n                        SYM_DOLLAR,\n                        str(arg_length).encode(),\n                        SYM_CRLF,\n                        arg,\n                        SYM_CRLF,\n                    )\n                )\n        output.append(buff)\n        return output\n\n    def pack_commands(self, commands: Iterable[Iterable[EncodableT]]) -> List[bytes]:\n        \"\"\"Pack multiple commands into the Redis protocol\"\"\"\n        output: List[bytes] = []\n        pieces: List[bytes] = []\n        buffer_length = 0\n        buffer_cutoff = self._buffer_cutoff\n\n        for cmd in commands:\n            for chunk in self.pack_command(*cmd):\n                chunklen = len(chunk)\n                if (\n                    buffer_length > buffer_cutoff\n                    or chunklen > buffer_cutoff\n                    or isinstance(chunk, memoryview)\n                ):\n                    if pieces:\n                        output.append(SYM_EMPTY.join(pieces))\n                    buffer_length = 0\n                    pieces = []\n\n                if chunklen > buffer_cutoff or isinstance(chunk, memoryview):\n                    output.append(chunk)\n                else:\n                    pieces.append(chunk)\n                    buffer_length += chunklen\n\n        if pieces:\n            output.append(SYM_EMPTY.join(pieces))\n        return output\n\n    def _socket_is_empty(self):\n        \"\"\"Check if the socket is empty\"\"\"\n        return len(self._reader._buffer) == 0\n\n    async def process_invalidation_messages(self):\n        while not self._socket_is_empty():\n            await self.read_response(push_request=True)\n\n    def set_re_auth_token(self, token: TokenInterface):\n        self._re_auth_token = token\n\n    async def re_auth(self):\n        if self._re_auth_token is not None:\n            await self.send_command(\n                \"AUTH\",\n                self._re_auth_token.try_get(\"oid\"),\n                self._re_auth_token.get_value(),\n            )\n            await self.read_response()\n            self._re_auth_token = None\n\n\nclass Connection(AbstractConnection):\n    \"Manages TCP communication to and from a Redis server\"\n\n    def __init__(\n        self,\n        *,\n        host: str = \"localhost\",\n        port: Union[str, int] = 6379,\n        socket_keepalive: bool = False,\n        socket_keepalive_options: Optional[Mapping[int, Union[int, bytes]]] = None,\n        socket_type: int = 0,\n        **kwargs,\n    ):\n        self.host = host\n        self.port = int(port)\n        self.socket_keepalive = socket_keepalive\n        self.socket_keepalive_options = socket_keepalive_options or {}\n        self.socket_type = socket_type\n        super().__init__(**kwargs)\n\n    def repr_pieces(self):\n        pieces = [(\"host\", self.host), (\"port\", self.port), (\"db\", self.db)]\n        if self.client_name:\n            pieces.append((\"client_name\", self.client_name))\n        return pieces\n\n    def _connection_arguments(self) -> Mapping:\n        return {\"host\": self.host, \"port\": self.port}\n\n    async def _connect(self):\n        \"\"\"Create a TCP socket connection\"\"\"\n        async with async_timeout(self.socket_connect_timeout):\n            reader, writer = await asyncio.open_connection(\n                **self._connection_arguments()\n            )\n        self._reader = reader\n        self._writer = writer\n        sock = writer.transport.get_extra_info(\"socket\")\n        if sock:\n            sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n            try:\n                # TCP_KEEPALIVE\n                if self.socket_keepalive:\n                    sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)\n                    for k, v in self.socket_keepalive_options.items():\n                        sock.setsockopt(socket.SOL_TCP, k, v)\n\n            except (OSError, TypeError):\n                # `socket_keepalive_options` might contain invalid options\n                # causing an error. Do not leave the connection open.\n                writer.close()\n                raise\n\n    def _host_error(self) -> str:\n        return f\"{self.host}:{self.port}\"\n\n\nclass SSLConnection(Connection):\n    \"\"\"Manages SSL connections to and from the Redis server(s).\n    This class extends the Connection class, adding SSL functionality, and making\n    use of ssl.SSLContext (https://docs.python.org/3/library/ssl.html#ssl.SSLContext)\n    \"\"\"\n\n    def __init__(\n        self,\n        ssl_keyfile: Optional[str] = None,\n        ssl_certfile: Optional[str] = None,\n        ssl_cert_reqs: Union[str, ssl.VerifyMode] = \"required\",\n        ssl_ca_certs: Optional[str] = None,\n        ssl_ca_data: Optional[str] = None,\n        ssl_check_hostname: bool = True,\n        ssl_min_version: Optional[TLSVersion] = None,\n        ssl_ciphers: Optional[str] = None,\n        **kwargs,\n    ):\n        if not SSL_AVAILABLE:\n            raise RedisError(\"Python wasn't built with SSL support\")\n\n        self.ssl_context: RedisSSLContext = RedisSSLContext(\n            keyfile=ssl_keyfile,\n            certfile=ssl_certfile,\n            cert_reqs=ssl_cert_reqs,\n            ca_certs=ssl_ca_certs,\n            ca_data=ssl_ca_data,\n            check_hostname=ssl_check_hostname,\n            min_version=ssl_min_version,\n            ciphers=ssl_ciphers,\n        )\n        super().__init__(**kwargs)\n\n    def _connection_arguments(self) -> Mapping:\n        kwargs = super()._connection_arguments()\n        kwargs[\"ssl\"] = self.ssl_context.get()\n        return kwargs\n\n    @property\n    def keyfile(self):\n        return self.ssl_context.keyfile\n\n    @property\n    def certfile(self):\n        return self.ssl_context.certfile\n\n    @property\n    def cert_reqs(self):\n        return self.ssl_context.cert_reqs\n\n    @property\n    def ca_certs(self):\n        return self.ssl_context.ca_certs\n\n    @property\n    def ca_data(self):\n        return self.ssl_context.ca_data\n\n    @property\n    def check_hostname(self):\n        return self.ssl_context.check_hostname\n\n    @property\n    def min_version(self):\n        return self.ssl_context.min_version\n\n\nclass RedisSSLContext:\n    __slots__ = (\n        \"keyfile\",\n        \"certfile\",\n        \"cert_reqs\",\n        \"ca_certs\",\n        \"ca_data\",\n        \"context\",\n        \"check_hostname\",\n        \"min_version\",\n        \"ciphers\",\n    )\n\n    def __init__(\n        self,\n        keyfile: Optional[str] = None,\n        certfile: Optional[str] = None,\n        cert_reqs: Optional[Union[str, ssl.VerifyMode]] = None,\n        ca_certs: Optional[str] = None,\n        ca_data: Optional[str] = None,\n        check_hostname: bool = False,\n        min_version: Optional[TLSVersion] = None,\n        ciphers: Optional[str] = None,\n    ):\n        if not SSL_AVAILABLE:\n            raise RedisError(\"Python wasn't built with SSL support\")\n\n        self.keyfile = keyfile\n        self.certfile = certfile\n        if cert_reqs is None:\n            cert_reqs = ssl.CERT_NONE\n        elif isinstance(cert_reqs, str):\n            CERT_REQS = {  # noqa: N806\n                \"none\": ssl.CERT_NONE,\n                \"optional\": ssl.CERT_OPTIONAL,\n                \"required\": ssl.CERT_REQUIRED,\n            }\n            if cert_reqs not in CERT_REQS:\n                raise RedisError(\n                    f\"Invalid SSL Certificate Requirements Flag: {cert_reqs}\"\n                )\n            cert_reqs = CERT_REQS[cert_reqs]\n        self.cert_reqs = cert_reqs\n        self.ca_certs = ca_certs\n        self.ca_data = ca_data\n        self.check_hostname = (\n            check_hostname if self.cert_reqs != ssl.CERT_NONE else False\n        )\n        self.min_version = min_version\n        self.ciphers = ciphers\n        self.context: Optional[SSLContext] = None\n\n    def get(self) -> SSLContext:\n        if not self.context:\n            context = ssl.create_default_context()\n            context.check_hostname = self.check_hostname\n            context.verify_mode = self.cert_reqs\n            if self.certfile and self.keyfile:\n                context.load_cert_chain(certfile=self.certfile, keyfile=self.keyfile)\n            if self.ca_certs or self.ca_data:\n                context.load_verify_locations(cafile=self.ca_certs, cadata=self.ca_data)\n            if self.min_version is not None:\n                context.minimum_version = self.min_version\n            if self.ciphers is not None:\n                context.set_ciphers(self.ciphers)\n            self.context = context\n        return self.context\n\n\nclass UnixDomainSocketConnection(AbstractConnection):\n    \"Manages UDS communication to and from a Redis server\"\n\n    def __init__(self, *, path: str = \"\", **kwargs):\n        self.path = path\n        super().__init__(**kwargs)\n\n    def repr_pieces(self) -> Iterable[Tuple[str, Union[str, int]]]:\n        pieces = [(\"path\", self.path), (\"db\", self.db)]\n        if self.client_name:\n            pieces.append((\"client_name\", self.client_name))\n        return pieces\n\n    async def _connect(self):\n        async with async_timeout(self.socket_connect_timeout):\n            reader, writer = await asyncio.open_unix_connection(path=self.path)\n        self._reader = reader\n        self._writer = writer\n        await self.on_connect()\n\n    def _host_error(self) -> str:\n        return self.path\n\n\nFALSE_STRINGS = (\"0\", \"F\", \"FALSE\", \"N\", \"NO\")\n\n\ndef to_bool(value) -> Optional[bool]:\n    if value is None or value == \"\":\n        return None\n    if isinstance(value, str) and value.upper() in FALSE_STRINGS:\n        return False\n    return bool(value)\n\n\nURL_QUERY_ARGUMENT_PARSERS: Mapping[str, Callable[..., object]] = MappingProxyType(\n    {\n        \"db\": int,\n        \"socket_timeout\": float,\n        \"socket_connect_timeout\": float,\n        \"socket_keepalive\": to_bool,\n        \"retry_on_timeout\": to_bool,\n        \"max_connections\": int,\n        \"health_check_interval\": int,\n        \"ssl_check_hostname\": to_bool,\n        \"timeout\": float,\n    }\n)\n\n\nclass ConnectKwargs(TypedDict, total=False):\n    username: str\n    password: str\n    connection_class: Type[AbstractConnection]\n    host: str\n    port: int\n    db: int\n    path: str\n\n\ndef parse_url(url: str) -> ConnectKwargs:\n    parsed: ParseResult = urlparse(url)\n    kwargs: ConnectKwargs = {}\n\n    for name, value_list in parse_qs(parsed.query).items():\n        if value_list and len(value_list) > 0:\n            value = unquote(value_list[0])\n            parser = URL_QUERY_ARGUMENT_PARSERS.get(name)\n            if parser:\n                try:\n                    kwargs[name] = parser(value)\n                except (TypeError, ValueError):\n                    raise ValueError(f\"Invalid value for '{name}' in connection URL.\")\n            else:\n                kwargs[name] = value\n\n    if parsed.username:\n        kwargs[\"username\"] = unquote(parsed.username)\n    if parsed.password:\n        kwargs[\"password\"] = unquote(parsed.password)\n\n    # We only support redis://, rediss:// and unix:// schemes.\n    if parsed.scheme == \"unix\":\n        if parsed.path:\n            kwargs[\"path\"] = unquote(parsed.path)\n        kwargs[\"connection_class\"] = UnixDomainSocketConnection\n\n    elif parsed.scheme in (\"redis\", \"rediss\"):\n        if parsed.hostname:\n            kwargs[\"host\"] = unquote(parsed.hostname)\n        if parsed.port:\n            kwargs[\"port\"] = int(parsed.port)\n\n        # If there's a path argument, use it as the db argument if a\n        # querystring value wasn't specified\n        if parsed.path and \"db\" not in kwargs:\n            try:\n                kwargs[\"db\"] = int(unquote(parsed.path).replace(\"/\", \"\"))\n            except (AttributeError, ValueError):\n                pass\n\n        if parsed.scheme == \"rediss\":\n            kwargs[\"connection_class\"] = SSLConnection\n    else:\n        valid_schemes = \"redis://, rediss://, unix://\"\n        raise ValueError(\n            f\"Redis URL must specify one of the following schemes ({valid_schemes})\"\n        )\n\n    return kwargs\n\n\n_CP = TypeVar(\"_CP\", bound=\"ConnectionPool\")\n\n\nclass ConnectionPool:\n    \"\"\"\n    Create a connection pool. ``If max_connections`` is set, then this\n    object raises :py:class:`~redis.ConnectionError` when the pool's\n    limit is reached.\n\n    By default, TCP connections are created unless ``connection_class``\n    is specified. Use :py:class:`~redis.UnixDomainSocketConnection` for\n    unix sockets.\n    :py:class:`~redis.SSLConnection` can be used for SSL enabled connections.\n\n    Any additional keyword arguments are passed to the constructor of\n    ``connection_class``.\n    \"\"\"\n\n    @classmethod\n    def from_url(cls: Type[_CP], url: str, **kwargs) -> _CP:\n        \"\"\"\n        Return a connection pool configured from the given URL.\n\n        For example::\n\n            redis://[[username]:[password]]@localhost:6379/0\n            rediss://[[username]:[password]]@localhost:6379/0\n            unix://[username@]/path/to/socket.sock?db=0[&password=password]\n\n        Three URL schemes are supported:\n\n        - `redis://` creates a TCP socket connection. See more at:\n          <https://www.iana.org/assignments/uri-schemes/prov/redis>\n        - `rediss://` creates a SSL wrapped TCP socket connection. See more at:\n          <https://www.iana.org/assignments/uri-schemes/prov/rediss>\n        - ``unix://``: creates a Unix Domain Socket connection.\n\n        The username, password, hostname, path and all querystring values\n        are passed through urllib.parse.unquote in order to replace any\n        percent-encoded values with their corresponding characters.\n\n        There are several ways to specify a database number. The first value\n        found will be used:\n\n        1. A ``db`` querystring option, e.g. redis://localhost?db=0\n\n        2. If using the redis:// or rediss:// schemes, the path argument\n               of the url, e.g. redis://localhost/0\n\n        3. A ``db`` keyword argument to this function.\n\n        If none of these options are specified, the default db=0 is used.\n\n        All querystring options are cast to their appropriate Python types.\n        Boolean arguments can be specified with string values \"True\"/\"False\"\n        or \"Yes\"/\"No\". Values that cannot be properly cast cause a\n        ``ValueError`` to be raised. Once parsed, the querystring arguments\n        and keyword arguments are passed to the ``ConnectionPool``'s\n        class initializer. In the case of conflicting arguments, querystring\n        arguments always win.\n        \"\"\"\n        url_options = parse_url(url)\n        kwargs.update(url_options)\n        return cls(**kwargs)\n\n    def __init__(\n        self,\n        connection_class: Type[AbstractConnection] = Connection,\n        max_connections: Optional[int] = None,\n        **connection_kwargs,\n    ):\n        max_connections = max_connections or 2**31\n        if not isinstance(max_connections, int) or max_connections < 0:\n            raise ValueError('\"max_connections\" must be a positive integer')\n\n        self.connection_class = connection_class\n        self.connection_kwargs = connection_kwargs\n        self.max_connections = max_connections\n\n        self._available_connections: List[AbstractConnection] = []\n        self._in_use_connections: Set[AbstractConnection] = set()\n        self.encoder_class = self.connection_kwargs.get(\"encoder_class\", Encoder)\n        self._lock = asyncio.Lock()\n        self._event_dispatcher = self.connection_kwargs.get(\"event_dispatcher\", None)\n        if self._event_dispatcher is None:\n            self._event_dispatcher = EventDispatcher()\n\n    def __repr__(self):\n        conn_kwargs = \",\".join([f\"{k}={v}\" for k, v in self.connection_kwargs.items()])\n        return (\n            f\"<{self.__class__.__module__}.{self.__class__.__name__}\"\n            f\"(<{self.connection_class.__module__}.{self.connection_class.__name__}\"\n            f\"({conn_kwargs})>)>\"\n        )\n\n    def reset(self):\n        self._available_connections = []\n        self._in_use_connections = weakref.WeakSet()\n\n    def can_get_connection(self) -> bool:\n        \"\"\"Return True if a connection can be retrieved from the pool.\"\"\"\n        return (\n            self._available_connections\n            or len(self._in_use_connections) < self.max_connections\n        )\n\n    @deprecated_args(\n        args_to_warn=[\"*\"],\n        reason=\"Use get_connection() without args instead\",\n        version=\"5.3.0\",\n    )\n    async def get_connection(self, command_name=None, *keys, **options):\n        async with self._lock:\n            \"\"\"Get a connected connection from the pool\"\"\"\n            connection = self.get_available_connection()\n            try:\n                await self.ensure_connection(connection)\n            except BaseException:\n                await self.release(connection)\n                raise\n\n        return connection\n\n    def get_available_connection(self):\n        \"\"\"Get a connection from the pool, without making sure it is connected\"\"\"\n        try:\n            connection = self._available_connections.pop()\n        except IndexError:\n            if len(self._in_use_connections) >= self.max_connections:\n                raise ConnectionError(\"Too many connections\") from None\n            connection = self.make_connection()\n        self._in_use_connections.add(connection)\n        return connection\n\n    def get_encoder(self):\n        \"\"\"Return an encoder based on encoding settings\"\"\"\n        kwargs = self.connection_kwargs\n        return self.encoder_class(\n            encoding=kwargs.get(\"encoding\", \"utf-8\"),\n            encoding_errors=kwargs.get(\"encoding_errors\", \"strict\"),\n            decode_responses=kwargs.get(\"decode_responses\", False),\n        )\n\n    def make_connection(self):\n        \"\"\"Create a new connection.  Can be overridden by child classes.\"\"\"\n        return self.connection_class(**self.connection_kwargs)\n\n    async def ensure_connection(self, connection: AbstractConnection):\n        \"\"\"Ensure that the connection object is connected and valid\"\"\"\n        await connection.connect()\n        # connections that the pool provides should be ready to send\n        # a command. if not, the connection was either returned to the\n        # pool before all data has been read or the socket has been\n        # closed. either way, reconnect and verify everything is good.\n        try:\n            if await connection.can_read_destructive():\n                raise ConnectionError(\"Connection has data\") from None\n        except (ConnectionError, TimeoutError, OSError):\n            await connection.disconnect()\n            await connection.connect()\n            if await connection.can_read_destructive():\n                raise ConnectionError(\"Connection not ready\") from None\n\n    async def release(self, connection: AbstractConnection):\n        \"\"\"Releases the connection back to the pool\"\"\"\n        # Connections should always be returned to the correct pool,\n        # not doing so is an error that will cause an exception here.\n        self._in_use_connections.remove(connection)\n        self._available_connections.append(connection)\n        await self._event_dispatcher.dispatch_async(\n            AsyncAfterConnectionReleasedEvent(connection)\n        )\n\n    async def disconnect(self, inuse_connections: bool = True):\n        \"\"\"\n        Disconnects connections in the pool\n\n        If ``inuse_connections`` is True, disconnect connections that are\n        current in use, potentially by other tasks. Otherwise only disconnect\n        connections that are idle in the pool.\n        \"\"\"\n        if inuse_connections:\n            connections: Iterable[AbstractConnection] = chain(\n                self._available_connections, self._in_use_connections\n            )\n        else:\n            connections = self._available_connections\n        resp = await asyncio.gather(\n            *(connection.disconnect() for connection in connections),\n            return_exceptions=True,\n        )\n        exc = next((r for r in resp if isinstance(r, BaseException)), None)\n        if exc:\n            raise exc\n\n    async def aclose(self) -> None:\n        \"\"\"Close the pool, disconnecting all connections\"\"\"\n        await self.disconnect()\n\n    def set_retry(self, retry: \"Retry\") -> None:\n        for conn in self._available_connections:\n            conn.retry = retry\n        for conn in self._in_use_connections:\n            conn.retry = retry\n\n    async def re_auth_callback(self, token: TokenInterface):\n        async with self._lock:\n            for conn in self._available_connections:\n                await conn.retry.call_with_retry(\n                    lambda: conn.send_command(\n                        \"AUTH\", token.try_get(\"oid\"), token.get_value()\n                    ),\n                    lambda error: self._mock(error),\n                )\n                await conn.retry.call_with_retry(\n                    lambda: conn.read_response(), lambda error: self._mock(error)\n                )\n            for conn in self._in_use_connections:\n                conn.set_re_auth_token(token)\n\n    async def _mock(self, error: RedisError):\n        \"\"\"\n        Dummy functions, needs to be passed as error callback to retry object.\n        :param error:\n        :return:\n        \"\"\"\n        pass\n\n\nclass BlockingConnectionPool(ConnectionPool):\n    \"\"\"\n    A blocking connection pool::\n\n        >>> from redis.asyncio import Redis, BlockingConnectionPool\n        >>> client = Redis.from_pool(BlockingConnectionPool())\n\n    It performs the same function as the default\n    :py:class:`~redis.asyncio.ConnectionPool` implementation, in that,\n    it maintains a pool of reusable connections that can be shared by\n    multiple async redis clients.\n\n    The difference is that, in the event that a client tries to get a\n    connection from the pool when all of connections are in use, rather than\n    raising a :py:class:`~redis.ConnectionError` (as the default\n    :py:class:`~redis.asyncio.ConnectionPool` implementation does), it\n    blocks the current `Task` for a specified number of seconds until\n    a connection becomes available.\n\n    Use ``max_connections`` to increase / decrease the pool size::\n\n        >>> pool = BlockingConnectionPool(max_connections=10)\n\n    Use ``timeout`` to tell it either how many seconds to wait for a connection\n    to become available, or to block forever:\n\n        >>> # Block forever.\n        >>> pool = BlockingConnectionPool(timeout=None)\n\n        >>> # Raise a ``ConnectionError`` after five seconds if a connection is\n        >>> # not available.\n        >>> pool = BlockingConnectionPool(timeout=5)\n    \"\"\"\n\n    def __init__(\n        self,\n        max_connections: int = 50,\n        timeout: Optional[int] = 20,\n        connection_class: Type[AbstractConnection] = Connection,\n        queue_class: Type[asyncio.Queue] = asyncio.LifoQueue,  # deprecated\n        **connection_kwargs,\n    ):\n        super().__init__(\n            connection_class=connection_class,\n            max_connections=max_connections,\n            **connection_kwargs,\n        )\n        self._condition = asyncio.Condition()\n        self.timeout = timeout\n\n    @deprecated_args(\n        args_to_warn=[\"*\"],\n        reason=\"Use get_connection() without args instead\",\n        version=\"5.3.0\",\n    )\n    async def get_connection(self, command_name=None, *keys, **options):\n        \"\"\"Gets a connection from the pool, blocking until one is available\"\"\"\n        try:\n            async with self._condition:\n                async with async_timeout(self.timeout):\n                    await self._condition.wait_for(self.can_get_connection)\n                    connection = super().get_available_connection()\n        except asyncio.TimeoutError as err:\n            raise ConnectionError(\"No connection available.\") from err\n\n        # We now perform the connection check outside of the lock.\n        try:\n            await self.ensure_connection(connection)\n            return connection\n        except BaseException:\n            await self.release(connection)\n            raise\n\n    async def release(self, connection: AbstractConnection):\n        \"\"\"Releases the connection back to the pool.\"\"\"\n        async with self._condition:\n            await super().release(connection)\n            self._condition.notify()\n", 1339], "/usr/local/lib/python3.11/asyncio/timeouts.py": ["import enum\n\nfrom types import TracebackType\nfrom typing import final, Optional, Type\n\nfrom . import events\nfrom . import exceptions\nfrom . import tasks\n\n\n__all__ = (\n    \"Timeout\",\n    \"timeout\",\n    \"timeout_at\",\n)\n\n\nclass _State(enum.Enum):\n    CREATED = \"created\"\n    ENTERED = \"active\"\n    EXPIRING = \"expiring\"\n    EXPIRED = \"expired\"\n    EXITED = \"finished\"\n\n\n@final\nclass Timeout:\n    \"\"\"Asynchronous context manager for cancelling overdue coroutines.\n\n    Use `timeout()` or `timeout_at()` rather than instantiating this class directly.\n    \"\"\"\n\n    def __init__(self, when: Optional[float]) -> None:\n        \"\"\"Schedule a timeout that will trigger at a given loop time.\n\n        - If `when` is `None`, the timeout will never trigger.\n        - If `when < loop.time()`, the timeout will trigger on the next\n          iteration of the event loop.\n        \"\"\"\n        self._state = _State.CREATED\n\n        self._timeout_handler: Optional[events.TimerHandle] = None\n        self._task: Optional[tasks.Task] = None\n        self._when = when\n\n    def when(self) -> Optional[float]:\n        \"\"\"Return the current deadline.\"\"\"\n        return self._when\n\n    def reschedule(self, when: Optional[float]) -> None:\n        \"\"\"Reschedule the timeout.\"\"\"\n        if self._state is not _State.ENTERED:\n            if self._state is _State.CREATED:\n                raise RuntimeError(\"Timeout has not been entered\")\n            raise RuntimeError(\n                f\"Cannot change state of {self._state.value} Timeout\",\n            )\n\n        self._when = when\n\n        if self._timeout_handler is not None:\n            self._timeout_handler.cancel()\n\n        if when is None:\n            self._timeout_handler = None\n        else:\n            loop = events.get_running_loop()\n            if when <= loop.time():\n                self._timeout_handler = loop.call_soon(self._on_timeout)\n            else:\n                self._timeout_handler = loop.call_at(when, self._on_timeout)\n\n    def expired(self) -> bool:\n        \"\"\"Is timeout expired during execution?\"\"\"\n        return self._state in (_State.EXPIRING, _State.EXPIRED)\n\n    def __repr__(self) -> str:\n        info = ['']\n        if self._state is _State.ENTERED:\n            when = round(self._when, 3) if self._when is not None else None\n            info.append(f\"when={when}\")\n        info_str = ' '.join(info)\n        return f\"<Timeout [{self._state.value}]{info_str}>\"\n\n    async def __aenter__(self) -> \"Timeout\":\n        if self._state is not _State.CREATED:\n            raise RuntimeError(\"Timeout has already been entered\")\n        task = tasks.current_task()\n        if task is None:\n            raise RuntimeError(\"Timeout should be used inside a task\")\n        self._state = _State.ENTERED\n        self._task = task\n        self._cancelling = self._task.cancelling()\n        self.reschedule(self._when)\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> Optional[bool]:\n        assert self._state in (_State.ENTERED, _State.EXPIRING)\n\n        if self._timeout_handler is not None:\n            self._timeout_handler.cancel()\n            self._timeout_handler = None\n\n        if self._state is _State.EXPIRING:\n            self._state = _State.EXPIRED\n\n            if self._task.uncancel() <= self._cancelling and exc_type is exceptions.CancelledError:\n                # Since there are no new cancel requests, we're\n                # handling this.\n                raise TimeoutError from exc_val\n        elif self._state is _State.ENTERED:\n            self._state = _State.EXITED\n\n        return None\n\n    def _on_timeout(self) -> None:\n        assert self._state is _State.ENTERED\n        self._task.cancel()\n        self._state = _State.EXPIRING\n        # drop the reference early\n        self._timeout_handler = None\n\n\ndef timeout(delay: Optional[float]) -> Timeout:\n    \"\"\"Timeout async context manager.\n\n    Useful in cases when you want to apply timeout logic around block\n    of code or in cases when asyncio.wait_for is not suitable. For example:\n\n    >>> async with asyncio.timeout(10):  # 10 seconds timeout\n    ...     await long_running_task()\n\n\n    delay - value in seconds or None to disable timeout logic\n\n    long_running_task() is interrupted by raising asyncio.CancelledError,\n    the top-most affected timeout() context manager converts CancelledError\n    into TimeoutError.\n    \"\"\"\n    loop = events.get_running_loop()\n    return Timeout(loop.time() + delay if delay is not None else None)\n\n\ndef timeout_at(when: Optional[float]) -> Timeout:\n    \"\"\"Schedule the timeout at absolute time.\n\n    Like timeout() but argument gives absolute time in the same clock system\n    as loop.time().\n\n    Please note: it is not POSIX time but a time with\n    undefined starting base, e.g. the time of the system power on.\n\n    >>> async with asyncio.timeout_at(loop.time() + 10):\n    ...     await long_running_task()\n\n\n    when - a deadline when timeout occurs or None to disable timeout logic\n\n    long_running_task() is interrupted by raising asyncio.CancelledError,\n    the top-most affected timeout() context manager converts CancelledError\n    into TimeoutError.\n    \"\"\"\n    return Timeout(when)\n", 168], "/usr/local/lib/python3.11/asyncio/streams.py": ["__all__ = (\n    'StreamReader', 'StreamWriter', 'StreamReaderProtocol',\n    'open_connection', 'start_server')\n\nimport collections\nimport socket\nimport sys\nimport warnings\nimport weakref\n\nif hasattr(socket, 'AF_UNIX'):\n    __all__ += ('open_unix_connection', 'start_unix_server')\n\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import format_helpers\nfrom . import protocols\nfrom .log import logger\nfrom .tasks import sleep\n\n\n_DEFAULT_LIMIT = 2 ** 16  # 64 KiB\n\n\nasync def open_connection(host=None, port=None, *,\n                          limit=_DEFAULT_LIMIT, **kwds):\n    \"\"\"A wrapper for create_connection() returning a (reader, writer) pair.\n\n    The reader returned is a StreamReader instance; the writer is a\n    StreamWriter instance.\n\n    The arguments are all the usual arguments to create_connection()\n    except protocol_factory; most common are positional host and port,\n    with various optional keyword arguments following.\n\n    Additional optional keyword arguments are loop (to set the event loop\n    instance to use) and limit (to set the buffer limit passed to the\n    StreamReader).\n\n    (If you want to customize the StreamReader and/or\n    StreamReaderProtocol classes, just copy the code -- there's\n    really nothing special here except some convenience.)\n    \"\"\"\n    loop = events.get_running_loop()\n    reader = StreamReader(limit=limit, loop=loop)\n    protocol = StreamReaderProtocol(reader, loop=loop)\n    transport, _ = await loop.create_connection(\n        lambda: protocol, host, port, **kwds)\n    writer = StreamWriter(transport, protocol, reader, loop)\n    return reader, writer\n\n\nasync def start_server(client_connected_cb, host=None, port=None, *,\n                       limit=_DEFAULT_LIMIT, **kwds):\n    \"\"\"Start a socket server, call back for each client connected.\n\n    The first parameter, `client_connected_cb`, takes two parameters:\n    client_reader, client_writer.  client_reader is a StreamReader\n    object, while client_writer is a StreamWriter object.  This\n    parameter can either be a plain callback function or a coroutine;\n    if it is a coroutine, it will be automatically converted into a\n    Task.\n\n    The rest of the arguments are all the usual arguments to\n    loop.create_server() except protocol_factory; most common are\n    positional host and port, with various optional keyword arguments\n    following.  The return value is the same as loop.create_server().\n\n    Additional optional keyword argument is limit (to set the buffer\n    limit passed to the StreamReader).\n\n    The return value is the same as loop.create_server(), i.e. a\n    Server object which can be used to stop the service.\n    \"\"\"\n    loop = events.get_running_loop()\n\n    def factory():\n        reader = StreamReader(limit=limit, loop=loop)\n        protocol = StreamReaderProtocol(reader, client_connected_cb,\n                                        loop=loop)\n        return protocol\n\n    return await loop.create_server(factory, host, port, **kwds)\n\n\nif hasattr(socket, 'AF_UNIX'):\n    # UNIX Domain Sockets are supported on this platform\n\n    async def open_unix_connection(path=None, *,\n                                   limit=_DEFAULT_LIMIT, **kwds):\n        \"\"\"Similar to `open_connection` but works with UNIX Domain Sockets.\"\"\"\n        loop = events.get_running_loop()\n\n        reader = StreamReader(limit=limit, loop=loop)\n        protocol = StreamReaderProtocol(reader, loop=loop)\n        transport, _ = await loop.create_unix_connection(\n            lambda: protocol, path, **kwds)\n        writer = StreamWriter(transport, protocol, reader, loop)\n        return reader, writer\n\n    async def start_unix_server(client_connected_cb, path=None, *,\n                                limit=_DEFAULT_LIMIT, **kwds):\n        \"\"\"Similar to `start_server` but works with UNIX Domain Sockets.\"\"\"\n        loop = events.get_running_loop()\n\n        def factory():\n            reader = StreamReader(limit=limit, loop=loop)\n            protocol = StreamReaderProtocol(reader, client_connected_cb,\n                                            loop=loop)\n            return protocol\n\n        return await loop.create_unix_server(factory, path, **kwds)\n\n\nclass FlowControlMixin(protocols.Protocol):\n    \"\"\"Reusable flow control logic for StreamWriter.drain().\n\n    This implements the protocol methods pause_writing(),\n    resume_writing() and connection_lost().  If the subclass overrides\n    these it must call the super methods.\n\n    StreamWriter.drain() must wait for _drain_helper() coroutine.\n    \"\"\"\n\n    def __init__(self, loop=None):\n        if loop is None:\n            self._loop = events._get_event_loop(stacklevel=4)\n        else:\n            self._loop = loop\n        self._paused = False\n        self._drain_waiters = collections.deque()\n        self._connection_lost = False\n\n    def pause_writing(self):\n        assert not self._paused\n        self._paused = True\n        if self._loop.get_debug():\n            logger.debug(\"%r pauses writing\", self)\n\n    def resume_writing(self):\n        assert self._paused\n        self._paused = False\n        if self._loop.get_debug():\n            logger.debug(\"%r resumes writing\", self)\n\n        for waiter in self._drain_waiters:\n            if not waiter.done():\n                waiter.set_result(None)\n\n    def connection_lost(self, exc):\n        self._connection_lost = True\n        # Wake up the writer(s) if currently paused.\n        if not self._paused:\n            return\n\n        for waiter in self._drain_waiters:\n            if not waiter.done():\n                if exc is None:\n                    waiter.set_result(None)\n                else:\n                    waiter.set_exception(exc)\n\n    async def _drain_helper(self):\n        if self._connection_lost:\n            raise ConnectionResetError('Connection lost')\n        if not self._paused:\n            return\n        waiter = self._loop.create_future()\n        self._drain_waiters.append(waiter)\n        try:\n            await waiter\n        finally:\n            self._drain_waiters.remove(waiter)\n\n    def _get_close_waiter(self, stream):\n        raise NotImplementedError\n\n\nclass StreamReaderProtocol(FlowControlMixin, protocols.Protocol):\n    \"\"\"Helper class to adapt between Protocol and StreamReader.\n\n    (This is a helper class instead of making StreamReader itself a\n    Protocol subclass, because the StreamReader has other potential\n    uses, and to prevent the user of the StreamReader to accidentally\n    call inappropriate methods of the protocol.)\n    \"\"\"\n\n    _source_traceback = None\n\n    def __init__(self, stream_reader, client_connected_cb=None, loop=None):\n        super().__init__(loop=loop)\n        if stream_reader is not None:\n            self._stream_reader_wr = weakref.ref(stream_reader)\n            self._source_traceback = stream_reader._source_traceback\n        else:\n            self._stream_reader_wr = None\n        if client_connected_cb is not None:\n            # This is a stream created by the `create_server()` function.\n            # Keep a strong reference to the reader until a connection\n            # is established.\n            self._strong_reader = stream_reader\n        self._reject_connection = False\n        self._stream_writer = None\n        self._task = None\n        self._transport = None\n        self._client_connected_cb = client_connected_cb\n        self._over_ssl = False\n        self._closed = self._loop.create_future()\n\n    @property\n    def _stream_reader(self):\n        if self._stream_reader_wr is None:\n            return None\n        return self._stream_reader_wr()\n\n    def _replace_writer(self, writer):\n        loop = self._loop\n        transport = writer.transport\n        self._stream_writer = writer\n        self._transport = transport\n        self._over_ssl = transport.get_extra_info('sslcontext') is not None\n\n    def connection_made(self, transport):\n        if self._reject_connection:\n            context = {\n                'message': ('An open stream was garbage collected prior to '\n                            'establishing network connection; '\n                            'call \"stream.close()\" explicitly.')\n            }\n            if self._source_traceback:\n                context['source_traceback'] = self._source_traceback\n            self._loop.call_exception_handler(context)\n            transport.abort()\n            return\n        self._transport = transport\n        reader = self._stream_reader\n        if reader is not None:\n            reader.set_transport(transport)\n        self._over_ssl = transport.get_extra_info('sslcontext') is not None\n        if self._client_connected_cb is not None:\n            self._stream_writer = StreamWriter(transport, self,\n                                               reader,\n                                               self._loop)\n            res = self._client_connected_cb(reader,\n                                            self._stream_writer)\n            if coroutines.iscoroutine(res):\n                def callback(task):\n                    if task.cancelled():\n                        transport.close()\n                        return\n                    exc = task.exception()\n                    if exc is not None:\n                        self._loop.call_exception_handler({\n                            'message': 'Unhandled exception in client_connected_cb',\n                            'exception': exc,\n                            'transport': transport,\n                        })\n                        transport.close()\n\n                self._task = self._loop.create_task(res)\n                self._task.add_done_callback(callback)\n\n            self._strong_reader = None\n\n    def connection_lost(self, exc):\n        reader = self._stream_reader\n        if reader is not None:\n            if exc is None:\n                reader.feed_eof()\n            else:\n                reader.set_exception(exc)\n        if not self._closed.done():\n            if exc is None:\n                self._closed.set_result(None)\n            else:\n                self._closed.set_exception(exc)\n        super().connection_lost(exc)\n        self._stream_reader_wr = None\n        self._stream_writer = None\n        self._task = None\n        self._transport = None\n\n    def data_received(self, data):\n        reader = self._stream_reader\n        if reader is not None:\n            reader.feed_data(data)\n\n    def eof_received(self):\n        reader = self._stream_reader\n        if reader is not None:\n            reader.feed_eof()\n        if self._over_ssl:\n            # Prevent a warning in SSLProtocol.eof_received:\n            # \"returning true from eof_received()\n            # has no effect when using ssl\"\n            return False\n        return True\n\n    def _get_close_waiter(self, stream):\n        return self._closed\n\n    def __del__(self):\n        # Prevent reports about unhandled exceptions.\n        # Better than self._closed._log_traceback = False hack\n        try:\n            closed = self._closed\n        except AttributeError:\n            pass  # failed constructor\n        else:\n            if closed.done() and not closed.cancelled():\n                closed.exception()\n\n\nclass StreamWriter:\n    \"\"\"Wraps a Transport.\n\n    This exposes write(), writelines(), [can_]write_eof(),\n    get_extra_info() and close().  It adds drain() which returns an\n    optional Future on which you can wait for flow control.  It also\n    adds a transport property which references the Transport\n    directly.\n    \"\"\"\n\n    def __init__(self, transport, protocol, reader, loop):\n        self._transport = transport\n        self._protocol = protocol\n        # drain() expects that the reader has an exception() method\n        assert reader is None or isinstance(reader, StreamReader)\n        self._reader = reader\n        self._loop = loop\n        self._complete_fut = self._loop.create_future()\n        self._complete_fut.set_result(None)\n\n    def __repr__(self):\n        info = [self.__class__.__name__, f'transport={self._transport!r}']\n        if self._reader is not None:\n            info.append(f'reader={self._reader!r}')\n        return '<{}>'.format(' '.join(info))\n\n    @property\n    def transport(self):\n        return self._transport\n\n    def write(self, data):\n        self._transport.write(data)\n\n    def writelines(self, data):\n        self._transport.writelines(data)\n\n    def write_eof(self):\n        return self._transport.write_eof()\n\n    def can_write_eof(self):\n        return self._transport.can_write_eof()\n\n    def close(self):\n        return self._transport.close()\n\n    def is_closing(self):\n        return self._transport.is_closing()\n\n    async def wait_closed(self):\n        await self._protocol._get_close_waiter(self)\n\n    def get_extra_info(self, name, default=None):\n        return self._transport.get_extra_info(name, default)\n\n    async def drain(self):\n        \"\"\"Flush the write buffer.\n\n        The intended use is to write\n\n          w.write(data)\n          await w.drain()\n        \"\"\"\n        if self._reader is not None:\n            exc = self._reader.exception()\n            if exc is not None:\n                raise exc\n        if self._transport.is_closing():\n            # Wait for protocol.connection_lost() call\n            # Raise connection closing error if any,\n            # ConnectionResetError otherwise\n            # Yield to the event loop so connection_lost() may be\n            # called.  Without this, _drain_helper() would return\n            # immediately, and code that calls\n            #     write(...); await drain()\n            # in a loop would never call connection_lost(), so it\n            # would not see an error when the socket is closed.\n            await sleep(0)\n        await self._protocol._drain_helper()\n\n    async def start_tls(self, sslcontext, *,\n                        server_hostname=None,\n                        ssl_handshake_timeout=None):\n        \"\"\"Upgrade an existing stream-based connection to TLS.\"\"\"\n        server_side = self._protocol._client_connected_cb is not None\n        protocol = self._protocol\n        await self.drain()\n        new_transport = await self._loop.start_tls(  # type: ignore\n            self._transport, protocol, sslcontext,\n            server_side=server_side, server_hostname=server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout)\n        self._transport = new_transport\n        protocol._replace_writer(self)\n\n    def __del__(self):\n        if not self._transport.is_closing():\n            if self._loop.is_closed():\n                warnings.warn(\"loop is closed\", ResourceWarning)\n            else:\n                self.close()\n                warnings.warn(f\"unclosed {self!r}\", ResourceWarning)\n\nclass StreamReader:\n\n    _source_traceback = None\n\n    def __init__(self, limit=_DEFAULT_LIMIT, loop=None):\n        # The line length limit is  a security feature;\n        # it also doubles as half the buffer limit.\n\n        if limit <= 0:\n            raise ValueError('Limit cannot be <= 0')\n\n        self._limit = limit\n        if loop is None:\n            self._loop = events._get_event_loop()\n        else:\n            self._loop = loop\n        self._buffer = bytearray()\n        self._eof = False    # Whether we're done.\n        self._waiter = None  # A future used by _wait_for_data()\n        self._exception = None\n        self._transport = None\n        self._paused = False\n        if self._loop.get_debug():\n            self._source_traceback = format_helpers.extract_stack(\n                sys._getframe(1))\n\n    def __repr__(self):\n        info = ['StreamReader']\n        if self._buffer:\n            info.append(f'{len(self._buffer)} bytes')\n        if self._eof:\n            info.append('eof')\n        if self._limit != _DEFAULT_LIMIT:\n            info.append(f'limit={self._limit}')\n        if self._waiter:\n            info.append(f'waiter={self._waiter!r}')\n        if self._exception:\n            info.append(f'exception={self._exception!r}')\n        if self._transport:\n            info.append(f'transport={self._transport!r}')\n        if self._paused:\n            info.append('paused')\n        return '<{}>'.format(' '.join(info))\n\n    def exception(self):\n        return self._exception\n\n    def set_exception(self, exc):\n        self._exception = exc\n\n        waiter = self._waiter\n        if waiter is not None:\n            self._waiter = None\n            if not waiter.cancelled():\n                waiter.set_exception(exc)\n\n    def _wakeup_waiter(self):\n        \"\"\"Wakeup read*() functions waiting for data or EOF.\"\"\"\n        waiter = self._waiter\n        if waiter is not None:\n            self._waiter = None\n            if not waiter.cancelled():\n                waiter.set_result(None)\n\n    def set_transport(self, transport):\n        assert self._transport is None, 'Transport already set'\n        self._transport = transport\n\n    def _maybe_resume_transport(self):\n        if self._paused and len(self._buffer) <= self._limit:\n            self._paused = False\n            self._transport.resume_reading()\n\n    def feed_eof(self):\n        self._eof = True\n        self._wakeup_waiter()\n\n    def at_eof(self):\n        \"\"\"Return True if the buffer is empty and 'feed_eof' was called.\"\"\"\n        return self._eof and not self._buffer\n\n    def feed_data(self, data):\n        assert not self._eof, 'feed_data after feed_eof'\n\n        if not data:\n            return\n\n        self._buffer.extend(data)\n        self._wakeup_waiter()\n\n        if (self._transport is not None and\n                not self._paused and\n                len(self._buffer) > 2 * self._limit):\n            try:\n                self._transport.pause_reading()\n            except NotImplementedError:\n                # The transport can't be paused.\n                # We'll just have to buffer all data.\n                # Forget the transport so we don't keep trying.\n                self._transport = None\n            else:\n                self._paused = True\n\n    async def _wait_for_data(self, func_name):\n        \"\"\"Wait until feed_data() or feed_eof() is called.\n\n        If stream was paused, automatically resume it.\n        \"\"\"\n        # StreamReader uses a future to link the protocol feed_data() method\n        # to a read coroutine. Running two read coroutines at the same time\n        # would have an unexpected behaviour. It would not possible to know\n        # which coroutine would get the next data.\n        if self._waiter is not None:\n            raise RuntimeError(\n                f'{func_name}() called while another coroutine is '\n                f'already waiting for incoming data')\n\n        assert not self._eof, '_wait_for_data after EOF'\n\n        # Waiting for data while paused will make deadlock, so prevent it.\n        # This is essential for readexactly(n) for case when n > self._limit.\n        if self._paused:\n            self._paused = False\n            self._transport.resume_reading()\n\n        self._waiter = self._loop.create_future()\n        try:\n            await self._waiter\n        finally:\n            self._waiter = None\n\n    async def readline(self):\n        \"\"\"Read chunk of data from the stream until newline (b'\\n') is found.\n\n        On success, return chunk that ends with newline. If only partial\n        line can be read due to EOF, return incomplete line without\n        terminating newline. When EOF was reached while no bytes read, empty\n        bytes object is returned.\n\n        If limit is reached, ValueError will be raised. In that case, if\n        newline was found, complete line including newline will be removed\n        from internal buffer. Else, internal buffer will be cleared. Limit is\n        compared against part of the line without newline.\n\n        If stream was paused, this function will automatically resume it if\n        needed.\n        \"\"\"\n        sep = b'\\n'\n        seplen = len(sep)\n        try:\n            line = await self.readuntil(sep)\n        except exceptions.IncompleteReadError as e:\n            return e.partial\n        except exceptions.LimitOverrunError as e:\n            if self._buffer.startswith(sep, e.consumed):\n                del self._buffer[:e.consumed + seplen]\n            else:\n                self._buffer.clear()\n            self._maybe_resume_transport()\n            raise ValueError(e.args[0])\n        return line\n\n    async def readuntil(self, separator=b'\\n'):\n        \"\"\"Read data from the stream until ``separator`` is found.\n\n        On success, the data and separator will be removed from the\n        internal buffer (consumed). Returned data will include the\n        separator at the end.\n\n        Configured stream limit is used to check result. Limit sets the\n        maximal length of data that can be returned, not counting the\n        separator.\n\n        If an EOF occurs and the complete separator is still not found,\n        an IncompleteReadError exception will be raised, and the internal\n        buffer will be reset.  The IncompleteReadError.partial attribute\n        may contain the separator partially.\n\n        If the data cannot be read because of over limit, a\n        LimitOverrunError exception  will be raised, and the data\n        will be left in the internal buffer, so it can be read again.\n        \"\"\"\n        seplen = len(separator)\n        if seplen == 0:\n            raise ValueError('Separator should be at least one-byte string')\n\n        if self._exception is not None:\n            raise self._exception\n\n        # Consume whole buffer except last bytes, which length is\n        # one less than seplen. Let's check corner cases with\n        # separator='SEPARATOR':\n        # * we have received almost complete separator (without last\n        #   byte). i.e buffer='some textSEPARATO'. In this case we\n        #   can safely consume len(separator) - 1 bytes.\n        # * last byte of buffer is first byte of separator, i.e.\n        #   buffer='abcdefghijklmnopqrS'. We may safely consume\n        #   everything except that last byte, but this require to\n        #   analyze bytes of buffer that match partial separator.\n        #   This is slow and/or require FSM. For this case our\n        #   implementation is not optimal, since require rescanning\n        #   of data that is known to not belong to separator. In\n        #   real world, separator will not be so long to notice\n        #   performance problems. Even when reading MIME-encoded\n        #   messages :)\n\n        # `offset` is the number of bytes from the beginning of the buffer\n        # where there is no occurrence of `separator`.\n        offset = 0\n\n        # Loop until we find `separator` in the buffer, exceed the buffer size,\n        # or an EOF has happened.\n        while True:\n            buflen = len(self._buffer)\n\n            # Check if we now have enough data in the buffer for `separator` to\n            # fit.\n            if buflen - offset >= seplen:\n                isep = self._buffer.find(separator, offset)\n\n                if isep != -1:\n                    # `separator` is in the buffer. `isep` will be used later\n                    # to retrieve the data.\n                    break\n\n                # see upper comment for explanation.\n                offset = buflen + 1 - seplen\n                if offset > self._limit:\n                    raise exceptions.LimitOverrunError(\n                        'Separator is not found, and chunk exceed the limit',\n                        offset)\n\n            # Complete message (with full separator) may be present in buffer\n            # even when EOF flag is set. This may happen when the last chunk\n            # adds data which makes separator be found. That's why we check for\n            # EOF *ater* inspecting the buffer.\n            if self._eof:\n                chunk = bytes(self._buffer)\n                self._buffer.clear()\n                raise exceptions.IncompleteReadError(chunk, None)\n\n            # _wait_for_data() will resume reading if stream was paused.\n            await self._wait_for_data('readuntil')\n\n        if isep > self._limit:\n            raise exceptions.LimitOverrunError(\n                'Separator is found, but chunk is longer than limit', isep)\n\n        chunk = self._buffer[:isep + seplen]\n        del self._buffer[:isep + seplen]\n        self._maybe_resume_transport()\n        return bytes(chunk)\n\n    async def read(self, n=-1):\n        \"\"\"Read up to `n` bytes from the stream.\n\n        If `n` is not provided or set to -1,\n        read until EOF, then return all read bytes.\n        If EOF was received and the internal buffer is empty,\n        return an empty bytes object.\n\n        If `n` is 0, return an empty bytes object immediately.\n\n        If `n` is positive, return at most `n` available bytes\n        as soon as at least 1 byte is available in the internal buffer.\n        If EOF is received before any byte is read, return an empty\n        bytes object.\n\n        Returned value is not limited with limit, configured at stream\n        creation.\n\n        If stream was paused, this function will automatically resume it if\n        needed.\n        \"\"\"\n\n        if self._exception is not None:\n            raise self._exception\n\n        if n == 0:\n            return b''\n\n        if n < 0:\n            # This used to just loop creating a new waiter hoping to\n            # collect everything in self._buffer, but that would\n            # deadlock if the subprocess sends more than self.limit\n            # bytes.  So just call self.read(self._limit) until EOF.\n            blocks = []\n            while True:\n                block = await self.read(self._limit)\n                if not block:\n                    break\n                blocks.append(block)\n            return b''.join(blocks)\n\n        if not self._buffer and not self._eof:\n            await self._wait_for_data('read')\n\n        # This will work right even if buffer is less than n bytes\n        data = bytes(self._buffer[:n])\n        del self._buffer[:n]\n\n        self._maybe_resume_transport()\n        return data\n\n    async def readexactly(self, n):\n        \"\"\"Read exactly `n` bytes.\n\n        Raise an IncompleteReadError if EOF is reached before `n` bytes can be\n        read. The IncompleteReadError.partial attribute of the exception will\n        contain the partial read bytes.\n\n        if n is zero, return empty bytes object.\n\n        Returned value is not limited with limit, configured at stream\n        creation.\n\n        If stream was paused, this function will automatically resume it if\n        needed.\n        \"\"\"\n        if n < 0:\n            raise ValueError('readexactly size can not be less than zero')\n\n        if self._exception is not None:\n            raise self._exception\n\n        if n == 0:\n            return b''\n\n        while len(self._buffer) < n:\n            if self._eof:\n                incomplete = bytes(self._buffer)\n                self._buffer.clear()\n                raise exceptions.IncompleteReadError(incomplete, n)\n\n            await self._wait_for_data('readexactly')\n\n        if len(self._buffer) == n:\n            data = bytes(self._buffer)\n            self._buffer.clear()\n        else:\n            data = bytes(self._buffer[:n])\n            del self._buffer[:n]\n        self._maybe_resume_transport()\n        return data\n\n    def __aiter__(self):\n        return self\n\n    async def __anext__(self):\n        val = await self.readline()\n        if val == b'':\n            raise StopAsyncIteration\n        return val\n", 768], "/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py": ["import sys\nfrom abc import ABC\nfrom asyncio import IncompleteReadError, StreamReader, TimeoutError\nfrom typing import Callable, List, Optional, Protocol, Union\n\nif sys.version_info.major >= 3 and sys.version_info.minor >= 11:\n    from asyncio import timeout as async_timeout\nelse:\n    from async_timeout import timeout as async_timeout\n\nfrom ..exceptions import (\n    AskError,\n    AuthenticationError,\n    AuthenticationWrongNumberOfArgsError,\n    BusyLoadingError,\n    ClusterCrossSlotError,\n    ClusterDownError,\n    ConnectionError,\n    ExecAbortError,\n    MasterDownError,\n    ModuleError,\n    MovedError,\n    NoPermissionError,\n    NoScriptError,\n    OutOfMemoryError,\n    ReadOnlyError,\n    RedisError,\n    ResponseError,\n    TryAgainError,\n)\nfrom ..typing import EncodableT\nfrom .encoders import Encoder\nfrom .socket import SERVER_CLOSED_CONNECTION_ERROR, SocketBuffer\n\nMODULE_LOAD_ERROR = \"Error loading the extension. Please check the server logs.\"\nNO_SUCH_MODULE_ERROR = \"Error unloading module: no such module with that name\"\nMODULE_UNLOAD_NOT_POSSIBLE_ERROR = \"Error unloading module: operation not possible.\"\nMODULE_EXPORTS_DATA_TYPES_ERROR = (\n    \"Error unloading module: the module \"\n    \"exports one or more module-side data \"\n    \"types, can't unload\"\n)\n# user send an AUTH cmd to a server without authorization configured\nNO_AUTH_SET_ERROR = {\n    # Redis >= 6.0\n    \"AUTH <password> called without any password \"\n    \"configured for the default user. Are you sure \"\n    \"your configuration is correct?\": AuthenticationError,\n    # Redis < 6.0\n    \"Client sent AUTH, but no password is set\": AuthenticationError,\n}\n\n\nclass BaseParser(ABC):\n    EXCEPTION_CLASSES = {\n        \"ERR\": {\n            \"max number of clients reached\": ConnectionError,\n            \"invalid password\": AuthenticationError,\n            # some Redis server versions report invalid command syntax\n            # in lowercase\n            \"wrong number of arguments \"\n            \"for 'auth' command\": AuthenticationWrongNumberOfArgsError,\n            # some Redis server versions report invalid command syntax\n            # in uppercase\n            \"wrong number of arguments \"\n            \"for 'AUTH' command\": AuthenticationWrongNumberOfArgsError,\n            MODULE_LOAD_ERROR: ModuleError,\n            MODULE_EXPORTS_DATA_TYPES_ERROR: ModuleError,\n            NO_SUCH_MODULE_ERROR: ModuleError,\n            MODULE_UNLOAD_NOT_POSSIBLE_ERROR: ModuleError,\n            **NO_AUTH_SET_ERROR,\n        },\n        \"OOM\": OutOfMemoryError,\n        \"WRONGPASS\": AuthenticationError,\n        \"EXECABORT\": ExecAbortError,\n        \"LOADING\": BusyLoadingError,\n        \"NOSCRIPT\": NoScriptError,\n        \"READONLY\": ReadOnlyError,\n        \"NOAUTH\": AuthenticationError,\n        \"NOPERM\": NoPermissionError,\n        \"ASK\": AskError,\n        \"TRYAGAIN\": TryAgainError,\n        \"MOVED\": MovedError,\n        \"CLUSTERDOWN\": ClusterDownError,\n        \"CROSSSLOT\": ClusterCrossSlotError,\n        \"MASTERDOWN\": MasterDownError,\n    }\n\n    @classmethod\n    def parse_error(cls, response):\n        \"Parse an error response\"\n        error_code = response.split(\" \")[0]\n        if error_code in cls.EXCEPTION_CLASSES:\n            response = response[len(error_code) + 1 :]\n            exception_class = cls.EXCEPTION_CLASSES[error_code]\n            if isinstance(exception_class, dict):\n                exception_class = exception_class.get(response, ResponseError)\n            return exception_class(response)\n        return ResponseError(response)\n\n    def on_disconnect(self):\n        raise NotImplementedError()\n\n    def on_connect(self, connection):\n        raise NotImplementedError()\n\n\nclass _RESPBase(BaseParser):\n    \"\"\"Base class for sync-based resp parsing\"\"\"\n\n    def __init__(self, socket_read_size):\n        self.socket_read_size = socket_read_size\n        self.encoder = None\n        self._sock = None\n        self._buffer = None\n\n    def __del__(self):\n        try:\n            self.on_disconnect()\n        except Exception:\n            pass\n\n    def on_connect(self, connection):\n        \"Called when the socket connects\"\n        self._sock = connection._sock\n        self._buffer = SocketBuffer(\n            self._sock, self.socket_read_size, connection.socket_timeout\n        )\n        self.encoder = connection.encoder\n\n    def on_disconnect(self):\n        \"Called when the socket disconnects\"\n        self._sock = None\n        if self._buffer is not None:\n            self._buffer.close()\n            self._buffer = None\n        self.encoder = None\n\n    def can_read(self, timeout):\n        return self._buffer and self._buffer.can_read(timeout)\n\n\nclass AsyncBaseParser(BaseParser):\n    \"\"\"Base parsing class for the python-backed async parser\"\"\"\n\n    __slots__ = \"_stream\", \"_read_size\"\n\n    def __init__(self, socket_read_size: int):\n        self._stream: Optional[StreamReader] = None\n        self._read_size = socket_read_size\n\n    async def can_read_destructive(self) -> bool:\n        raise NotImplementedError()\n\n    async def read_response(\n        self, disable_decoding: bool = False\n    ) -> Union[EncodableT, ResponseError, None, List[EncodableT]]:\n        raise NotImplementedError()\n\n\n_INVALIDATION_MESSAGE = [b\"invalidate\", \"invalidate\"]\n\n\nclass PushNotificationsParser(Protocol):\n    \"\"\"Protocol defining RESP3-specific parsing functionality\"\"\"\n\n    pubsub_push_handler_func: Callable\n    invalidation_push_handler_func: Optional[Callable] = None\n\n    def handle_pubsub_push_response(self, response):\n        \"\"\"Handle pubsub push responses\"\"\"\n        raise NotImplementedError()\n\n    def handle_push_response(self, response, **kwargs):\n        if response[0] not in _INVALIDATION_MESSAGE:\n            return self.pubsub_push_handler_func(response)\n        if self.invalidation_push_handler_func:\n            return self.invalidation_push_handler_func(response)\n\n    def set_pubsub_push_handler(self, pubsub_push_handler_func):\n        self.pubsub_push_handler_func = pubsub_push_handler_func\n\n    def set_invalidation_push_handler(self, invalidation_push_handler_func):\n        self.invalidation_push_handler_func = invalidation_push_handler_func\n\n\nclass AsyncPushNotificationsParser(Protocol):\n    \"\"\"Protocol defining async RESP3-specific parsing functionality\"\"\"\n\n    pubsub_push_handler_func: Callable\n    invalidation_push_handler_func: Optional[Callable] = None\n\n    async def handle_pubsub_push_response(self, response):\n        \"\"\"Handle pubsub push responses asynchronously\"\"\"\n        raise NotImplementedError()\n\n    async def handle_push_response(self, response, **kwargs):\n        \"\"\"Handle push responses asynchronously\"\"\"\n        if response[0] not in _INVALIDATION_MESSAGE:\n            return await self.pubsub_push_handler_func(response)\n        if self.invalidation_push_handler_func:\n            return await self.invalidation_push_handler_func(response)\n\n    def set_pubsub_push_handler(self, pubsub_push_handler_func):\n        \"\"\"Set the pubsub push handler function\"\"\"\n        self.pubsub_push_handler_func = pubsub_push_handler_func\n\n    def set_invalidation_push_handler(self, invalidation_push_handler_func):\n        \"\"\"Set the invalidation push handler function\"\"\"\n        self.invalidation_push_handler_func = invalidation_push_handler_func\n\n\nclass _AsyncRESPBase(AsyncBaseParser):\n    \"\"\"Base class for async resp parsing\"\"\"\n\n    __slots__ = AsyncBaseParser.__slots__ + (\"encoder\", \"_buffer\", \"_pos\", \"_chunks\")\n\n    def __init__(self, socket_read_size: int):\n        super().__init__(socket_read_size)\n        self.encoder: Optional[Encoder] = None\n        self._buffer = b\"\"\n        self._chunks = []\n        self._pos = 0\n\n    def _clear(self):\n        self._buffer = b\"\"\n        self._chunks.clear()\n\n    def on_connect(self, connection):\n        \"\"\"Called when the stream connects\"\"\"\n        self._stream = connection._reader\n        if self._stream is None:\n            raise RedisError(\"Buffer is closed.\")\n        self.encoder = connection.encoder\n        self._clear()\n        self._connected = True\n\n    def on_disconnect(self):\n        \"\"\"Called when the stream disconnects\"\"\"\n        self._connected = False\n\n    async def can_read_destructive(self) -> bool:\n        if not self._connected:\n            raise RedisError(\"Buffer is closed.\")\n        if self._buffer:\n            return True\n        try:\n            async with async_timeout(0):\n                return self._stream.at_eof()\n        except TimeoutError:\n            return False\n\n    async def _read(self, length: int) -> bytes:\n        \"\"\"\n        Read `length` bytes of data.  These are assumed to be followed\n        by a '\\r\\n' terminator which is subsequently discarded.\n        \"\"\"\n        want = length + 2\n        end = self._pos + want\n        if len(self._buffer) >= end:\n            result = self._buffer[self._pos : end - 2]\n        else:\n            tail = self._buffer[self._pos :]\n            try:\n                data = await self._stream.readexactly(want - len(tail))\n            except IncompleteReadError as error:\n                raise ConnectionError(SERVER_CLOSED_CONNECTION_ERROR) from error\n            result = (tail + data)[:-2]\n            self._chunks.append(data)\n        self._pos += want\n        return result\n\n    async def _readline(self) -> bytes:\n        \"\"\"\n        read an unknown number of bytes up to the next '\\r\\n'\n        line separator, which is discarded.\n        \"\"\"\n        found = self._buffer.find(b\"\\r\\n\", self._pos)\n        if found >= 0:\n            result = self._buffer[self._pos : found]\n        else:\n            tail = self._buffer[self._pos :]\n            data = await self._stream.readline()\n            if not data.endswith(b\"\\r\\n\"):\n                raise ConnectionError(SERVER_CLOSED_CONNECTION_ERROR)\n            result = (tail + data)[:-2]\n            self._chunks.append(data)\n        self._pos += len(result) + 2\n        return result\n", 289], "/usr/local/lib/python3.11/site-packages/redis/backoff.py": ["import random\nfrom abc import ABC, abstractmethod\n\n# Maximum backoff between each retry in seconds\nDEFAULT_CAP = 0.512\n# Minimum backoff between each retry in seconds\nDEFAULT_BASE = 0.008\n\n\nclass AbstractBackoff(ABC):\n    \"\"\"Backoff interface\"\"\"\n\n    def reset(self):\n        \"\"\"\n        Reset internal state before an operation.\n        `reset` is called once at the beginning of\n        every call to `Retry.call_with_retry`\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def compute(self, failures: int) -> float:\n        \"\"\"Compute backoff in seconds upon failure\"\"\"\n        pass\n\n\nclass ConstantBackoff(AbstractBackoff):\n    \"\"\"Constant backoff upon failure\"\"\"\n\n    def __init__(self, backoff: float) -> None:\n        \"\"\"`backoff`: backoff time in seconds\"\"\"\n        self._backoff = backoff\n\n    def __hash__(self) -> int:\n        return hash((self._backoff,))\n\n    def __eq__(self, other) -> bool:\n        if not isinstance(other, ConstantBackoff):\n            return NotImplemented\n\n        return self._backoff == other._backoff\n\n    def compute(self, failures: int) -> float:\n        return self._backoff\n\n\nclass NoBackoff(ConstantBackoff):\n    \"\"\"No backoff upon failure\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__(0)\n\n\nclass ExponentialBackoff(AbstractBackoff):\n    \"\"\"Exponential backoff upon failure\"\"\"\n\n    def __init__(self, cap: float = DEFAULT_CAP, base: float = DEFAULT_BASE):\n        \"\"\"\n        `cap`: maximum backoff time in seconds\n        `base`: base backoff time in seconds\n        \"\"\"\n        self._cap = cap\n        self._base = base\n\n    def __hash__(self) -> int:\n        return hash((self._base, self._cap))\n\n    def __eq__(self, other) -> bool:\n        if not isinstance(other, ExponentialBackoff):\n            return NotImplemented\n\n        return self._base == other._base and self._cap == other._cap\n\n    def compute(self, failures: int) -> float:\n        return min(self._cap, self._base * 2**failures)\n\n\nclass FullJitterBackoff(AbstractBackoff):\n    \"\"\"Full jitter backoff upon failure\"\"\"\n\n    def __init__(self, cap: float = DEFAULT_CAP, base: float = DEFAULT_BASE) -> None:\n        \"\"\"\n        `cap`: maximum backoff time in seconds\n        `base`: base backoff time in seconds\n        \"\"\"\n        self._cap = cap\n        self._base = base\n\n    def __hash__(self) -> int:\n        return hash((self._base, self._cap))\n\n    def __eq__(self, other) -> bool:\n        if not isinstance(other, FullJitterBackoff):\n            return NotImplemented\n\n        return self._base == other._base and self._cap == other._cap\n\n    def compute(self, failures: int) -> float:\n        return random.uniform(0, min(self._cap, self._base * 2**failures))\n\n\nclass EqualJitterBackoff(AbstractBackoff):\n    \"\"\"Equal jitter backoff upon failure\"\"\"\n\n    def __init__(self, cap: float = DEFAULT_CAP, base: float = DEFAULT_BASE) -> None:\n        \"\"\"\n        `cap`: maximum backoff time in seconds\n        `base`: base backoff time in seconds\n        \"\"\"\n        self._cap = cap\n        self._base = base\n\n    def __hash__(self) -> int:\n        return hash((self._base, self._cap))\n\n    def __eq__(self, other) -> bool:\n        if not isinstance(other, EqualJitterBackoff):\n            return NotImplemented\n\n        return self._base == other._base and self._cap == other._cap\n\n    def compute(self, failures: int) -> float:\n        temp = min(self._cap, self._base * 2**failures) / 2\n        return temp + random.uniform(0, temp)\n\n\nclass DecorrelatedJitterBackoff(AbstractBackoff):\n    \"\"\"Decorrelated jitter backoff upon failure\"\"\"\n\n    def __init__(self, cap: float = DEFAULT_CAP, base: float = DEFAULT_BASE) -> None:\n        \"\"\"\n        `cap`: maximum backoff time in seconds\n        `base`: base backoff time in seconds\n        \"\"\"\n        self._cap = cap\n        self._base = base\n        self._previous_backoff = 0\n\n    def __hash__(self) -> int:\n        return hash((self._base, self._cap))\n\n    def __eq__(self, other) -> bool:\n        if not isinstance(other, DecorrelatedJitterBackoff):\n            return NotImplemented\n\n        return self._base == other._base and self._cap == other._cap\n\n    def reset(self) -> None:\n        self._previous_backoff = 0\n\n    def compute(self, failures: int) -> float:\n        max_backoff = max(self._base, self._previous_backoff * 3)\n        temp = random.uniform(self._base, max_backoff)\n        self._previous_backoff = min(self._cap, temp)\n        return self._previous_backoff\n\n\nclass ExponentialWithJitterBackoff(AbstractBackoff):\n    \"\"\"Exponential backoff upon failure, with jitter\"\"\"\n\n    def __init__(self, cap: float = DEFAULT_CAP, base: float = DEFAULT_BASE) -> None:\n        \"\"\"\n        `cap`: maximum backoff time in seconds\n        `base`: base backoff time in seconds\n        \"\"\"\n        self._cap = cap\n        self._base = base\n\n    def __hash__(self) -> int:\n        return hash((self._base, self._cap))\n\n    def __eq__(self, other) -> bool:\n        if not isinstance(other, ExponentialWithJitterBackoff):\n            return NotImplemented\n\n        return self._base == other._base and self._cap == other._cap\n\n    def compute(self, failures: int) -> float:\n        return min(self._cap, random.random() * self._base * 2**failures)\n\n\ndef default_backoff():\n    return EqualJitterBackoff()\n", 183], "/usr/local/lib/python3.11/site-packages/redis/_parsers/encoders.py": ["from ..exceptions import DataError\n\n\nclass Encoder:\n    \"Encode strings to bytes-like and decode bytes-like to strings\"\n\n    __slots__ = \"encoding\", \"encoding_errors\", \"decode_responses\"\n\n    def __init__(self, encoding, encoding_errors, decode_responses):\n        self.encoding = encoding\n        self.encoding_errors = encoding_errors\n        self.decode_responses = decode_responses\n\n    def encode(self, value):\n        \"Return a bytestring or bytes-like representation of the value\"\n        if isinstance(value, (bytes, memoryview)):\n            return value\n        elif isinstance(value, bool):\n            # special case bool since it is a subclass of int\n            raise DataError(\n                \"Invalid input of type: 'bool'. Convert to a \"\n                \"bytes, string, int or float first.\"\n            )\n        elif isinstance(value, (int, float)):\n            value = repr(value).encode()\n        elif not isinstance(value, str):\n            # a value we don't know how to deal with. throw an error\n            typename = type(value).__name__\n            raise DataError(\n                f\"Invalid input of type: '{typename}'. \"\n                f\"Convert to a bytes, string, int or float first.\"\n            )\n        if isinstance(value, str):\n            value = value.encode(self.encoding, self.encoding_errors)\n        return value\n\n    def decode(self, value, force=False):\n        \"Return a unicode string from the bytes-like representation\"\n        if self.decode_responses or force:\n            if isinstance(value, memoryview):\n                value = value.tobytes()\n            if isinstance(value, bytes):\n                value = value.decode(self.encoding, self.encoding_errors)\n        return value\n", 44], "/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py": ["from typing import Any, Union\n\nfrom ..exceptions import ConnectionError, InvalidResponse, ResponseError\nfrom ..typing import EncodableT\nfrom .base import _AsyncRESPBase, _RESPBase\nfrom .socket import SERVER_CLOSED_CONNECTION_ERROR\n\n\nclass _RESP2Parser(_RESPBase):\n    \"\"\"RESP2 protocol implementation\"\"\"\n\n    def read_response(self, disable_decoding=False):\n        pos = self._buffer.get_pos() if self._buffer else None\n        try:\n            result = self._read_response(disable_decoding=disable_decoding)\n        except BaseException:\n            if self._buffer:\n                self._buffer.rewind(pos)\n            raise\n        else:\n            self._buffer.purge()\n            return result\n\n    def _read_response(self, disable_decoding=False):\n        raw = self._buffer.readline()\n        if not raw:\n            raise ConnectionError(SERVER_CLOSED_CONNECTION_ERROR)\n\n        byte, response = raw[:1], raw[1:]\n\n        # server returned an error\n        if byte == b\"-\":\n            response = response.decode(\"utf-8\", errors=\"replace\")\n            error = self.parse_error(response)\n            # if the error is a ConnectionError, raise immediately so the user\n            # is notified\n            if isinstance(error, ConnectionError):\n                raise error\n            # otherwise, we're dealing with a ResponseError that might belong\n            # inside a pipeline response. the connection's read_response()\n            # and/or the pipeline's execute() will raise this error if\n            # necessary, so just return the exception instance here.\n            return error\n        # single value\n        elif byte == b\"+\":\n            pass\n        # int value\n        elif byte == b\":\":\n            return int(response)\n        # bulk response\n        elif byte == b\"$\" and response == b\"-1\":\n            return None\n        elif byte == b\"$\":\n            response = self._buffer.read(int(response))\n        # multi-bulk response\n        elif byte == b\"*\" and response == b\"-1\":\n            return None\n        elif byte == b\"*\":\n            response = [\n                self._read_response(disable_decoding=disable_decoding)\n                for i in range(int(response))\n            ]\n        else:\n            raise InvalidResponse(f\"Protocol Error: {raw!r}\")\n\n        if disable_decoding is False:\n            response = self.encoder.decode(response)\n        return response\n\n\nclass _AsyncRESP2Parser(_AsyncRESPBase):\n    \"\"\"Async class for the RESP2 protocol\"\"\"\n\n    async def read_response(self, disable_decoding: bool = False):\n        if not self._connected:\n            raise ConnectionError(SERVER_CLOSED_CONNECTION_ERROR)\n        if self._chunks:\n            # augment parsing buffer with previously read data\n            self._buffer += b\"\".join(self._chunks)\n            self._chunks.clear()\n        self._pos = 0\n        response = await self._read_response(disable_decoding=disable_decoding)\n        # Successfully parsing a response allows us to clear our parsing buffer\n        self._clear()\n        return response\n\n    async def _read_response(\n        self, disable_decoding: bool = False\n    ) -> Union[EncodableT, ResponseError, None]:\n        raw = await self._readline()\n        response: Any\n        byte, response = raw[:1], raw[1:]\n\n        # server returned an error\n        if byte == b\"-\":\n            response = response.decode(\"utf-8\", errors=\"replace\")\n            error = self.parse_error(response)\n            # if the error is a ConnectionError, raise immediately so the user\n            # is notified\n            if isinstance(error, ConnectionError):\n                self._clear()  # Successful parse\n                raise error\n            # otherwise, we're dealing with a ResponseError that might belong\n            # inside a pipeline response. the connection's read_response()\n            # and/or the pipeline's execute() will raise this error if\n            # necessary, so just return the exception instance here.\n            return error\n        # single value\n        elif byte == b\"+\":\n            pass\n        # int value\n        elif byte == b\":\":\n            return int(response)\n        # bulk response\n        elif byte == b\"$\" and response == b\"-1\":\n            return None\n        elif byte == b\"$\":\n            response = await self._read(int(response))\n        # multi-bulk response\n        elif byte == b\"*\" and response == b\"-1\":\n            return None\n        elif byte == b\"*\":\n            response = [\n                (await self._read_response(disable_decoding))\n                for _ in range(int(response))  # noqa\n            ]\n        else:\n            raise InvalidResponse(f\"Protocol Error: {raw!r}\")\n\n        if disable_decoding is False:\n            response = self.encoder.decode(response)\n        return response\n", 132], "/usr/local/lib/python3.11/site-packages/redis/asyncio/retry.py": ["from asyncio import sleep\nfrom typing import TYPE_CHECKING, Any, Awaitable, Callable, Tuple, Type, TypeVar\n\nfrom redis.exceptions import ConnectionError, RedisError, TimeoutError\nfrom redis.retry import AbstractRetry\n\nT = TypeVar(\"T\")\n\nif TYPE_CHECKING:\n    from redis.backoff import AbstractBackoff\n\n\nclass Retry(AbstractRetry[RedisError]):\n    __hash__ = AbstractRetry.__hash__\n\n    def __init__(\n        self,\n        backoff: \"AbstractBackoff\",\n        retries: int,\n        supported_errors: Tuple[Type[RedisError], ...] = (\n            ConnectionError,\n            TimeoutError,\n        ),\n    ):\n        super().__init__(backoff, retries, supported_errors)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, Retry):\n            return NotImplemented\n\n        return (\n            self._backoff == other._backoff\n            and self._retries == other._retries\n            and set(self._supported_errors) == set(other._supported_errors)\n        )\n\n    async def call_with_retry(\n        self, do: Callable[[], Awaitable[T]], fail: Callable[[RedisError], Any]\n    ) -> T:\n        \"\"\"\n        Execute an operation that might fail and returns its result, or\n        raise the exception that was thrown depending on the `Backoff` object.\n        `do`: the operation to call. Expects no argument.\n        `fail`: the failure handler, expects the last error that was thrown\n        \"\"\"\n        self._backoff.reset()\n        failures = 0\n        while True:\n            try:\n                return await do()\n            except self._supported_errors as error:\n                failures += 1\n                await fail(error)\n                if self._retries >= 0 and failures > self._retries:\n                    raise error\n                backoff = self._backoff.compute(failures)\n                if backoff > 0:\n                    await sleep(backoff)\n", 58], "/app/app/services/data_collector.py": ["\"\"\"\nData Collector Service - M2/M3 Implementation\nHandles RLC job consumption, gap detection, and backfill orchestration.\n\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional\n\nfrom app.core.config import get_settings\nfrom app.observability.metrics import (\n    BACKFILL_COMPLETED,\n    BACKFILL_ENQUEUED,\n    BACKFILL_OLDEST_AGE,\n    BACKFILL_QUEUE_DEPTH,\n    GAPS_FOUND,\n    JOBS_PROCESSED,\n)\nfrom app.services.database import DatabaseService\n\nlogger = logging.getLogger(__name__)\n\n\ndef _delta_from_interval(interval: str) -> timedelta:\n    \"\"\"Convert interval string to timedelta.\"\"\"\n    if interval == \"1m\":\n        return timedelta(minutes=1)\n    elif interval == \"5m\":\n        return timedelta(minutes=5)\n    elif interval == \"15m\":\n        return timedelta(minutes=15)\n    elif interval == \"1h\":\n        return timedelta(hours=1)\n    elif interval == \"1d\":\n        return timedelta(days=1)\n    else:\n        return timedelta(minutes=1)  # default\n\n\nclass CollectorJobs:\n    \"\"\"Simple Redis-backed job queue for RLC integration and backfills.\"\"\"\n\n    def __init__(self, redis_url: str, jobs_key: str, backfill_keys: Dict[str, str]):\n        try:\n            import redis.asyncio as aioredis\n            self.r = aioredis.from_url(redis_url, encoding=\"utf-8\", decode_responses=True)\n        except ImportError:\n            logger.warning(\"redis not installed; RLC mode disabled\")\n            self.r = None\n        self.jobs_key = jobs_key\n        self.backfill_keys = backfill_keys  # {\"T0\": \"market:backfills:T0\", ...}\n\n    async def push_job(self, job: Dict) -> None:\n        if not self.r:\n            return\n        await self.r.lpush(self.jobs_key, json.dumps(job))\n\n    async def pop_job(self, timeout: int = 1) -> Optional[Dict]:\n        if not self.r:\n            return None\n        try:\n            v = await self.r.brpop(self.jobs_key, timeout=timeout)\n            if not v:\n                return None\n            _, payload = v\n            return json.loads(payload)\n        except Exception as exc:\n            logger.debug(\"pop_job error: %s\", exc)\n            return None\n\n    async def push_backfill(self, tier: str, job: Dict) -> None:\n        if not self.r:\n            return\n        key = self.backfill_keys.get(tier, self.backfill_keys[\"T2\"])\n        await self.r.lpush(key, json.dumps(job))\n\n    async def pop_backfill(self, tiers: List[str], timeout: int = 1) -> Optional[Dict]:\n        if not self.r:\n            return None\n        # Try tiers in priority order\n        for tier in sorted(tiers, key=lambda t: {\"T0\": 0, \"T1\": 1, \"T2\": 2}.get(t, 9)):\n            key = self.backfill_keys.get(tier)\n            if not key:\n                continue\n            try:\n                v = await self.r.brpop(key, timeout=0)  # non-blocking check\n                if v:\n                    _, payload = v\n                    return json.loads(payload)\n            except Exception:\n                continue\n        return None\n\n    async def backfill_depth(self, tier: str) -> int:\n        if not self.r:\n            return 0\n        key = self.backfill_keys.get(tier)\n        if not key:\n            return 0\n        try:\n            return await self.r.llen(key)\n        except Exception:\n            return 0\n\n    async def backfill_oldest(self, tier: str) -> Optional[Dict]:\n        if not self.r:\n            return None\n        key = self.backfill_keys.get(tier)\n        if not key:\n            return None\n        try:\n            oldest_raw = await self.r.lindex(key, -1)  # oldest at tail\n            if oldest_raw:\n                return json.loads(oldest_raw)\n        except Exception:\n            pass\n        return None\n\n\nclass DataCollectorService:\n    \"\"\"\n    Background service for collecting and storing market data.\n    Implements M2 (RLC jobs + local sweep) and M3 (gap detection + backfill).\n    \"\"\"\n\n    def __init__(self, db: DatabaseService, registry=None):\n        self.db = db\n        self.registry = registry  # provider registry from main\n        self.running = False\n        self.task: Optional[asyncio.Task] = None\n\n        s = get_settings()\n        self.jobs = CollectorJobs(\n            s.RLC_REDIS_URL,\n            s.RLC_REDIS_JOBS_KEY,\n            s.RLC_REDIS_BACKFILL_KEYS,\n        )\n\n        # Backfill concurrency semaphores (per tier)\n        self.sem = {\n            \"T0\": asyncio.Semaphore(s.BACKFILL_MAX_CONCURRENCY_T0),\n            \"T1\": asyncio.Semaphore(s.BACKFILL_MAX_CONCURRENCY_T1),\n            \"T2\": asyncio.Semaphore(s.BACKFILL_MAX_CONCURRENCY_T2),\n        }\n        self.dispatch_tokens = s.BACKFILL_DISPATCH_RATE_PER_SEC\n        self._last_dispatch = datetime.utcnow()\n\n    # --------- PUBLIC TASKS ----------\n\n    async def run(self) -> None:\n        \"\"\"Run job consumer + optional local sweeper + backfill worker.\"\"\"\n        if self.running:\n            logger.warning(\"Data collector already running\")\n            return\n\n        s = get_settings()\n        self.running = True\n\n        tasks = [asyncio.create_task(self.consume_jobs())]\n        if s.LOCAL_SWEEP_ENABLED and not s.USE_RLC:\n            tasks.append(asyncio.create_task(self.local_sweeper()))\n        tasks.append(asyncio.create_task(self.backfill_worker()))\n        tasks.append(asyncio.create_task(self.metrics_reporter()))\n\n        self.task = asyncio.gather(*tasks, return_exceptions=True)\n        logger.info(\"Data collector started (USE_RLC=%s, LOCAL_SWEEP=%s)\", s.USE_RLC, s.LOCAL_SWEEP_ENABLED)\n\n    async def stop(self) -> None:\n        \"\"\"Stop the data collection background task.\"\"\"\n        self.running = False\n        if self.task:\n            self.task.cancel()\n            try:\n                await self.task\n            except asyncio.CancelledError:\n                pass\n        logger.info(\"Data collector stopped\")\n\n    # --------- JOB CONSUMER ----------\n\n    async def consume_jobs(self) -> None:\n        \"\"\"Consume RLC jobs; fallback: no-op if queue is empty.\"\"\"\n        while self.running:\n            job = await self.jobs.pop_job(timeout=2)\n            if not job:\n                await asyncio.sleep(0.1)\n                continue\n            try:\n                t = job.get(\"type\")\n                if t == \"bars_fetch\":\n                    await self._handle_bars_job(job)\n                elif t == \"quotes_fetch\":\n                    await self._handle_quotes_job(job)\n                elif t == \"backfill\":\n                    # push into backfill stream (throttled)\n                    tier = job.get(\"priority\", \"T2\")\n                    await self.jobs.push_backfill(tier, job)\n                else:\n                    logger.debug(\"Unknown job type: %s\", t)\n            except Exception as exc:\n                logger.error(\"Error handling job: %s\", exc, exc_info=True)\n                # TODO: emit metric + optional DLQ\n                continue\n\n    # --------- LOCAL SWEEPER (DEV) ----------\n\n    async def local_sweeper(self) -> None:\n        \"\"\"Emit simple bars_fetch jobs by tier and cadence when RLC is off.\"\"\"\n        s = get_settings()\n        offset = {\"T0\": 0, \"T1\": 0, \"T2\": 0}\n        while self.running:\n            now = datetime.utcnow()\n            for tier, cadence in [(\"T0\", s.CADENCE_T0), (\"T1\", s.CADENCE_T1), (\"T2\", s.CADENCE_T2)]:\n                if \"bars_1m\" not in cadence and \"eod\" not in cadence:\n                    continue\n                try:\n                    batch = await self.db.get_symbols_by_tier(tier, s.LOCAL_SWEEP_BATCH, offset[tier])\n                except Exception as exc:\n                    logger.warning(\"Error getting symbols for tier %s: %s\", tier, exc)\n                    offset[tier] = 0\n                    continue\n\n                if not batch:\n                    offset[tier] = 0\n                    continue\n                offset[tier] += len(batch)\n\n                end = now.replace(second=0, microsecond=0)\n                start = end - timedelta(minutes=5) if tier != \"T2\" else end - timedelta(days=2)\n\n                job = {\n                    \"type\": \"bars_fetch\",\n                    \"symbols\": batch,\n                    \"interval\": \"1m\" if tier != \"T2\" else \"1d\",\n                    \"time_window\": {\"start\": start.isoformat() + \"Z\", \"end\": end.isoformat() + \"Z\"},\n                    \"priority\": tier,\n                    \"provider_hint\": None,\n                }\n                await self.jobs.push_job(job)\n\n            await asyncio.sleep(s.LOCAL_SWEEP_TICK_SEC)\n\n    # --------- JOB HANDLERS ----------\n\n    async def _handle_bars_job(self, job: Dict) -> None:\n        \"\"\"Handle bars_fetch job: rank providers, fetch, write, detect gaps.\"\"\"\n        if not self.registry:\n            logger.warning(\"Registry not available; skipping bars job\")\n            return\n\n        sym_list: List[str] = job.get(\"symbols\", [])\n        interval: str = job.get(\"interval\", \"1m\")\n        start = datetime.fromisoformat(job[\"time_window\"][\"start\"].replace(\"Z\", \"\"))\n        end = datetime.fromisoformat(job[\"time_window\"][\"end\"].replace(\"Z\", \"\"))\n        tier = job.get(\"priority\", \"T1\")\n        hint = job.get(\"provider_hint\")\n\n        # Rank providers once for this capability\n        capability = \"bars_1m\" if interval == \"1m\" else \"eod\"\n        providers = self.registry.rank(capability, provider_hint=hint)\n        if not providers:\n            logger.warning(\"No providers available for capability %s\", capability)\n            return\n\n        for symbol in sym_list:\n            provider_used = None\n            bars_all: List[Dict] = []\n            for pname in providers:\n                adapter = self.registry.providers[pname].adapter\n                try:\n                    # Fetch bars from provider\n                    data = await adapter.fetch_bars(symbol=symbol, start=start, end=end, interval=interval)\n                    if not data:\n                        continue\n                    provider_used = pname\n                    bars_all = data\n                    JOBS_PROCESSED.labels(type=\"bars_fetch\", provider=pname).inc()\n                    break\n                except Exception as exc:\n                    logger.debug(\"Provider %s failed for %s: %s\", pname, symbol, exc)\n                    continue\n\n            if not bars_all:\n                # Could emit a small gap/backfill from start\u2192end for this symbol\n                await self._enqueue_gap(symbol, interval, start, end, tier)\n                continue\n\n            # Gap detection & upserts\n            await self._write_with_gaps(symbol, interval, provider_used, bars_all, tier)\n\n    async def _handle_quotes_job(self, job: Dict) -> None:\n        \"\"\"Handle quotes_fetch job: similar pattern to bars.\"\"\"\n        # Placeholder: same routing pattern, write to quotes table, etc.\n        logger.debug(\"quotes_fetch job received but not implemented: %s\", job)\n        return\n\n    # --------- GAP & BACKFILL CORE ----------\n\n    async def _write_with_gaps(\n        self,\n        symbol: str,\n        interval: str,\n        provider_used: str,\n        bars: List[Dict],\n        tier: str,\n    ) -> None:\n        \"\"\"Write bars and detect gaps.\"\"\"\n        # Sort bars and load cursor\n        bars.sort(key=lambda b: b.get(\"ts\", datetime.min))\n        last_ts = await self.db.get_cursor(symbol, interval, provider_used)\n        dt = _delta_from_interval(interval)\n\n        # If no cursor, set it to first-interval-before first bar\n        if last_ts is None and bars:\n            last_ts = bars[0][\"ts\"] - dt\n\n        expected = last_ts + dt if last_ts else (bars[0][\"ts\"] if bars else datetime.utcnow())\n\n        # Detect gap before first bar\n        if bars and bars[0][\"ts\"] > expected:\n            await self._enqueue_gap(symbol, interval, expected, bars[0][\"ts\"] - dt, tier)\n\n        # Bulk write all bars (idempotent)\n        s = get_settings()\n        await self.db.upsert_bars_bulk(bars, batch_size=s.LIVE_BATCH_SIZE, provider_used=provider_used)\n\n        # Detect gaps between bars\n        if len(bars) > 1:\n            prev = bars[0][\"ts\"]\n            for b in bars[1:]:\n                want = prev + dt\n                if b[\"ts\"] > want:\n                    await self._enqueue_gap(symbol, interval, want, b[\"ts\"] - dt, tier)\n                prev = b[\"ts\"]\n\n        # Advance cursor\n        if bars:\n            await self.db.update_cursor(symbol, interval, provider_used, bars[-1][\"ts\"], status=\"ok\")\n\n    async def _enqueue_gap(\n        self,\n        symbol: str,\n        interval: str,\n        start_ts: datetime,\n        end_ts: datetime,\n        tier: str,\n    ) -> None:\n        \"\"\"Split large gaps into chunks and enqueue backfill jobs.\"\"\"\n        s = get_settings()\n        chunk_duration = timedelta(minutes=s.BACKFILL_CHUNK_MINUTES) if interval == \"1m\" else timedelta(days=30)\n        cur = start_ts\n        count = 0\n        while cur <= end_ts:\n            nxt = min(end_ts, cur + chunk_duration)\n            job = {\n                \"type\": \"backfill\",\n                \"symbol\": symbol,\n                \"interval\": interval,\n                \"time_window\": {\"start\": cur.isoformat() + \"Z\", \"end\": nxt.isoformat() + \"Z\"},\n                \"priority\": tier,\n            }\n            await self.jobs.push_backfill(tier, job)\n            GAPS_FOUND.labels(interval=interval).inc()\n            BACKFILL_ENQUEUED.labels(tier=tier).inc()\n            count += 1\n            cur = nxt + _delta_from_interval(interval)\n\n        if count > 0:\n            logger.info(\"Enqueued %d backfill chunks for %s [%s] %s\u2192%s\", count, symbol, interval, start_ts, end_ts)\n\n    # --------- BACKFILL WORKER ----------\n\n    async def backfill_worker(self) -> None:\n        \"\"\"Backfill worker with rate-limiting and concurrency control.\"\"\"\n        s = get_settings()\n        bucket = 0.0\n        last = datetime.utcnow()\n        while self.running:\n            # refill tokens\n            now = datetime.utcnow()\n            bucket += (now - last).total_seconds() * s.BACKFILL_DISPATCH_RATE_PER_SEC\n            bucket = min(bucket, 10.0)\n            last = now\n\n            if bucket < 1.0:\n                await asyncio.sleep(0.1)\n                continue\n\n            job = await self.jobs.pop_backfill([\"T0\", \"T1\", \"T2\"], timeout=1)\n            if not job:\n                await asyncio.sleep(0.1)\n                continue\n\n            bucket -= 1.0\n            tier = job.get(\"priority\", \"T2\")\n            async with self.sem.get(tier, self.sem[\"T2\"]):\n                await self._run_backfill(job)\n\n    async def _run_backfill(self, job: Dict) -> None:\n        \"\"\"Execute a single backfill job.\"\"\"\n        if not self.registry:\n            logger.warning(\"Registry not available; skipping backfill\")\n            BACKFILL_COMPLETED.labels(tier=job.get(\"priority\", \"T2\"), status=\"failed\").inc()\n            return\n\n        symbol = job[\"symbol\"]\n        interval = job.get(\"interval\", \"1m\")\n        start = datetime.fromisoformat(job[\"time_window\"][\"start\"].replace(\"Z\", \"\"))\n        end = datetime.fromisoformat(job[\"time_window\"][\"end\"].replace(\"Z\", \"\"))\n        tier = job.get(\"priority\", \"T2\")\n\n        providers = self.registry.rank(\"bars_1m\" if interval == \"1m\" else \"eod\", provider_hint=None)\n        s = get_settings()\n\n        for pname in providers:\n            adapter = self.registry.providers[pname].adapter\n            try:\n                data = await adapter.fetch_bars(symbol=symbol, start=start, end=end, interval=interval)\n                if not data:\n                    continue\n                await self.db.upsert_bars_bulk(\n                    data,\n                    batch_size=s.BACKFILL_BATCH_SIZE,\n                    provider_used=pname,\n                )\n                # advance cursor conservatively if needed\n                if data:\n                    max_ts = max(b[\"ts\"] for b in data)\n                    await self.db.update_cursor(symbol, interval, pname, max_ts, status=\"backfilled\")\n                BACKFILL_COMPLETED.labels(tier=tier, status=\"done\").inc()\n                JOBS_PROCESSED.labels(type=\"backfill\", provider=pname).inc()\n                logger.debug(\"Backfill completed for %s [%s] %s\u2192%s via %s\", symbol, interval, start, end, pname)\n                return\n            except Exception as exc:\n                logger.debug(\"Backfill provider %s failed for %s: %s\", pname, symbol, exc)\n                continue\n\n        # If we reach here, all providers failed\n        BACKFILL_COMPLETED.labels(tier=tier, status=\"failed\").inc()\n        logger.warning(\"Backfill failed for %s [%s] %s\u2192%s (all providers failed)\", symbol, interval, start, end)\n\n    # --------- METRICS REPORTER ----------\n\n    async def metrics_reporter(self) -> None:\n        \"\"\"Periodically update backfill queue depth and oldest age metrics.\"\"\"\n        while self.running:\n            try:\n                for tier in [\"T0\", \"T1\", \"T2\"]:\n                    depth = await self.jobs.backfill_depth(tier)\n                    BACKFILL_QUEUE_DEPTH.set(depth)\n\n                    if depth > 0:\n                        oldest = await self.jobs.backfill_oldest(tier)\n                        if oldest:\n                            start_str = oldest.get(\"time_window\", {}).get(\"start\")\n                            if start_str:\n                                start_dt = datetime.fromisoformat(start_str.replace(\"Z\", \"\"))\n                                age = (datetime.utcnow() - start_dt).total_seconds()\n                                BACKFILL_OLDEST_AGE.set(max(age, 0.0))\n            except Exception as exc:\n                logger.debug(\"Error updating backfill metrics: %s\", exc)\n\n            await asyncio.sleep(10)\n\n    # --------- LEGACY API (for backwards compatibility) ----------\n\n    async def start(self) -> None:\n        \"\"\"Legacy start method.\"\"\"\n        await self.run()\n\n    async def collect_symbol_data(\n        self,\n        symbol: str,\n        days_back: int = 365,\n        force_update: bool = False,\n    ) -> bool:\n        \"\"\"\n        Legacy method for on-demand symbol collection.\n        Now delegates to job queue.\n        \"\"\"\n        logger.info(\"Legacy collect_symbol_data called for %s\", symbol)\n        now = datetime.utcnow()\n        start = now - timedelta(days=days_back)\n        job = {\n            \"type\": \"bars_fetch\",\n            \"symbols\": [symbol.upper()],\n            \"interval\": \"1d\",\n            \"time_window\": {\"start\": start.isoformat() + \"Z\", \"end\": now.isoformat() + \"Z\"},\n            \"priority\": \"T2\",\n            \"provider_hint\": None,\n        }\n        await self.jobs.push_job(job)\n        return True\n\n    async def get_collection_status(self) -> dict:\n        \"\"\"Get status information about the data collector.\"\"\"\n        s = get_settings()\n        depths = {}\n        for tier in [\"T0\", \"T1\", \"T2\"]:\n            depths[tier] = await self.jobs.backfill_depth(tier)\n\n        return {\n            \"running\": self.running,\n            \"use_rlc\": s.USE_RLC,\n            \"local_sweep_enabled\": s.LOCAL_SWEEP_ENABLED,\n            \"backfill_queue_depths\": depths,\n            \"policy_version\": s.policy_version,\n        }\n", 511], "/usr/local/lib/python3.11/site-packages/redis/client.py": ["import copy\nimport re\nimport threading\nimport time\nfrom itertools import chain\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    List,\n    Mapping,\n    Optional,\n    Set,\n    Type,\n    Union,\n)\n\nfrom redis._parsers.encoders import Encoder\nfrom redis._parsers.helpers import (\n    _RedisCallbacks,\n    _RedisCallbacksRESP2,\n    _RedisCallbacksRESP3,\n    bool_ok,\n)\nfrom redis.backoff import ExponentialWithJitterBackoff\nfrom redis.cache import CacheConfig, CacheInterface\nfrom redis.commands import (\n    CoreCommands,\n    RedisModuleCommands,\n    SentinelCommands,\n    list_or_args,\n)\nfrom redis.commands.core import Script\nfrom redis.connection import (\n    AbstractConnection,\n    Connection,\n    ConnectionPool,\n    SSLConnection,\n    UnixDomainSocketConnection,\n)\nfrom redis.credentials import CredentialProvider\nfrom redis.event import (\n    AfterPooledConnectionsInstantiationEvent,\n    AfterPubSubConnectionInstantiationEvent,\n    AfterSingleConnectionInstantiationEvent,\n    ClientType,\n    EventDispatcher,\n)\nfrom redis.exceptions import (\n    ConnectionError,\n    ExecAbortError,\n    PubSubError,\n    RedisError,\n    ResponseError,\n    WatchError,\n)\nfrom redis.lock import Lock\nfrom redis.retry import Retry\nfrom redis.utils import (\n    _set_info_logger,\n    deprecated_args,\n    get_lib_version,\n    safe_str,\n    str_if_bytes,\n    truncate_text,\n)\n\nif TYPE_CHECKING:\n    import ssl\n\n    import OpenSSL\n\nSYM_EMPTY = b\"\"\nEMPTY_RESPONSE = \"EMPTY_RESPONSE\"\n\n# some responses (ie. dump) are binary, and just meant to never be decoded\nNEVER_DECODE = \"NEVER_DECODE\"\n\n\nclass CaseInsensitiveDict(dict):\n    \"Case insensitive dict implementation. Assumes string keys only.\"\n\n    def __init__(self, data: Dict[str, str]) -> None:\n        for k, v in data.items():\n            self[k.upper()] = v\n\n    def __contains__(self, k):\n        return super().__contains__(k.upper())\n\n    def __delitem__(self, k):\n        super().__delitem__(k.upper())\n\n    def __getitem__(self, k):\n        return super().__getitem__(k.upper())\n\n    def get(self, k, default=None):\n        return super().get(k.upper(), default)\n\n    def __setitem__(self, k, v):\n        super().__setitem__(k.upper(), v)\n\n    def update(self, data):\n        data = CaseInsensitiveDict(data)\n        super().update(data)\n\n\nclass AbstractRedis:\n    pass\n\n\nclass Redis(RedisModuleCommands, CoreCommands, SentinelCommands):\n    \"\"\"\n    Implementation of the Redis protocol.\n\n    This abstract class provides a Python interface to all Redis commands\n    and an implementation of the Redis protocol.\n\n    Pipelines derive from this, implementing how\n    the commands are sent and received to the Redis server. Based on\n    configuration, an instance will either use a ConnectionPool, or\n    Connection object to talk to redis.\n\n    It is not safe to pass PubSub or Pipeline objects between threads.\n    \"\"\"\n\n    @classmethod\n    def from_url(cls, url: str, **kwargs) -> \"Redis\":\n        \"\"\"\n        Return a Redis client object configured from the given URL\n\n        For example::\n\n            redis://[[username]:[password]]@localhost:6379/0\n            rediss://[[username]:[password]]@localhost:6379/0\n            unix://[username@]/path/to/socket.sock?db=0[&password=password]\n\n        Three URL schemes are supported:\n\n        - `redis://` creates a TCP socket connection. See more at:\n          <https://www.iana.org/assignments/uri-schemes/prov/redis>\n        - `rediss://` creates a SSL wrapped TCP socket connection. See more at:\n          <https://www.iana.org/assignments/uri-schemes/prov/rediss>\n        - ``unix://``: creates a Unix Domain Socket connection.\n\n        The username, password, hostname, path and all querystring values\n        are passed through urllib.parse.unquote in order to replace any\n        percent-encoded values with their corresponding characters.\n\n        There are several ways to specify a database number. The first value\n        found will be used:\n\n            1. A ``db`` querystring option, e.g. redis://localhost?db=0\n            2. If using the redis:// or rediss:// schemes, the path argument\n               of the url, e.g. redis://localhost/0\n            3. A ``db`` keyword argument to this function.\n\n        If none of these options are specified, the default db=0 is used.\n\n        All querystring options are cast to their appropriate Python types.\n        Boolean arguments can be specified with string values \"True\"/\"False\"\n        or \"Yes\"/\"No\". Values that cannot be properly cast cause a\n        ``ValueError`` to be raised. Once parsed, the querystring arguments\n        and keyword arguments are passed to the ``ConnectionPool``'s\n        class initializer. In the case of conflicting arguments, querystring\n        arguments always win.\n\n        \"\"\"\n        single_connection_client = kwargs.pop(\"single_connection_client\", False)\n        connection_pool = ConnectionPool.from_url(url, **kwargs)\n        client = cls(\n            connection_pool=connection_pool,\n            single_connection_client=single_connection_client,\n        )\n        client.auto_close_connection_pool = True\n        return client\n\n    @classmethod\n    def from_pool(\n        cls: Type[\"Redis\"],\n        connection_pool: ConnectionPool,\n    ) -> \"Redis\":\n        \"\"\"\n        Return a Redis client from the given connection pool.\n        The Redis client will take ownership of the connection pool and\n        close it when the Redis client is closed.\n        \"\"\"\n        client = cls(\n            connection_pool=connection_pool,\n        )\n        client.auto_close_connection_pool = True\n        return client\n\n    @deprecated_args(\n        args_to_warn=[\"retry_on_timeout\"],\n        reason=\"TimeoutError is included by default.\",\n        version=\"6.0.0\",\n    )\n    def __init__(\n        self,\n        host: str = \"localhost\",\n        port: int = 6379,\n        db: int = 0,\n        password: Optional[str] = None,\n        socket_timeout: Optional[float] = None,\n        socket_connect_timeout: Optional[float] = None,\n        socket_keepalive: Optional[bool] = None,\n        socket_keepalive_options: Optional[Mapping[int, Union[int, bytes]]] = None,\n        connection_pool: Optional[ConnectionPool] = None,\n        unix_socket_path: Optional[str] = None,\n        encoding: str = \"utf-8\",\n        encoding_errors: str = \"strict\",\n        decode_responses: bool = False,\n        retry_on_timeout: bool = False,\n        retry: Retry = Retry(\n            backoff=ExponentialWithJitterBackoff(base=1, cap=10), retries=3\n        ),\n        retry_on_error: Optional[List[Type[Exception]]] = None,\n        ssl: bool = False,\n        ssl_keyfile: Optional[str] = None,\n        ssl_certfile: Optional[str] = None,\n        ssl_cert_reqs: Union[str, \"ssl.VerifyMode\"] = \"required\",\n        ssl_ca_certs: Optional[str] = None,\n        ssl_ca_path: Optional[str] = None,\n        ssl_ca_data: Optional[str] = None,\n        ssl_check_hostname: bool = True,\n        ssl_password: Optional[str] = None,\n        ssl_validate_ocsp: bool = False,\n        ssl_validate_ocsp_stapled: bool = False,\n        ssl_ocsp_context: Optional[\"OpenSSL.SSL.Context\"] = None,\n        ssl_ocsp_expected_cert: Optional[str] = None,\n        ssl_min_version: Optional[\"ssl.TLSVersion\"] = None,\n        ssl_ciphers: Optional[str] = None,\n        max_connections: Optional[int] = None,\n        single_connection_client: bool = False,\n        health_check_interval: int = 0,\n        client_name: Optional[str] = None,\n        lib_name: Optional[str] = \"redis-py\",\n        lib_version: Optional[str] = get_lib_version(),\n        username: Optional[str] = None,\n        redis_connect_func: Optional[Callable[[], None]] = None,\n        credential_provider: Optional[CredentialProvider] = None,\n        protocol: Optional[int] = 2,\n        cache: Optional[CacheInterface] = None,\n        cache_config: Optional[CacheConfig] = None,\n        event_dispatcher: Optional[EventDispatcher] = None,\n    ) -> None:\n        \"\"\"\n        Initialize a new Redis client.\n\n        To specify a retry policy for specific errors, you have two options:\n\n        1. Set the `retry_on_error` to a list of the error/s to retry on, and\n        you can also set `retry` to a valid `Retry` object(in case the default\n        one is not appropriate) - with this approach the retries will be triggered\n        on the default errors specified in the Retry object enriched with the\n        errors specified in `retry_on_error`.\n\n        2. Define a `Retry` object with configured 'supported_errors' and set\n        it to the `retry` parameter - with this approach you completely redefine\n        the errors on which retries will happen.\n\n        `retry_on_timeout` is deprecated - please include the TimeoutError\n        either in the Retry object or in the `retry_on_error` list.\n\n        When 'connection_pool' is provided - the retry configuration of the\n        provided pool will be used.\n\n        Args:\n\n        single_connection_client:\n            if `True`, connection pool is not used. In that case `Redis`\n            instance use is not thread safe.\n        \"\"\"\n        if event_dispatcher is None:\n            self._event_dispatcher = EventDispatcher()\n        else:\n            self._event_dispatcher = event_dispatcher\n        if not connection_pool:\n            if not retry_on_error:\n                retry_on_error = []\n            kwargs = {\n                \"db\": db,\n                \"username\": username,\n                \"password\": password,\n                \"socket_timeout\": socket_timeout,\n                \"encoding\": encoding,\n                \"encoding_errors\": encoding_errors,\n                \"decode_responses\": decode_responses,\n                \"retry_on_error\": retry_on_error,\n                \"retry\": copy.deepcopy(retry),\n                \"max_connections\": max_connections,\n                \"health_check_interval\": health_check_interval,\n                \"client_name\": client_name,\n                \"lib_name\": lib_name,\n                \"lib_version\": lib_version,\n                \"redis_connect_func\": redis_connect_func,\n                \"credential_provider\": credential_provider,\n                \"protocol\": protocol,\n            }\n            # based on input, setup appropriate connection args\n            if unix_socket_path is not None:\n                kwargs.update(\n                    {\n                        \"path\": unix_socket_path,\n                        \"connection_class\": UnixDomainSocketConnection,\n                    }\n                )\n            else:\n                # TCP specific options\n                kwargs.update(\n                    {\n                        \"host\": host,\n                        \"port\": port,\n                        \"socket_connect_timeout\": socket_connect_timeout,\n                        \"socket_keepalive\": socket_keepalive,\n                        \"socket_keepalive_options\": socket_keepalive_options,\n                    }\n                )\n\n                if ssl:\n                    kwargs.update(\n                        {\n                            \"connection_class\": SSLConnection,\n                            \"ssl_keyfile\": ssl_keyfile,\n                            \"ssl_certfile\": ssl_certfile,\n                            \"ssl_cert_reqs\": ssl_cert_reqs,\n                            \"ssl_ca_certs\": ssl_ca_certs,\n                            \"ssl_ca_data\": ssl_ca_data,\n                            \"ssl_check_hostname\": ssl_check_hostname,\n                            \"ssl_password\": ssl_password,\n                            \"ssl_ca_path\": ssl_ca_path,\n                            \"ssl_validate_ocsp_stapled\": ssl_validate_ocsp_stapled,\n                            \"ssl_validate_ocsp\": ssl_validate_ocsp,\n                            \"ssl_ocsp_context\": ssl_ocsp_context,\n                            \"ssl_ocsp_expected_cert\": ssl_ocsp_expected_cert,\n                            \"ssl_min_version\": ssl_min_version,\n                            \"ssl_ciphers\": ssl_ciphers,\n                        }\n                    )\n                if (cache_config or cache) and protocol in [3, \"3\"]:\n                    kwargs.update(\n                        {\n                            \"cache\": cache,\n                            \"cache_config\": cache_config,\n                        }\n                    )\n            connection_pool = ConnectionPool(**kwargs)\n            self._event_dispatcher.dispatch(\n                AfterPooledConnectionsInstantiationEvent(\n                    [connection_pool], ClientType.SYNC, credential_provider\n                )\n            )\n            self.auto_close_connection_pool = True\n        else:\n            self.auto_close_connection_pool = False\n            self._event_dispatcher.dispatch(\n                AfterPooledConnectionsInstantiationEvent(\n                    [connection_pool], ClientType.SYNC, credential_provider\n                )\n            )\n\n        self.connection_pool = connection_pool\n\n        if (cache_config or cache) and self.connection_pool.get_protocol() not in [\n            3,\n            \"3\",\n        ]:\n            raise RedisError(\"Client caching is only supported with RESP version 3\")\n\n        self.single_connection_lock = threading.RLock()\n        self.connection = None\n        self._single_connection_client = single_connection_client\n        if self._single_connection_client:\n            self.connection = self.connection_pool.get_connection()\n            self._event_dispatcher.dispatch(\n                AfterSingleConnectionInstantiationEvent(\n                    self.connection, ClientType.SYNC, self.single_connection_lock\n                )\n            )\n\n        self.response_callbacks = CaseInsensitiveDict(_RedisCallbacks)\n\n        if self.connection_pool.connection_kwargs.get(\"protocol\") in [\"3\", 3]:\n            self.response_callbacks.update(_RedisCallbacksRESP3)\n        else:\n            self.response_callbacks.update(_RedisCallbacksRESP2)\n\n    def __repr__(self) -> str:\n        return (\n            f\"<{type(self).__module__}.{type(self).__name__}\"\n            f\"({repr(self.connection_pool)})>\"\n        )\n\n    def get_encoder(self) -> \"Encoder\":\n        \"\"\"Get the connection pool's encoder\"\"\"\n        return self.connection_pool.get_encoder()\n\n    def get_connection_kwargs(self) -> Dict:\n        \"\"\"Get the connection's key-word arguments\"\"\"\n        return self.connection_pool.connection_kwargs\n\n    def get_retry(self) -> Optional[Retry]:\n        return self.get_connection_kwargs().get(\"retry\")\n\n    def set_retry(self, retry: Retry) -> None:\n        self.get_connection_kwargs().update({\"retry\": retry})\n        self.connection_pool.set_retry(retry)\n\n    def set_response_callback(self, command: str, callback: Callable) -> None:\n        \"\"\"Set a custom Response Callback\"\"\"\n        self.response_callbacks[command] = callback\n\n    def load_external_module(self, funcname, func) -> None:\n        \"\"\"\n        This function can be used to add externally defined redis modules,\n        and their namespaces to the redis client.\n\n        funcname - A string containing the name of the function to create\n        func - The function, being added to this class.\n\n        ex: Assume that one has a custom redis module named foomod that\n        creates command named 'foo.dothing' and 'foo.anotherthing' in redis.\n        To load function functions into this namespace:\n\n        from redis import Redis\n        from foomodule import F\n        r = Redis()\n        r.load_external_module(\"foo\", F)\n        r.foo().dothing('your', 'arguments')\n\n        For a concrete example see the reimport of the redisjson module in\n        tests/test_connection.py::test_loading_external_modules\n        \"\"\"\n        setattr(self, funcname, func)\n\n    def pipeline(self, transaction=True, shard_hint=None) -> \"Pipeline\":\n        \"\"\"\n        Return a new pipeline object that can queue multiple commands for\n        later execution. ``transaction`` indicates whether all commands\n        should be executed atomically. Apart from making a group of operations\n        atomic, pipelines are useful for reducing the back-and-forth overhead\n        between the client and server.\n        \"\"\"\n        return Pipeline(\n            self.connection_pool, self.response_callbacks, transaction, shard_hint\n        )\n\n    def transaction(\n        self, func: Callable[[\"Pipeline\"], None], *watches, **kwargs\n    ) -> Union[List[Any], Any, None]:\n        \"\"\"\n        Convenience method for executing the callable `func` as a transaction\n        while watching all keys specified in `watches`. The 'func' callable\n        should expect a single argument which is a Pipeline object.\n        \"\"\"\n        shard_hint = kwargs.pop(\"shard_hint\", None)\n        value_from_callable = kwargs.pop(\"value_from_callable\", False)\n        watch_delay = kwargs.pop(\"watch_delay\", None)\n        with self.pipeline(True, shard_hint) as pipe:\n            while True:\n                try:\n                    if watches:\n                        pipe.watch(*watches)\n                    func_value = func(pipe)\n                    exec_value = pipe.execute()\n                    return func_value if value_from_callable else exec_value\n                except WatchError:\n                    if watch_delay is not None and watch_delay > 0:\n                        time.sleep(watch_delay)\n                    continue\n\n    def lock(\n        self,\n        name: str,\n        timeout: Optional[float] = None,\n        sleep: float = 0.1,\n        blocking: bool = True,\n        blocking_timeout: Optional[float] = None,\n        lock_class: Union[None, Any] = None,\n        thread_local: bool = True,\n        raise_on_release_error: bool = True,\n    ):\n        \"\"\"\n        Return a new Lock object using key ``name`` that mimics\n        the behavior of threading.Lock.\n\n        If specified, ``timeout`` indicates a maximum life for the lock.\n        By default, it will remain locked until release() is called.\n\n        ``sleep`` indicates the amount of time to sleep per loop iteration\n        when the lock is in blocking mode and another client is currently\n        holding the lock.\n\n        ``blocking`` indicates whether calling ``acquire`` should block until\n        the lock has been acquired or to fail immediately, causing ``acquire``\n        to return False and the lock not being acquired. Defaults to True.\n        Note this value can be overridden by passing a ``blocking``\n        argument to ``acquire``.\n\n        ``blocking_timeout`` indicates the maximum amount of time in seconds to\n        spend trying to acquire the lock. A value of ``None`` indicates\n        continue trying forever. ``blocking_timeout`` can be specified as a\n        float or integer, both representing the number of seconds to wait.\n\n        ``lock_class`` forces the specified lock implementation. Note that as\n        of redis-py 3.0, the only lock class we implement is ``Lock`` (which is\n        a Lua-based lock). So, it's unlikely you'll need this parameter, unless\n        you have created your own custom lock class.\n\n        ``thread_local`` indicates whether the lock token is placed in\n        thread-local storage. By default, the token is placed in thread local\n        storage so that a thread only sees its token, not a token set by\n        another thread. Consider the following timeline:\n\n            time: 0, thread-1 acquires `my-lock`, with a timeout of 5 seconds.\n                     thread-1 sets the token to \"abc\"\n            time: 1, thread-2 blocks trying to acquire `my-lock` using the\n                     Lock instance.\n            time: 5, thread-1 has not yet completed. redis expires the lock\n                     key.\n            time: 5, thread-2 acquired `my-lock` now that it's available.\n                     thread-2 sets the token to \"xyz\"\n            time: 6, thread-1 finishes its work and calls release(). if the\n                     token is *not* stored in thread local storage, then\n                     thread-1 would see the token value as \"xyz\" and would be\n                     able to successfully release the thread-2's lock.\n\n        ``raise_on_release_error`` indicates whether to raise an exception when\n        the lock is no longer owned when exiting the context manager. By default,\n        this is True, meaning an exception will be raised. If False, the warning\n        will be logged and the exception will be suppressed.\n\n        In some use cases it's necessary to disable thread local storage. For\n        example, if you have code where one thread acquires a lock and passes\n        that lock instance to a worker thread to release later. If thread\n        local storage isn't disabled in this case, the worker thread won't see\n        the token set by the thread that acquired the lock. Our assumption\n        is that these cases aren't common and as such default to using\n        thread local storage.\"\"\"\n        if lock_class is None:\n            lock_class = Lock\n        return lock_class(\n            self,\n            name,\n            timeout=timeout,\n            sleep=sleep,\n            blocking=blocking,\n            blocking_timeout=blocking_timeout,\n            thread_local=thread_local,\n            raise_on_release_error=raise_on_release_error,\n        )\n\n    def pubsub(self, **kwargs):\n        \"\"\"\n        Return a Publish/Subscribe object. With this object, you can\n        subscribe to channels and listen for messages that get published to\n        them.\n        \"\"\"\n        return PubSub(\n            self.connection_pool, event_dispatcher=self._event_dispatcher, **kwargs\n        )\n\n    def monitor(self):\n        return Monitor(self.connection_pool)\n\n    def client(self):\n        return self.__class__(\n            connection_pool=self.connection_pool, single_connection_client=True\n        )\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n\n    def __del__(self):\n        try:\n            self.close()\n        except Exception:\n            pass\n\n    def close(self) -> None:\n        # In case a connection property does not yet exist\n        # (due to a crash earlier in the Redis() constructor), return\n        # immediately as there is nothing to clean-up.\n        if not hasattr(self, \"connection\"):\n            return\n\n        conn = self.connection\n        if conn:\n            self.connection = None\n            self.connection_pool.release(conn)\n\n        if self.auto_close_connection_pool:\n            self.connection_pool.disconnect()\n\n    def _send_command_parse_response(self, conn, command_name, *args, **options):\n        \"\"\"\n        Send a command and parse the response\n        \"\"\"\n        conn.send_command(*args, **options)\n        return self.parse_response(conn, command_name, **options)\n\n    def _close_connection(self, conn) -> None:\n        \"\"\"\n        Close the connection before retrying.\n\n        The supported exceptions are already checked in the\n        retry object so we don't need to do it here.\n\n        After we disconnect the connection, it will try to reconnect and\n        do a health check as part of the send_command logic(on connection level).\n        \"\"\"\n\n        conn.disconnect()\n\n    # COMMAND EXECUTION AND PROTOCOL PARSING\n    def execute_command(self, *args, **options):\n        return self._execute_command(*args, **options)\n\n    def _execute_command(self, *args, **options):\n        \"\"\"Execute a command and return a parsed response\"\"\"\n        pool = self.connection_pool\n        command_name = args[0]\n        conn = self.connection or pool.get_connection()\n\n        if self._single_connection_client:\n            self.single_connection_lock.acquire()\n        try:\n            return conn.retry.call_with_retry(\n                lambda: self._send_command_parse_response(\n                    conn, command_name, *args, **options\n                ),\n                lambda _: self._close_connection(conn),\n            )\n        finally:\n            if self._single_connection_client:\n                self.single_connection_lock.release()\n            if not self.connection:\n                pool.release(conn)\n\n    def parse_response(self, connection, command_name, **options):\n        \"\"\"Parses a response from the Redis server\"\"\"\n        try:\n            if NEVER_DECODE in options:\n                response = connection.read_response(disable_decoding=True)\n                options.pop(NEVER_DECODE)\n            else:\n                response = connection.read_response()\n        except ResponseError:\n            if EMPTY_RESPONSE in options:\n                return options[EMPTY_RESPONSE]\n            raise\n\n        if EMPTY_RESPONSE in options:\n            options.pop(EMPTY_RESPONSE)\n\n        # Remove keys entry, it needs only for cache.\n        options.pop(\"keys\", None)\n\n        if command_name in self.response_callbacks:\n            return self.response_callbacks[command_name](response, **options)\n        return response\n\n    def get_cache(self) -> Optional[CacheInterface]:\n        return self.connection_pool.cache\n\n\nStrictRedis = Redis\n\n\nclass Monitor:\n    \"\"\"\n    Monitor is useful for handling the MONITOR command to the redis server.\n    next_command() method returns one command from monitor\n    listen() method yields commands from monitor.\n    \"\"\"\n\n    monitor_re = re.compile(r\"\\[(\\d+) (.*?)\\] (.*)\")\n    command_re = re.compile(r'\"(.*?)(?<!\\\\)\"')\n\n    def __init__(self, connection_pool):\n        self.connection_pool = connection_pool\n        self.connection = self.connection_pool.get_connection()\n\n    def __enter__(self):\n        self.connection.send_command(\"MONITOR\")\n        # check that monitor returns 'OK', but don't return it to user\n        response = self.connection.read_response()\n        if not bool_ok(response):\n            raise RedisError(f\"MONITOR failed: {response}\")\n        return self\n\n    def __exit__(self, *args):\n        self.connection.disconnect()\n        self.connection_pool.release(self.connection)\n\n    def next_command(self):\n        \"\"\"Parse the response from a monitor command\"\"\"\n        response = self.connection.read_response()\n        if isinstance(response, bytes):\n            response = self.connection.encoder.decode(response, force=True)\n        command_time, command_data = response.split(\" \", 1)\n        m = self.monitor_re.match(command_data)\n        db_id, client_info, command = m.groups()\n        command = \" \".join(self.command_re.findall(command))\n        # Redis escapes double quotes because each piece of the command\n        # string is surrounded by double quotes. We don't have that\n        # requirement so remove the escaping and leave the quote.\n        command = command.replace('\\\\\"', '\"')\n\n        if client_info == \"lua\":\n            client_address = \"lua\"\n            client_port = \"\"\n            client_type = \"lua\"\n        elif client_info.startswith(\"unix\"):\n            client_address = \"unix\"\n            client_port = client_info[5:]\n            client_type = \"unix\"\n        else:\n            # use rsplit as ipv6 addresses contain colons\n            client_address, client_port = client_info.rsplit(\":\", 1)\n            client_type = \"tcp\"\n        return {\n            \"time\": float(command_time),\n            \"db\": int(db_id),\n            \"client_address\": client_address,\n            \"client_port\": client_port,\n            \"client_type\": client_type,\n            \"command\": command,\n        }\n\n    def listen(self):\n        \"\"\"Listen for commands coming to the server.\"\"\"\n        while True:\n            yield self.next_command()\n\n\nclass PubSub:\n    \"\"\"\n    PubSub provides publish, subscribe and listen support to Redis channels.\n\n    After subscribing to one or more channels, the listen() method will block\n    until a message arrives on one of the subscribed channels. That message\n    will be returned and it's safe to start listening again.\n    \"\"\"\n\n    PUBLISH_MESSAGE_TYPES = (\"message\", \"pmessage\", \"smessage\")\n    UNSUBSCRIBE_MESSAGE_TYPES = (\"unsubscribe\", \"punsubscribe\", \"sunsubscribe\")\n    HEALTH_CHECK_MESSAGE = \"redis-py-health-check\"\n\n    def __init__(\n        self,\n        connection_pool,\n        shard_hint=None,\n        ignore_subscribe_messages: bool = False,\n        encoder: Optional[\"Encoder\"] = None,\n        push_handler_func: Union[None, Callable[[str], None]] = None,\n        event_dispatcher: Optional[\"EventDispatcher\"] = None,\n    ):\n        self.connection_pool = connection_pool\n        self.shard_hint = shard_hint\n        self.ignore_subscribe_messages = ignore_subscribe_messages\n        self.connection = None\n        self.subscribed_event = threading.Event()\n        # we need to know the encoding options for this connection in order\n        # to lookup channel and pattern names for callback handlers.\n        self.encoder = encoder\n        self.push_handler_func = push_handler_func\n        if event_dispatcher is None:\n            self._event_dispatcher = EventDispatcher()\n        else:\n            self._event_dispatcher = event_dispatcher\n\n        self._lock = threading.RLock()\n        if self.encoder is None:\n            self.encoder = self.connection_pool.get_encoder()\n        self.health_check_response_b = self.encoder.encode(self.HEALTH_CHECK_MESSAGE)\n        if self.encoder.decode_responses:\n            self.health_check_response = [\"pong\", self.HEALTH_CHECK_MESSAGE]\n        else:\n            self.health_check_response = [b\"pong\", self.health_check_response_b]\n        if self.push_handler_func is None:\n            _set_info_logger()\n        self.reset()\n\n    def __enter__(self) -> \"PubSub\":\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback) -> None:\n        self.reset()\n\n    def __del__(self) -> None:\n        try:\n            # if this object went out of scope prior to shutting down\n            # subscriptions, close the connection manually before\n            # returning it to the connection pool\n            self.reset()\n        except Exception:\n            pass\n\n    def reset(self) -> None:\n        if self.connection:\n            self.connection.disconnect()\n            self.connection.deregister_connect_callback(self.on_connect)\n            self.connection_pool.release(self.connection)\n            self.connection = None\n        self.health_check_response_counter = 0\n        self.channels = {}\n        self.pending_unsubscribe_channels = set()\n        self.shard_channels = {}\n        self.pending_unsubscribe_shard_channels = set()\n        self.patterns = {}\n        self.pending_unsubscribe_patterns = set()\n        self.subscribed_event.clear()\n\n    def close(self) -> None:\n        self.reset()\n\n    def on_connect(self, connection) -> None:\n        \"Re-subscribe to any channels and patterns previously subscribed to\"\n        # NOTE: for python3, we can't pass bytestrings as keyword arguments\n        # so we need to decode channel/pattern names back to unicode strings\n        # before passing them to [p]subscribe.\n        self.pending_unsubscribe_channels.clear()\n        self.pending_unsubscribe_patterns.clear()\n        self.pending_unsubscribe_shard_channels.clear()\n        if self.channels:\n            channels = {\n                self.encoder.decode(k, force=True): v for k, v in self.channels.items()\n            }\n            self.subscribe(**channels)\n        if self.patterns:\n            patterns = {\n                self.encoder.decode(k, force=True): v for k, v in self.patterns.items()\n            }\n            self.psubscribe(**patterns)\n        if self.shard_channels:\n            shard_channels = {\n                self.encoder.decode(k, force=True): v\n                for k, v in self.shard_channels.items()\n            }\n            self.ssubscribe(**shard_channels)\n\n    @property\n    def subscribed(self) -> bool:\n        \"\"\"Indicates if there are subscriptions to any channels or patterns\"\"\"\n        return self.subscribed_event.is_set()\n\n    def execute_command(self, *args):\n        \"\"\"Execute a publish/subscribe command\"\"\"\n\n        # NOTE: don't parse the response in this function -- it could pull a\n        # legitimate message off the stack if the connection is already\n        # subscribed to one or more channels\n\n        if self.connection is None:\n            self.connection = self.connection_pool.get_connection()\n            # register a callback that re-subscribes to any channels we\n            # were listening to when we were disconnected\n            self.connection.register_connect_callback(self.on_connect)\n            if self.push_handler_func is not None:\n                self.connection._parser.set_pubsub_push_handler(self.push_handler_func)\n            self._event_dispatcher.dispatch(\n                AfterPubSubConnectionInstantiationEvent(\n                    self.connection, self.connection_pool, ClientType.SYNC, self._lock\n                )\n            )\n        connection = self.connection\n        kwargs = {\"check_health\": not self.subscribed}\n        if not self.subscribed:\n            self.clean_health_check_responses()\n        with self._lock:\n            self._execute(connection, connection.send_command, *args, **kwargs)\n\n    def clean_health_check_responses(self) -> None:\n        \"\"\"\n        If any health check responses are present, clean them\n        \"\"\"\n        ttl = 10\n        conn = self.connection\n        while self.health_check_response_counter > 0 and ttl > 0:\n            if self._execute(conn, conn.can_read, timeout=conn.socket_timeout):\n                response = self._execute(conn, conn.read_response)\n                if self.is_health_check_response(response):\n                    self.health_check_response_counter -= 1\n                else:\n                    raise PubSubError(\n                        \"A non health check response was cleaned by \"\n                        \"execute_command: {}\".format(response)\n                    )\n            ttl -= 1\n\n    def _reconnect(self, conn) -> None:\n        \"\"\"\n        The supported exceptions are already checked in the\n        retry object so we don't need to do it here.\n\n        In this error handler we are trying to reconnect to the server.\n        \"\"\"\n        conn.disconnect()\n        conn.connect()\n\n    def _execute(self, conn, command, *args, **kwargs):\n        \"\"\"\n        Connect manually upon disconnection. If the Redis server is down,\n        this will fail and raise a ConnectionError as desired.\n        After reconnection, the ``on_connect`` callback should have been\n        called by the # connection to resubscribe us to any channels and\n        patterns we were previously listening to\n        \"\"\"\n        return conn.retry.call_with_retry(\n            lambda: command(*args, **kwargs),\n            lambda _: self._reconnect(conn),\n        )\n\n    def parse_response(self, block=True, timeout=0):\n        \"\"\"Parse the response from a publish/subscribe command\"\"\"\n        conn = self.connection\n        if conn is None:\n            raise RuntimeError(\n                \"pubsub connection not set: \"\n                \"did you forget to call subscribe() or psubscribe()?\"\n            )\n\n        self.check_health()\n\n        def try_read():\n            if not block:\n                if not conn.can_read(timeout=timeout):\n                    return None\n            else:\n                conn.connect()\n            return conn.read_response(disconnect_on_error=False, push_request=True)\n\n        response = self._execute(conn, try_read)\n\n        if self.is_health_check_response(response):\n            # ignore the health check message as user might not expect it\n            self.health_check_response_counter -= 1\n            return None\n        return response\n\n    def is_health_check_response(self, response) -> bool:\n        \"\"\"\n        Check if the response is a health check response.\n        If there are no subscriptions redis responds to PING command with a\n        bulk response, instead of a multi-bulk with \"pong\" and the response.\n        \"\"\"\n        return response in [\n            self.health_check_response,  # If there was a subscription\n            self.health_check_response_b,  # If there wasn't\n        ]\n\n    def check_health(self) -> None:\n        conn = self.connection\n        if conn is None:\n            raise RuntimeError(\n                \"pubsub connection not set: \"\n                \"did you forget to call subscribe() or psubscribe()?\"\n            )\n\n        if conn.health_check_interval and time.monotonic() > conn.next_health_check:\n            conn.send_command(\"PING\", self.HEALTH_CHECK_MESSAGE, check_health=False)\n            self.health_check_response_counter += 1\n\n    def _normalize_keys(self, data) -> Dict:\n        \"\"\"\n        normalize channel/pattern names to be either bytes or strings\n        based on whether responses are automatically decoded. this saves us\n        from coercing the value for each message coming in.\n        \"\"\"\n        encode = self.encoder.encode\n        decode = self.encoder.decode\n        return {decode(encode(k)): v for k, v in data.items()}\n\n    def psubscribe(self, *args, **kwargs):\n        \"\"\"\n        Subscribe to channel patterns. Patterns supplied as keyword arguments\n        expect a pattern name as the key and a callable as the value. A\n        pattern's callable will be invoked automatically when a message is\n        received on that pattern rather than producing a message via\n        ``listen()``.\n        \"\"\"\n        if args:\n            args = list_or_args(args[0], args[1:])\n        new_patterns = dict.fromkeys(args)\n        new_patterns.update(kwargs)\n        ret_val = self.execute_command(\"PSUBSCRIBE\", *new_patterns.keys())\n        # update the patterns dict AFTER we send the command. we don't want to\n        # subscribe twice to these patterns, once for the command and again\n        # for the reconnection.\n        new_patterns = self._normalize_keys(new_patterns)\n        self.patterns.update(new_patterns)\n        if not self.subscribed:\n            # Set the subscribed_event flag to True\n            self.subscribed_event.set()\n            # Clear the health check counter\n            self.health_check_response_counter = 0\n        self.pending_unsubscribe_patterns.difference_update(new_patterns)\n        return ret_val\n\n    def punsubscribe(self, *args):\n        \"\"\"\n        Unsubscribe from the supplied patterns. If empty, unsubscribe from\n        all patterns.\n        \"\"\"\n        if args:\n            args = list_or_args(args[0], args[1:])\n            patterns = self._normalize_keys(dict.fromkeys(args))\n        else:\n            patterns = self.patterns\n        self.pending_unsubscribe_patterns.update(patterns)\n        return self.execute_command(\"PUNSUBSCRIBE\", *args)\n\n    def subscribe(self, *args, **kwargs):\n        \"\"\"\n        Subscribe to channels. Channels supplied as keyword arguments expect\n        a channel name as the key and a callable as the value. A channel's\n        callable will be invoked automatically when a message is received on\n        that channel rather than producing a message via ``listen()`` or\n        ``get_message()``.\n        \"\"\"\n        if args:\n            args = list_or_args(args[0], args[1:])\n        new_channels = dict.fromkeys(args)\n        new_channels.update(kwargs)\n        ret_val = self.execute_command(\"SUBSCRIBE\", *new_channels.keys())\n        # update the channels dict AFTER we send the command. we don't want to\n        # subscribe twice to these channels, once for the command and again\n        # for the reconnection.\n        new_channels = self._normalize_keys(new_channels)\n        self.channels.update(new_channels)\n        if not self.subscribed:\n            # Set the subscribed_event flag to True\n            self.subscribed_event.set()\n            # Clear the health check counter\n            self.health_check_response_counter = 0\n        self.pending_unsubscribe_channels.difference_update(new_channels)\n        return ret_val\n\n    def unsubscribe(self, *args):\n        \"\"\"\n        Unsubscribe from the supplied channels. If empty, unsubscribe from\n        all channels\n        \"\"\"\n        if args:\n            args = list_or_args(args[0], args[1:])\n            channels = self._normalize_keys(dict.fromkeys(args))\n        else:\n            channels = self.channels\n        self.pending_unsubscribe_channels.update(channels)\n        return self.execute_command(\"UNSUBSCRIBE\", *args)\n\n    def ssubscribe(self, *args, target_node=None, **kwargs):\n        \"\"\"\n        Subscribes the client to the specified shard channels.\n        Channels supplied as keyword arguments expect a channel name as the key\n        and a callable as the value. A channel's callable will be invoked automatically\n        when a message is received on that channel rather than producing a message via\n        ``listen()`` or ``get_sharded_message()``.\n        \"\"\"\n        if args:\n            args = list_or_args(args[0], args[1:])\n        new_s_channels = dict.fromkeys(args)\n        new_s_channels.update(kwargs)\n        ret_val = self.execute_command(\"SSUBSCRIBE\", *new_s_channels.keys())\n        # update the s_channels dict AFTER we send the command. we don't want to\n        # subscribe twice to these channels, once for the command and again\n        # for the reconnection.\n        new_s_channels = self._normalize_keys(new_s_channels)\n        self.shard_channels.update(new_s_channels)\n        if not self.subscribed:\n            # Set the subscribed_event flag to True\n            self.subscribed_event.set()\n            # Clear the health check counter\n            self.health_check_response_counter = 0\n        self.pending_unsubscribe_shard_channels.difference_update(new_s_channels)\n        return ret_val\n\n    def sunsubscribe(self, *args, target_node=None):\n        \"\"\"\n        Unsubscribe from the supplied shard_channels. If empty, unsubscribe from\n        all shard_channels\n        \"\"\"\n        if args:\n            args = list_or_args(args[0], args[1:])\n            s_channels = self._normalize_keys(dict.fromkeys(args))\n        else:\n            s_channels = self.shard_channels\n        self.pending_unsubscribe_shard_channels.update(s_channels)\n        return self.execute_command(\"SUNSUBSCRIBE\", *args)\n\n    def listen(self):\n        \"Listen for messages on channels this client has been subscribed to\"\n        while self.subscribed:\n            response = self.handle_message(self.parse_response(block=True))\n            if response is not None:\n                yield response\n\n    def get_message(\n        self, ignore_subscribe_messages: bool = False, timeout: float = 0.0\n    ):\n        \"\"\"\n        Get the next message if one is available, otherwise None.\n\n        If timeout is specified, the system will wait for `timeout` seconds\n        before returning. Timeout should be specified as a floating point\n        number, or None, to wait indefinitely.\n        \"\"\"\n        if not self.subscribed:\n            # Wait for subscription\n            start_time = time.monotonic()\n            if self.subscribed_event.wait(timeout) is True:\n                # The connection was subscribed during the timeout time frame.\n                # The timeout should be adjusted based on the time spent\n                # waiting for the subscription\n                time_spent = time.monotonic() - start_time\n                timeout = max(0.0, timeout - time_spent)\n            else:\n                # The connection isn't subscribed to any channels or patterns,\n                # so no messages are available\n                return None\n\n        response = self.parse_response(block=(timeout is None), timeout=timeout)\n        if response:\n            return self.handle_message(response, ignore_subscribe_messages)\n        return None\n\n    get_sharded_message = get_message\n\n    def ping(self, message: Union[str, None] = None) -> bool:\n        \"\"\"\n        Ping the Redis server\n        \"\"\"\n        args = [\"PING\", message] if message is not None else [\"PING\"]\n        return self.execute_command(*args)\n\n    def handle_message(self, response, ignore_subscribe_messages=False):\n        \"\"\"\n        Parses a pub/sub message. If the channel or pattern was subscribed to\n        with a message handler, the handler is invoked instead of a parsed\n        message being returned.\n        \"\"\"\n        if response is None:\n            return None\n        if isinstance(response, bytes):\n            response = [b\"pong\", response] if response != b\"PONG\" else [b\"pong\", b\"\"]\n        message_type = str_if_bytes(response[0])\n        if message_type == \"pmessage\":\n            message = {\n                \"type\": message_type,\n                \"pattern\": response[1],\n                \"channel\": response[2],\n                \"data\": response[3],\n            }\n        elif message_type == \"pong\":\n            message = {\n                \"type\": message_type,\n                \"pattern\": None,\n                \"channel\": None,\n                \"data\": response[1],\n            }\n        else:\n            message = {\n                \"type\": message_type,\n                \"pattern\": None,\n                \"channel\": response[1],\n                \"data\": response[2],\n            }\n\n        # if this is an unsubscribe message, remove it from memory\n        if message_type in self.UNSUBSCRIBE_MESSAGE_TYPES:\n            if message_type == \"punsubscribe\":\n                pattern = response[1]\n                if pattern in self.pending_unsubscribe_patterns:\n                    self.pending_unsubscribe_patterns.remove(pattern)\n                    self.patterns.pop(pattern, None)\n            elif message_type == \"sunsubscribe\":\n                s_channel = response[1]\n                if s_channel in self.pending_unsubscribe_shard_channels:\n                    self.pending_unsubscribe_shard_channels.remove(s_channel)\n                    self.shard_channels.pop(s_channel, None)\n            else:\n                channel = response[1]\n                if channel in self.pending_unsubscribe_channels:\n                    self.pending_unsubscribe_channels.remove(channel)\n                    self.channels.pop(channel, None)\n            if not self.channels and not self.patterns and not self.shard_channels:\n                # There are no subscriptions anymore, set subscribed_event flag\n                # to false\n                self.subscribed_event.clear()\n\n        if message_type in self.PUBLISH_MESSAGE_TYPES:\n            # if there's a message handler, invoke it\n            if message_type == \"pmessage\":\n                handler = self.patterns.get(message[\"pattern\"], None)\n            elif message_type == \"smessage\":\n                handler = self.shard_channels.get(message[\"channel\"], None)\n            else:\n                handler = self.channels.get(message[\"channel\"], None)\n            if handler:\n                handler(message)\n                return None\n        elif message_type != \"pong\":\n            # this is a subscribe/unsubscribe message. ignore if we don't\n            # want them\n            if ignore_subscribe_messages or self.ignore_subscribe_messages:\n                return None\n\n        return message\n\n    def run_in_thread(\n        self,\n        sleep_time: float = 0.0,\n        daemon: bool = False,\n        exception_handler: Optional[Callable] = None,\n    ) -> \"PubSubWorkerThread\":\n        for channel, handler in self.channels.items():\n            if handler is None:\n                raise PubSubError(f\"Channel: '{channel}' has no handler registered\")\n        for pattern, handler in self.patterns.items():\n            if handler is None:\n                raise PubSubError(f\"Pattern: '{pattern}' has no handler registered\")\n        for s_channel, handler in self.shard_channels.items():\n            if handler is None:\n                raise PubSubError(\n                    f\"Shard Channel: '{s_channel}' has no handler registered\"\n                )\n\n        thread = PubSubWorkerThread(\n            self, sleep_time, daemon=daemon, exception_handler=exception_handler\n        )\n        thread.start()\n        return thread\n\n\nclass PubSubWorkerThread(threading.Thread):\n    def __init__(\n        self,\n        pubsub,\n        sleep_time: float,\n        daemon: bool = False,\n        exception_handler: Union[\n            Callable[[Exception, \"PubSub\", \"PubSubWorkerThread\"], None], None\n        ] = None,\n    ):\n        super().__init__()\n        self.daemon = daemon\n        self.pubsub = pubsub\n        self.sleep_time = sleep_time\n        self.exception_handler = exception_handler\n        self._running = threading.Event()\n\n    def run(self) -> None:\n        if self._running.is_set():\n            return\n        self._running.set()\n        pubsub = self.pubsub\n        sleep_time = self.sleep_time\n        while self._running.is_set():\n            try:\n                pubsub.get_message(ignore_subscribe_messages=True, timeout=sleep_time)\n            except BaseException as e:\n                if self.exception_handler is None:\n                    raise\n                self.exception_handler(e, pubsub, self)\n        pubsub.close()\n\n    def stop(self) -> None:\n        # trip the flag so the run loop exits. the run loop will\n        # close the pubsub connection, which disconnects the socket\n        # and returns the connection to the pool.\n        self._running.clear()\n\n\nclass Pipeline(Redis):\n    \"\"\"\n    Pipelines provide a way to transmit multiple commands to the Redis server\n    in one transmission.  This is convenient for batch processing, such as\n    saving all the values in a list to Redis.\n\n    All commands executed within a pipeline(when running in transactional mode,\n    which is the default behavior) are wrapped with MULTI and EXEC\n    calls. This guarantees all commands executed in the pipeline will be\n    executed atomically.\n\n    Any command raising an exception does *not* halt the execution of\n    subsequent commands in the pipeline. Instead, the exception is caught\n    and its instance is placed into the response list returned by execute().\n    Code iterating over the response list should be able to deal with an\n    instance of an exception as a potential value. In general, these will be\n    ResponseError exceptions, such as those raised when issuing a command\n    on a key of a different datatype.\n    \"\"\"\n\n    UNWATCH_COMMANDS = {\"DISCARD\", \"EXEC\", \"UNWATCH\"}\n\n    def __init__(\n        self,\n        connection_pool: ConnectionPool,\n        response_callbacks,\n        transaction,\n        shard_hint,\n    ):\n        self.connection_pool = connection_pool\n        self.connection: Optional[Connection] = None\n        self.response_callbacks = response_callbacks\n        self.transaction = transaction\n        self.shard_hint = shard_hint\n        self.watching = False\n        self.command_stack = []\n        self.scripts: Set[Script] = set()\n        self.explicit_transaction = False\n\n    def __enter__(self) -> \"Pipeline\":\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.reset()\n\n    def __del__(self):\n        try:\n            self.reset()\n        except Exception:\n            pass\n\n    def __len__(self) -> int:\n        return len(self.command_stack)\n\n    def __bool__(self) -> bool:\n        \"\"\"Pipeline instances should always evaluate to True\"\"\"\n        return True\n\n    def reset(self) -> None:\n        self.command_stack = []\n        self.scripts = set()\n        # make sure to reset the connection state in the event that we were\n        # watching something\n        if self.watching and self.connection:\n            try:\n                # call this manually since our unwatch or\n                # immediate_execute_command methods can call reset()\n                self.connection.send_command(\"UNWATCH\")\n                self.connection.read_response()\n            except ConnectionError:\n                # disconnect will also remove any previous WATCHes\n                self.connection.disconnect()\n        # clean up the other instance attributes\n        self.watching = False\n        self.explicit_transaction = False\n        # we can safely return the connection to the pool here since we're\n        # sure we're no longer WATCHing anything\n        if self.connection:\n            self.connection_pool.release(self.connection)\n            self.connection = None\n\n    def close(self) -> None:\n        \"\"\"Close the pipeline\"\"\"\n        self.reset()\n\n    def multi(self) -> None:\n        \"\"\"\n        Start a transactional block of the pipeline after WATCH commands\n        are issued. End the transactional block with `execute`.\n        \"\"\"\n        if self.explicit_transaction:\n            raise RedisError(\"Cannot issue nested calls to MULTI\")\n        if self.command_stack:\n            raise RedisError(\n                \"Commands without an initial WATCH have already been issued\"\n            )\n        self.explicit_transaction = True\n\n    def execute_command(self, *args, **kwargs):\n        if (self.watching or args[0] == \"WATCH\") and not self.explicit_transaction:\n            return self.immediate_execute_command(*args, **kwargs)\n        return self.pipeline_execute_command(*args, **kwargs)\n\n    def _disconnect_reset_raise_on_watching(\n        self,\n        conn: AbstractConnection,\n        error: Exception,\n    ) -> None:\n        \"\"\"\n        Close the connection reset watching state and\n        raise an exception if we were watching.\n\n        The supported exceptions are already checked in the\n        retry object so we don't need to do it here.\n\n        After we disconnect the connection, it will try to reconnect and\n        do a health check as part of the send_command logic(on connection level).\n        \"\"\"\n        conn.disconnect()\n\n        # if we were already watching a variable, the watch is no longer\n        # valid since this connection has died. raise a WatchError, which\n        # indicates the user should retry this transaction.\n        if self.watching:\n            self.reset()\n            raise WatchError(\n                f\"A {type(error).__name__} occurred while watching one or more keys\"\n            )\n\n    def immediate_execute_command(self, *args, **options):\n        \"\"\"\n        Execute a command immediately, but don't auto-retry on the supported\n        errors for retry if we're already WATCHing a variable.\n        Used when issuing WATCH or subsequent commands retrieving their values but before\n        MULTI is called.\n        \"\"\"\n        command_name = args[0]\n        conn = self.connection\n        # if this is the first call, we need a connection\n        if not conn:\n            conn = self.connection_pool.get_connection()\n            self.connection = conn\n\n        return conn.retry.call_with_retry(\n            lambda: self._send_command_parse_response(\n                conn, command_name, *args, **options\n            ),\n            lambda error: self._disconnect_reset_raise_on_watching(conn, error),\n        )\n\n    def pipeline_execute_command(self, *args, **options) -> \"Pipeline\":\n        \"\"\"\n        Stage a command to be executed when execute() is next called\n\n        Returns the current Pipeline object back so commands can be\n        chained together, such as:\n\n        pipe = pipe.set('foo', 'bar').incr('baz').decr('bang')\n\n        At some other point, you can then run: pipe.execute(),\n        which will execute all commands queued in the pipe.\n        \"\"\"\n        self.command_stack.append((args, options))\n        return self\n\n    def _execute_transaction(\n        self, connection: Connection, commands, raise_on_error\n    ) -> List:\n        cmds = chain([((\"MULTI\",), {})], commands, [((\"EXEC\",), {})])\n        all_cmds = connection.pack_commands(\n            [args for args, options in cmds if EMPTY_RESPONSE not in options]\n        )\n        connection.send_packed_command(all_cmds)\n        errors = []\n\n        # parse off the response for MULTI\n        # NOTE: we need to handle ResponseErrors here and continue\n        # so that we read all the additional command messages from\n        # the socket\n        try:\n            self.parse_response(connection, \"_\")\n        except ResponseError as e:\n            errors.append((0, e))\n\n        # and all the other commands\n        for i, command in enumerate(commands):\n            if EMPTY_RESPONSE in command[1]:\n                errors.append((i, command[1][EMPTY_RESPONSE]))\n            else:\n                try:\n                    self.parse_response(connection, \"_\")\n                except ResponseError as e:\n                    self.annotate_exception(e, i + 1, command[0])\n                    errors.append((i, e))\n\n        # parse the EXEC.\n        try:\n            response = self.parse_response(connection, \"_\")\n        except ExecAbortError:\n            if errors:\n                raise errors[0][1]\n            raise\n\n        # EXEC clears any watched keys\n        self.watching = False\n\n        if response is None:\n            raise WatchError(\"Watched variable changed.\")\n\n        # put any parse errors into the response\n        for i, e in errors:\n            response.insert(i, e)\n\n        if len(response) != len(commands):\n            self.connection.disconnect()\n            raise ResponseError(\n                \"Wrong number of response items from pipeline execution\"\n            )\n\n        # find any errors in the response and raise if necessary\n        if raise_on_error:\n            self.raise_first_error(commands, response)\n\n        # We have to run response callbacks manually\n        data = []\n        for r, cmd in zip(response, commands):\n            if not isinstance(r, Exception):\n                args, options = cmd\n                # Remove keys entry, it needs only for cache.\n                options.pop(\"keys\", None)\n                command_name = args[0]\n                if command_name in self.response_callbacks:\n                    r = self.response_callbacks[command_name](r, **options)\n            data.append(r)\n        return data\n\n    def _execute_pipeline(self, connection, commands, raise_on_error):\n        # build up all commands into a single request to increase network perf\n        all_cmds = connection.pack_commands([args for args, _ in commands])\n        connection.send_packed_command(all_cmds)\n\n        response = []\n        for args, options in commands:\n            try:\n                response.append(self.parse_response(connection, args[0], **options))\n            except ResponseError as e:\n                response.append(e)\n\n        if raise_on_error:\n            self.raise_first_error(commands, response)\n        return response\n\n    def raise_first_error(self, commands, response):\n        for i, r in enumerate(response):\n            if isinstance(r, ResponseError):\n                self.annotate_exception(r, i + 1, commands[i][0])\n                raise r\n\n    def annotate_exception(self, exception, number, command):\n        cmd = \" \".join(map(safe_str, command))\n        msg = (\n            f\"Command # {number} ({truncate_text(cmd)}) of pipeline \"\n            f\"caused error: {exception.args[0]}\"\n        )\n        exception.args = (msg,) + exception.args[1:]\n\n    def parse_response(self, connection, command_name, **options):\n        result = Redis.parse_response(self, connection, command_name, **options)\n        if command_name in self.UNWATCH_COMMANDS:\n            self.watching = False\n        elif command_name == \"WATCH\":\n            self.watching = True\n        return result\n\n    def load_scripts(self):\n        # make sure all scripts that are about to be run on this pipeline exist\n        scripts = list(self.scripts)\n        immediate = self.immediate_execute_command\n        shas = [s.sha for s in scripts]\n        # we can't use the normal script_* methods because they would just\n        # get buffered in the pipeline.\n        exists = immediate(\"SCRIPT EXISTS\", *shas)\n        if not all(exists):\n            for s, exist in zip(scripts, exists):\n                if not exist:\n                    s.sha = immediate(\"SCRIPT LOAD\", s.script)\n\n    def _disconnect_raise_on_watching(\n        self,\n        conn: AbstractConnection,\n        error: Exception,\n    ) -> None:\n        \"\"\"\n        Close the connection, raise an exception if we were watching.\n\n        The supported exceptions are already checked in the\n        retry object so we don't need to do it here.\n\n        After we disconnect the connection, it will try to reconnect and\n        do a health check as part of the send_command logic(on connection level).\n        \"\"\"\n        conn.disconnect()\n        # if we were watching a variable, the watch is no longer valid\n        # since this connection has died. raise a WatchError, which\n        # indicates the user should retry this transaction.\n        if self.watching:\n            raise WatchError(\n                f\"A {type(error).__name__} occurred while watching one or more keys\"\n            )\n\n    def execute(self, raise_on_error: bool = True) -> List[Any]:\n        \"\"\"Execute all the commands in the current pipeline\"\"\"\n        stack = self.command_stack\n        if not stack and not self.watching:\n            return []\n        if self.scripts:\n            self.load_scripts()\n        if self.transaction or self.explicit_transaction:\n            execute = self._execute_transaction\n        else:\n            execute = self._execute_pipeline\n\n        conn = self.connection\n        if not conn:\n            conn = self.connection_pool.get_connection()\n            # assign to self.connection so reset() releases the connection\n            # back to the pool after we're done\n            self.connection = conn\n\n        try:\n            return conn.retry.call_with_retry(\n                lambda: execute(conn, stack, raise_on_error),\n                lambda error: self._disconnect_raise_on_watching(conn, error),\n            )\n        finally:\n            self.reset()\n\n    def discard(self):\n        \"\"\"\n        Flushes all previously queued commands\n        See: https://redis.io/commands/DISCARD\n        \"\"\"\n        self.execute_command(\"DISCARD\")\n\n    def watch(self, *names):\n        \"\"\"Watches the values at keys ``names``\"\"\"\n        if self.explicit_transaction:\n            raise RedisError(\"Cannot issue a WATCH after a MULTI\")\n        return self.execute_command(\"WATCH\", *names)\n\n    def unwatch(self) -> bool:\n        \"\"\"Unwatches all previously specified keys\"\"\"\n        return self.watching and self.execute_command(\"UNWATCH\") or True\n", 1631], "/usr/local/lib/python3.11/site-packages/redis/event.py": ["import asyncio\nimport threading\nfrom abc import ABC, abstractmethod\nfrom enum import Enum\nfrom typing import List, Optional, Union\n\nfrom redis.auth.token import TokenInterface\nfrom redis.credentials import CredentialProvider, StreamingCredentialProvider\n\n\nclass EventListenerInterface(ABC):\n    \"\"\"\n    Represents a listener for given event object.\n    \"\"\"\n\n    @abstractmethod\n    def listen(self, event: object):\n        pass\n\n\nclass AsyncEventListenerInterface(ABC):\n    \"\"\"\n    Represents an async listener for given event object.\n    \"\"\"\n\n    @abstractmethod\n    async def listen(self, event: object):\n        pass\n\n\nclass EventDispatcherInterface(ABC):\n    \"\"\"\n    Represents a dispatcher that dispatches events to listeners\n    associated with given event.\n    \"\"\"\n\n    @abstractmethod\n    def dispatch(self, event: object):\n        pass\n\n    @abstractmethod\n    async def dispatch_async(self, event: object):\n        pass\n\n\nclass EventException(Exception):\n    \"\"\"\n    Exception wrapper that adds an event object into exception context.\n    \"\"\"\n\n    def __init__(self, exception: Exception, event: object):\n        self.exception = exception\n        self.event = event\n        super().__init__(exception)\n\n\nclass EventDispatcher(EventDispatcherInterface):\n    # TODO: Make dispatcher to accept external mappings.\n    def __init__(self):\n        \"\"\"\n        Mapping should be extended for any new events or listeners to be added.\n        \"\"\"\n        self._event_listeners_mapping = {\n            AfterConnectionReleasedEvent: [\n                ReAuthConnectionListener(),\n            ],\n            AfterPooledConnectionsInstantiationEvent: [\n                RegisterReAuthForPooledConnections()\n            ],\n            AfterSingleConnectionInstantiationEvent: [\n                RegisterReAuthForSingleConnection()\n            ],\n            AfterPubSubConnectionInstantiationEvent: [RegisterReAuthForPubSub()],\n            AfterAsyncClusterInstantiationEvent: [RegisterReAuthForAsyncClusterNodes()],\n            AsyncAfterConnectionReleasedEvent: [\n                AsyncReAuthConnectionListener(),\n            ],\n        }\n\n    def dispatch(self, event: object):\n        listeners = self._event_listeners_mapping.get(type(event))\n\n        for listener in listeners:\n            listener.listen(event)\n\n    async def dispatch_async(self, event: object):\n        listeners = self._event_listeners_mapping.get(type(event))\n\n        for listener in listeners:\n            await listener.listen(event)\n\n\nclass AfterConnectionReleasedEvent:\n    \"\"\"\n    Event that will be fired before each command execution.\n    \"\"\"\n\n    def __init__(self, connection):\n        self._connection = connection\n\n    @property\n    def connection(self):\n        return self._connection\n\n\nclass AsyncAfterConnectionReleasedEvent(AfterConnectionReleasedEvent):\n    pass\n\n\nclass ClientType(Enum):\n    SYNC = (\"sync\",)\n    ASYNC = (\"async\",)\n\n\nclass AfterPooledConnectionsInstantiationEvent:\n    \"\"\"\n    Event that will be fired after pooled connection instances was created.\n    \"\"\"\n\n    def __init__(\n        self,\n        connection_pools: List,\n        client_type: ClientType,\n        credential_provider: Optional[CredentialProvider] = None,\n    ):\n        self._connection_pools = connection_pools\n        self._client_type = client_type\n        self._credential_provider = credential_provider\n\n    @property\n    def connection_pools(self):\n        return self._connection_pools\n\n    @property\n    def client_type(self) -> ClientType:\n        return self._client_type\n\n    @property\n    def credential_provider(self) -> Union[CredentialProvider, None]:\n        return self._credential_provider\n\n\nclass AfterSingleConnectionInstantiationEvent:\n    \"\"\"\n    Event that will be fired after single connection instances was created.\n\n    :param connection_lock: For sync client thread-lock should be provided,\n    for async asyncio.Lock\n    \"\"\"\n\n    def __init__(\n        self,\n        connection,\n        client_type: ClientType,\n        connection_lock: Union[threading.RLock, asyncio.Lock],\n    ):\n        self._connection = connection\n        self._client_type = client_type\n        self._connection_lock = connection_lock\n\n    @property\n    def connection(self):\n        return self._connection\n\n    @property\n    def client_type(self) -> ClientType:\n        return self._client_type\n\n    @property\n    def connection_lock(self) -> Union[threading.RLock, asyncio.Lock]:\n        return self._connection_lock\n\n\nclass AfterPubSubConnectionInstantiationEvent:\n    def __init__(\n        self,\n        pubsub_connection,\n        connection_pool,\n        client_type: ClientType,\n        connection_lock: Union[threading.RLock, asyncio.Lock],\n    ):\n        self._pubsub_connection = pubsub_connection\n        self._connection_pool = connection_pool\n        self._client_type = client_type\n        self._connection_lock = connection_lock\n\n    @property\n    def pubsub_connection(self):\n        return self._pubsub_connection\n\n    @property\n    def connection_pool(self):\n        return self._connection_pool\n\n    @property\n    def client_type(self) -> ClientType:\n        return self._client_type\n\n    @property\n    def connection_lock(self) -> Union[threading.RLock, asyncio.Lock]:\n        return self._connection_lock\n\n\nclass AfterAsyncClusterInstantiationEvent:\n    \"\"\"\n    Event that will be fired after async cluster instance was created.\n\n    Async cluster doesn't use connection pools,\n    instead ClusterNode object manages connections.\n    \"\"\"\n\n    def __init__(\n        self,\n        nodes: dict,\n        credential_provider: Optional[CredentialProvider] = None,\n    ):\n        self._nodes = nodes\n        self._credential_provider = credential_provider\n\n    @property\n    def nodes(self) -> dict:\n        return self._nodes\n\n    @property\n    def credential_provider(self) -> Union[CredentialProvider, None]:\n        return self._credential_provider\n\n\nclass ReAuthConnectionListener(EventListenerInterface):\n    \"\"\"\n    Listener that performs re-authentication of given connection.\n    \"\"\"\n\n    def listen(self, event: AfterConnectionReleasedEvent):\n        event.connection.re_auth()\n\n\nclass AsyncReAuthConnectionListener(AsyncEventListenerInterface):\n    \"\"\"\n    Async listener that performs re-authentication of given connection.\n    \"\"\"\n\n    async def listen(self, event: AsyncAfterConnectionReleasedEvent):\n        await event.connection.re_auth()\n\n\nclass RegisterReAuthForPooledConnections(EventListenerInterface):\n    \"\"\"\n    Listener that registers a re-authentication callback for pooled connections.\n    Required by :class:`StreamingCredentialProvider`.\n    \"\"\"\n\n    def __init__(self):\n        self._event = None\n\n    def listen(self, event: AfterPooledConnectionsInstantiationEvent):\n        if isinstance(event.credential_provider, StreamingCredentialProvider):\n            self._event = event\n\n            if event.client_type == ClientType.SYNC:\n                event.credential_provider.on_next(self._re_auth)\n                event.credential_provider.on_error(self._raise_on_error)\n            else:\n                event.credential_provider.on_next(self._re_auth_async)\n                event.credential_provider.on_error(self._raise_on_error_async)\n\n    def _re_auth(self, token):\n        for pool in self._event.connection_pools:\n            pool.re_auth_callback(token)\n\n    async def _re_auth_async(self, token):\n        for pool in self._event.connection_pools:\n            await pool.re_auth_callback(token)\n\n    def _raise_on_error(self, error: Exception):\n        raise EventException(error, self._event)\n\n    async def _raise_on_error_async(self, error: Exception):\n        raise EventException(error, self._event)\n\n\nclass RegisterReAuthForSingleConnection(EventListenerInterface):\n    \"\"\"\n    Listener that registers a re-authentication callback for single connection.\n    Required by :class:`StreamingCredentialProvider`.\n    \"\"\"\n\n    def __init__(self):\n        self._event = None\n\n    def listen(self, event: AfterSingleConnectionInstantiationEvent):\n        if isinstance(\n            event.connection.credential_provider, StreamingCredentialProvider\n        ):\n            self._event = event\n\n            if event.client_type == ClientType.SYNC:\n                event.connection.credential_provider.on_next(self._re_auth)\n                event.connection.credential_provider.on_error(self._raise_on_error)\n            else:\n                event.connection.credential_provider.on_next(self._re_auth_async)\n                event.connection.credential_provider.on_error(\n                    self._raise_on_error_async\n                )\n\n    def _re_auth(self, token):\n        with self._event.connection_lock:\n            self._event.connection.send_command(\n                \"AUTH\", token.try_get(\"oid\"), token.get_value()\n            )\n            self._event.connection.read_response()\n\n    async def _re_auth_async(self, token):\n        async with self._event.connection_lock:\n            await self._event.connection.send_command(\n                \"AUTH\", token.try_get(\"oid\"), token.get_value()\n            )\n            await self._event.connection.read_response()\n\n    def _raise_on_error(self, error: Exception):\n        raise EventException(error, self._event)\n\n    async def _raise_on_error_async(self, error: Exception):\n        raise EventException(error, self._event)\n\n\nclass RegisterReAuthForAsyncClusterNodes(EventListenerInterface):\n    def __init__(self):\n        self._event = None\n\n    def listen(self, event: AfterAsyncClusterInstantiationEvent):\n        if isinstance(event.credential_provider, StreamingCredentialProvider):\n            self._event = event\n            event.credential_provider.on_next(self._re_auth)\n            event.credential_provider.on_error(self._raise_on_error)\n\n    async def _re_auth(self, token: TokenInterface):\n        for key in self._event.nodes:\n            await self._event.nodes[key].re_auth_callback(token)\n\n    async def _raise_on_error(self, error: Exception):\n        raise EventException(error, self._event)\n\n\nclass RegisterReAuthForPubSub(EventListenerInterface):\n    def __init__(self):\n        self._connection = None\n        self._connection_pool = None\n        self._client_type = None\n        self._connection_lock = None\n        self._event = None\n\n    def listen(self, event: AfterPubSubConnectionInstantiationEvent):\n        if isinstance(\n            event.pubsub_connection.credential_provider, StreamingCredentialProvider\n        ) and event.pubsub_connection.get_protocol() in [3, \"3\"]:\n            self._event = event\n            self._connection = event.pubsub_connection\n            self._connection_pool = event.connection_pool\n            self._client_type = event.client_type\n            self._connection_lock = event.connection_lock\n\n            if self._client_type == ClientType.SYNC:\n                self._connection.credential_provider.on_next(self._re_auth)\n                self._connection.credential_provider.on_error(self._raise_on_error)\n            else:\n                self._connection.credential_provider.on_next(self._re_auth_async)\n                self._connection.credential_provider.on_error(\n                    self._raise_on_error_async\n                )\n\n    def _re_auth(self, token: TokenInterface):\n        with self._connection_lock:\n            self._connection.send_command(\n                \"AUTH\", token.try_get(\"oid\"), token.get_value()\n            )\n            self._connection.read_response()\n\n        self._connection_pool.re_auth_callback(token)\n\n    async def _re_auth_async(self, token: TokenInterface):\n        async with self._connection_lock:\n            await self._connection.send_command(\n                \"AUTH\", token.try_get(\"oid\"), token.get_value()\n            )\n            await self._connection.read_response()\n\n        await self._connection_pool.re_auth_callback(token)\n\n    def _raise_on_error(self, error: Exception):\n        raise EventException(error, self._event)\n\n    async def _raise_on_error_async(self, error: Exception):\n        raise EventException(error, self._event)\n", 394], "/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py": ["import os\nfrom threading import Lock\nimport time\nimport types\nfrom typing import (\n    Any, Callable, Dict, Iterable, List, Literal, Optional, Sequence, Tuple,\n    Type, TypeVar, Union,\n)\nimport warnings\n\nfrom . import values  # retain this import style for testability\nfrom .context_managers import ExceptionCounter, InprogressTracker, Timer\nfrom .metrics_core import Metric\nfrom .registry import Collector, CollectorRegistry, REGISTRY\nfrom .samples import Exemplar, Sample\nfrom .utils import floatToGoString, INF\nfrom .validation import (\n    _validate_exemplar, _validate_labelnames, _validate_metric_name,\n)\n\nT = TypeVar('T', bound='MetricWrapperBase')\nF = TypeVar(\"F\", bound=Callable[..., Any])\n\n\ndef _build_full_name(metric_type, name, namespace, subsystem, unit):\n    if not name:\n        raise ValueError('Metric name should not be empty')\n    full_name = ''\n    if namespace:\n        full_name += namespace + '_'\n    if subsystem:\n        full_name += subsystem + '_'\n    full_name += name\n    if metric_type == 'counter' and full_name.endswith('_total'):\n        full_name = full_name[:-6]  # Munge to OpenMetrics.\n    if unit and not full_name.endswith(\"_\" + unit):\n        full_name += \"_\" + unit\n    if unit and metric_type in ('info', 'stateset'):\n        raise ValueError('Metric name is of a type that cannot have a unit: ' + full_name)\n    return full_name\n\n\n\ndef _get_use_created() -> bool:\n    return os.environ.get(\"PROMETHEUS_DISABLE_CREATED_SERIES\", 'False').lower() not in ('true', '1', 't')\n\n\n_use_created = _get_use_created()\n\n\ndef disable_created_metrics():\n    \"\"\"Disable exporting _created metrics on counters, histograms, and summaries.\"\"\"\n    global _use_created\n    _use_created = False\n\n\ndef enable_created_metrics():\n    \"\"\"Enable exporting _created metrics on counters, histograms, and summaries.\"\"\"\n    global _use_created\n    _use_created = True\n\n\nclass MetricWrapperBase(Collector):\n    _type: Optional[str] = None\n    _reserved_labelnames: Sequence[str] = ()\n\n    def _is_observable(self):\n        # Whether this metric is observable, i.e.\n        # * a metric without label names and values, or\n        # * the child of a labelled metric.\n        return not self._labelnames or (self._labelnames and self._labelvalues)\n\n    def _raise_if_not_observable(self):\n        # Functions that mutate the state of the metric, for example incrementing\n        # a counter, will fail if the metric is not observable, because only if a\n        # metric is observable will the value be initialized.\n        if not self._is_observable():\n            raise ValueError('%s metric is missing label values' % str(self._type))\n\n    def _is_parent(self):\n        return self._labelnames and not self._labelvalues\n\n    def _get_metric(self):\n        return Metric(self._name, self._documentation, self._type, self._unit)\n\n    def describe(self) -> Iterable[Metric]:\n        return [self._get_metric()]\n\n    def collect(self) -> Iterable[Metric]:\n        metric = self._get_metric()\n        for suffix, labels, value, timestamp, exemplar, native_histogram_value in self._samples():\n            metric.add_sample(self._name + suffix, labels, value, timestamp, exemplar, native_histogram_value)\n        return [metric]\n\n    def __str__(self) -> str:\n        return f\"{self._type}:{self._name}\"\n\n    def __repr__(self) -> str:\n        metric_type = type(self)\n        return f\"{metric_type.__module__}.{metric_type.__name__}({self._name})\"\n\n    def __init__(self: T,\n                 name: str,\n                 documentation: str,\n                 labelnames: Iterable[str] = (),\n                 namespace: str = '',\n                 subsystem: str = '',\n                 unit: str = '',\n                 registry: Optional[CollectorRegistry] = REGISTRY,\n                 _labelvalues: Optional[Sequence[str]] = None,\n                 ) -> None:\n        self._name = _build_full_name(self._type, name, namespace, subsystem, unit)\n        self._labelnames = _validate_labelnames(self, labelnames)\n        self._labelvalues = tuple(_labelvalues or ())\n        self._kwargs: Dict[str, Any] = {}\n        self._documentation = documentation\n        self._unit = unit\n\n        _validate_metric_name(self._name)\n\n        if self._is_parent():\n            # Prepare the fields needed for child metrics.\n            self._lock = Lock()\n            self._metrics: Dict[Sequence[str], T] = {}\n\n        if self._is_observable():\n            self._metric_init()\n\n        if not self._labelvalues:\n            # Register the multi-wrapper parent metric, or if a label-less metric, the whole shebang.\n            if registry:\n                registry.register(self)\n\n    def labels(self: T, *labelvalues: Any, **labelkwargs: Any) -> T:\n        \"\"\"Return the child for the given labelset.\n\n        All metrics can have labels, allowing grouping of related time series.\n        Taking a counter as an example:\n\n            from prometheus_client import Counter\n\n            c = Counter('my_requests_total', 'HTTP Failures', ['method', 'endpoint'])\n            c.labels('get', '/').inc()\n            c.labels('post', '/submit').inc()\n\n        Labels can also be provided as keyword arguments:\n\n            from prometheus_client import Counter\n\n            c = Counter('my_requests_total', 'HTTP Failures', ['method', 'endpoint'])\n            c.labels(method='get', endpoint='/').inc()\n            c.labels(method='post', endpoint='/submit').inc()\n\n        See the best practices on [naming](http://prometheus.io/docs/practices/naming/)\n        and [labels](http://prometheus.io/docs/practices/instrumentation/#use-labels).\n        \"\"\"\n        if not self._labelnames:\n            raise ValueError('No label names were set when constructing %s' % self)\n\n        if self._labelvalues:\n            raise ValueError('{} already has labels set ({}); can not chain calls to .labels()'.format(\n                self,\n                dict(zip(self._labelnames, self._labelvalues))\n            ))\n\n        if labelvalues and labelkwargs:\n            raise ValueError(\"Can't pass both *args and **kwargs\")\n\n        if labelkwargs:\n            if sorted(labelkwargs) != sorted(self._labelnames):\n                raise ValueError('Incorrect label names')\n            labelvalues = tuple(str(labelkwargs[l]) for l in self._labelnames)\n        else:\n            if len(labelvalues) != len(self._labelnames):\n                raise ValueError('Incorrect label count')\n            labelvalues = tuple(str(l) for l in labelvalues)\n        with self._lock:\n            if labelvalues not in self._metrics:\n                self._metrics[labelvalues] = self.__class__(\n                    self._name,\n                    documentation=self._documentation,\n                    labelnames=self._labelnames,\n                    unit=self._unit,\n                    _labelvalues=labelvalues,\n                    **self._kwargs\n                )\n            return self._metrics[labelvalues]\n\n    def remove(self, *labelvalues: Any) -> None:\n        if 'prometheus_multiproc_dir' in os.environ or 'PROMETHEUS_MULTIPROC_DIR' in os.environ:\n            warnings.warn(\n                \"Removal of labels has not been implemented in  multi-process mode yet.\",\n                UserWarning)\n\n        if not self._labelnames:\n            raise ValueError('No label names were set when constructing %s' % self)\n\n        \"\"\"Remove the given labelset from the metric.\"\"\"\n        if len(labelvalues) != len(self._labelnames):\n            raise ValueError('Incorrect label count (expected %d, got %s)' % (len(self._labelnames), labelvalues))\n        labelvalues = tuple(str(l) for l in labelvalues)\n        with self._lock:\n            if labelvalues in self._metrics:\n                del self._metrics[labelvalues]\n\n    def clear(self) -> None:\n        \"\"\"Remove all labelsets from the metric\"\"\"\n        if 'prometheus_multiproc_dir' in os.environ or 'PROMETHEUS_MULTIPROC_DIR' in os.environ:\n            warnings.warn(\n                \"Clearing labels has not been implemented in multi-process mode yet\",\n                UserWarning)\n        with self._lock:\n            self._metrics = {}\n\n    def _samples(self) -> Iterable[Sample]:\n        if self._is_parent():\n            return self._multi_samples()\n        else:\n            return self._child_samples()\n\n    def _multi_samples(self) -> Iterable[Sample]:\n        with self._lock:\n            metrics = self._metrics.copy()\n        for labels, metric in metrics.items():\n            series_labels = list(zip(self._labelnames, labels))\n            for suffix, sample_labels, value, timestamp, exemplar, native_histogram_value in metric._samples():\n                yield Sample(suffix, dict(series_labels + list(sample_labels.items())), value, timestamp, exemplar, native_histogram_value)\n\n    def _child_samples(self) -> Iterable[Sample]:  # pragma: no cover\n        raise NotImplementedError('_child_samples() must be implemented by %r' % self)\n\n    def _metric_init(self):  # pragma: no cover\n        \"\"\"\n        Initialize the metric object as a child, i.e. when it has labels (if any) set.\n\n        This is factored as a separate function to allow for deferred initialization.\n        \"\"\"\n        raise NotImplementedError('_metric_init() must be implemented by %r' % self)\n\n\nclass Counter(MetricWrapperBase):\n    \"\"\"A Counter tracks counts of events or running totals.\n\n    Example use cases for Counters:\n    - Number of requests processed\n    - Number of items that were inserted into a queue\n    - Total amount of data that a system has processed\n\n    Counters can only go up (and be reset when the process restarts). If your use case can go down,\n    you should use a Gauge instead.\n\n    An example for a Counter:\n\n        from prometheus_client import Counter\n\n        c = Counter('my_failures_total', 'Description of counter')\n        c.inc()     # Increment by 1\n        c.inc(1.6)  # Increment by given value\n\n    There are utilities to count exceptions raised:\n\n        @c.count_exceptions()\n        def f():\n            pass\n\n        with c.count_exceptions():\n            pass\n\n        # Count only one type of exception\n        with c.count_exceptions(ValueError):\n            pass\n            \n    You can also reset the counter to zero in case your logical \"process\" restarts\n    without restarting the actual python process.\n\n       c.reset()\n\n    \"\"\"\n    _type = 'counter'\n\n    def _metric_init(self) -> None:\n        self._value = values.ValueClass(self._type, self._name, self._name + '_total', self._labelnames,\n                                        self._labelvalues, self._documentation)\n        self._created = time.time()\n\n    def inc(self, amount: float = 1, exemplar: Optional[Dict[str, str]] = None) -> None:\n        \"\"\"Increment counter by the given amount.\"\"\"\n        self._raise_if_not_observable()\n        if amount < 0:\n            raise ValueError('Counters can only be incremented by non-negative amounts.')\n        self._value.inc(amount)\n        if exemplar:\n            _validate_exemplar(exemplar)\n            self._value.set_exemplar(Exemplar(exemplar, amount, time.time()))\n\n    def reset(self) -> None:\n        \"\"\"Reset the counter to zero. Use this when a logical process restarts without restarting the actual python process.\"\"\"\n        self._value.set(0)\n        self._created = time.time()\n\n    def count_exceptions(self, exception: Union[Type[BaseException], Tuple[Type[BaseException], ...]] = Exception) -> ExceptionCounter:\n        \"\"\"Count exceptions in a block of code or function.\n\n        Can be used as a function decorator or context manager.\n        Increments the counter when an exception of the given\n        type is raised up out of the code.\n        \"\"\"\n        self._raise_if_not_observable()\n        return ExceptionCounter(self, exception)\n\n    def _child_samples(self) -> Iterable[Sample]:\n        sample = Sample('_total', {}, self._value.get(), None, self._value.get_exemplar())\n        if _use_created:\n            return (\n                sample,\n                Sample('_created', {}, self._created, None, None)\n            )\n        return (sample,)\n\n\nclass Gauge(MetricWrapperBase):\n    \"\"\"Gauge metric, to report instantaneous values.\n\n     Examples of Gauges include:\n        - Inprogress requests\n        - Number of items in a queue\n        - Free memory\n        - Total memory\n        - Temperature\n\n     Gauges can go both up and down.\n\n        from prometheus_client import Gauge\n\n        g = Gauge('my_inprogress_requests', 'Description of gauge')\n        g.inc()      # Increment by 1\n        g.dec(10)    # Decrement by given value\n        g.set(4.2)   # Set to a given value\n\n     There are utilities for common use cases:\n\n        g.set_to_current_time()   # Set to current unixtime\n\n        # Increment when entered, decrement when exited.\n        @g.track_inprogress()\n        def f():\n            pass\n\n        with g.track_inprogress():\n            pass\n\n     A Gauge can also take its value from a callback:\n\n        d = Gauge('data_objects', 'Number of objects')\n        my_dict = {}\n        d.set_function(lambda: len(my_dict))\n    \"\"\"\n    _type = 'gauge'\n    _MULTIPROC_MODES = frozenset(('all', 'liveall', 'min', 'livemin', 'max', 'livemax', 'sum', 'livesum', 'mostrecent', 'livemostrecent'))\n    _MOST_RECENT_MODES = frozenset(('mostrecent', 'livemostrecent'))\n\n    def __init__(self,\n                 name: str,\n                 documentation: str,\n                 labelnames: Iterable[str] = (),\n                 namespace: str = '',\n                 subsystem: str = '',\n                 unit: str = '',\n                 registry: Optional[CollectorRegistry] = REGISTRY,\n                 _labelvalues: Optional[Sequence[str]] = None,\n                 multiprocess_mode: Literal['all', 'liveall', 'min', 'livemin', 'max', 'livemax', 'sum', 'livesum', 'mostrecent', 'livemostrecent'] = 'all',\n                 ):\n        self._multiprocess_mode = multiprocess_mode\n        if multiprocess_mode not in self._MULTIPROC_MODES:\n            raise ValueError('Invalid multiprocess mode: ' + multiprocess_mode)\n        super().__init__(\n            name=name,\n            documentation=documentation,\n            labelnames=labelnames,\n            namespace=namespace,\n            subsystem=subsystem,\n            unit=unit,\n            registry=registry,\n            _labelvalues=_labelvalues,\n        )\n        self._kwargs['multiprocess_mode'] = self._multiprocess_mode\n        self._is_most_recent = self._multiprocess_mode in self._MOST_RECENT_MODES\n\n    def _metric_init(self) -> None:\n        self._value = values.ValueClass(\n            self._type, self._name, self._name, self._labelnames, self._labelvalues,\n            self._documentation, multiprocess_mode=self._multiprocess_mode\n        )\n\n    def inc(self, amount: float = 1) -> None:\n        \"\"\"Increment gauge by the given amount.\"\"\"\n        if self._is_most_recent:\n            raise RuntimeError(\"inc must not be used with the mostrecent mode\")\n        self._raise_if_not_observable()\n        self._value.inc(amount)\n\n    def dec(self, amount: float = 1) -> None:\n        \"\"\"Decrement gauge by the given amount.\"\"\"\n        if self._is_most_recent:\n            raise RuntimeError(\"dec must not be used with the mostrecent mode\")\n        self._raise_if_not_observable()\n        self._value.inc(-amount)\n\n    def set(self, value: float) -> None:\n        \"\"\"Set gauge to the given value.\"\"\"\n        self._raise_if_not_observable()\n        if self._is_most_recent:\n            self._value.set(float(value), timestamp=time.time())\n        else:\n            self._value.set(float(value))\n\n    def set_to_current_time(self) -> None:\n        \"\"\"Set gauge to the current unixtime.\"\"\"\n        self.set(time.time())\n\n    def track_inprogress(self) -> InprogressTracker:\n        \"\"\"Track inprogress blocks of code or functions.\n\n        Can be used as a function decorator or context manager.\n        Increments the gauge when the code is entered,\n        and decrements when it is exited.\n        \"\"\"\n        self._raise_if_not_observable()\n        return InprogressTracker(self)\n\n    def time(self) -> Timer:\n        \"\"\"Time a block of code or function, and set the duration in seconds.\n\n        Can be used as a function decorator or context manager.\n        \"\"\"\n        return Timer(self, 'set')\n\n    def set_function(self, f: Callable[[], float]) -> None:\n        \"\"\"Call the provided function to return the Gauge value.\n\n        The function must return a float, and may be called from\n        multiple threads. All other methods of the Gauge become NOOPs.\n        \"\"\"\n\n        self._raise_if_not_observable()\n\n        def samples(_: Gauge) -> Iterable[Sample]:\n            return (Sample('', {}, float(f()), None, None),)\n\n        self._child_samples = types.MethodType(samples, self)  # type: ignore\n\n    def _child_samples(self) -> Iterable[Sample]:\n        return (Sample('', {}, self._value.get(), None, None),)\n\n\nclass Summary(MetricWrapperBase):\n    \"\"\"A Summary tracks the size and number of events.\n\n    Example use cases for Summaries:\n    - Response latency\n    - Request size\n\n    Example for a Summary:\n\n        from prometheus_client import Summary\n\n        s = Summary('request_size_bytes', 'Request size (bytes)')\n        s.observe(512)  # Observe 512 (bytes)\n\n    Example for a Summary using time:\n\n        from prometheus_client import Summary\n\n        REQUEST_TIME = Summary('response_latency_seconds', 'Response latency (seconds)')\n\n        @REQUEST_TIME.time()\n        def create_response(request):\n          '''A dummy function'''\n          time.sleep(1)\n\n    Example for using the same Summary object as a context manager:\n\n        with REQUEST_TIME.time():\n            pass  # Logic to be timed\n    \"\"\"\n    _type = 'summary'\n    _reserved_labelnames = ['quantile']\n\n    def _metric_init(self) -> None:\n        self._count = values.ValueClass(self._type, self._name, self._name + '_count', self._labelnames,\n                                        self._labelvalues, self._documentation)\n        self._sum = values.ValueClass(self._type, self._name, self._name + '_sum', self._labelnames, self._labelvalues, self._documentation)\n        self._created = time.time()\n\n    def observe(self, amount: float) -> None:\n        \"\"\"Observe the given amount.\n\n        The amount is usually positive or zero. Negative values are\n        accepted but prevent current versions of Prometheus from\n        properly detecting counter resets in the sum of\n        observations. See\n        https://prometheus.io/docs/practices/histograms/#count-and-sum-of-observations\n        for details.\n        \"\"\"\n        self._raise_if_not_observable()\n        self._count.inc(1)\n        self._sum.inc(amount)\n\n    def time(self) -> Timer:\n        \"\"\"Time a block of code or function, and observe the duration in seconds.\n\n        Can be used as a function decorator or context manager.\n        \"\"\"\n        return Timer(self, 'observe')\n\n    def _child_samples(self) -> Iterable[Sample]:\n        samples = [\n            Sample('_count', {}, self._count.get(), None, None),\n            Sample('_sum', {}, self._sum.get(), None, None),\n        ]\n        if _use_created:\n            samples.append(Sample('_created', {}, self._created, None, None))\n        return tuple(samples)\n\n\nclass Histogram(MetricWrapperBase):\n    \"\"\"A Histogram tracks the size and number of events in buckets.\n\n    You can use Histograms for aggregatable calculation of quantiles.\n\n    Example use cases:\n    - Response latency\n    - Request size\n\n    Example for a Histogram:\n\n        from prometheus_client import Histogram\n\n        h = Histogram('request_size_bytes', 'Request size (bytes)')\n        h.observe(512)  # Observe 512 (bytes)\n\n    Example for a Histogram using time:\n\n        from prometheus_client import Histogram\n\n        REQUEST_TIME = Histogram('response_latency_seconds', 'Response latency (seconds)')\n\n        @REQUEST_TIME.time()\n        def create_response(request):\n          '''A dummy function'''\n          time.sleep(1)\n\n    Example of using the same Histogram object as a context manager:\n\n        with REQUEST_TIME.time():\n            pass  # Logic to be timed\n\n    The default buckets are intended to cover a typical web/rpc request from milliseconds to seconds.\n    They can be overridden by passing `buckets` keyword argument to `Histogram`.\n    \"\"\"\n    _type = 'histogram'\n    _reserved_labelnames = ['le']\n    DEFAULT_BUCKETS = (.005, .01, .025, .05, .075, .1, .25, .5, .75, 1.0, 2.5, 5.0, 7.5, 10.0, INF)\n\n    def __init__(self,\n                 name: str,\n                 documentation: str,\n                 labelnames: Iterable[str] = (),\n                 namespace: str = '',\n                 subsystem: str = '',\n                 unit: str = '',\n                 registry: Optional[CollectorRegistry] = REGISTRY,\n                 _labelvalues: Optional[Sequence[str]] = None,\n                 buckets: Sequence[Union[float, str]] = DEFAULT_BUCKETS,\n                 ):\n        self._prepare_buckets(buckets)\n        super().__init__(\n            name=name,\n            documentation=documentation,\n            labelnames=labelnames,\n            namespace=namespace,\n            subsystem=subsystem,\n            unit=unit,\n            registry=registry,\n            _labelvalues=_labelvalues,\n        )\n        self._kwargs['buckets'] = buckets\n\n    def _prepare_buckets(self, source_buckets: Sequence[Union[float, str]]) -> None:\n        buckets = [float(b) for b in source_buckets]\n        if buckets != sorted(buckets):\n            # This is probably an error on the part of the user,\n            # so raise rather than sorting for them.\n            raise ValueError('Buckets not in sorted order')\n        if buckets and buckets[-1] != INF:\n            buckets.append(INF)\n        if len(buckets) < 2:\n            raise ValueError('Must have at least two buckets')\n        self._upper_bounds = buckets\n\n    def _metric_init(self) -> None:\n        self._buckets: List[values.ValueClass] = []\n        self._created = time.time()\n        bucket_labelnames = self._labelnames + ('le',)\n        self._sum = values.ValueClass(self._type, self._name, self._name + '_sum', self._labelnames, self._labelvalues, self._documentation)\n        for b in self._upper_bounds:\n            self._buckets.append(values.ValueClass(\n                self._type,\n                self._name,\n                self._name + '_bucket',\n                bucket_labelnames,\n                self._labelvalues + (floatToGoString(b),),\n                self._documentation)\n            )\n\n    def observe(self, amount: float, exemplar: Optional[Dict[str, str]] = None) -> None:\n        \"\"\"Observe the given amount.\n\n        The amount is usually positive or zero. Negative values are\n        accepted but prevent current versions of Prometheus from\n        properly detecting counter resets in the sum of\n        observations. See\n        https://prometheus.io/docs/practices/histograms/#count-and-sum-of-observations\n        for details.\n        \"\"\"\n        self._raise_if_not_observable()\n        self._sum.inc(amount)\n        for i, bound in enumerate(self._upper_bounds):\n            if amount <= bound:\n                self._buckets[i].inc(1)\n                if exemplar:\n                    _validate_exemplar(exemplar)\n                    self._buckets[i].set_exemplar(Exemplar(exemplar, amount, time.time()))\n                break\n\n    def time(self) -> Timer:\n        \"\"\"Time a block of code or function, and observe the duration in seconds.\n\n        Can be used as a function decorator or context manager.\n        \"\"\"\n        return Timer(self, 'observe')\n\n    def _child_samples(self) -> Iterable[Sample]:\n        samples = []\n        acc = 0.0\n        for i, bound in enumerate(self._upper_bounds):\n            acc += self._buckets[i].get()\n            samples.append(Sample('_bucket', {'le': floatToGoString(bound)}, acc, None, self._buckets[i].get_exemplar()))\n        samples.append(Sample('_count', {}, acc, None, None))\n        if self._upper_bounds[0] >= 0:\n            samples.append(Sample('_sum', {}, self._sum.get(), None, None))\n        if _use_created:\n            samples.append(Sample('_created', {}, self._created, None, None))\n        return tuple(samples)\n\n\nclass Info(MetricWrapperBase):\n    \"\"\"Info metric, key-value pairs.\n\n     Examples of Info include:\n        - Build information\n        - Version information\n        - Potential target metadata\n\n     Example usage:\n        from prometheus_client import Info\n\n        i = Info('my_build', 'Description of info')\n        i.info({'version': '1.2.3', 'buildhost': 'foo@bar'})\n\n     Info metrics do not work in multiprocess mode.\n    \"\"\"\n    _type = 'info'\n\n    def _metric_init(self):\n        self._labelname_set = set(self._labelnames)\n        self._lock = Lock()\n        self._value = {}\n\n    def info(self, val: Dict[str, str]) -> None:\n        \"\"\"Set info metric.\"\"\"\n        if self._labelname_set.intersection(val.keys()):\n            raise ValueError('Overlapping labels for Info metric, metric: {} child: {}'.format(\n                self._labelnames, val))\n        if any(i is None for i in val.values()):\n            raise ValueError('Label value cannot be None')\n        with self._lock:\n            self._value = dict(val)\n\n    def _child_samples(self) -> Iterable[Sample]:\n        with self._lock:\n            return (Sample('_info', self._value, 1.0, None, None),)\n\n\nclass Enum(MetricWrapperBase):\n    \"\"\"Enum metric, which of a set of states is true.\n\n     Example usage:\n        from prometheus_client import Enum\n\n        e = Enum('task_state', 'Description of enum',\n          states=['starting', 'running', 'stopped'])\n        e.state('running')\n\n     The first listed state will be the default.\n     Enum metrics do not work in multiprocess mode.\n    \"\"\"\n    _type = 'stateset'\n\n    def __init__(self,\n                 name: str,\n                 documentation: str,\n                 labelnames: Sequence[str] = (),\n                 namespace: str = '',\n                 subsystem: str = '',\n                 unit: str = '',\n                 registry: Optional[CollectorRegistry] = REGISTRY,\n                 _labelvalues: Optional[Sequence[str]] = None,\n                 states: Optional[Sequence[str]] = None,\n                 ):\n        super().__init__(\n            name=name,\n            documentation=documentation,\n            labelnames=labelnames,\n            namespace=namespace,\n            subsystem=subsystem,\n            unit=unit,\n            registry=registry,\n            _labelvalues=_labelvalues,\n        )\n        if name in labelnames:\n            raise ValueError(f'Overlapping labels for Enum metric: {name}')\n        if not states:\n            raise ValueError(f'No states provided for Enum metric: {name}')\n        self._kwargs['states'] = self._states = states\n\n    def _metric_init(self) -> None:\n        self._value = 0\n        self._lock = Lock()\n\n    def state(self, state: str) -> None:\n        \"\"\"Set enum metric state.\"\"\"\n        self._raise_if_not_observable()\n        with self._lock:\n            self._value = self._states.index(state)\n\n    def _child_samples(self) -> Iterable[Sample]:\n        with self._lock:\n            return [\n                Sample('', {self._name: s}, 1 if i == self._value else 0, None, None)\n                for i, s\n                in enumerate(self._states)\n            ]\n", 753], "/usr/local/lib/python3.11/site-packages/prometheus_client/values.py": ["import os\nfrom threading import Lock\nimport warnings\n\nfrom .mmap_dict import mmap_key, MmapedDict\n\n\nclass MutexValue:\n    \"\"\"A float protected by a mutex.\"\"\"\n\n    _multiprocess = False\n\n    def __init__(self, typ, metric_name, name, labelnames, labelvalues, help_text, **kwargs):\n        self._value = 0.0\n        self._exemplar = None\n        self._lock = Lock()\n\n    def inc(self, amount):\n        with self._lock:\n            self._value += amount\n\n    def set(self, value, timestamp=None):\n        with self._lock:\n            self._value = value\n\n    def set_exemplar(self, exemplar):\n        with self._lock:\n            self._exemplar = exemplar\n\n    def get(self):\n        with self._lock:\n            return self._value\n\n    def get_exemplar(self):\n        with self._lock:\n            return self._exemplar\n\n\ndef MultiProcessValue(process_identifier=os.getpid):\n    \"\"\"Returns a MmapedValue class based on a process_identifier function.\n\n    The 'process_identifier' function MUST comply with this simple rule:\n    when called in simultaneously running processes it MUST return distinct values.\n\n    Using a different function than the default 'os.getpid' is at your own risk.\n    \"\"\"\n    files = {}\n    values = []\n    pid = {'value': process_identifier()}\n    # Use a single global lock when in multi-processing mode\n    # as we presume this means there is no threading going on.\n    # This avoids the need to also have mutexes in __MmapDict.\n    lock = Lock()\n\n    class MmapedValue:\n        \"\"\"A float protected by a mutex backed by a per-process mmaped file.\"\"\"\n\n        _multiprocess = True\n\n        def __init__(self, typ, metric_name, name, labelnames, labelvalues, help_text, multiprocess_mode='', **kwargs):\n            self._params = typ, metric_name, name, labelnames, labelvalues, help_text, multiprocess_mode\n            # This deprecation warning can go away in a few releases when removing the compatibility\n            if 'prometheus_multiproc_dir' in os.environ and 'PROMETHEUS_MULTIPROC_DIR' not in os.environ:\n                os.environ['PROMETHEUS_MULTIPROC_DIR'] = os.environ['prometheus_multiproc_dir']\n                warnings.warn(\"prometheus_multiproc_dir variable has been deprecated in favor of the upper case naming PROMETHEUS_MULTIPROC_DIR\", DeprecationWarning)\n            with lock:\n                self.__check_for_pid_change()\n                self.__reset()\n                values.append(self)\n\n        def __reset(self):\n            typ, metric_name, name, labelnames, labelvalues, help_text, multiprocess_mode = self._params\n            if typ == 'gauge':\n                file_prefix = typ + '_' + multiprocess_mode\n            else:\n                file_prefix = typ\n            if file_prefix not in files:\n                filename = os.path.join(\n                    os.environ.get('PROMETHEUS_MULTIPROC_DIR'),\n                    '{}_{}.db'.format(file_prefix, pid['value']))\n\n                files[file_prefix] = MmapedDict(filename)\n            self._file = files[file_prefix]\n            self._key = mmap_key(metric_name, name, labelnames, labelvalues, help_text)\n            self._value, self._timestamp = self._file.read_value(self._key)\n\n        def __check_for_pid_change(self):\n            actual_pid = process_identifier()\n            if pid['value'] != actual_pid:\n                pid['value'] = actual_pid\n                # There has been a fork(), reset all the values.\n                for f in files.values():\n                    f.close()\n                files.clear()\n                for value in values:\n                    value.__reset()\n\n        def inc(self, amount):\n            with lock:\n                self.__check_for_pid_change()\n                self._value += amount\n                self._timestamp = 0.0\n                self._file.write_value(self._key, self._value, self._timestamp)\n\n        def set(self, value, timestamp=None):\n            with lock:\n                self.__check_for_pid_change()\n                self._value = value\n                self._timestamp = timestamp or 0.0\n                self._file.write_value(self._key, self._value, self._timestamp)\n\n        def set_exemplar(self, exemplar):\n            # TODO: Implement exemplars for multiprocess mode.\n            return\n\n        def get(self):\n            with lock:\n                self.__check_for_pid_change()\n                return self._value\n\n        def get_exemplar(self):\n            # TODO: Implement exemplars for multiprocess mode.\n            return None\n\n    return MmapedValue\n\n\ndef get_value_class():\n    # Should we enable multi-process mode?\n    # This needs to be chosen before the first metric is constructed,\n    # and as that may be in some arbitrary library the user/admin has\n    # no control over we use an environment variable.\n    if 'prometheus_multiproc_dir' in os.environ or 'PROMETHEUS_MULTIPROC_DIR' in os.environ:\n        return MultiProcessValue()\n    else:\n        return MutexValue\n\n\nValueClass = get_value_class()\n", 139], "/usr/local/lib/python3.11/email/utils.py": ["# Copyright (C) 2001-2010 Python Software Foundation\n# Author: Barry Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"Miscellaneous utilities.\"\"\"\n\n__all__ = [\n    'collapse_rfc2231_value',\n    'decode_params',\n    'decode_rfc2231',\n    'encode_rfc2231',\n    'formataddr',\n    'formatdate',\n    'format_datetime',\n    'getaddresses',\n    'make_msgid',\n    'mktime_tz',\n    'parseaddr',\n    'parsedate',\n    'parsedate_tz',\n    'parsedate_to_datetime',\n    'unquote',\n    ]\n\nimport os\nimport re\nimport time\nimport random\nimport socket\nimport datetime\nimport urllib.parse\n\nfrom email._parseaddr import quote\nfrom email._parseaddr import AddressList as _AddressList\nfrom email._parseaddr import mktime_tz\n\nfrom email._parseaddr import parsedate, parsedate_tz, _parsedate_tz\n\n# Intrapackage imports\nfrom email.charset import Charset\n\nCOMMASPACE = ', '\nEMPTYSTRING = ''\nUEMPTYSTRING = ''\nCRLF = '\\r\\n'\nTICK = \"'\"\n\nspecialsre = re.compile(r'[][\\\\()<>@,:;\".]')\nescapesre = re.compile(r'[\\\\\"]')\n\n\ndef _has_surrogates(s):\n    \"\"\"Return True if s may contain surrogate-escaped binary data.\"\"\"\n    # This check is based on the fact that unless there are surrogates, utf8\n    # (Python's default encoding) can encode any string.  This is the fastest\n    # way to check for surrogates, see bpo-11454 (moved to gh-55663) for timings.\n    try:\n        s.encode()\n        return False\n    except UnicodeEncodeError:\n        return True\n\n# How to deal with a string containing bytes before handing it to the\n# application through the 'normal' interface.\ndef _sanitize(string):\n    # Turn any escaped bytes into unicode 'unknown' char.  If the escaped\n    # bytes happen to be utf-8 they will instead get decoded, even if they\n    # were invalid in the charset the source was supposed to be in.  This\n    # seems like it is not a bad thing; a defect was still registered.\n    original_bytes = string.encode('utf-8', 'surrogateescape')\n    return original_bytes.decode('utf-8', 'replace')\n\n\n\n# Helpers\n\ndef formataddr(pair, charset='utf-8'):\n    \"\"\"The inverse of parseaddr(), this takes a 2-tuple of the form\n    (realname, email_address) and returns the string value suitable\n    for an RFC 2822 From, To or Cc header.\n\n    If the first element of pair is false, then the second element is\n    returned unmodified.\n\n    The optional charset is the character set that is used to encode\n    realname in case realname is not ASCII safe.  Can be an instance of str or\n    a Charset-like object which has a header_encode method.  Default is\n    'utf-8'.\n    \"\"\"\n    name, address = pair\n    # The address MUST (per RFC) be ascii, so raise a UnicodeError if it isn't.\n    address.encode('ascii')\n    if name:\n        try:\n            name.encode('ascii')\n        except UnicodeEncodeError:\n            if isinstance(charset, str):\n                charset = Charset(charset)\n            encoded_name = charset.header_encode(name)\n            return \"%s <%s>\" % (encoded_name, address)\n        else:\n            quotes = ''\n            if specialsre.search(name):\n                quotes = '\"'\n            name = escapesre.sub(r'\\\\\\g<0>', name)\n            return '%s%s%s <%s>' % (quotes, name, quotes, address)\n    return address\n\n\ndef _iter_escaped_chars(addr):\n    pos = 0\n    escape = False\n    for pos, ch in enumerate(addr):\n        if escape:\n            yield (pos, '\\\\' + ch)\n            escape = False\n        elif ch == '\\\\':\n            escape = True\n        else:\n            yield (pos, ch)\n    if escape:\n        yield (pos, '\\\\')\n\n\ndef _strip_quoted_realnames(addr):\n    \"\"\"Strip real names between quotes.\"\"\"\n    if '\"' not in addr:\n        # Fast path\n        return addr\n\n    start = 0\n    open_pos = None\n    result = []\n    for pos, ch in _iter_escaped_chars(addr):\n        if ch == '\"':\n            if open_pos is None:\n                open_pos = pos\n            else:\n                if start != open_pos:\n                    result.append(addr[start:open_pos])\n                start = pos + 1\n                open_pos = None\n\n    if start < len(addr):\n        result.append(addr[start:])\n\n    return ''.join(result)\n\n\nsupports_strict_parsing = True\n\ndef getaddresses(fieldvalues, *, strict=True):\n    \"\"\"Return a list of (REALNAME, EMAIL) or ('','') for each fieldvalue.\n\n    When parsing fails for a fieldvalue, a 2-tuple of ('', '') is returned in\n    its place.\n\n    If strict is true, use a strict parser which rejects malformed inputs.\n    \"\"\"\n\n    # If strict is true, if the resulting list of parsed addresses is greater\n    # than the number of fieldvalues in the input list, a parsing error has\n    # occurred and consequently a list containing a single empty 2-tuple [('',\n    # '')] is returned in its place. This is done to avoid invalid output.\n    #\n    # Malformed input: getaddresses(['alice@example.com <bob@example.com>'])\n    # Invalid output: [('', 'alice@example.com'), ('', 'bob@example.com')]\n    # Safe output: [('', '')]\n\n    if not strict:\n        all = COMMASPACE.join(str(v) for v in fieldvalues)\n        a = _AddressList(all)\n        return a.addresslist\n\n    fieldvalues = [str(v) for v in fieldvalues]\n    fieldvalues = _pre_parse_validation(fieldvalues)\n    addr = COMMASPACE.join(fieldvalues)\n    a = _AddressList(addr)\n    result = _post_parse_validation(a.addresslist)\n\n    # Treat output as invalid if the number of addresses is not equal to the\n    # expected number of addresses.\n    n = 0\n    for v in fieldvalues:\n        # When a comma is used in the Real Name part it is not a deliminator.\n        # So strip those out before counting the commas.\n        v = _strip_quoted_realnames(v)\n        # Expected number of addresses: 1 + number of commas\n        n += 1 + v.count(',')\n    if len(result) != n:\n        return [('', '')]\n\n    return result\n\n\ndef _check_parenthesis(addr):\n    # Ignore parenthesis in quoted real names.\n    addr = _strip_quoted_realnames(addr)\n\n    opens = 0\n    for pos, ch in _iter_escaped_chars(addr):\n        if ch == '(':\n            opens += 1\n        elif ch == ')':\n            opens -= 1\n            if opens < 0:\n                return False\n    return (opens == 0)\n\n\ndef _pre_parse_validation(email_header_fields):\n    accepted_values = []\n    for v in email_header_fields:\n        if not _check_parenthesis(v):\n            v = \"('', '')\"\n        accepted_values.append(v)\n\n    return accepted_values\n\n\ndef _post_parse_validation(parsed_email_header_tuples):\n    accepted_values = []\n    # The parser would have parsed a correctly formatted domain-literal\n    # The existence of an [ after parsing indicates a parsing failure\n    for v in parsed_email_header_tuples:\n        if '[' in v[1]:\n            v = ('', '')\n        accepted_values.append(v)\n\n    return accepted_values\n\n\ndef _format_timetuple_and_zone(timetuple, zone):\n    return '%s, %02d %s %04d %02d:%02d:%02d %s' % (\n        ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'][timetuple[6]],\n        timetuple[2],\n        ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n         'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'][timetuple[1] - 1],\n        timetuple[0], timetuple[3], timetuple[4], timetuple[5],\n        zone)\n\ndef formatdate(timeval=None, localtime=False, usegmt=False):\n    \"\"\"Returns a date string as specified by RFC 2822, e.g.:\n\n    Fri, 09 Nov 2001 01:08:47 -0000\n\n    Optional timeval if given is a floating point time value as accepted by\n    gmtime() and localtime(), otherwise the current time is used.\n\n    Optional localtime is a flag that when True, interprets timeval, and\n    returns a date relative to the local timezone instead of UTC, properly\n    taking daylight savings time into account.\n\n    Optional argument usegmt means that the timezone is written out as\n    an ascii string, not numeric one (so \"GMT\" instead of \"+0000\"). This\n    is needed for HTTP, and is only used when localtime==False.\n    \"\"\"\n    # Note: we cannot use strftime() because that honors the locale and RFC\n    # 2822 requires that day and month names be the English abbreviations.\n    if timeval is None:\n        timeval = time.time()\n    if localtime or usegmt:\n        dt = datetime.datetime.fromtimestamp(timeval, datetime.timezone.utc)\n    else:\n        dt = datetime.datetime.utcfromtimestamp(timeval)\n    if localtime:\n        dt = dt.astimezone()\n        usegmt = False\n    return format_datetime(dt, usegmt)\n\ndef format_datetime(dt, usegmt=False):\n    \"\"\"Turn a datetime into a date string as specified in RFC 2822.\n\n    If usegmt is True, dt must be an aware datetime with an offset of zero.  In\n    this case 'GMT' will be rendered instead of the normal +0000 required by\n    RFC2822.  This is to support HTTP headers involving date stamps.\n    \"\"\"\n    now = dt.timetuple()\n    if usegmt:\n        if dt.tzinfo is None or dt.tzinfo != datetime.timezone.utc:\n            raise ValueError(\"usegmt option requires a UTC datetime\")\n        zone = 'GMT'\n    elif dt.tzinfo is None:\n        zone = '-0000'\n    else:\n        zone = dt.strftime(\"%z\")\n    return _format_timetuple_and_zone(now, zone)\n\n\ndef make_msgid(idstring=None, domain=None):\n    \"\"\"Returns a string suitable for RFC 2822 compliant Message-ID, e.g:\n\n    <142480216486.20800.16526388040877946887@nightshade.la.mastaler.com>\n\n    Optional idstring if given is a string used to strengthen the\n    uniqueness of the message id.  Optional domain if given provides the\n    portion of the message id after the '@'.  It defaults to the locally\n    defined hostname.\n    \"\"\"\n    timeval = int(time.time()*100)\n    pid = os.getpid()\n    randint = random.getrandbits(64)\n    if idstring is None:\n        idstring = ''\n    else:\n        idstring = '.' + idstring\n    if domain is None:\n        domain = socket.getfqdn()\n    msgid = '<%d.%d.%d%s@%s>' % (timeval, pid, randint, idstring, domain)\n    return msgid\n\n\ndef parsedate_to_datetime(data):\n    parsed_date_tz = _parsedate_tz(data)\n    if parsed_date_tz is None:\n        raise ValueError('Invalid date value or format \"%s\"' % str(data))\n    *dtuple, tz = parsed_date_tz\n    if tz is None:\n        return datetime.datetime(*dtuple[:6])\n    return datetime.datetime(*dtuple[:6],\n            tzinfo=datetime.timezone(datetime.timedelta(seconds=tz)))\n\n\ndef parseaddr(addr, *, strict=True):\n    \"\"\"\n    Parse addr into its constituent realname and email address parts.\n\n    Return a tuple of realname and email address, unless the parse fails, in\n    which case return a 2-tuple of ('', '').\n\n    If strict is True, use a strict parser which rejects malformed inputs.\n    \"\"\"\n    if not strict:\n        addrs = _AddressList(addr).addresslist\n        if not addrs:\n            return ('', '')\n        return addrs[0]\n\n    if isinstance(addr, list):\n        addr = addr[0]\n\n    if not isinstance(addr, str):\n        return ('', '')\n\n    addr = _pre_parse_validation([addr])[0]\n    addrs = _post_parse_validation(_AddressList(addr).addresslist)\n\n    if not addrs or len(addrs) > 1:\n        return ('', '')\n\n    return addrs[0]\n\n\n# rfc822.unquote() doesn't properly de-backslash-ify in Python pre-2.3.\ndef unquote(str):\n    \"\"\"Remove quotes from a string.\"\"\"\n    if len(str) > 1:\n        if str.startswith('\"') and str.endswith('\"'):\n            return str[1:-1].replace('\\\\\\\\', '\\\\').replace('\\\\\"', '\"')\n        if str.startswith('<') and str.endswith('>'):\n            return str[1:-1]\n    return str\n\n\n\n# RFC2231-related functions - parameter encoding and decoding\ndef decode_rfc2231(s):\n    \"\"\"Decode string according to RFC 2231\"\"\"\n    parts = s.split(TICK, 2)\n    if len(parts) <= 2:\n        return None, None, s\n    return parts\n\n\ndef encode_rfc2231(s, charset=None, language=None):\n    \"\"\"Encode string according to RFC 2231.\n\n    If neither charset nor language is given, then s is returned as-is.  If\n    charset is given but not language, the string is encoded using the empty\n    string for language.\n    \"\"\"\n    s = urllib.parse.quote(s, safe='', encoding=charset or 'ascii')\n    if charset is None and language is None:\n        return s\n    if language is None:\n        language = ''\n    return \"%s'%s'%s\" % (charset, language, s)\n\n\nrfc2231_continuation = re.compile(r'^(?P<name>\\w+)\\*((?P<num>[0-9]+)\\*?)?$',\n    re.ASCII)\n\ndef decode_params(params):\n    \"\"\"Decode parameters list according to RFC 2231.\n\n    params is a sequence of 2-tuples containing (param name, string value).\n    \"\"\"\n    new_params = [params[0]]\n    # Map parameter's name to a list of continuations.  The values are a\n    # 3-tuple of the continuation number, the string value, and a flag\n    # specifying whether a particular segment is %-encoded.\n    rfc2231_params = {}\n    for name, value in params[1:]:\n        encoded = name.endswith('*')\n        value = unquote(value)\n        mo = rfc2231_continuation.match(name)\n        if mo:\n            name, num = mo.group('name', 'num')\n            if num is not None:\n                num = int(num)\n            rfc2231_params.setdefault(name, []).append((num, value, encoded))\n        else:\n            new_params.append((name, '\"%s\"' % quote(value)))\n    if rfc2231_params:\n        for name, continuations in rfc2231_params.items():\n            value = []\n            extended = False\n            # Sort by number\n            continuations.sort()\n            # And now append all values in numerical order, converting\n            # %-encodings for the encoded segments.  If any of the\n            # continuation names ends in a *, then the entire string, after\n            # decoding segments and concatenating, must have the charset and\n            # language specifiers at the beginning of the string.\n            for num, s, encoded in continuations:\n                if encoded:\n                    # Decode as \"latin-1\", so the characters in s directly\n                    # represent the percent-encoded octet values.\n                    # collapse_rfc2231_value treats this as an octet sequence.\n                    s = urllib.parse.unquote(s, encoding=\"latin-1\")\n                    extended = True\n                value.append(s)\n            value = quote(EMPTYSTRING.join(value))\n            if extended:\n                charset, language, value = decode_rfc2231(value)\n                new_params.append((name, (charset, language, '\"%s\"' % value)))\n            else:\n                new_params.append((name, '\"%s\"' % value))\n    return new_params\n\ndef collapse_rfc2231_value(value, errors='replace',\n                           fallback_charset='us-ascii'):\n    if not isinstance(value, tuple) or len(value) != 3:\n        return unquote(value)\n    # While value comes to us as a unicode string, we need it to be a bytes\n    # object.  We do not want bytes() normal utf-8 decoder, we want a straight\n    # interpretation of the string as character bytes.\n    charset, language, text = value\n    if charset is None:\n        # Issue 17369: if charset/lang is None, decode_rfc2231 couldn't parse\n        # the value, so use the fallback_charset.\n        charset = fallback_charset\n    rawbytes = bytes(text, 'raw-unicode-escape')\n    try:\n        return str(rawbytes, charset, errors)\n    except LookupError:\n        # charset is not a known codec.\n        return unquote(text)\n\n\n#\n# datetime doesn't provide a localtime function yet, so provide one.  Code\n# adapted from the patch in issue 9527.  This may not be perfect, but it is\n# better than not having it.\n#\n\ndef localtime(dt=None, isdst=-1):\n    \"\"\"Return local time as an aware datetime object.\n\n    If called without arguments, return current time.  Otherwise *dt*\n    argument should be a datetime instance, and it is converted to the\n    local time zone according to the system time zone database.  If *dt* is\n    naive (that is, dt.tzinfo is None), it is assumed to be in local time.\n    In this case, a positive or zero value for *isdst* causes localtime to\n    presume initially that summer time (for example, Daylight Saving Time)\n    is or is not (respectively) in effect for the specified time.  A\n    negative value for *isdst* causes the localtime() function to attempt\n    to divine whether summer time is in effect for the specified time.\n\n    \"\"\"\n    if dt is None:\n        return datetime.datetime.now(datetime.timezone.utc).astimezone()\n    if dt.tzinfo is not None:\n        return dt.astimezone()\n    # We have a naive datetime.  Convert to a (localtime) timetuple and pass to\n    # system mktime together with the isdst hint.  System mktime will return\n    # seconds since epoch.\n    tm = dt.timetuple()[:-1] + (isdst,)\n    seconds = time.mktime(tm)\n    localtm = time.localtime(seconds)\n    try:\n        delta = datetime.timedelta(seconds=localtm.tm_gmtoff)\n        tz = datetime.timezone(delta, localtm.tm_zone)\n    except AttributeError:\n        # Compute UTC offset and compare with the value implied by tm_isdst.\n        # If the values match, use the zone name implied by tm_isdst.\n        delta = dt - datetime.datetime(*time.gmtime(seconds)[:6])\n        dst = time.daylight and localtm.tm_isdst > 0\n        gmtoff = -(time.altzone if dst else time.timezone)\n        if delta == datetime.timedelta(seconds=gmtoff):\n            tz = datetime.timezone(delta, time.tzname[dst])\n        else:\n            tz = datetime.timezone(delta)\n    return dt.replace(tzinfo=tz)\n", 504], "<frozen abc>": ["# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Abstract Base Classes (ABCs) according to PEP 3119.\"\"\"\n\n\ndef abstractmethod(funcobj):\n    \"\"\"A decorator indicating abstract methods.\n\n    Requires that the metaclass is ABCMeta or derived from it.  A\n    class that has a metaclass derived from ABCMeta cannot be\n    instantiated unless all of its abstract methods are overridden.\n    The abstract methods can be called using any of the normal\n    'super' call mechanisms.  abstractmethod() may be used to declare\n    abstract methods for properties and descriptors.\n\n    Usage:\n\n        class C(metaclass=ABCMeta):\n            @abstractmethod\n            def my_abstract_method(self, arg1, arg2, argN):\n                ...\n    \"\"\"\n    funcobj.__isabstractmethod__ = True\n    return funcobj\n\n\nclass abstractclassmethod(classmethod):\n    \"\"\"A decorator indicating abstract classmethods.\n\n    Deprecated, use 'classmethod' with 'abstractmethod' instead:\n\n        class C(ABC):\n            @classmethod\n            @abstractmethod\n            def my_abstract_classmethod(cls, ...):\n                ...\n\n    \"\"\"\n\n    __isabstractmethod__ = True\n\n    def __init__(self, callable):\n        callable.__isabstractmethod__ = True\n        super().__init__(callable)\n\n\nclass abstractstaticmethod(staticmethod):\n    \"\"\"A decorator indicating abstract staticmethods.\n\n    Deprecated, use 'staticmethod' with 'abstractmethod' instead:\n\n        class C(ABC):\n            @staticmethod\n            @abstractmethod\n            def my_abstract_staticmethod(...):\n                ...\n\n    \"\"\"\n\n    __isabstractmethod__ = True\n\n    def __init__(self, callable):\n        callable.__isabstractmethod__ = True\n        super().__init__(callable)\n\n\nclass abstractproperty(property):\n    \"\"\"A decorator indicating abstract properties.\n\n    Deprecated, use 'property' with 'abstractmethod' instead:\n\n        class C(ABC):\n            @property\n            @abstractmethod\n            def my_abstract_property(self):\n                ...\n\n    \"\"\"\n\n    __isabstractmethod__ = True\n\n\ntry:\n    from _abc import (get_cache_token, _abc_init, _abc_register,\n                      _abc_instancecheck, _abc_subclasscheck, _get_dump,\n                      _reset_registry, _reset_caches)\nexcept ImportError:\n    from _py_abc import ABCMeta, get_cache_token\n    ABCMeta.__module__ = 'abc'\nelse:\n    class ABCMeta(type):\n        \"\"\"Metaclass for defining Abstract Base Classes (ABCs).\n\n        Use this metaclass to create an ABC.  An ABC can be subclassed\n        directly, and then acts as a mix-in class.  You can also register\n        unrelated concrete classes (even built-in classes) and unrelated\n        ABCs as 'virtual subclasses' -- these and their descendants will\n        be considered subclasses of the registering ABC by the built-in\n        issubclass() function, but the registering ABC won't show up in\n        their MRO (Method Resolution Order) nor will method\n        implementations defined by the registering ABC be callable (not\n        even via super()).\n        \"\"\"\n        def __new__(mcls, name, bases, namespace, /, **kwargs):\n            cls = super().__new__(mcls, name, bases, namespace, **kwargs)\n            _abc_init(cls)\n            return cls\n\n        def register(cls, subclass):\n            \"\"\"Register a virtual subclass of an ABC.\n\n            Returns the subclass, to allow usage as a class decorator.\n            \"\"\"\n            return _abc_register(cls, subclass)\n\n        def __instancecheck__(cls, instance):\n            \"\"\"Override for isinstance(instance, cls).\"\"\"\n            return _abc_instancecheck(cls, instance)\n\n        def __subclasscheck__(cls, subclass):\n            \"\"\"Override for issubclass(subclass, cls).\"\"\"\n            return _abc_subclasscheck(cls, subclass)\n\n        def _dump_registry(cls, file=None):\n            \"\"\"Debug helper to print the ABC registry.\"\"\"\n            print(f\"Class: {cls.__module__}.{cls.__qualname__}\", file=file)\n            print(f\"Inv. counter: {get_cache_token()}\", file=file)\n            (_abc_registry, _abc_cache, _abc_negative_cache,\n             _abc_negative_cache_version) = _get_dump(cls)\n            print(f\"_abc_registry: {_abc_registry!r}\", file=file)\n            print(f\"_abc_cache: {_abc_cache!r}\", file=file)\n            print(f\"_abc_negative_cache: {_abc_negative_cache!r}\", file=file)\n            print(f\"_abc_negative_cache_version: {_abc_negative_cache_version!r}\",\n                  file=file)\n\n        def _abc_registry_clear(cls):\n            \"\"\"Clear the registry (for debugging or testing).\"\"\"\n            _reset_registry(cls)\n\n        def _abc_caches_clear(cls):\n            \"\"\"Clear the caches (for debugging or testing).\"\"\"\n            _reset_caches(cls)\n\n\ndef update_abstractmethods(cls):\n    \"\"\"Recalculate the set of abstract methods of an abstract class.\n\n    If a class has had one of its abstract methods implemented after the\n    class was created, the method will not be considered implemented until\n    this function is called. Alternatively, if a new abstract method has been\n    added to the class, it will only be considered an abstract method of the\n    class after this function is called.\n\n    This function should be called before any use is made of the class,\n    usually in class decorators that add methods to the subject class.\n\n    Returns cls, to allow usage as a class decorator.\n\n    If cls is not an instance of ABCMeta, does nothing.\n    \"\"\"\n    if not hasattr(cls, '__abstractmethods__'):\n        # We check for __abstractmethods__ here because cls might by a C\n        # implementation or a python implementation (especially during\n        # testing), and we want to handle both cases.\n        return cls\n\n    abstracts = set()\n    # Check the existing abstract methods of the parents, keep only the ones\n    # that are not implemented.\n    for scls in cls.__bases__:\n        for name in getattr(scls, '__abstractmethods__', ()):\n            value = getattr(cls, name, None)\n            if getattr(value, \"__isabstractmethod__\", False):\n                abstracts.add(name)\n    # Also add any other newly added abstract methods.\n    for name, value in cls.__dict__.items():\n        if getattr(value, \"__isabstractmethod__\", False):\n            abstracts.add(name)\n    cls.__abstractmethods__ = frozenset(abstracts)\n    return cls\n\n\nclass ABC(metaclass=ABCMeta):\n    \"\"\"Helper class that provides a standard way to create an ABC using\n    inheritance.\n    \"\"\"\n    __slots__ = ()\n", 188], "/usr/local/lib/python3.11/asyncio/coroutines.py": ["__all__ = 'iscoroutinefunction', 'iscoroutine'\n\nimport collections.abc\nimport inspect\nimport os\nimport sys\nimport traceback\nimport types\n\n\ndef _is_debug_mode():\n    # See: https://docs.python.org/3/library/asyncio-dev.html#asyncio-debug-mode.\n    return sys.flags.dev_mode or (not sys.flags.ignore_environment and\n                                  bool(os.environ.get('PYTHONASYNCIODEBUG')))\n\n\n# A marker for iscoroutinefunction.\n_is_coroutine = object()\n\n\ndef iscoroutinefunction(func):\n    \"\"\"Return True if func is a decorated coroutine function.\"\"\"\n    return (inspect.iscoroutinefunction(func) or\n            getattr(func, '_is_coroutine', None) is _is_coroutine)\n\n\n# Prioritize native coroutine check to speed-up\n# asyncio.iscoroutine.\n_COROUTINE_TYPES = (types.CoroutineType, types.GeneratorType,\n                    collections.abc.Coroutine)\n_iscoroutine_typecache = set()\n\n\ndef iscoroutine(obj):\n    \"\"\"Return True if obj is a coroutine object.\"\"\"\n    if type(obj) in _iscoroutine_typecache:\n        return True\n\n    if isinstance(obj, _COROUTINE_TYPES):\n        # Just in case we don't want to cache more than 100\n        # positive types.  That shouldn't ever happen, unless\n        # someone stressing the system on purpose.\n        if len(_iscoroutine_typecache) < 100:\n            _iscoroutine_typecache.add(type(obj))\n        return True\n    else:\n        return False\n\n\ndef _format_coroutine(coro):\n    assert iscoroutine(coro)\n\n    def get_name(coro):\n        # Coroutines compiled with Cython sometimes don't have\n        # proper __qualname__ or __name__.  While that is a bug\n        # in Cython, asyncio shouldn't crash with an AttributeError\n        # in its __repr__ functions.\n        if hasattr(coro, '__qualname__') and coro.__qualname__:\n            coro_name = coro.__qualname__\n        elif hasattr(coro, '__name__') and coro.__name__:\n            coro_name = coro.__name__\n        else:\n            # Stop masking Cython bugs, expose them in a friendly way.\n            coro_name = f'<{type(coro).__name__} without __name__>'\n        return f'{coro_name}()'\n\n    def is_running(coro):\n        try:\n            return coro.cr_running\n        except AttributeError:\n            try:\n                return coro.gi_running\n            except AttributeError:\n                return False\n\n    coro_code = None\n    if hasattr(coro, 'cr_code') and coro.cr_code:\n        coro_code = coro.cr_code\n    elif hasattr(coro, 'gi_code') and coro.gi_code:\n        coro_code = coro.gi_code\n\n    coro_name = get_name(coro)\n\n    if not coro_code:\n        # Built-in types might not have __qualname__ or __name__.\n        if is_running(coro):\n            return f'{coro_name} running'\n        else:\n            return coro_name\n\n    coro_frame = None\n    if hasattr(coro, 'gi_frame') and coro.gi_frame:\n        coro_frame = coro.gi_frame\n    elif hasattr(coro, 'cr_frame') and coro.cr_frame:\n        coro_frame = coro.cr_frame\n\n    # If Cython's coroutine has a fake code object without proper\n    # co_filename -- expose that.\n    filename = coro_code.co_filename or '<empty co_filename>'\n\n    lineno = 0\n\n    if coro_frame is not None:\n        lineno = coro_frame.f_lineno\n        coro_repr = f'{coro_name} running at {filename}:{lineno}'\n\n    else:\n        lineno = coro_code.co_firstlineno\n        coro_repr = f'{coro_name} done, defined at {filename}:{lineno}'\n\n    return coro_repr\n", 111], "/usr/local/lib/python3.11/inspect.py": ["\"\"\"Get useful information from live Python objects.\n\nThis module encapsulates the interface provided by the internal special\nattributes (co_*, im_*, tb_*, etc.) in a friendlier fashion.\nIt also provides some help for examining source code and class layout.\n\nHere are some of the useful functions provided by this module:\n\n    ismodule(), isclass(), ismethod(), isfunction(), isgeneratorfunction(),\n        isgenerator(), istraceback(), isframe(), iscode(), isbuiltin(),\n        isroutine() - check object types\n    getmembers() - get members of an object that satisfy a given condition\n\n    getfile(), getsourcefile(), getsource() - find an object's source code\n    getdoc(), getcomments() - get documentation on an object\n    getmodule() - determine the module that an object came from\n    getclasstree() - arrange classes so as to represent their hierarchy\n\n    getargvalues(), getcallargs() - get info about function arguments\n    getfullargspec() - same, with support for Python 3 features\n    formatargvalues() - format an argument spec\n    getouterframes(), getinnerframes() - get info about frames\n    currentframe() - get the current stack frame\n    stack(), trace() - get info about frames on the stack or in a traceback\n\n    signature() - get a Signature object for the callable\n\n    get_annotations() - safely compute an object's annotations\n\"\"\"\n\n# This module is in the public domain.  No warranties.\n\n__author__ = ('Ka-Ping Yee <ping@lfw.org>',\n              'Yury Selivanov <yselivanov@sprymix.com>')\n\n__all__ = [\n    \"ArgInfo\",\n    \"Arguments\",\n    \"Attribute\",\n    \"BlockFinder\",\n    \"BoundArguments\",\n    \"CORO_CLOSED\",\n    \"CORO_CREATED\",\n    \"CORO_RUNNING\",\n    \"CORO_SUSPENDED\",\n    \"CO_ASYNC_GENERATOR\",\n    \"CO_COROUTINE\",\n    \"CO_GENERATOR\",\n    \"CO_ITERABLE_COROUTINE\",\n    \"CO_NESTED\",\n    \"CO_NEWLOCALS\",\n    \"CO_NOFREE\",\n    \"CO_OPTIMIZED\",\n    \"CO_VARARGS\",\n    \"CO_VARKEYWORDS\",\n    \"ClassFoundException\",\n    \"ClosureVars\",\n    \"EndOfBlock\",\n    \"FrameInfo\",\n    \"FullArgSpec\",\n    \"GEN_CLOSED\",\n    \"GEN_CREATED\",\n    \"GEN_RUNNING\",\n    \"GEN_SUSPENDED\",\n    \"Parameter\",\n    \"Signature\",\n    \"TPFLAGS_IS_ABSTRACT\",\n    \"Traceback\",\n    \"classify_class_attrs\",\n    \"cleandoc\",\n    \"currentframe\",\n    \"findsource\",\n    \"formatannotation\",\n    \"formatannotationrelativeto\",\n    \"formatargvalues\",\n    \"get_annotations\",\n    \"getabsfile\",\n    \"getargs\",\n    \"getargvalues\",\n    \"getattr_static\",\n    \"getblock\",\n    \"getcallargs\",\n    \"getclasstree\",\n    \"getclosurevars\",\n    \"getcomments\",\n    \"getcoroutinelocals\",\n    \"getcoroutinestate\",\n    \"getdoc\",\n    \"getfile\",\n    \"getframeinfo\",\n    \"getfullargspec\",\n    \"getgeneratorlocals\",\n    \"getgeneratorstate\",\n    \"getinnerframes\",\n    \"getlineno\",\n    \"getmembers\",\n    \"getmembers_static\",\n    \"getmodule\",\n    \"getmodulename\",\n    \"getmro\",\n    \"getouterframes\",\n    \"getsource\",\n    \"getsourcefile\",\n    \"getsourcelines\",\n    \"indentsize\",\n    \"isabstract\",\n    \"isasyncgen\",\n    \"isasyncgenfunction\",\n    \"isawaitable\",\n    \"isbuiltin\",\n    \"isclass\",\n    \"iscode\",\n    \"iscoroutine\",\n    \"iscoroutinefunction\",\n    \"isdatadescriptor\",\n    \"isframe\",\n    \"isfunction\",\n    \"isgenerator\",\n    \"isgeneratorfunction\",\n    \"isgetsetdescriptor\",\n    \"ismemberdescriptor\",\n    \"ismethod\",\n    \"ismethoddescriptor\",\n    \"ismethodwrapper\",\n    \"ismodule\",\n    \"isroutine\",\n    \"istraceback\",\n    \"signature\",\n    \"stack\",\n    \"trace\",\n    \"unwrap\",\n    \"walktree\",\n]\n\n\nimport abc\nimport ast\nimport dis\nimport collections.abc\nimport enum\nimport importlib.machinery\nimport itertools\nimport linecache\nimport os\nimport re\nimport sys\nimport tokenize\nimport token\nimport types\nimport functools\nimport builtins\nfrom keyword import iskeyword\nfrom operator import attrgetter\nfrom collections import namedtuple, OrderedDict\n\n# Create constants for the compiler flags in Include/code.h\n# We try to get them from dis to avoid duplication\nmod_dict = globals()\nfor k, v in dis.COMPILER_FLAG_NAMES.items():\n    mod_dict[\"CO_\" + v] = k\ndel k, v, mod_dict\n\n# See Include/object.h\nTPFLAGS_IS_ABSTRACT = 1 << 20\n\n\ndef get_annotations(obj, *, globals=None, locals=None, eval_str=False):\n    \"\"\"Compute the annotations dict for an object.\n\n    obj may be a callable, class, or module.\n    Passing in an object of any other type raises TypeError.\n\n    Returns a dict.  get_annotations() returns a new dict every time\n    it's called; calling it twice on the same object will return two\n    different but equivalent dicts.\n\n    This function handles several details for you:\n\n      * If eval_str is true, values of type str will\n        be un-stringized using eval().  This is intended\n        for use with stringized annotations\n        (\"from __future__ import annotations\").\n      * If obj doesn't have an annotations dict, returns an\n        empty dict.  (Functions and methods always have an\n        annotations dict; classes, modules, and other types of\n        callables may not.)\n      * Ignores inherited annotations on classes.  If a class\n        doesn't have its own annotations dict, returns an empty dict.\n      * All accesses to object members and dict values are done\n        using getattr() and dict.get() for safety.\n      * Always, always, always returns a freshly-created dict.\n\n    eval_str controls whether or not values of type str are replaced\n    with the result of calling eval() on those values:\n\n      * If eval_str is true, eval() is called on values of type str.\n      * If eval_str is false (the default), values of type str are unchanged.\n\n    globals and locals are passed in to eval(); see the documentation\n    for eval() for more information.  If either globals or locals is\n    None, this function may replace that value with a context-specific\n    default, contingent on type(obj):\n\n      * If obj is a module, globals defaults to obj.__dict__.\n      * If obj is a class, globals defaults to\n        sys.modules[obj.__module__].__dict__ and locals\n        defaults to the obj class namespace.\n      * If obj is a callable, globals defaults to obj.__globals__,\n        although if obj is a wrapped function (using\n        functools.update_wrapper()) it is first unwrapped.\n    \"\"\"\n    if isinstance(obj, type):\n        # class\n        obj_dict = getattr(obj, '__dict__', None)\n        if obj_dict and hasattr(obj_dict, 'get'):\n            ann = obj_dict.get('__annotations__', None)\n            if isinstance(ann, types.GetSetDescriptorType):\n                ann = None\n        else:\n            ann = None\n\n        obj_globals = None\n        module_name = getattr(obj, '__module__', None)\n        if module_name:\n            module = sys.modules.get(module_name, None)\n            if module:\n                obj_globals = getattr(module, '__dict__', None)\n        obj_locals = dict(vars(obj))\n        unwrap = obj\n    elif isinstance(obj, types.ModuleType):\n        # module\n        ann = getattr(obj, '__annotations__', None)\n        obj_globals = getattr(obj, '__dict__')\n        obj_locals = None\n        unwrap = None\n    elif callable(obj):\n        # this includes types.Function, types.BuiltinFunctionType,\n        # types.BuiltinMethodType, functools.partial, functools.singledispatch,\n        # \"class funclike\" from Lib/test/test_inspect... on and on it goes.\n        ann = getattr(obj, '__annotations__', None)\n        obj_globals = getattr(obj, '__globals__', None)\n        obj_locals = None\n        unwrap = obj\n    else:\n        raise TypeError(f\"{obj!r} is not a module, class, or callable.\")\n\n    if ann is None:\n        return {}\n\n    if not isinstance(ann, dict):\n        raise ValueError(f\"{obj!r}.__annotations__ is neither a dict nor None\")\n\n    if not ann:\n        return {}\n\n    if not eval_str:\n        return dict(ann)\n\n    if unwrap is not None:\n        while True:\n            if hasattr(unwrap, '__wrapped__'):\n                unwrap = unwrap.__wrapped__\n                continue\n            if isinstance(unwrap, functools.partial):\n                unwrap = unwrap.func\n                continue\n            break\n        if hasattr(unwrap, \"__globals__\"):\n            obj_globals = unwrap.__globals__\n\n    if globals is None:\n        globals = obj_globals\n    if locals is None:\n        locals = obj_locals\n\n    return_value = {key:\n        value if not isinstance(value, str) else eval(value, globals, locals)\n        for key, value in ann.items() }\n    return return_value\n\n\n# ----------------------------------------------------------- type-checking\ndef ismodule(object):\n    \"\"\"Return true if the object is a module.\n\n    Module objects provide these attributes:\n        __cached__      pathname to byte compiled file\n        __doc__         documentation string\n        __file__        filename (missing for built-in modules)\"\"\"\n    return isinstance(object, types.ModuleType)\n\ndef isclass(object):\n    \"\"\"Return true if the object is a class.\n\n    Class objects provide these attributes:\n        __doc__         documentation string\n        __module__      name of module in which this class was defined\"\"\"\n    return isinstance(object, type)\n\ndef ismethod(object):\n    \"\"\"Return true if the object is an instance method.\n\n    Instance method objects provide these attributes:\n        __doc__         documentation string\n        __name__        name with which this method was defined\n        __func__        function object containing implementation of method\n        __self__        instance to which this method is bound\"\"\"\n    return isinstance(object, types.MethodType)\n\ndef ismethoddescriptor(object):\n    \"\"\"Return true if the object is a method descriptor.\n\n    But not if ismethod() or isclass() or isfunction() are true.\n\n    This is new in Python 2.2, and, for example, is true of int.__add__.\n    An object passing this test has a __get__ attribute but not a __set__\n    attribute, but beyond that the set of attributes varies.  __name__ is\n    usually sensible, and __doc__ often is.\n\n    Methods implemented via descriptors that also pass one of the other\n    tests return false from the ismethoddescriptor() test, simply because\n    the other tests promise more -- you can, e.g., count on having the\n    __func__ attribute (etc) when an object passes ismethod().\"\"\"\n    if isclass(object) or ismethod(object) or isfunction(object):\n        # mutual exclusion\n        return False\n    tp = type(object)\n    return hasattr(tp, \"__get__\") and not hasattr(tp, \"__set__\")\n\ndef isdatadescriptor(object):\n    \"\"\"Return true if the object is a data descriptor.\n\n    Data descriptors have a __set__ or a __delete__ attribute.  Examples are\n    properties (defined in Python) and getsets and members (defined in C).\n    Typically, data descriptors will also have __name__ and __doc__ attributes\n    (properties, getsets, and members have both of these attributes), but this\n    is not guaranteed.\"\"\"\n    if isclass(object) or ismethod(object) or isfunction(object):\n        # mutual exclusion\n        return False\n    tp = type(object)\n    return hasattr(tp, \"__set__\") or hasattr(tp, \"__delete__\")\n\nif hasattr(types, 'MemberDescriptorType'):\n    # CPython and equivalent\n    def ismemberdescriptor(object):\n        \"\"\"Return true if the object is a member descriptor.\n\n        Member descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return isinstance(object, types.MemberDescriptorType)\nelse:\n    # Other implementations\n    def ismemberdescriptor(object):\n        \"\"\"Return true if the object is a member descriptor.\n\n        Member descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return False\n\nif hasattr(types, 'GetSetDescriptorType'):\n    # CPython and equivalent\n    def isgetsetdescriptor(object):\n        \"\"\"Return true if the object is a getset descriptor.\n\n        getset descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return isinstance(object, types.GetSetDescriptorType)\nelse:\n    # Other implementations\n    def isgetsetdescriptor(object):\n        \"\"\"Return true if the object is a getset descriptor.\n\n        getset descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return False\n\ndef isfunction(object):\n    \"\"\"Return true if the object is a user-defined function.\n\n    Function objects provide these attributes:\n        __doc__         documentation string\n        __name__        name with which this function was defined\n        __code__        code object containing compiled function bytecode\n        __defaults__    tuple of any default values for arguments\n        __globals__     global namespace in which this function was defined\n        __annotations__ dict of parameter annotations\n        __kwdefaults__  dict of keyword only parameters with defaults\"\"\"\n    return isinstance(object, types.FunctionType)\n\ndef _has_code_flag(f, flag):\n    \"\"\"Return true if ``f`` is a function (or a method or functools.partial\n    wrapper wrapping a function) whose code object has the given ``flag``\n    set in its flags.\"\"\"\n    while ismethod(f):\n        f = f.__func__\n    f = functools._unwrap_partial(f)\n    if not (isfunction(f) or _signature_is_functionlike(f)):\n        return False\n    return bool(f.__code__.co_flags & flag)\n\ndef isgeneratorfunction(obj):\n    \"\"\"Return true if the object is a user-defined generator function.\n\n    Generator function objects provide the same attributes as functions.\n    See help(isfunction) for a list of attributes.\"\"\"\n    return _has_code_flag(obj, CO_GENERATOR)\n\ndef iscoroutinefunction(obj):\n    \"\"\"Return true if the object is a coroutine function.\n\n    Coroutine functions are defined with \"async def\" syntax.\n    \"\"\"\n    return _has_code_flag(obj, CO_COROUTINE)\n\ndef isasyncgenfunction(obj):\n    \"\"\"Return true if the object is an asynchronous generator function.\n\n    Asynchronous generator functions are defined with \"async def\"\n    syntax and have \"yield\" expressions in their body.\n    \"\"\"\n    return _has_code_flag(obj, CO_ASYNC_GENERATOR)\n\ndef isasyncgen(object):\n    \"\"\"Return true if the object is an asynchronous generator.\"\"\"\n    return isinstance(object, types.AsyncGeneratorType)\n\ndef isgenerator(object):\n    \"\"\"Return true if the object is a generator.\n\n    Generator objects provide these attributes:\n        __iter__        defined to support iteration over container\n        close           raises a new GeneratorExit exception inside the\n                        generator to terminate the iteration\n        gi_code         code object\n        gi_frame        frame object or possibly None once the generator has\n                        been exhausted\n        gi_running      set to 1 when generator is executing, 0 otherwise\n        next            return the next item from the container\n        send            resumes the generator and \"sends\" a value that becomes\n                        the result of the current yield-expression\n        throw           used to raise an exception inside the generator\"\"\"\n    return isinstance(object, types.GeneratorType)\n\ndef iscoroutine(object):\n    \"\"\"Return true if the object is a coroutine.\"\"\"\n    return isinstance(object, types.CoroutineType)\n\ndef isawaitable(object):\n    \"\"\"Return true if object can be passed to an ``await`` expression.\"\"\"\n    return (isinstance(object, types.CoroutineType) or\n            isinstance(object, types.GeneratorType) and\n                bool(object.gi_code.co_flags & CO_ITERABLE_COROUTINE) or\n            isinstance(object, collections.abc.Awaitable))\n\ndef istraceback(object):\n    \"\"\"Return true if the object is a traceback.\n\n    Traceback objects provide these attributes:\n        tb_frame        frame object at this level\n        tb_lasti        index of last attempted instruction in bytecode\n        tb_lineno       current line number in Python source code\n        tb_next         next inner traceback object (called by this level)\"\"\"\n    return isinstance(object, types.TracebackType)\n\ndef isframe(object):\n    \"\"\"Return true if the object is a frame object.\n\n    Frame objects provide these attributes:\n        f_back          next outer frame object (this frame's caller)\n        f_builtins      built-in namespace seen by this frame\n        f_code          code object being executed in this frame\n        f_globals       global namespace seen by this frame\n        f_lasti         index of last attempted instruction in bytecode\n        f_lineno        current line number in Python source code\n        f_locals        local namespace seen by this frame\n        f_trace         tracing function for this frame, or None\"\"\"\n    return isinstance(object, types.FrameType)\n\ndef iscode(object):\n    \"\"\"Return true if the object is a code object.\n\n    Code objects provide these attributes:\n        co_argcount         number of arguments (not including *, ** args\n                            or keyword only arguments)\n        co_code             string of raw compiled bytecode\n        co_cellvars         tuple of names of cell variables\n        co_consts           tuple of constants used in the bytecode\n        co_filename         name of file in which this code object was created\n        co_firstlineno      number of first line in Python source code\n        co_flags            bitmap: 1=optimized | 2=newlocals | 4=*arg | 8=**arg\n                            | 16=nested | 32=generator | 64=nofree | 128=coroutine\n                            | 256=iterable_coroutine | 512=async_generator\n        co_freevars         tuple of names of free variables\n        co_posonlyargcount  number of positional only arguments\n        co_kwonlyargcount   number of keyword only arguments (not including ** arg)\n        co_lnotab           encoded mapping of line numbers to bytecode indices\n        co_name             name with which this code object was defined\n        co_names            tuple of names other than arguments and function locals\n        co_nlocals          number of local variables\n        co_stacksize        virtual machine stack space required\n        co_varnames         tuple of names of arguments and local variables\"\"\"\n    return isinstance(object, types.CodeType)\n\ndef isbuiltin(object):\n    \"\"\"Return true if the object is a built-in function or method.\n\n    Built-in functions and methods provide these attributes:\n        __doc__         documentation string\n        __name__        original name of this function or method\n        __self__        instance to which a method is bound, or None\"\"\"\n    return isinstance(object, types.BuiltinFunctionType)\n\ndef ismethodwrapper(object):\n    \"\"\"Return true if the object is a method wrapper.\"\"\"\n    return isinstance(object, types.MethodWrapperType)\n\ndef isroutine(object):\n    \"\"\"Return true if the object is any kind of function or method.\"\"\"\n    return (isbuiltin(object)\n            or isfunction(object)\n            or ismethod(object)\n            or ismethoddescriptor(object)\n            or ismethodwrapper(object))\n\ndef isabstract(object):\n    \"\"\"Return true if the object is an abstract base class (ABC).\"\"\"\n    if not isinstance(object, type):\n        return False\n    if object.__flags__ & TPFLAGS_IS_ABSTRACT:\n        return True\n    if not issubclass(type(object), abc.ABCMeta):\n        return False\n    if hasattr(object, '__abstractmethods__'):\n        # It looks like ABCMeta.__new__ has finished running;\n        # TPFLAGS_IS_ABSTRACT should have been accurate.\n        return False\n    # It looks like ABCMeta.__new__ has not finished running yet; we're\n    # probably in __init_subclass__. We'll look for abstractmethods manually.\n    for name, value in object.__dict__.items():\n        if getattr(value, \"__isabstractmethod__\", False):\n            return True\n    for base in object.__bases__:\n        for name in getattr(base, \"__abstractmethods__\", ()):\n            value = getattr(object, name, None)\n            if getattr(value, \"__isabstractmethod__\", False):\n                return True\n    return False\n\ndef _getmembers(object, predicate, getter):\n    results = []\n    processed = set()\n    names = dir(object)\n    if isclass(object):\n        mro = (object,) + getmro(object)\n        # add any DynamicClassAttributes to the list of names if object is a class;\n        # this may result in duplicate entries if, for example, a virtual\n        # attribute with the same name as a DynamicClassAttribute exists\n        try:\n            for base in object.__bases__:\n                for k, v in base.__dict__.items():\n                    if isinstance(v, types.DynamicClassAttribute):\n                        names.append(k)\n        except AttributeError:\n            pass\n    else:\n        mro = ()\n    for key in names:\n        # First try to get the value via getattr.  Some descriptors don't\n        # like calling their __get__ (see bug #1785), so fall back to\n        # looking in the __dict__.\n        try:\n            value = getter(object, key)\n            # handle the duplicate key\n            if key in processed:\n                raise AttributeError\n        except AttributeError:\n            for base in mro:\n                if key in base.__dict__:\n                    value = base.__dict__[key]\n                    break\n            else:\n                # could be a (currently) missing slot member, or a buggy\n                # __dir__; discard and move on\n                continue\n        if not predicate or predicate(value):\n            results.append((key, value))\n        processed.add(key)\n    results.sort(key=lambda pair: pair[0])\n    return results\n\ndef getmembers(object, predicate=None):\n    \"\"\"Return all members of an object as (name, value) pairs sorted by name.\n    Optionally, only return members that satisfy a given predicate.\"\"\"\n    return _getmembers(object, predicate, getattr)\n\ndef getmembers_static(object, predicate=None):\n    \"\"\"Return all members of an object as (name, value) pairs sorted by name\n    without triggering dynamic lookup via the descriptor protocol,\n    __getattr__ or __getattribute__. Optionally, only return members that\n    satisfy a given predicate.\n\n    Note: this function may not be able to retrieve all members\n       that getmembers can fetch (like dynamically created attributes)\n       and may find members that getmembers can't (like descriptors\n       that raise AttributeError). It can also return descriptor objects\n       instead of instance members in some cases.\n    \"\"\"\n    return _getmembers(object, predicate, getattr_static)\n\nAttribute = namedtuple('Attribute', 'name kind defining_class object')\n\ndef classify_class_attrs(cls):\n    \"\"\"Return list of attribute-descriptor tuples.\n\n    For each name in dir(cls), the return list contains a 4-tuple\n    with these elements:\n\n        0. The name (a string).\n\n        1. The kind of attribute this is, one of these strings:\n               'class method'    created via classmethod()\n               'static method'   created via staticmethod()\n               'property'        created via property()\n               'method'          any other flavor of method or descriptor\n               'data'            not a method\n\n        2. The class which defined this attribute (a class).\n\n        3. The object as obtained by calling getattr; if this fails, or if the\n           resulting object does not live anywhere in the class' mro (including\n           metaclasses) then the object is looked up in the defining class's\n           dict (found by walking the mro).\n\n    If one of the items in dir(cls) is stored in the metaclass it will now\n    be discovered and not have None be listed as the class in which it was\n    defined.  Any items whose home class cannot be discovered are skipped.\n    \"\"\"\n\n    mro = getmro(cls)\n    metamro = getmro(type(cls)) # for attributes stored in the metaclass\n    metamro = tuple(cls for cls in metamro if cls not in (type, object))\n    class_bases = (cls,) + mro\n    all_bases = class_bases + metamro\n    names = dir(cls)\n    # :dd any DynamicClassAttributes to the list of names;\n    # this may result in duplicate entries if, for example, a virtual\n    # attribute with the same name as a DynamicClassAttribute exists.\n    for base in mro:\n        for k, v in base.__dict__.items():\n            if isinstance(v, types.DynamicClassAttribute) and v.fget is not None:\n                names.append(k)\n    result = []\n    processed = set()\n\n    for name in names:\n        # Get the object associated with the name, and where it was defined.\n        # Normal objects will be looked up with both getattr and directly in\n        # its class' dict (in case getattr fails [bug #1785], and also to look\n        # for a docstring).\n        # For DynamicClassAttributes on the second pass we only look in the\n        # class's dict.\n        #\n        # Getting an obj from the __dict__ sometimes reveals more than\n        # using getattr.  Static and class methods are dramatic examples.\n        homecls = None\n        get_obj = None\n        dict_obj = None\n        if name not in processed:\n            try:\n                if name == '__dict__':\n                    raise Exception(\"__dict__ is special, don't want the proxy\")\n                get_obj = getattr(cls, name)\n            except Exception as exc:\n                pass\n            else:\n                homecls = getattr(get_obj, \"__objclass__\", homecls)\n                if homecls not in class_bases:\n                    # if the resulting object does not live somewhere in the\n                    # mro, drop it and search the mro manually\n                    homecls = None\n                    last_cls = None\n                    # first look in the classes\n                    for srch_cls in class_bases:\n                        srch_obj = getattr(srch_cls, name, None)\n                        if srch_obj is get_obj:\n                            last_cls = srch_cls\n                    # then check the metaclasses\n                    for srch_cls in metamro:\n                        try:\n                            srch_obj = srch_cls.__getattr__(cls, name)\n                        except AttributeError:\n                            continue\n                        if srch_obj is get_obj:\n                            last_cls = srch_cls\n                    if last_cls is not None:\n                        homecls = last_cls\n        for base in all_bases:\n            if name in base.__dict__:\n                dict_obj = base.__dict__[name]\n                if homecls not in metamro:\n                    homecls = base\n                break\n        if homecls is None:\n            # unable to locate the attribute anywhere, most likely due to\n            # buggy custom __dir__; discard and move on\n            continue\n        obj = get_obj if get_obj is not None else dict_obj\n        # Classify the object or its descriptor.\n        if isinstance(dict_obj, (staticmethod, types.BuiltinMethodType)):\n            kind = \"static method\"\n            obj = dict_obj\n        elif isinstance(dict_obj, (classmethod, types.ClassMethodDescriptorType)):\n            kind = \"class method\"\n            obj = dict_obj\n        elif isinstance(dict_obj, property):\n            kind = \"property\"\n            obj = dict_obj\n        elif isroutine(obj):\n            kind = \"method\"\n        else:\n            kind = \"data\"\n        result.append(Attribute(name, kind, homecls, obj))\n        processed.add(name)\n    return result\n\n# ----------------------------------------------------------- class helpers\n\ndef getmro(cls):\n    \"Return tuple of base classes (including cls) in method resolution order.\"\n    return cls.__mro__\n\n# -------------------------------------------------------- function helpers\n\ndef unwrap(func, *, stop=None):\n    \"\"\"Get the object wrapped by *func*.\n\n   Follows the chain of :attr:`__wrapped__` attributes returning the last\n   object in the chain.\n\n   *stop* is an optional callback accepting an object in the wrapper chain\n   as its sole argument that allows the unwrapping to be terminated early if\n   the callback returns a true value. If the callback never returns a true\n   value, the last object in the chain is returned as usual. For example,\n   :func:`signature` uses this to stop unwrapping if any object in the\n   chain has a ``__signature__`` attribute defined.\n\n   :exc:`ValueError` is raised if a cycle is encountered.\n\n    \"\"\"\n    f = func  # remember the original func for error reporting\n    # Memoise by id to tolerate non-hashable objects, but store objects to\n    # ensure they aren't destroyed, which would allow their IDs to be reused.\n    memo = {id(f): f}\n    recursion_limit = sys.getrecursionlimit()\n    while not isinstance(func, type) and hasattr(func, '__wrapped__'):\n        if stop is not None and stop(func):\n            break\n        func = func.__wrapped__\n        id_func = id(func)\n        if (id_func in memo) or (len(memo) >= recursion_limit):\n            raise ValueError('wrapper loop when unwrapping {!r}'.format(f))\n        memo[id_func] = func\n    return func\n\n# -------------------------------------------------- source code extraction\ndef indentsize(line):\n    \"\"\"Return the indent size, in spaces, at the start of a line of text.\"\"\"\n    expline = line.expandtabs()\n    return len(expline) - len(expline.lstrip())\n\ndef _findclass(func):\n    cls = sys.modules.get(func.__module__)\n    if cls is None:\n        return None\n    for name in func.__qualname__.split('.')[:-1]:\n        cls = getattr(cls, name)\n    if not isclass(cls):\n        return None\n    return cls\n\ndef _finddoc(obj):\n    if isclass(obj):\n        for base in obj.__mro__:\n            if base is not object:\n                try:\n                    doc = base.__doc__\n                except AttributeError:\n                    continue\n                if doc is not None:\n                    return doc\n        return None\n\n    if ismethod(obj):\n        name = obj.__func__.__name__\n        self = obj.__self__\n        if (isclass(self) and\n            getattr(getattr(self, name, None), '__func__') is obj.__func__):\n            # classmethod\n            cls = self\n        else:\n            cls = self.__class__\n    elif isfunction(obj):\n        name = obj.__name__\n        cls = _findclass(obj)\n        if cls is None or getattr(cls, name) is not obj:\n            return None\n    elif isbuiltin(obj):\n        name = obj.__name__\n        self = obj.__self__\n        if (isclass(self) and\n            self.__qualname__ + '.' + name == obj.__qualname__):\n            # classmethod\n            cls = self\n        else:\n            cls = self.__class__\n    # Should be tested before isdatadescriptor().\n    elif isinstance(obj, property):\n        func = obj.fget\n        name = func.__name__\n        cls = _findclass(func)\n        if cls is None or getattr(cls, name) is not obj:\n            return None\n    elif ismethoddescriptor(obj) or isdatadescriptor(obj):\n        name = obj.__name__\n        cls = obj.__objclass__\n        if getattr(cls, name) is not obj:\n            return None\n        if ismemberdescriptor(obj):\n            slots = getattr(cls, '__slots__', None)\n            if isinstance(slots, dict) and name in slots:\n                return slots[name]\n    else:\n        return None\n    for base in cls.__mro__:\n        try:\n            doc = getattr(base, name).__doc__\n        except AttributeError:\n            continue\n        if doc is not None:\n            return doc\n    return None\n\ndef getdoc(object):\n    \"\"\"Get the documentation string for an object.\n\n    All tabs are expanded to spaces.  To clean up docstrings that are\n    indented to line up with blocks of code, any whitespace than can be\n    uniformly removed from the second line onwards is removed.\"\"\"\n    try:\n        doc = object.__doc__\n    except AttributeError:\n        return None\n    if doc is None:\n        try:\n            doc = _finddoc(object)\n        except (AttributeError, TypeError):\n            return None\n    if not isinstance(doc, str):\n        return None\n    return cleandoc(doc)\n\ndef cleandoc(doc):\n    \"\"\"Clean up indentation from docstrings.\n\n    Any whitespace that can be uniformly removed from the second line\n    onwards is removed.\"\"\"\n    try:\n        lines = doc.expandtabs().split('\\n')\n    except UnicodeError:\n        return None\n    else:\n        # Find minimum indentation of any non-blank lines after first line.\n        margin = sys.maxsize\n        for line in lines[1:]:\n            content = len(line.lstrip())\n            if content:\n                indent = len(line) - content\n                margin = min(margin, indent)\n        # Remove indentation.\n        if lines:\n            lines[0] = lines[0].lstrip()\n        if margin < sys.maxsize:\n            for i in range(1, len(lines)): lines[i] = lines[i][margin:]\n        # Remove any trailing or leading blank lines.\n        while lines and not lines[-1]:\n            lines.pop()\n        while lines and not lines[0]:\n            lines.pop(0)\n        return '\\n'.join(lines)\n\ndef getfile(object):\n    \"\"\"Work out which source or compiled file an object was defined in.\"\"\"\n    if ismodule(object):\n        if getattr(object, '__file__', None):\n            return object.__file__\n        raise TypeError('{!r} is a built-in module'.format(object))\n    if isclass(object):\n        if hasattr(object, '__module__'):\n            module = sys.modules.get(object.__module__)\n            if getattr(module, '__file__', None):\n                return module.__file__\n            if object.__module__ == '__main__':\n                raise OSError('source code not available')\n        raise TypeError('{!r} is a built-in class'.format(object))\n    if ismethod(object):\n        object = object.__func__\n    if isfunction(object):\n        object = object.__code__\n    if istraceback(object):\n        object = object.tb_frame\n    if isframe(object):\n        object = object.f_code\n    if iscode(object):\n        return object.co_filename\n    raise TypeError('module, class, method, function, traceback, frame, or '\n                    'code object was expected, got {}'.format(\n                    type(object).__name__))\n\ndef getmodulename(path):\n    \"\"\"Return the module name for a given file, or None.\"\"\"\n    fname = os.path.basename(path)\n    # Check for paths that look like an actual module file\n    suffixes = [(-len(suffix), suffix)\n                    for suffix in importlib.machinery.all_suffixes()]\n    suffixes.sort() # try longest suffixes first, in case they overlap\n    for neglen, suffix in suffixes:\n        if fname.endswith(suffix):\n            return fname[:neglen]\n    return None\n\ndef getsourcefile(object):\n    \"\"\"Return the filename that can be used to locate an object's source.\n    Return None if no way can be identified to get the source.\n    \"\"\"\n    filename = getfile(object)\n    all_bytecode_suffixes = importlib.machinery.DEBUG_BYTECODE_SUFFIXES[:]\n    all_bytecode_suffixes += importlib.machinery.OPTIMIZED_BYTECODE_SUFFIXES[:]\n    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n        filename = (os.path.splitext(filename)[0] +\n                    importlib.machinery.SOURCE_SUFFIXES[0])\n    elif any(filename.endswith(s) for s in\n                 importlib.machinery.EXTENSION_SUFFIXES):\n        return None\n    if os.path.exists(filename):\n        return filename\n    # only return a non-existent filename if the module has a PEP 302 loader\n    module = getmodule(object, filename)\n    if getattr(module, '__loader__', None) is not None:\n        return filename\n    elif getattr(getattr(module, \"__spec__\", None), \"loader\", None) is not None:\n        return filename\n    # or it is in the linecache\n    elif filename in linecache.cache:\n        return filename\n\ndef getabsfile(object, _filename=None):\n    \"\"\"Return an absolute path to the source or compiled file for an object.\n\n    The idea is for each object to have a unique origin, so this routine\n    normalizes the result as much as possible.\"\"\"\n    if _filename is None:\n        _filename = getsourcefile(object) or getfile(object)\n    return os.path.normcase(os.path.abspath(_filename))\n\nmodulesbyfile = {}\n_filesbymodname = {}\n\ndef getmodule(object, _filename=None):\n    \"\"\"Return the module an object was defined in, or None if not found.\"\"\"\n    if ismodule(object):\n        return object\n    if hasattr(object, '__module__'):\n        return sys.modules.get(object.__module__)\n    # Try the filename to modulename cache\n    if _filename is not None and _filename in modulesbyfile:\n        return sys.modules.get(modulesbyfile[_filename])\n    # Try the cache again with the absolute file name\n    try:\n        file = getabsfile(object, _filename)\n    except (TypeError, FileNotFoundError):\n        return None\n    if file in modulesbyfile:\n        return sys.modules.get(modulesbyfile[file])\n    # Update the filename to module name cache and check yet again\n    # Copy sys.modules in order to cope with changes while iterating\n    for modname, module in sys.modules.copy().items():\n        if ismodule(module) and hasattr(module, '__file__'):\n            f = module.__file__\n            if f == _filesbymodname.get(modname, None):\n                # Have already mapped this module, so skip it\n                continue\n            _filesbymodname[modname] = f\n            f = getabsfile(module)\n            # Always map to the name the module knows itself by\n            modulesbyfile[f] = modulesbyfile[\n                os.path.realpath(f)] = module.__name__\n    if file in modulesbyfile:\n        return sys.modules.get(modulesbyfile[file])\n    # Check the main module\n    main = sys.modules['__main__']\n    if not hasattr(object, '__name__'):\n        return None\n    if hasattr(main, object.__name__):\n        mainobject = getattr(main, object.__name__)\n        if mainobject is object:\n            return main\n    # Check builtins\n    builtin = sys.modules['builtins']\n    if hasattr(builtin, object.__name__):\n        builtinobject = getattr(builtin, object.__name__)\n        if builtinobject is object:\n            return builtin\n\n\nclass ClassFoundException(Exception):\n    pass\n\n\nclass _ClassFinder(ast.NodeVisitor):\n\n    def __init__(self, qualname):\n        self.stack = []\n        self.qualname = qualname\n\n    def visit_FunctionDef(self, node):\n        self.stack.append(node.name)\n        self.stack.append('<locals>')\n        self.generic_visit(node)\n        self.stack.pop()\n        self.stack.pop()\n\n    visit_AsyncFunctionDef = visit_FunctionDef\n\n    def visit_ClassDef(self, node):\n        self.stack.append(node.name)\n        if self.qualname == '.'.join(self.stack):\n            # Return the decorator for the class if present\n            if node.decorator_list:\n                line_number = node.decorator_list[0].lineno\n            else:\n                line_number = node.lineno\n\n            # decrement by one since lines starts with indexing by zero\n            line_number -= 1\n            raise ClassFoundException(line_number)\n        self.generic_visit(node)\n        self.stack.pop()\n\n\ndef findsource(object):\n    \"\"\"Return the entire source file and starting line number for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a list of all the lines\n    in the file and the line number indexes a line in that list.  An OSError\n    is raised if the source code cannot be retrieved.\"\"\"\n\n    file = getsourcefile(object)\n    if file:\n        # Invalidate cache if needed.\n        linecache.checkcache(file)\n    else:\n        file = getfile(object)\n        # Allow filenames in form of \"<something>\" to pass through.\n        # `doctest` monkeypatches `linecache` module to enable\n        # inspection, so let `linecache.getlines` to be called.\n        if not (file.startswith('<') and file.endswith('>')):\n            raise OSError('source code not available')\n\n    module = getmodule(object, file)\n    if module:\n        lines = linecache.getlines(file, module.__dict__)\n    else:\n        lines = linecache.getlines(file)\n    if not lines:\n        raise OSError('could not get source code')\n\n    if ismodule(object):\n        return lines, 0\n\n    if isclass(object):\n        qualname = object.__qualname__\n        source = ''.join(lines)\n        tree = ast.parse(source)\n        class_finder = _ClassFinder(qualname)\n        try:\n            class_finder.visit(tree)\n        except ClassFoundException as e:\n            line_number = e.args[0]\n            return lines, line_number\n        else:\n            raise OSError('could not find class definition')\n\n    if ismethod(object):\n        object = object.__func__\n    if isfunction(object):\n        object = object.__code__\n    if istraceback(object):\n        object = object.tb_frame\n    if isframe(object):\n        object = object.f_code\n    if iscode(object):\n        if not hasattr(object, 'co_firstlineno'):\n            raise OSError('could not find function definition')\n        lnum = object.co_firstlineno - 1\n        pat = re.compile(r'^(\\s*def\\s)|(\\s*async\\s+def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')\n        while lnum > 0:\n            try:\n                line = lines[lnum]\n            except IndexError:\n                raise OSError('lineno is out of bounds')\n            if pat.match(line):\n                break\n            lnum = lnum - 1\n        return lines, lnum\n    raise OSError('could not find code object')\n\ndef getcomments(object):\n    \"\"\"Get lines of comments immediately preceding an object's source code.\n\n    Returns None when source can't be found.\n    \"\"\"\n    try:\n        lines, lnum = findsource(object)\n    except (OSError, TypeError):\n        return None\n\n    if ismodule(object):\n        # Look for a comment block at the top of the file.\n        start = 0\n        if lines and lines[0][:2] == '#!': start = 1\n        while start < len(lines) and lines[start].strip() in ('', '#'):\n            start = start + 1\n        if start < len(lines) and lines[start][:1] == '#':\n            comments = []\n            end = start\n            while end < len(lines) and lines[end][:1] == '#':\n                comments.append(lines[end].expandtabs())\n                end = end + 1\n            return ''.join(comments)\n\n    # Look for a preceding block of comments at the same indentation.\n    elif lnum > 0:\n        indent = indentsize(lines[lnum])\n        end = lnum - 1\n        if end >= 0 and lines[end].lstrip()[:1] == '#' and \\\n            indentsize(lines[end]) == indent:\n            comments = [lines[end].expandtabs().lstrip()]\n            if end > 0:\n                end = end - 1\n                comment = lines[end].expandtabs().lstrip()\n                while comment[:1] == '#' and indentsize(lines[end]) == indent:\n                    comments[:0] = [comment]\n                    end = end - 1\n                    if end < 0: break\n                    comment = lines[end].expandtabs().lstrip()\n            while comments and comments[0].strip() == '#':\n                comments[:1] = []\n            while comments and comments[-1].strip() == '#':\n                comments[-1:] = []\n            return ''.join(comments)\n\nclass EndOfBlock(Exception): pass\n\nclass BlockFinder:\n    \"\"\"Provide a tokeneater() method to detect the end of a code block.\"\"\"\n    def __init__(self):\n        self.indent = 0\n        self.islambda = False\n        self.started = False\n        self.passline = False\n        self.indecorator = False\n        self.last = 1\n        self.body_col0 = None\n\n    def tokeneater(self, type, token, srowcol, erowcol, line):\n        if not self.started and not self.indecorator:\n            # skip any decorators\n            if token == \"@\":\n                self.indecorator = True\n            # look for the first \"def\", \"class\" or \"lambda\"\n            elif token in (\"def\", \"class\", \"lambda\"):\n                if token == \"lambda\":\n                    self.islambda = True\n                self.started = True\n            self.passline = True    # skip to the end of the line\n        elif type == tokenize.NEWLINE:\n            self.passline = False   # stop skipping when a NEWLINE is seen\n            self.last = srowcol[0]\n            if self.islambda:       # lambdas always end at the first NEWLINE\n                raise EndOfBlock\n            # hitting a NEWLINE when in a decorator without args\n            # ends the decorator\n            if self.indecorator:\n                self.indecorator = False\n        elif self.passline:\n            pass\n        elif type == tokenize.INDENT:\n            if self.body_col0 is None and self.started:\n                self.body_col0 = erowcol[1]\n            self.indent = self.indent + 1\n            self.passline = True\n        elif type == tokenize.DEDENT:\n            self.indent = self.indent - 1\n            # the end of matching indent/dedent pairs end a block\n            # (note that this only works for \"def\"/\"class\" blocks,\n            #  not e.g. for \"if: else:\" or \"try: finally:\" blocks)\n            if self.indent <= 0:\n                raise EndOfBlock\n        elif type == tokenize.COMMENT:\n            if self.body_col0 is not None and srowcol[1] >= self.body_col0:\n                # Include comments if indented at least as much as the block\n                self.last = srowcol[0]\n        elif self.indent == 0 and type not in (tokenize.COMMENT, tokenize.NL):\n            # any other token on the same indentation level end the previous\n            # block as well, except the pseudo-tokens COMMENT and NL.\n            raise EndOfBlock\n\ndef getblock(lines):\n    \"\"\"Extract the block of code at the top of the given list of lines.\"\"\"\n    blockfinder = BlockFinder()\n    try:\n        tokens = tokenize.generate_tokens(iter(lines).__next__)\n        for _token in tokens:\n            blockfinder.tokeneater(*_token)\n    except (EndOfBlock, IndentationError):\n        pass\n    return lines[:blockfinder.last]\n\ndef getsourcelines(object):\n    \"\"\"Return a list of source lines and starting line number for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a list of the lines\n    corresponding to the object and the line number indicates where in the\n    original source file the first line of code was found.  An OSError is\n    raised if the source code cannot be retrieved.\"\"\"\n    object = unwrap(object)\n    lines, lnum = findsource(object)\n\n    if istraceback(object):\n        object = object.tb_frame\n\n    # for module or frame that corresponds to module, return all source lines\n    if (ismodule(object) or\n        (isframe(object) and object.f_code.co_name == \"<module>\")):\n        return lines, 0\n    else:\n        return getblock(lines[lnum:]), lnum + 1\n\ndef getsource(object):\n    \"\"\"Return the text of the source code for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a single string.  An\n    OSError is raised if the source code cannot be retrieved.\"\"\"\n    lines, lnum = getsourcelines(object)\n    return ''.join(lines)\n\n# --------------------------------------------------- class tree extraction\ndef walktree(classes, children, parent):\n    \"\"\"Recursive helper function for getclasstree().\"\"\"\n    results = []\n    classes.sort(key=attrgetter('__module__', '__name__'))\n    for c in classes:\n        results.append((c, c.__bases__))\n        if c in children:\n            results.append(walktree(children[c], children, c))\n    return results\n\ndef getclasstree(classes, unique=False):\n    \"\"\"Arrange the given list of classes into a hierarchy of nested lists.\n\n    Where a nested list appears, it contains classes derived from the class\n    whose entry immediately precedes the list.  Each entry is a 2-tuple\n    containing a class and a tuple of its base classes.  If the 'unique'\n    argument is true, exactly one entry appears in the returned structure\n    for each class in the given list.  Otherwise, classes using multiple\n    inheritance and their descendants will appear multiple times.\"\"\"\n    children = {}\n    roots = []\n    for c in classes:\n        if c.__bases__:\n            for parent in c.__bases__:\n                if parent not in children:\n                    children[parent] = []\n                if c not in children[parent]:\n                    children[parent].append(c)\n                if unique and parent in classes: break\n        elif c not in roots:\n            roots.append(c)\n    for parent in children:\n        if parent not in classes:\n            roots.append(parent)\n    return walktree(roots, children, None)\n\n# ------------------------------------------------ argument list extraction\nArguments = namedtuple('Arguments', 'args, varargs, varkw')\n\ndef getargs(co):\n    \"\"\"Get information about the arguments accepted by a code object.\n\n    Three things are returned: (args, varargs, varkw), where\n    'args' is the list of argument names. Keyword-only arguments are\n    appended. 'varargs' and 'varkw' are the names of the * and **\n    arguments or None.\"\"\"\n    if not iscode(co):\n        raise TypeError('{!r} is not a code object'.format(co))\n\n    names = co.co_varnames\n    nargs = co.co_argcount\n    nkwargs = co.co_kwonlyargcount\n    args = list(names[:nargs])\n    kwonlyargs = list(names[nargs:nargs+nkwargs])\n    step = 0\n\n    nargs += nkwargs\n    varargs = None\n    if co.co_flags & CO_VARARGS:\n        varargs = co.co_varnames[nargs]\n        nargs = nargs + 1\n    varkw = None\n    if co.co_flags & CO_VARKEYWORDS:\n        varkw = co.co_varnames[nargs]\n    return Arguments(args + kwonlyargs, varargs, varkw)\n\n\nFullArgSpec = namedtuple('FullArgSpec',\n    'args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations')\n\ndef getfullargspec(func):\n    \"\"\"Get the names and default values of a callable object's parameters.\n\n    A tuple of seven things is returned:\n    (args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations).\n    'args' is a list of the parameter names.\n    'varargs' and 'varkw' are the names of the * and ** parameters or None.\n    'defaults' is an n-tuple of the default values of the last n parameters.\n    'kwonlyargs' is a list of keyword-only parameter names.\n    'kwonlydefaults' is a dictionary mapping names from kwonlyargs to defaults.\n    'annotations' is a dictionary mapping parameter names to annotations.\n\n    Notable differences from inspect.signature():\n      - the \"self\" parameter is always reported, even for bound methods\n      - wrapper chains defined by __wrapped__ *not* unwrapped automatically\n    \"\"\"\n    try:\n        # Re: `skip_bound_arg=False`\n        #\n        # There is a notable difference in behaviour between getfullargspec\n        # and Signature: the former always returns 'self' parameter for bound\n        # methods, whereas the Signature always shows the actual calling\n        # signature of the passed object.\n        #\n        # To simulate this behaviour, we \"unbind\" bound methods, to trick\n        # inspect.signature to always return their first parameter (\"self\",\n        # usually)\n\n        # Re: `follow_wrapper_chains=False`\n        #\n        # getfullargspec() historically ignored __wrapped__ attributes,\n        # so we ensure that remains the case in 3.3+\n\n        sig = _signature_from_callable(func,\n                                       follow_wrapper_chains=False,\n                                       skip_bound_arg=False,\n                                       sigcls=Signature,\n                                       eval_str=False)\n    except Exception as ex:\n        # Most of the times 'signature' will raise ValueError.\n        # But, it can also raise AttributeError, and, maybe something\n        # else. So to be fully backwards compatible, we catch all\n        # possible exceptions here, and reraise a TypeError.\n        raise TypeError('unsupported callable') from ex\n\n    args = []\n    varargs = None\n    varkw = None\n    posonlyargs = []\n    kwonlyargs = []\n    annotations = {}\n    defaults = ()\n    kwdefaults = {}\n\n    if sig.return_annotation is not sig.empty:\n        annotations['return'] = sig.return_annotation\n\n    for param in sig.parameters.values():\n        kind = param.kind\n        name = param.name\n\n        if kind is _POSITIONAL_ONLY:\n            posonlyargs.append(name)\n            if param.default is not param.empty:\n                defaults += (param.default,)\n        elif kind is _POSITIONAL_OR_KEYWORD:\n            args.append(name)\n            if param.default is not param.empty:\n                defaults += (param.default,)\n        elif kind is _VAR_POSITIONAL:\n            varargs = name\n        elif kind is _KEYWORD_ONLY:\n            kwonlyargs.append(name)\n            if param.default is not param.empty:\n                kwdefaults[name] = param.default\n        elif kind is _VAR_KEYWORD:\n            varkw = name\n\n        if param.annotation is not param.empty:\n            annotations[name] = param.annotation\n\n    if not kwdefaults:\n        # compatibility with 'func.__kwdefaults__'\n        kwdefaults = None\n\n    if not defaults:\n        # compatibility with 'func.__defaults__'\n        defaults = None\n\n    return FullArgSpec(posonlyargs + args, varargs, varkw, defaults,\n                       kwonlyargs, kwdefaults, annotations)\n\n\nArgInfo = namedtuple('ArgInfo', 'args varargs keywords locals')\n\ndef getargvalues(frame):\n    \"\"\"Get information about arguments passed into a particular frame.\n\n    A tuple of four things is returned: (args, varargs, varkw, locals).\n    'args' is a list of the argument names.\n    'varargs' and 'varkw' are the names of the * and ** arguments or None.\n    'locals' is the locals dictionary of the given frame.\"\"\"\n    args, varargs, varkw = getargs(frame.f_code)\n    return ArgInfo(args, varargs, varkw, frame.f_locals)\n\ndef formatannotation(annotation, base_module=None):\n    if getattr(annotation, '__module__', None) == 'typing':\n        def repl(match):\n            text = match.group()\n            return text.removeprefix('typing.')\n        return re.sub(r'[\\w\\.]+', repl, repr(annotation))\n    if isinstance(annotation, types.GenericAlias):\n        return str(annotation)\n    if isinstance(annotation, type):\n        if annotation.__module__ in ('builtins', base_module):\n            return annotation.__qualname__\n        return annotation.__module__+'.'+annotation.__qualname__\n    return repr(annotation)\n\ndef formatannotationrelativeto(object):\n    module = getattr(object, '__module__', None)\n    def _formatannotation(annotation):\n        return formatannotation(annotation, module)\n    return _formatannotation\n\n\ndef formatargvalues(args, varargs, varkw, locals,\n                    formatarg=str,\n                    formatvarargs=lambda name: '*' + name,\n                    formatvarkw=lambda name: '**' + name,\n                    formatvalue=lambda value: '=' + repr(value)):\n    \"\"\"Format an argument spec from the 4 values returned by getargvalues.\n\n    The first four arguments are (args, varargs, varkw, locals).  The\n    next four arguments are the corresponding optional formatting functions\n    that are called to turn names and values into strings.  The ninth\n    argument is an optional function to format the sequence of arguments.\"\"\"\n    def convert(name, locals=locals,\n                formatarg=formatarg, formatvalue=formatvalue):\n        return formatarg(name) + formatvalue(locals[name])\n    specs = []\n    for i in range(len(args)):\n        specs.append(convert(args[i]))\n    if varargs:\n        specs.append(formatvarargs(varargs) + formatvalue(locals[varargs]))\n    if varkw:\n        specs.append(formatvarkw(varkw) + formatvalue(locals[varkw]))\n    return '(' + ', '.join(specs) + ')'\n\ndef _missing_arguments(f_name, argnames, pos, values):\n    names = [repr(name) for name in argnames if name not in values]\n    missing = len(names)\n    if missing == 1:\n        s = names[0]\n    elif missing == 2:\n        s = \"{} and {}\".format(*names)\n    else:\n        tail = \", {} and {}\".format(*names[-2:])\n        del names[-2:]\n        s = \", \".join(names) + tail\n    raise TypeError(\"%s() missing %i required %s argument%s: %s\" %\n                    (f_name, missing,\n                      \"positional\" if pos else \"keyword-only\",\n                      \"\" if missing == 1 else \"s\", s))\n\ndef _too_many(f_name, args, kwonly, varargs, defcount, given, values):\n    atleast = len(args) - defcount\n    kwonly_given = len([arg for arg in kwonly if arg in values])\n    if varargs:\n        plural = atleast != 1\n        sig = \"at least %d\" % (atleast,)\n    elif defcount:\n        plural = True\n        sig = \"from %d to %d\" % (atleast, len(args))\n    else:\n        plural = len(args) != 1\n        sig = str(len(args))\n    kwonly_sig = \"\"\n    if kwonly_given:\n        msg = \" positional argument%s (and %d keyword-only argument%s)\"\n        kwonly_sig = (msg % (\"s\" if given != 1 else \"\", kwonly_given,\n                             \"s\" if kwonly_given != 1 else \"\"))\n    raise TypeError(\"%s() takes %s positional argument%s but %d%s %s given\" %\n            (f_name, sig, \"s\" if plural else \"\", given, kwonly_sig,\n             \"was\" if given == 1 and not kwonly_given else \"were\"))\n\ndef getcallargs(func, /, *positional, **named):\n    \"\"\"Get the mapping of arguments to values.\n\n    A dict is returned, with keys the function argument names (including the\n    names of the * and ** arguments, if any), and values the respective bound\n    values from 'positional' and 'named'.\"\"\"\n    spec = getfullargspec(func)\n    args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, ann = spec\n    f_name = func.__name__\n    arg2value = {}\n\n\n    if ismethod(func) and func.__self__ is not None:\n        # implicit 'self' (or 'cls' for classmethods) argument\n        positional = (func.__self__,) + positional\n    num_pos = len(positional)\n    num_args = len(args)\n    num_defaults = len(defaults) if defaults else 0\n\n    n = min(num_pos, num_args)\n    for i in range(n):\n        arg2value[args[i]] = positional[i]\n    if varargs:\n        arg2value[varargs] = tuple(positional[n:])\n    possible_kwargs = set(args + kwonlyargs)\n    if varkw:\n        arg2value[varkw] = {}\n    for kw, value in named.items():\n        if kw not in possible_kwargs:\n            if not varkw:\n                raise TypeError(\"%s() got an unexpected keyword argument %r\" %\n                                (f_name, kw))\n            arg2value[varkw][kw] = value\n            continue\n        if kw in arg2value:\n            raise TypeError(\"%s() got multiple values for argument %r\" %\n                            (f_name, kw))\n        arg2value[kw] = value\n    if num_pos > num_args and not varargs:\n        _too_many(f_name, args, kwonlyargs, varargs, num_defaults,\n                   num_pos, arg2value)\n    if num_pos < num_args:\n        req = args[:num_args - num_defaults]\n        for arg in req:\n            if arg not in arg2value:\n                _missing_arguments(f_name, req, True, arg2value)\n        for i, arg in enumerate(args[num_args - num_defaults:]):\n            if arg not in arg2value:\n                arg2value[arg] = defaults[i]\n    missing = 0\n    for kwarg in kwonlyargs:\n        if kwarg not in arg2value:\n            if kwonlydefaults and kwarg in kwonlydefaults:\n                arg2value[kwarg] = kwonlydefaults[kwarg]\n            else:\n                missing += 1\n    if missing:\n        _missing_arguments(f_name, kwonlyargs, False, arg2value)\n    return arg2value\n\nClosureVars = namedtuple('ClosureVars', 'nonlocals globals builtins unbound')\n\ndef getclosurevars(func):\n    \"\"\"\n    Get the mapping of free variables to their current values.\n\n    Returns a named tuple of dicts mapping the current nonlocal, global\n    and builtin references as seen by the body of the function. A final\n    set of unbound names that could not be resolved is also provided.\n    \"\"\"\n\n    if ismethod(func):\n        func = func.__func__\n\n    if not isfunction(func):\n        raise TypeError(\"{!r} is not a Python function\".format(func))\n\n    code = func.__code__\n    # Nonlocal references are named in co_freevars and resolved\n    # by looking them up in __closure__ by positional index\n    if func.__closure__ is None:\n        nonlocal_vars = {}\n    else:\n        nonlocal_vars = {\n            var : cell.cell_contents\n            for var, cell in zip(code.co_freevars, func.__closure__)\n       }\n\n    # Global and builtin references are named in co_names and resolved\n    # by looking them up in __globals__ or __builtins__\n    global_ns = func.__globals__\n    builtin_ns = global_ns.get(\"__builtins__\", builtins.__dict__)\n    if ismodule(builtin_ns):\n        builtin_ns = builtin_ns.__dict__\n    global_vars = {}\n    builtin_vars = {}\n    unbound_names = set()\n    for name in code.co_names:\n        if name in (\"None\", \"True\", \"False\"):\n            # Because these used to be builtins instead of keywords, they\n            # may still show up as name references. We ignore them.\n            continue\n        try:\n            global_vars[name] = global_ns[name]\n        except KeyError:\n            try:\n                builtin_vars[name] = builtin_ns[name]\n            except KeyError:\n                unbound_names.add(name)\n\n    return ClosureVars(nonlocal_vars, global_vars,\n                       builtin_vars, unbound_names)\n\n# -------------------------------------------------- stack frame extraction\n\n_Traceback = namedtuple('_Traceback', 'filename lineno function code_context index')\n\nclass Traceback(_Traceback):\n    def __new__(cls, filename, lineno, function, code_context, index, *, positions=None):\n        instance = super().__new__(cls, filename, lineno, function, code_context, index)\n        instance.positions = positions\n        return instance\n\n    def __repr__(self):\n        return ('Traceback(filename={!r}, lineno={!r}, function={!r}, '\n               'code_context={!r}, index={!r}, positions={!r})'.format(\n                self.filename, self.lineno, self.function, self.code_context,\n                self.index, self.positions))\n\ndef _get_code_position_from_tb(tb):\n    code, instruction_index = tb.tb_frame.f_code, tb.tb_lasti\n    return _get_code_position(code, instruction_index)\n\ndef _get_code_position(code, instruction_index):\n    if instruction_index < 0:\n        return (None, None, None, None)\n    positions_gen = code.co_positions()\n    # The nth entry in code.co_positions() corresponds to instruction (2*n)th since Python 3.10+\n    return next(itertools.islice(positions_gen, instruction_index // 2, None))\n\ndef getframeinfo(frame, context=1):\n    \"\"\"Get information about a frame or traceback object.\n\n    A tuple of five things is returned: the filename, the line number of\n    the current line, the function name, a list of lines of context from\n    the source code, and the index of the current line within that list.\n    The optional second argument specifies the number of lines of context\n    to return, which are centered around the current line.\"\"\"\n    if istraceback(frame):\n        positions = _get_code_position_from_tb(frame)\n        lineno = frame.tb_lineno\n        frame = frame.tb_frame\n    else:\n        lineno = frame.f_lineno\n        positions = _get_code_position(frame.f_code, frame.f_lasti)\n\n    if positions[0] is None:\n        frame, *positions = (frame, lineno, *positions[1:])\n    else:\n        frame, *positions = (frame, *positions)\n\n    lineno = positions[0]\n\n    if not isframe(frame):\n        raise TypeError('{!r} is not a frame or traceback object'.format(frame))\n\n    filename = getsourcefile(frame) or getfile(frame)\n    if context > 0:\n        start = lineno - 1 - context//2\n        try:\n            lines, lnum = findsource(frame)\n        except OSError:\n            lines = index = None\n        else:\n            start = max(0, min(start, len(lines) - context))\n            lines = lines[start:start+context]\n            index = lineno - 1 - start\n    else:\n        lines = index = None\n\n    return Traceback(filename, lineno, frame.f_code.co_name, lines,\n                     index, positions=dis.Positions(*positions))\n\ndef getlineno(frame):\n    \"\"\"Get the line number from a frame object, allowing for optimization.\"\"\"\n    # FrameType.f_lineno is now a descriptor that grovels co_lnotab\n    return frame.f_lineno\n\n_FrameInfo = namedtuple('_FrameInfo', ('frame',) + Traceback._fields)\nclass FrameInfo(_FrameInfo):\n    def __new__(cls, frame, filename, lineno, function, code_context, index, *, positions=None):\n        instance = super().__new__(cls, frame, filename, lineno, function, code_context, index)\n        instance.positions = positions\n        return instance\n\n    def __repr__(self):\n        return ('FrameInfo(frame={!r}, filename={!r}, lineno={!r}, function={!r}, '\n               'code_context={!r}, index={!r}, positions={!r})'.format(\n                self.frame, self.filename, self.lineno, self.function,\n                self.code_context, self.index, self.positions))\n\ndef getouterframes(frame, context=1):\n    \"\"\"Get a list of records for a frame and all higher (calling) frames.\n\n    Each record contains a frame object, filename, line number, function\n    name, a list of lines of context, and index within the context.\"\"\"\n    framelist = []\n    while frame:\n        traceback_info = getframeinfo(frame, context)\n        frameinfo = (frame,) + traceback_info\n        framelist.append(FrameInfo(*frameinfo, positions=traceback_info.positions))\n        frame = frame.f_back\n    return framelist\n\ndef getinnerframes(tb, context=1):\n    \"\"\"Get a list of records for a traceback's frame and all lower frames.\n\n    Each record contains a frame object, filename, line number, function\n    name, a list of lines of context, and index within the context.\"\"\"\n    framelist = []\n    while tb:\n        traceback_info = getframeinfo(tb, context)\n        frameinfo = (tb.tb_frame,) + traceback_info\n        framelist.append(FrameInfo(*frameinfo, positions=traceback_info.positions))\n        tb = tb.tb_next\n    return framelist\n\ndef currentframe():\n    \"\"\"Return the frame of the caller or None if this is not possible.\"\"\"\n    return sys._getframe(1) if hasattr(sys, \"_getframe\") else None\n\ndef stack(context=1):\n    \"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\n    return getouterframes(sys._getframe(1), context)\n\ndef trace(context=1):\n    \"\"\"Return a list of records for the stack below the current exception.\"\"\"\n    return getinnerframes(sys.exc_info()[2], context)\n\n\n# ------------------------------------------------ static version of getattr\n\n_sentinel = object()\n\ndef _static_getmro(klass):\n    return type.__dict__['__mro__'].__get__(klass)\n\ndef _check_instance(obj, attr):\n    instance_dict = {}\n    try:\n        instance_dict = object.__getattribute__(obj, \"__dict__\")\n    except AttributeError:\n        pass\n    return dict.get(instance_dict, attr, _sentinel)\n\n\ndef _check_class(klass, attr):\n    for entry in _static_getmro(klass):\n        if _shadowed_dict(type(entry)) is _sentinel:\n            try:\n                return entry.__dict__[attr]\n            except KeyError:\n                pass\n    return _sentinel\n\ndef _is_type(obj):\n    try:\n        _static_getmro(obj)\n    except TypeError:\n        return False\n    return True\n\ndef _shadowed_dict(klass):\n    dict_attr = type.__dict__[\"__dict__\"]\n    for entry in _static_getmro(klass):\n        try:\n            class_dict = dict_attr.__get__(entry)[\"__dict__\"]\n        except KeyError:\n            pass\n        else:\n            if not (type(class_dict) is types.GetSetDescriptorType and\n                    class_dict.__name__ == \"__dict__\" and\n                    class_dict.__objclass__ is entry):\n                return class_dict\n    return _sentinel\n\ndef getattr_static(obj, attr, default=_sentinel):\n    \"\"\"Retrieve attributes without triggering dynamic lookup via the\n       descriptor protocol,  __getattr__ or __getattribute__.\n\n       Note: this function may not be able to retrieve all attributes\n       that getattr can fetch (like dynamically created attributes)\n       and may find attributes that getattr can't (like descriptors\n       that raise AttributeError). It can also return descriptor objects\n       instead of instance members in some cases. See the\n       documentation for details.\n    \"\"\"\n    instance_result = _sentinel\n    if not _is_type(obj):\n        klass = type(obj)\n        dict_attr = _shadowed_dict(klass)\n        if (dict_attr is _sentinel or\n            type(dict_attr) is types.MemberDescriptorType):\n            instance_result = _check_instance(obj, attr)\n    else:\n        klass = obj\n\n    klass_result = _check_class(klass, attr)\n\n    if instance_result is not _sentinel and klass_result is not _sentinel:\n        if _check_class(type(klass_result), \"__get__\") is not _sentinel and (\n            _check_class(type(klass_result), \"__set__\") is not _sentinel\n            or _check_class(type(klass_result), \"__delete__\") is not _sentinel\n        ):\n            return klass_result\n\n    if instance_result is not _sentinel:\n        return instance_result\n    if klass_result is not _sentinel:\n        return klass_result\n\n    if obj is klass:\n        # for types we check the metaclass too\n        for entry in _static_getmro(type(klass)):\n            if _shadowed_dict(type(entry)) is _sentinel:\n                try:\n                    return entry.__dict__[attr]\n                except KeyError:\n                    pass\n    if default is not _sentinel:\n        return default\n    raise AttributeError(attr)\n\n\n# ------------------------------------------------ generator introspection\n\nGEN_CREATED = 'GEN_CREATED'\nGEN_RUNNING = 'GEN_RUNNING'\nGEN_SUSPENDED = 'GEN_SUSPENDED'\nGEN_CLOSED = 'GEN_CLOSED'\n\ndef getgeneratorstate(generator):\n    \"\"\"Get current state of a generator-iterator.\n\n    Possible states are:\n      GEN_CREATED: Waiting to start execution.\n      GEN_RUNNING: Currently being executed by the interpreter.\n      GEN_SUSPENDED: Currently suspended at a yield expression.\n      GEN_CLOSED: Execution has completed.\n    \"\"\"\n    if generator.gi_running:\n        return GEN_RUNNING\n    if generator.gi_suspended:\n        return GEN_SUSPENDED\n    if generator.gi_frame is None:\n        return GEN_CLOSED\n    return GEN_CREATED\n\n\ndef getgeneratorlocals(generator):\n    \"\"\"\n    Get the mapping of generator local variables to their current values.\n\n    A dict is returned, with the keys the local variable names and values the\n    bound values.\"\"\"\n\n    if not isgenerator(generator):\n        raise TypeError(\"{!r} is not a Python generator\".format(generator))\n\n    frame = getattr(generator, \"gi_frame\", None)\n    if frame is not None:\n        return generator.gi_frame.f_locals\n    else:\n        return {}\n\n\n# ------------------------------------------------ coroutine introspection\n\nCORO_CREATED = 'CORO_CREATED'\nCORO_RUNNING = 'CORO_RUNNING'\nCORO_SUSPENDED = 'CORO_SUSPENDED'\nCORO_CLOSED = 'CORO_CLOSED'\n\ndef getcoroutinestate(coroutine):\n    \"\"\"Get current state of a coroutine object.\n\n    Possible states are:\n      CORO_CREATED: Waiting to start execution.\n      CORO_RUNNING: Currently being executed by the interpreter.\n      CORO_SUSPENDED: Currently suspended at an await expression.\n      CORO_CLOSED: Execution has completed.\n    \"\"\"\n    if coroutine.cr_running:\n        return CORO_RUNNING\n    if coroutine.cr_suspended:\n        return CORO_SUSPENDED\n    if coroutine.cr_frame is None:\n        return CORO_CLOSED\n    return CORO_CREATED\n\n\ndef getcoroutinelocals(coroutine):\n    \"\"\"\n    Get the mapping of coroutine local variables to their current values.\n\n    A dict is returned, with the keys the local variable names and values the\n    bound values.\"\"\"\n    frame = getattr(coroutine, \"cr_frame\", None)\n    if frame is not None:\n        return frame.f_locals\n    else:\n        return {}\n\n\n###############################################################################\n### Function Signature Object (PEP 362)\n###############################################################################\n\n\n_NonUserDefinedCallables = (types.WrapperDescriptorType,\n                            types.MethodWrapperType,\n                            types.ClassMethodDescriptorType,\n                            types.BuiltinFunctionType)\n\n\ndef _signature_get_user_defined_method(cls, method_name):\n    \"\"\"Private helper. Checks if ``cls`` has an attribute\n    named ``method_name`` and returns it only if it is a\n    pure python function.\n    \"\"\"\n    if method_name == '__new__':\n        meth = getattr(cls, method_name, None)\n    else:\n        meth = getattr_static(cls, method_name, None)\n    if meth is None or isinstance(meth, _NonUserDefinedCallables):\n        # Once '__signature__' will be added to 'C'-level\n        # callables, this check won't be necessary\n        return None\n    if method_name != '__new__':\n        meth = _descriptor_get(meth, cls)\n    return meth\n\n\ndef _signature_get_partial(wrapped_sig, partial, extra_args=()):\n    \"\"\"Private helper to calculate how 'wrapped_sig' signature will\n    look like after applying a 'functools.partial' object (or alike)\n    on it.\n    \"\"\"\n\n    old_params = wrapped_sig.parameters\n    new_params = OrderedDict(old_params.items())\n\n    partial_args = partial.args or ()\n    partial_keywords = partial.keywords or {}\n\n    if extra_args:\n        partial_args = extra_args + partial_args\n\n    try:\n        ba = wrapped_sig.bind_partial(*partial_args, **partial_keywords)\n    except TypeError as ex:\n        msg = 'partial object {!r} has incorrect arguments'.format(partial)\n        raise ValueError(msg) from ex\n\n\n    transform_to_kwonly = False\n    for param_name, param in old_params.items():\n        try:\n            arg_value = ba.arguments[param_name]\n        except KeyError:\n            pass\n        else:\n            if param.kind is _POSITIONAL_ONLY:\n                # If positional-only parameter is bound by partial,\n                # it effectively disappears from the signature\n                new_params.pop(param_name)\n                continue\n\n            if param.kind is _POSITIONAL_OR_KEYWORD:\n                if param_name in partial_keywords:\n                    # This means that this parameter, and all parameters\n                    # after it should be keyword-only (and var-positional\n                    # should be removed). Here's why. Consider the following\n                    # function:\n                    #     foo(a, b, *args, c):\n                    #         pass\n                    #\n                    # \"partial(foo, a='spam')\" will have the following\n                    # signature: \"(*, a='spam', b, c)\". Because attempting\n                    # to call that partial with \"(10, 20)\" arguments will\n                    # raise a TypeError, saying that \"a\" argument received\n                    # multiple values.\n                    transform_to_kwonly = True\n                    # Set the new default value\n                    new_params[param_name] = param.replace(default=arg_value)\n                else:\n                    # was passed as a positional argument\n                    new_params.pop(param.name)\n                    continue\n\n            if param.kind is _KEYWORD_ONLY:\n                # Set the new default value\n                new_params[param_name] = param.replace(default=arg_value)\n\n        if transform_to_kwonly:\n            assert param.kind is not _POSITIONAL_ONLY\n\n            if param.kind is _POSITIONAL_OR_KEYWORD:\n                new_param = new_params[param_name].replace(kind=_KEYWORD_ONLY)\n                new_params[param_name] = new_param\n                new_params.move_to_end(param_name)\n            elif param.kind in (_KEYWORD_ONLY, _VAR_KEYWORD):\n                new_params.move_to_end(param_name)\n            elif param.kind is _VAR_POSITIONAL:\n                new_params.pop(param.name)\n\n    return wrapped_sig.replace(parameters=new_params.values())\n\n\ndef _signature_bound_method(sig):\n    \"\"\"Private helper to transform signatures for unbound\n    functions to bound methods.\n    \"\"\"\n\n    params = tuple(sig.parameters.values())\n\n    if not params or params[0].kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n        raise ValueError('invalid method signature')\n\n    kind = params[0].kind\n    if kind in (_POSITIONAL_OR_KEYWORD, _POSITIONAL_ONLY):\n        # Drop first parameter:\n        # '(p1, p2[, ...])' -> '(p2[, ...])'\n        params = params[1:]\n    else:\n        if kind is not _VAR_POSITIONAL:\n            # Unless we add a new parameter type we never\n            # get here\n            raise ValueError('invalid argument type')\n        # It's a var-positional parameter.\n        # Do nothing. '(*args[, ...])' -> '(*args[, ...])'\n\n    return sig.replace(parameters=params)\n\n\ndef _signature_is_builtin(obj):\n    \"\"\"Private helper to test if `obj` is a callable that might\n    support Argument Clinic's __text_signature__ protocol.\n    \"\"\"\n    return (isbuiltin(obj) or\n            ismethoddescriptor(obj) or\n            isinstance(obj, _NonUserDefinedCallables) or\n            # Can't test 'isinstance(type)' here, as it would\n            # also be True for regular python classes\n            obj in (type, object))\n\n\ndef _signature_is_functionlike(obj):\n    \"\"\"Private helper to test if `obj` is a duck type of FunctionType.\n    A good example of such objects are functions compiled with\n    Cython, which have all attributes that a pure Python function\n    would have, but have their code statically compiled.\n    \"\"\"\n\n    if not callable(obj) or isclass(obj):\n        # All function-like objects are obviously callables,\n        # and not classes.\n        return False\n\n    name = getattr(obj, '__name__', None)\n    code = getattr(obj, '__code__', None)\n    defaults = getattr(obj, '__defaults__', _void) # Important to use _void ...\n    kwdefaults = getattr(obj, '__kwdefaults__', _void) # ... and not None here\n    annotations = getattr(obj, '__annotations__', None)\n\n    return (isinstance(code, types.CodeType) and\n            isinstance(name, str) and\n            (defaults is None or isinstance(defaults, tuple)) and\n            (kwdefaults is None or isinstance(kwdefaults, dict)) and\n            (isinstance(annotations, (dict)) or annotations is None) )\n\n\ndef _signature_strip_non_python_syntax(signature):\n    \"\"\"\n    Private helper function. Takes a signature in Argument Clinic's\n    extended signature format.\n\n    Returns a tuple of three things:\n      * that signature re-rendered in standard Python syntax,\n      * the index of the \"self\" parameter (generally 0), or None if\n        the function does not have a \"self\" parameter, and\n      * the index of the last \"positional only\" parameter,\n        or None if the signature has no positional-only parameters.\n    \"\"\"\n\n    if not signature:\n        return signature, None, None\n\n    self_parameter = None\n    last_positional_only = None\n\n    lines = [l.encode('ascii') for l in signature.split('\\n') if l]\n    generator = iter(lines).__next__\n    token_stream = tokenize.tokenize(generator)\n\n    delayed_comma = False\n    skip_next_comma = False\n    text = []\n    add = text.append\n\n    current_parameter = 0\n    OP = token.OP\n    ERRORTOKEN = token.ERRORTOKEN\n\n    # token stream always starts with ENCODING token, skip it\n    t = next(token_stream)\n    assert t.type == tokenize.ENCODING\n\n    for t in token_stream:\n        type, string = t.type, t.string\n\n        if type == OP:\n            if string == ',':\n                if skip_next_comma:\n                    skip_next_comma = False\n                else:\n                    assert not delayed_comma\n                    delayed_comma = True\n                    current_parameter += 1\n                continue\n\n            if string == '/':\n                assert not skip_next_comma\n                assert last_positional_only is None\n                skip_next_comma = True\n                last_positional_only = current_parameter - 1\n                continue\n\n        if (type == ERRORTOKEN) and (string == '$'):\n            assert self_parameter is None\n            self_parameter = current_parameter\n            continue\n\n        if delayed_comma:\n            delayed_comma = False\n            if not ((type == OP) and (string == ')')):\n                add(', ')\n        add(string)\n        if (string == ','):\n            add(' ')\n    clean_signature = ''.join(text)\n    return clean_signature, self_parameter, last_positional_only\n\n\ndef _signature_fromstr(cls, obj, s, skip_bound_arg=True):\n    \"\"\"Private helper to parse content of '__text_signature__'\n    and return a Signature based on it.\n    \"\"\"\n    Parameter = cls._parameter_cls\n\n    clean_signature, self_parameter, last_positional_only = \\\n        _signature_strip_non_python_syntax(s)\n\n    program = \"def foo\" + clean_signature + \": pass\"\n\n    try:\n        module = ast.parse(program)\n    except SyntaxError:\n        module = None\n\n    if not isinstance(module, ast.Module):\n        raise ValueError(\"{!r} builtin has invalid signature\".format(obj))\n\n    f = module.body[0]\n\n    parameters = []\n    empty = Parameter.empty\n\n    module = None\n    module_dict = {}\n    module_name = getattr(obj, '__module__', None)\n    if module_name:\n        module = sys.modules.get(module_name, None)\n        if module:\n            module_dict = module.__dict__\n    sys_module_dict = sys.modules.copy()\n\n    def parse_name(node):\n        assert isinstance(node, ast.arg)\n        if node.annotation is not None:\n            raise ValueError(\"Annotations are not currently supported\")\n        return node.arg\n\n    def wrap_value(s):\n        try:\n            value = eval(s, module_dict)\n        except NameError:\n            try:\n                value = eval(s, sys_module_dict)\n            except NameError:\n                raise ValueError\n\n        if isinstance(value, (str, int, float, bytes, bool, type(None))):\n            return ast.Constant(value)\n        raise ValueError\n\n    class RewriteSymbolics(ast.NodeTransformer):\n        def visit_Attribute(self, node):\n            a = []\n            n = node\n            while isinstance(n, ast.Attribute):\n                a.append(n.attr)\n                n = n.value\n            if not isinstance(n, ast.Name):\n                raise ValueError\n            a.append(n.id)\n            value = \".\".join(reversed(a))\n            return wrap_value(value)\n\n        def visit_Name(self, node):\n            if not isinstance(node.ctx, ast.Load):\n                raise ValueError()\n            return wrap_value(node.id)\n\n        def visit_BinOp(self, node):\n            # Support constant folding of a couple simple binary operations\n            # commonly used to define default values in text signatures\n            left = self.visit(node.left)\n            right = self.visit(node.right)\n            if not isinstance(left, ast.Constant) or not isinstance(right, ast.Constant):\n                raise ValueError\n            if isinstance(node.op, ast.Add):\n                return ast.Constant(left.value + right.value)\n            elif isinstance(node.op, ast.Sub):\n                return ast.Constant(left.value - right.value)\n            elif isinstance(node.op, ast.BitOr):\n                return ast.Constant(left.value | right.value)\n            raise ValueError\n\n    def p(name_node, default_node, default=empty):\n        name = parse_name(name_node)\n        if default_node and default_node is not _empty:\n            try:\n                default_node = RewriteSymbolics().visit(default_node)\n                default = ast.literal_eval(default_node)\n            except ValueError:\n                raise ValueError(\"{!r} builtin has invalid signature\".format(obj)) from None\n        parameters.append(Parameter(name, kind, default=default, annotation=empty))\n\n    # non-keyword-only parameters\n    args = reversed(f.args.args)\n    defaults = reversed(f.args.defaults)\n    iter = itertools.zip_longest(args, defaults, fillvalue=None)\n    if last_positional_only is not None:\n        kind = Parameter.POSITIONAL_ONLY\n    else:\n        kind = Parameter.POSITIONAL_OR_KEYWORD\n    for i, (name, default) in enumerate(reversed(list(iter))):\n        p(name, default)\n        if i == last_positional_only:\n            kind = Parameter.POSITIONAL_OR_KEYWORD\n\n    # *args\n    if f.args.vararg:\n        kind = Parameter.VAR_POSITIONAL\n        p(f.args.vararg, empty)\n\n    # keyword-only arguments\n    kind = Parameter.KEYWORD_ONLY\n    for name, default in zip(f.args.kwonlyargs, f.args.kw_defaults):\n        p(name, default)\n\n    # **kwargs\n    if f.args.kwarg:\n        kind = Parameter.VAR_KEYWORD\n        p(f.args.kwarg, empty)\n\n    if self_parameter is not None:\n        # Possibly strip the bound argument:\n        #    - We *always* strip first bound argument if\n        #      it is a module.\n        #    - We don't strip first bound argument if\n        #      skip_bound_arg is False.\n        assert parameters\n        _self = getattr(obj, '__self__', None)\n        self_isbound = _self is not None\n        self_ismodule = ismodule(_self)\n        if self_isbound and (self_ismodule or skip_bound_arg):\n            parameters.pop(0)\n        else:\n            # for builtins, self parameter is always positional-only!\n            p = parameters[0].replace(kind=Parameter.POSITIONAL_ONLY)\n            parameters[0] = p\n\n    return cls(parameters, return_annotation=cls.empty)\n\n\ndef _signature_from_builtin(cls, func, skip_bound_arg=True):\n    \"\"\"Private helper function to get signature for\n    builtin callables.\n    \"\"\"\n\n    if not _signature_is_builtin(func):\n        raise TypeError(\"{!r} is not a Python builtin \"\n                        \"function\".format(func))\n\n    s = getattr(func, \"__text_signature__\", None)\n    if not s:\n        raise ValueError(\"no signature found for builtin {!r}\".format(func))\n\n    return _signature_fromstr(cls, func, s, skip_bound_arg)\n\n\ndef _signature_from_function(cls, func, skip_bound_arg=True,\n                             globals=None, locals=None, eval_str=False):\n    \"\"\"Private helper: constructs Signature for the given python function.\"\"\"\n\n    is_duck_function = False\n    if not isfunction(func):\n        if _signature_is_functionlike(func):\n            is_duck_function = True\n        else:\n            # If it's not a pure Python function, and not a duck type\n            # of pure function:\n            raise TypeError('{!r} is not a Python function'.format(func))\n\n    s = getattr(func, \"__text_signature__\", None)\n    if s:\n        return _signature_fromstr(cls, func, s, skip_bound_arg)\n\n    Parameter = cls._parameter_cls\n\n    # Parameter information.\n    func_code = func.__code__\n    pos_count = func_code.co_argcount\n    arg_names = func_code.co_varnames\n    posonly_count = func_code.co_posonlyargcount\n    positional = arg_names[:pos_count]\n    keyword_only_count = func_code.co_kwonlyargcount\n    keyword_only = arg_names[pos_count:pos_count + keyword_only_count]\n    annotations = get_annotations(func, globals=globals, locals=locals, eval_str=eval_str)\n    defaults = func.__defaults__\n    kwdefaults = func.__kwdefaults__\n\n    if defaults:\n        pos_default_count = len(defaults)\n    else:\n        pos_default_count = 0\n\n    parameters = []\n\n    non_default_count = pos_count - pos_default_count\n    posonly_left = posonly_count\n\n    # Non-keyword-only parameters w/o defaults.\n    for name in positional[:non_default_count]:\n        kind = _POSITIONAL_ONLY if posonly_left else _POSITIONAL_OR_KEYWORD\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=kind))\n        if posonly_left:\n            posonly_left -= 1\n\n    # ... w/ defaults.\n    for offset, name in enumerate(positional[non_default_count:]):\n        kind = _POSITIONAL_ONLY if posonly_left else _POSITIONAL_OR_KEYWORD\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=kind,\n                                    default=defaults[offset]))\n        if posonly_left:\n            posonly_left -= 1\n\n    # *args\n    if func_code.co_flags & CO_VARARGS:\n        name = arg_names[pos_count + keyword_only_count]\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=_VAR_POSITIONAL))\n\n    # Keyword-only parameters.\n    for name in keyword_only:\n        default = _empty\n        if kwdefaults is not None:\n            default = kwdefaults.get(name, _empty)\n\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=_KEYWORD_ONLY,\n                                    default=default))\n    # **kwargs\n    if func_code.co_flags & CO_VARKEYWORDS:\n        index = pos_count + keyword_only_count\n        if func_code.co_flags & CO_VARARGS:\n            index += 1\n\n        name = arg_names[index]\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=_VAR_KEYWORD))\n\n    # Is 'func' is a pure Python function - don't validate the\n    # parameters list (for correct order and defaults), it should be OK.\n    return cls(parameters,\n               return_annotation=annotations.get('return', _empty),\n               __validate_parameters__=is_duck_function)\n\n\ndef _descriptor_get(descriptor, obj):\n    if isclass(descriptor):\n        return descriptor\n    get = getattr(type(descriptor), '__get__', _sentinel)\n    if get is _sentinel:\n        return descriptor\n    return get(descriptor, obj, type(obj))\n\n\ndef _signature_from_callable(obj, *,\n                             follow_wrapper_chains=True,\n                             skip_bound_arg=True,\n                             globals=None,\n                             locals=None,\n                             eval_str=False,\n                             sigcls):\n\n    \"\"\"Private helper function to get signature for arbitrary\n    callable objects.\n    \"\"\"\n\n    _get_signature_of = functools.partial(_signature_from_callable,\n                                follow_wrapper_chains=follow_wrapper_chains,\n                                skip_bound_arg=skip_bound_arg,\n                                globals=globals,\n                                locals=locals,\n                                sigcls=sigcls,\n                                eval_str=eval_str)\n\n    if not callable(obj):\n        raise TypeError('{!r} is not a callable object'.format(obj))\n\n    if isinstance(obj, types.MethodType):\n        # In this case we skip the first parameter of the underlying\n        # function (usually `self` or `cls`).\n        sig = _get_signature_of(obj.__func__)\n\n        if skip_bound_arg:\n            return _signature_bound_method(sig)\n        else:\n            return sig\n\n    # Was this function wrapped by a decorator?\n    if follow_wrapper_chains:\n        # Unwrap until we find an explicit signature or a MethodType (which will be\n        # handled explicitly below).\n        obj = unwrap(obj, stop=(lambda f: hasattr(f, \"__signature__\")\n                                or isinstance(f, types.MethodType)))\n        if isinstance(obj, types.MethodType):\n            # If the unwrapped object is a *method*, we might want to\n            # skip its first parameter (self).\n            # See test_signature_wrapped_bound_method for details.\n            return _get_signature_of(obj)\n\n    try:\n        sig = obj.__signature__\n    except AttributeError:\n        pass\n    else:\n        if sig is not None:\n            if not isinstance(sig, Signature):\n                raise TypeError(\n                    'unexpected object {!r} in __signature__ '\n                    'attribute'.format(sig))\n            return sig\n\n    try:\n        partialmethod = obj._partialmethod\n    except AttributeError:\n        pass\n    else:\n        if isinstance(partialmethod, functools.partialmethod):\n            # Unbound partialmethod (see functools.partialmethod)\n            # This means, that we need to calculate the signature\n            # as if it's a regular partial object, but taking into\n            # account that the first positional argument\n            # (usually `self`, or `cls`) will not be passed\n            # automatically (as for boundmethods)\n\n            wrapped_sig = _get_signature_of(partialmethod.func)\n\n            sig = _signature_get_partial(wrapped_sig, partialmethod, (None,))\n            first_wrapped_param = tuple(wrapped_sig.parameters.values())[0]\n            if first_wrapped_param.kind is Parameter.VAR_POSITIONAL:\n                # First argument of the wrapped callable is `*args`, as in\n                # `partialmethod(lambda *args)`.\n                return sig\n            else:\n                sig_params = tuple(sig.parameters.values())\n                assert (not sig_params or\n                        first_wrapped_param is not sig_params[0])\n                new_params = (first_wrapped_param,) + sig_params\n                return sig.replace(parameters=new_params)\n\n    if isfunction(obj) or _signature_is_functionlike(obj):\n        # If it's a pure Python function, or an object that is duck type\n        # of a Python function (Cython functions, for instance), then:\n        return _signature_from_function(sigcls, obj,\n                                        skip_bound_arg=skip_bound_arg,\n                                        globals=globals, locals=locals, eval_str=eval_str)\n\n    if _signature_is_builtin(obj):\n        return _signature_from_builtin(sigcls, obj,\n                                       skip_bound_arg=skip_bound_arg)\n\n    if isinstance(obj, functools.partial):\n        wrapped_sig = _get_signature_of(obj.func)\n        return _signature_get_partial(wrapped_sig, obj)\n\n    if isinstance(obj, type):\n        # obj is a class or a metaclass\n\n        # First, let's see if it has an overloaded __call__ defined\n        # in its metaclass\n        call = _signature_get_user_defined_method(type(obj), '__call__')\n        if call is not None:\n            return _get_signature_of(call)\n\n        new = _signature_get_user_defined_method(obj, '__new__')\n        init = _signature_get_user_defined_method(obj, '__init__')\n\n        # Go through the MRO and see if any class has user-defined\n        # pure Python __new__ or __init__ method\n        for base in obj.__mro__:\n            # Now we check if the 'obj' class has an own '__new__' method\n            if new is not None and '__new__' in base.__dict__:\n                sig = _get_signature_of(new)\n                if skip_bound_arg:\n                    sig = _signature_bound_method(sig)\n                return sig\n            # or an own '__init__' method\n            elif init is not None and '__init__' in base.__dict__:\n                return _get_signature_of(init)\n\n        # At this point we know, that `obj` is a class, with no user-\n        # defined '__init__', '__new__', or class-level '__call__'\n\n        for base in obj.__mro__[:-1]:\n            # Since '__text_signature__' is implemented as a\n            # descriptor that extracts text signature from the\n            # class docstring, if 'obj' is derived from a builtin\n            # class, its own '__text_signature__' may be 'None'.\n            # Therefore, we go through the MRO (except the last\n            # class in there, which is 'object') to find the first\n            # class with non-empty text signature.\n            try:\n                text_sig = base.__text_signature__\n            except AttributeError:\n                pass\n            else:\n                if text_sig:\n                    # If 'base' class has a __text_signature__ attribute:\n                    # return a signature based on it\n                    return _signature_fromstr(sigcls, base, text_sig)\n\n        # No '__text_signature__' was found for the 'obj' class.\n        # Last option is to check if its '__init__' is\n        # object.__init__ or type.__init__.\n        if type not in obj.__mro__:\n            # We have a class (not metaclass), but no user-defined\n            # __init__ or __new__ for it\n            if (obj.__init__ is object.__init__ and\n                obj.__new__ is object.__new__):\n                # Return a signature of 'object' builtin.\n                return sigcls.from_callable(object)\n            else:\n                raise ValueError(\n                    'no signature found for builtin type {!r}'.format(obj))\n\n    else:\n        # An object with __call__\n        call = getattr_static(type(obj), '__call__', None)\n        if call is not None:\n            call = _descriptor_get(call, obj)\n            return _get_signature_of(call)\n\n    raise ValueError('callable {!r} is not supported by signature'.format(obj))\n\n\nclass _void:\n    \"\"\"A private marker - used in Parameter & Signature.\"\"\"\n\n\nclass _empty:\n    \"\"\"Marker object for Signature.empty and Parameter.empty.\"\"\"\n\n\nclass _ParameterKind(enum.IntEnum):\n    POSITIONAL_ONLY = 'positional-only'\n    POSITIONAL_OR_KEYWORD = 'positional or keyword'\n    VAR_POSITIONAL = 'variadic positional'\n    KEYWORD_ONLY = 'keyword-only'\n    VAR_KEYWORD = 'variadic keyword'\n\n    def __new__(cls, description):\n        value = len(cls.__members__)\n        member = int.__new__(cls, value)\n        member._value_ = value\n        member.description = description\n        return member\n\n    def __str__(self):\n        return self.name\n\n_POSITIONAL_ONLY         = _ParameterKind.POSITIONAL_ONLY\n_POSITIONAL_OR_KEYWORD   = _ParameterKind.POSITIONAL_OR_KEYWORD\n_VAR_POSITIONAL          = _ParameterKind.VAR_POSITIONAL\n_KEYWORD_ONLY            = _ParameterKind.KEYWORD_ONLY\n_VAR_KEYWORD             = _ParameterKind.VAR_KEYWORD\n\n\nclass Parameter:\n    \"\"\"Represents a parameter in a function signature.\n\n    Has the following public attributes:\n\n    * name : str\n        The name of the parameter as a string.\n    * default : object\n        The default value for the parameter if specified.  If the\n        parameter has no default value, this attribute is set to\n        `Parameter.empty`.\n    * annotation\n        The annotation for the parameter if specified.  If the\n        parameter has no annotation, this attribute is set to\n        `Parameter.empty`.\n    * kind : str\n        Describes how argument values are bound to the parameter.\n        Possible values: `Parameter.POSITIONAL_ONLY`,\n        `Parameter.POSITIONAL_OR_KEYWORD`, `Parameter.VAR_POSITIONAL`,\n        `Parameter.KEYWORD_ONLY`, `Parameter.VAR_KEYWORD`.\n    \"\"\"\n\n    __slots__ = ('_name', '_kind', '_default', '_annotation')\n\n    POSITIONAL_ONLY         = _POSITIONAL_ONLY\n    POSITIONAL_OR_KEYWORD   = _POSITIONAL_OR_KEYWORD\n    VAR_POSITIONAL          = _VAR_POSITIONAL\n    KEYWORD_ONLY            = _KEYWORD_ONLY\n    VAR_KEYWORD             = _VAR_KEYWORD\n\n    empty = _empty\n\n    def __init__(self, name, kind, *, default=_empty, annotation=_empty):\n        try:\n            self._kind = _ParameterKind(kind)\n        except ValueError:\n            raise ValueError(f'value {kind!r} is not a valid Parameter.kind')\n        if default is not _empty:\n            if self._kind in (_VAR_POSITIONAL, _VAR_KEYWORD):\n                msg = '{} parameters cannot have default values'\n                msg = msg.format(self._kind.description)\n                raise ValueError(msg)\n        self._default = default\n        self._annotation = annotation\n\n        if name is _empty:\n            raise ValueError('name is a required attribute for Parameter')\n\n        if not isinstance(name, str):\n            msg = 'name must be a str, not a {}'.format(type(name).__name__)\n            raise TypeError(msg)\n\n        if name[0] == '.' and name[1:].isdigit():\n            # These are implicit arguments generated by comprehensions. In\n            # order to provide a friendlier interface to users, we recast\n            # their name as \"implicitN\" and treat them as positional-only.\n            # See issue 19611.\n            if self._kind != _POSITIONAL_OR_KEYWORD:\n                msg = (\n                    'implicit arguments must be passed as '\n                    'positional or keyword arguments, not {}'\n                )\n                msg = msg.format(self._kind.description)\n                raise ValueError(msg)\n            self._kind = _POSITIONAL_ONLY\n            name = 'implicit{}'.format(name[1:])\n\n        # It's possible for C functions to have a positional-only parameter\n        # where the name is a keyword, so for compatibility we'll allow it.\n        is_keyword = iskeyword(name) and self._kind is not _POSITIONAL_ONLY\n        if is_keyword or not name.isidentifier():\n            raise ValueError('{!r} is not a valid parameter name'.format(name))\n\n        self._name = name\n\n    def __reduce__(self):\n        return (type(self),\n                (self._name, self._kind),\n                {'_default': self._default,\n                 '_annotation': self._annotation})\n\n    def __setstate__(self, state):\n        self._default = state['_default']\n        self._annotation = state['_annotation']\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def default(self):\n        return self._default\n\n    @property\n    def annotation(self):\n        return self._annotation\n\n    @property\n    def kind(self):\n        return self._kind\n\n    def replace(self, *, name=_void, kind=_void,\n                annotation=_void, default=_void):\n        \"\"\"Creates a customized copy of the Parameter.\"\"\"\n\n        if name is _void:\n            name = self._name\n\n        if kind is _void:\n            kind = self._kind\n\n        if annotation is _void:\n            annotation = self._annotation\n\n        if default is _void:\n            default = self._default\n\n        return type(self)(name, kind, default=default, annotation=annotation)\n\n    def __str__(self):\n        kind = self.kind\n        formatted = self._name\n\n        # Add annotation and default value\n        if self._annotation is not _empty:\n            formatted = '{}: {}'.format(formatted,\n                                       formatannotation(self._annotation))\n\n        if self._default is not _empty:\n            if self._annotation is not _empty:\n                formatted = '{} = {}'.format(formatted, repr(self._default))\n            else:\n                formatted = '{}={}'.format(formatted, repr(self._default))\n\n        if kind == _VAR_POSITIONAL:\n            formatted = '*' + formatted\n        elif kind == _VAR_KEYWORD:\n            formatted = '**' + formatted\n\n        return formatted\n\n    def __repr__(self):\n        return '<{} \"{}\">'.format(self.__class__.__name__, self)\n\n    def __hash__(self):\n        return hash((self.name, self.kind, self.annotation, self.default))\n\n    def __eq__(self, other):\n        if self is other:\n            return True\n        if not isinstance(other, Parameter):\n            return NotImplemented\n        return (self._name == other._name and\n                self._kind == other._kind and\n                self._default == other._default and\n                self._annotation == other._annotation)\n\n\nclass BoundArguments:\n    \"\"\"Result of `Signature.bind` call.  Holds the mapping of arguments\n    to the function's parameters.\n\n    Has the following public attributes:\n\n    * arguments : dict\n        An ordered mutable mapping of parameters' names to arguments' values.\n        Does not contain arguments' default values.\n    * signature : Signature\n        The Signature object that created this instance.\n    * args : tuple\n        Tuple of positional arguments values.\n    * kwargs : dict\n        Dict of keyword arguments values.\n    \"\"\"\n\n    __slots__ = ('arguments', '_signature', '__weakref__')\n\n    def __init__(self, signature, arguments):\n        self.arguments = arguments\n        self._signature = signature\n\n    @property\n    def signature(self):\n        return self._signature\n\n    @property\n    def args(self):\n        args = []\n        for param_name, param in self._signature.parameters.items():\n            if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n                break\n\n            try:\n                arg = self.arguments[param_name]\n            except KeyError:\n                # We're done here. Other arguments\n                # will be mapped in 'BoundArguments.kwargs'\n                break\n            else:\n                if param.kind == _VAR_POSITIONAL:\n                    # *args\n                    args.extend(arg)\n                else:\n                    # plain argument\n                    args.append(arg)\n\n        return tuple(args)\n\n    @property\n    def kwargs(self):\n        kwargs = {}\n        kwargs_started = False\n        for param_name, param in self._signature.parameters.items():\n            if not kwargs_started:\n                if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n                    kwargs_started = True\n                else:\n                    if param_name not in self.arguments:\n                        kwargs_started = True\n                        continue\n\n            if not kwargs_started:\n                continue\n\n            try:\n                arg = self.arguments[param_name]\n            except KeyError:\n                pass\n            else:\n                if param.kind == _VAR_KEYWORD:\n                    # **kwargs\n                    kwargs.update(arg)\n                else:\n                    # plain keyword argument\n                    kwargs[param_name] = arg\n\n        return kwargs\n\n    def apply_defaults(self):\n        \"\"\"Set default values for missing arguments.\n\n        For variable-positional arguments (*args) the default is an\n        empty tuple.\n\n        For variable-keyword arguments (**kwargs) the default is an\n        empty dict.\n        \"\"\"\n        arguments = self.arguments\n        new_arguments = []\n        for name, param in self._signature.parameters.items():\n            try:\n                new_arguments.append((name, arguments[name]))\n            except KeyError:\n                if param.default is not _empty:\n                    val = param.default\n                elif param.kind is _VAR_POSITIONAL:\n                    val = ()\n                elif param.kind is _VAR_KEYWORD:\n                    val = {}\n                else:\n                    # This BoundArguments was likely produced by\n                    # Signature.bind_partial().\n                    continue\n                new_arguments.append((name, val))\n        self.arguments = dict(new_arguments)\n\n    def __eq__(self, other):\n        if self is other:\n            return True\n        if not isinstance(other, BoundArguments):\n            return NotImplemented\n        return (self.signature == other.signature and\n                self.arguments == other.arguments)\n\n    def __setstate__(self, state):\n        self._signature = state['_signature']\n        self.arguments = state['arguments']\n\n    def __getstate__(self):\n        return {'_signature': self._signature, 'arguments': self.arguments}\n\n    def __repr__(self):\n        args = []\n        for arg, value in self.arguments.items():\n            args.append('{}={!r}'.format(arg, value))\n        return '<{} ({})>'.format(self.__class__.__name__, ', '.join(args))\n\n\nclass Signature:\n    \"\"\"A Signature object represents the overall signature of a function.\n    It stores a Parameter object for each parameter accepted by the\n    function, as well as information specific to the function itself.\n\n    A Signature object has the following public attributes and methods:\n\n    * parameters : OrderedDict\n        An ordered mapping of parameters' names to the corresponding\n        Parameter objects (keyword-only arguments are in the same order\n        as listed in `code.co_varnames`).\n    * return_annotation : object\n        The annotation for the return type of the function if specified.\n        If the function has no annotation for its return type, this\n        attribute is set to `Signature.empty`.\n    * bind(*args, **kwargs) -> BoundArguments\n        Creates a mapping from positional and keyword arguments to\n        parameters.\n    * bind_partial(*args, **kwargs) -> BoundArguments\n        Creates a partial mapping from positional and keyword arguments\n        to parameters (simulating 'functools.partial' behavior.)\n    \"\"\"\n\n    __slots__ = ('_return_annotation', '_parameters')\n\n    _parameter_cls = Parameter\n    _bound_arguments_cls = BoundArguments\n\n    empty = _empty\n\n    def __init__(self, parameters=None, *, return_annotation=_empty,\n                 __validate_parameters__=True):\n        \"\"\"Constructs Signature from the given list of Parameter\n        objects and 'return_annotation'.  All arguments are optional.\n        \"\"\"\n\n        if parameters is None:\n            params = OrderedDict()\n        else:\n            if __validate_parameters__:\n                params = OrderedDict()\n                top_kind = _POSITIONAL_ONLY\n                seen_default = False\n\n                for param in parameters:\n                    kind = param.kind\n                    name = param.name\n\n                    if kind < top_kind:\n                        msg = (\n                            'wrong parameter order: {} parameter before {} '\n                            'parameter'\n                        )\n                        msg = msg.format(top_kind.description,\n                                         kind.description)\n                        raise ValueError(msg)\n                    elif kind > top_kind:\n                        top_kind = kind\n\n                    if kind in (_POSITIONAL_ONLY, _POSITIONAL_OR_KEYWORD):\n                        if param.default is _empty:\n                            if seen_default:\n                                # No default for this parameter, but the\n                                # previous parameter of had a default\n                                msg = 'non-default argument follows default ' \\\n                                      'argument'\n                                raise ValueError(msg)\n                        else:\n                            # There is a default for this parameter.\n                            seen_default = True\n\n                    if name in params:\n                        msg = 'duplicate parameter name: {!r}'.format(name)\n                        raise ValueError(msg)\n\n                    params[name] = param\n            else:\n                params = OrderedDict((param.name, param) for param in parameters)\n\n        self._parameters = types.MappingProxyType(params)\n        self._return_annotation = return_annotation\n\n    @classmethod\n    def from_callable(cls, obj, *,\n                      follow_wrapped=True, globals=None, locals=None, eval_str=False):\n        \"\"\"Constructs Signature for the given callable object.\"\"\"\n        return _signature_from_callable(obj, sigcls=cls,\n                                        follow_wrapper_chains=follow_wrapped,\n                                        globals=globals, locals=locals, eval_str=eval_str)\n\n    @property\n    def parameters(self):\n        return self._parameters\n\n    @property\n    def return_annotation(self):\n        return self._return_annotation\n\n    def replace(self, *, parameters=_void, return_annotation=_void):\n        \"\"\"Creates a customized copy of the Signature.\n        Pass 'parameters' and/or 'return_annotation' arguments\n        to override them in the new copy.\n        \"\"\"\n\n        if parameters is _void:\n            parameters = self.parameters.values()\n\n        if return_annotation is _void:\n            return_annotation = self._return_annotation\n\n        return type(self)(parameters,\n                          return_annotation=return_annotation)\n\n    def _hash_basis(self):\n        params = tuple(param for param in self.parameters.values()\n                             if param.kind != _KEYWORD_ONLY)\n\n        kwo_params = {param.name: param for param in self.parameters.values()\n                                        if param.kind == _KEYWORD_ONLY}\n\n        return params, kwo_params, self.return_annotation\n\n    def __hash__(self):\n        params, kwo_params, return_annotation = self._hash_basis()\n        kwo_params = frozenset(kwo_params.values())\n        return hash((params, kwo_params, return_annotation))\n\n    def __eq__(self, other):\n        if self is other:\n            return True\n        if not isinstance(other, Signature):\n            return NotImplemented\n        return self._hash_basis() == other._hash_basis()\n\n    def _bind(self, args, kwargs, *, partial=False):\n        \"\"\"Private method. Don't use directly.\"\"\"\n\n        arguments = {}\n\n        parameters = iter(self.parameters.values())\n        parameters_ex = ()\n        arg_vals = iter(args)\n\n        while True:\n            # Let's iterate through the positional arguments and corresponding\n            # parameters\n            try:\n                arg_val = next(arg_vals)\n            except StopIteration:\n                # No more positional arguments\n                try:\n                    param = next(parameters)\n                except StopIteration:\n                    # No more parameters. That's it. Just need to check that\n                    # we have no `kwargs` after this while loop\n                    break\n                else:\n                    if param.kind == _VAR_POSITIONAL:\n                        # That's OK, just empty *args.  Let's start parsing\n                        # kwargs\n                        break\n                    elif param.name in kwargs:\n                        if param.kind == _POSITIONAL_ONLY:\n                            msg = '{arg!r} parameter is positional only, ' \\\n                                  'but was passed as a keyword'\n                            msg = msg.format(arg=param.name)\n                            raise TypeError(msg) from None\n                        parameters_ex = (param,)\n                        break\n                    elif (param.kind == _VAR_KEYWORD or\n                                                param.default is not _empty):\n                        # That's fine too - we have a default value for this\n                        # parameter.  So, lets start parsing `kwargs`, starting\n                        # with the current parameter\n                        parameters_ex = (param,)\n                        break\n                    else:\n                        # No default, not VAR_KEYWORD, not VAR_POSITIONAL,\n                        # not in `kwargs`\n                        if partial:\n                            parameters_ex = (param,)\n                            break\n                        else:\n                            msg = 'missing a required argument: {arg!r}'\n                            msg = msg.format(arg=param.name)\n                            raise TypeError(msg) from None\n            else:\n                # We have a positional argument to process\n                try:\n                    param = next(parameters)\n                except StopIteration:\n                    raise TypeError('too many positional arguments') from None\n                else:\n                    if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n                        # Looks like we have no parameter for this positional\n                        # argument\n                        raise TypeError(\n                            'too many positional arguments') from None\n\n                    if param.kind == _VAR_POSITIONAL:\n                        # We have an '*args'-like argument, let's fill it with\n                        # all positional arguments we have left and move on to\n                        # the next phase\n                        values = [arg_val]\n                        values.extend(arg_vals)\n                        arguments[param.name] = tuple(values)\n                        break\n\n                    if param.name in kwargs and param.kind != _POSITIONAL_ONLY:\n                        raise TypeError(\n                            'multiple values for argument {arg!r}'.format(\n                                arg=param.name)) from None\n\n                    arguments[param.name] = arg_val\n\n        # Now, we iterate through the remaining parameters to process\n        # keyword arguments\n        kwargs_param = None\n        for param in itertools.chain(parameters_ex, parameters):\n            if param.kind == _VAR_KEYWORD:\n                # Memorize that we have a '**kwargs'-like parameter\n                kwargs_param = param\n                continue\n\n            if param.kind == _VAR_POSITIONAL:\n                # Named arguments don't refer to '*args'-like parameters.\n                # We only arrive here if the positional arguments ended\n                # before reaching the last parameter before *args.\n                continue\n\n            param_name = param.name\n            try:\n                arg_val = kwargs.pop(param_name)\n            except KeyError:\n                # We have no value for this parameter.  It's fine though,\n                # if it has a default value, or it is an '*args'-like\n                # parameter, left alone by the processing of positional\n                # arguments.\n                if (not partial and param.kind != _VAR_POSITIONAL and\n                                                    param.default is _empty):\n                    raise TypeError('missing a required argument: {arg!r}'. \\\n                                    format(arg=param_name)) from None\n\n            else:\n                if param.kind == _POSITIONAL_ONLY:\n                    # This should never happen in case of a properly built\n                    # Signature object (but let's have this check here\n                    # to ensure correct behaviour just in case)\n                    raise TypeError('{arg!r} parameter is positional only, '\n                                    'but was passed as a keyword'. \\\n                                    format(arg=param.name))\n\n                arguments[param_name] = arg_val\n\n        if kwargs:\n            if kwargs_param is not None:\n                # Process our '**kwargs'-like parameter\n                arguments[kwargs_param.name] = kwargs\n            else:\n                raise TypeError(\n                    'got an unexpected keyword argument {arg!r}'.format(\n                        arg=next(iter(kwargs))))\n\n        return self._bound_arguments_cls(self, arguments)\n\n    def bind(self, /, *args, **kwargs):\n        \"\"\"Get a BoundArguments object, that maps the passed `args`\n        and `kwargs` to the function's signature.  Raises `TypeError`\n        if the passed arguments can not be bound.\n        \"\"\"\n        return self._bind(args, kwargs)\n\n    def bind_partial(self, /, *args, **kwargs):\n        \"\"\"Get a BoundArguments object, that partially maps the\n        passed `args` and `kwargs` to the function's signature.\n        Raises `TypeError` if the passed arguments can not be bound.\n        \"\"\"\n        return self._bind(args, kwargs, partial=True)\n\n    def __reduce__(self):\n        return (type(self),\n                (tuple(self._parameters.values()),),\n                {'_return_annotation': self._return_annotation})\n\n    def __setstate__(self, state):\n        self._return_annotation = state['_return_annotation']\n\n    def __repr__(self):\n        return '<{} {}>'.format(self.__class__.__name__, self)\n\n    def __str__(self):\n        result = []\n        render_pos_only_separator = False\n        render_kw_only_separator = True\n        for param in self.parameters.values():\n            formatted = str(param)\n\n            kind = param.kind\n\n            if kind == _POSITIONAL_ONLY:\n                render_pos_only_separator = True\n            elif render_pos_only_separator:\n                # It's not a positional-only parameter, and the flag\n                # is set to 'True' (there were pos-only params before.)\n                result.append('/')\n                render_pos_only_separator = False\n\n            if kind == _VAR_POSITIONAL:\n                # OK, we have an '*args'-like parameter, so we won't need\n                # a '*' to separate keyword-only arguments\n                render_kw_only_separator = False\n            elif kind == _KEYWORD_ONLY and render_kw_only_separator:\n                # We have a keyword-only parameter to render and we haven't\n                # rendered an '*args'-like parameter before, so add a '*'\n                # separator to the parameters list (\"foo(arg1, *, arg2)\" case)\n                result.append('*')\n                # This condition should be only triggered once, so\n                # reset the flag\n                render_kw_only_separator = False\n\n            result.append(formatted)\n\n        if render_pos_only_separator:\n            # There were only positional-only parameters, hence the\n            # flag was not reset to 'False'\n            result.append('/')\n\n        rendered = '({})'.format(', '.join(result))\n\n        if self.return_annotation is not _empty:\n            anno = formatannotation(self.return_annotation)\n            rendered += ' -> {}'.format(anno)\n\n        return rendered\n\n\ndef signature(obj, *, follow_wrapped=True, globals=None, locals=None, eval_str=False):\n    \"\"\"Get a signature object for the passed callable.\"\"\"\n    return Signature.from_callable(obj, follow_wrapped=follow_wrapped,\n                                   globals=globals, locals=locals, eval_str=eval_str)\n\n\ndef _main():\n    \"\"\" Logic for inspecting an object given at command line \"\"\"\n    import argparse\n    import importlib\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        'object',\n         help=\"The object to be analysed. \"\n              \"It supports the 'module:qualname' syntax\")\n    parser.add_argument(\n        '-d', '--details', action='store_true',\n        help='Display info about the module rather than its source code')\n\n    args = parser.parse_args()\n\n    target = args.object\n    mod_name, has_attrs, attrs = target.partition(\":\")\n    try:\n        obj = module = importlib.import_module(mod_name)\n    except Exception as exc:\n        msg = \"Failed to import {} ({}: {})\".format(mod_name,\n                                                    type(exc).__name__,\n                                                    exc)\n        print(msg, file=sys.stderr)\n        sys.exit(2)\n\n    if has_attrs:\n        parts = attrs.split(\".\")\n        obj = module\n        for part in parts:\n            obj = getattr(obj, part)\n\n    if module.__name__ in sys.builtin_module_names:\n        print(\"Can't get info for builtin modules.\", file=sys.stderr)\n        sys.exit(1)\n\n    if args.details:\n        print('Target: {}'.format(target))\n        print('Origin: {}'.format(getsourcefile(module)))\n        print('Cached: {}'.format(module.__cached__))\n        if obj is module:\n            print('Loader: {}'.format(repr(module.__loader__)))\n            if hasattr(module, '__path__'):\n                print('Submodule search path: {}'.format(module.__path__))\n        else:\n            try:\n                __, lineno = findsource(obj)\n            except Exception:\n                pass\n            else:\n                print('Line: {}'.format(lineno))\n\n        print('\\n')\n    else:\n        print(getsource(obj))\n\n\nif __name__ == \"__main__\":\n    _main()\n", 3326], "/usr/local/lib/python3.11/functools.py": ["\"\"\"functools.py - Tools for working with functions and callable objects\n\"\"\"\n# Python module wrapper for _functools C module\n# to allow utilities written in Python to be added\n# to the functools module.\n# Written by Nick Coghlan <ncoghlan at gmail.com>,\n# Raymond Hettinger <python at rcn.com>,\n# and \u0141ukasz Langa <lukasz at langa.pl>.\n#   Copyright (C) 2006-2013 Python Software Foundation.\n# See C source code for _functools credits/copyright\n\n__all__ = ['update_wrapper', 'wraps', 'WRAPPER_ASSIGNMENTS', 'WRAPPER_UPDATES',\n           'total_ordering', 'cache', 'cmp_to_key', 'lru_cache', 'reduce',\n           'partial', 'partialmethod', 'singledispatch', 'singledispatchmethod',\n           'cached_property']\n\nfrom abc import get_cache_token\nfrom collections import namedtuple\n# import types, weakref  # Deferred to single_dispatch()\nfrom reprlib import recursive_repr\nfrom _thread import RLock\nfrom types import GenericAlias\n\n\n################################################################################\n### update_wrapper() and wraps() decorator\n################################################################################\n\n# update_wrapper() and wraps() are tools to help write\n# wrapper functions that can handle naive introspection\n\nWRAPPER_ASSIGNMENTS = ('__module__', '__name__', '__qualname__', '__doc__',\n                       '__annotations__')\nWRAPPER_UPDATES = ('__dict__',)\ndef update_wrapper(wrapper,\n                   wrapped,\n                   assigned = WRAPPER_ASSIGNMENTS,\n                   updated = WRAPPER_UPDATES):\n    \"\"\"Update a wrapper function to look like the wrapped function\n\n       wrapper is the function to be updated\n       wrapped is the original function\n       assigned is a tuple naming the attributes assigned directly\n       from the wrapped function to the wrapper function (defaults to\n       functools.WRAPPER_ASSIGNMENTS)\n       updated is a tuple naming the attributes of the wrapper that\n       are updated with the corresponding attribute from the wrapped\n       function (defaults to functools.WRAPPER_UPDATES)\n    \"\"\"\n    for attr in assigned:\n        try:\n            value = getattr(wrapped, attr)\n        except AttributeError:\n            pass\n        else:\n            setattr(wrapper, attr, value)\n    for attr in updated:\n        getattr(wrapper, attr).update(getattr(wrapped, attr, {}))\n    # Issue #17482: set __wrapped__ last so we don't inadvertently copy it\n    # from the wrapped function when updating __dict__\n    wrapper.__wrapped__ = wrapped\n    # Return the wrapper so this can be used as a decorator via partial()\n    return wrapper\n\ndef wraps(wrapped,\n          assigned = WRAPPER_ASSIGNMENTS,\n          updated = WRAPPER_UPDATES):\n    \"\"\"Decorator factory to apply update_wrapper() to a wrapper function\n\n       Returns a decorator that invokes update_wrapper() with the decorated\n       function as the wrapper argument and the arguments to wraps() as the\n       remaining arguments. Default arguments are as for update_wrapper().\n       This is a convenience function to simplify applying partial() to\n       update_wrapper().\n    \"\"\"\n    return partial(update_wrapper, wrapped=wrapped,\n                   assigned=assigned, updated=updated)\n\n\n################################################################################\n### total_ordering class decorator\n################################################################################\n\n# The total ordering functions all invoke the root magic method directly\n# rather than using the corresponding operator.  This avoids possible\n# infinite recursion that could occur when the operator dispatch logic\n# detects a NotImplemented result and then calls a reflected method.\n\ndef _gt_from_lt(self, other):\n    'Return a > b.  Computed by @total_ordering from (not a < b) and (a != b).'\n    op_result = type(self).__lt__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result and self != other\n\ndef _le_from_lt(self, other):\n    'Return a <= b.  Computed by @total_ordering from (a < b) or (a == b).'\n    op_result = type(self).__lt__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return op_result or self == other\n\ndef _ge_from_lt(self, other):\n    'Return a >= b.  Computed by @total_ordering from (not a < b).'\n    op_result = type(self).__lt__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result\n\ndef _ge_from_le(self, other):\n    'Return a >= b.  Computed by @total_ordering from (not a <= b) or (a == b).'\n    op_result = type(self).__le__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result or self == other\n\ndef _lt_from_le(self, other):\n    'Return a < b.  Computed by @total_ordering from (a <= b) and (a != b).'\n    op_result = type(self).__le__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return op_result and self != other\n\ndef _gt_from_le(self, other):\n    'Return a > b.  Computed by @total_ordering from (not a <= b).'\n    op_result = type(self).__le__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result\n\ndef _lt_from_gt(self, other):\n    'Return a < b.  Computed by @total_ordering from (not a > b) and (a != b).'\n    op_result = type(self).__gt__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result and self != other\n\ndef _ge_from_gt(self, other):\n    'Return a >= b.  Computed by @total_ordering from (a > b) or (a == b).'\n    op_result = type(self).__gt__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return op_result or self == other\n\ndef _le_from_gt(self, other):\n    'Return a <= b.  Computed by @total_ordering from (not a > b).'\n    op_result = type(self).__gt__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result\n\ndef _le_from_ge(self, other):\n    'Return a <= b.  Computed by @total_ordering from (not a >= b) or (a == b).'\n    op_result = type(self).__ge__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result or self == other\n\ndef _gt_from_ge(self, other):\n    'Return a > b.  Computed by @total_ordering from (a >= b) and (a != b).'\n    op_result = type(self).__ge__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return op_result and self != other\n\ndef _lt_from_ge(self, other):\n    'Return a < b.  Computed by @total_ordering from (not a >= b).'\n    op_result = type(self).__ge__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result\n\n_convert = {\n    '__lt__': [('__gt__', _gt_from_lt),\n               ('__le__', _le_from_lt),\n               ('__ge__', _ge_from_lt)],\n    '__le__': [('__ge__', _ge_from_le),\n               ('__lt__', _lt_from_le),\n               ('__gt__', _gt_from_le)],\n    '__gt__': [('__lt__', _lt_from_gt),\n               ('__ge__', _ge_from_gt),\n               ('__le__', _le_from_gt)],\n    '__ge__': [('__le__', _le_from_ge),\n               ('__gt__', _gt_from_ge),\n               ('__lt__', _lt_from_ge)]\n}\n\ndef total_ordering(cls):\n    \"\"\"Class decorator that fills in missing ordering methods\"\"\"\n    # Find user-defined comparisons (not those inherited from object).\n    roots = {op for op in _convert if getattr(cls, op, None) is not getattr(object, op, None)}\n    if not roots:\n        raise ValueError('must define at least one ordering operation: < > <= >=')\n    root = max(roots)       # prefer __lt__ to __le__ to __gt__ to __ge__\n    for opname, opfunc in _convert[root]:\n        if opname not in roots:\n            opfunc.__name__ = opname\n            setattr(cls, opname, opfunc)\n    return cls\n\n\n################################################################################\n### cmp_to_key() function converter\n################################################################################\n\ndef cmp_to_key(mycmp):\n    \"\"\"Convert a cmp= function into a key= function\"\"\"\n    class K(object):\n        __slots__ = ['obj']\n        def __init__(self, obj):\n            self.obj = obj\n        def __lt__(self, other):\n            return mycmp(self.obj, other.obj) < 0\n        def __gt__(self, other):\n            return mycmp(self.obj, other.obj) > 0\n        def __eq__(self, other):\n            return mycmp(self.obj, other.obj) == 0\n        def __le__(self, other):\n            return mycmp(self.obj, other.obj) <= 0\n        def __ge__(self, other):\n            return mycmp(self.obj, other.obj) >= 0\n        __hash__ = None\n    return K\n\ntry:\n    from _functools import cmp_to_key\nexcept ImportError:\n    pass\n\n\n################################################################################\n### reduce() sequence to a single item\n################################################################################\n\n_initial_missing = object()\n\ndef reduce(function, sequence, initial=_initial_missing):\n    \"\"\"\n    reduce(function, iterable[, initial]) -> value\n\n    Apply a function of two arguments cumulatively to the items of a sequence\n    or iterable, from left to right, so as to reduce the iterable to a single\n    value.  For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates\n    ((((1+2)+3)+4)+5).  If initial is present, it is placed before the items\n    of the iterable in the calculation, and serves as a default when the\n    iterable is empty.\n    \"\"\"\n\n    it = iter(sequence)\n\n    if initial is _initial_missing:\n        try:\n            value = next(it)\n        except StopIteration:\n            raise TypeError(\n                \"reduce() of empty iterable with no initial value\") from None\n    else:\n        value = initial\n\n    for element in it:\n        value = function(value, element)\n\n    return value\n\ntry:\n    from _functools import reduce\nexcept ImportError:\n    pass\n\n\n################################################################################\n### partial() argument application\n################################################################################\n\n# Purely functional, no descriptor behaviour\nclass partial:\n    \"\"\"New function with partial application of the given arguments\n    and keywords.\n    \"\"\"\n\n    __slots__ = \"func\", \"args\", \"keywords\", \"__dict__\", \"__weakref__\"\n\n    def __new__(cls, func, /, *args, **keywords):\n        if not callable(func):\n            raise TypeError(\"the first argument must be callable\")\n\n        if hasattr(func, \"func\"):\n            args = func.args + args\n            keywords = {**func.keywords, **keywords}\n            func = func.func\n\n        self = super(partial, cls).__new__(cls)\n\n        self.func = func\n        self.args = args\n        self.keywords = keywords\n        return self\n\n    def __call__(self, /, *args, **keywords):\n        keywords = {**self.keywords, **keywords}\n        return self.func(*self.args, *args, **keywords)\n\n    @recursive_repr()\n    def __repr__(self):\n        qualname = type(self).__qualname__\n        args = [repr(self.func)]\n        args.extend(repr(x) for x in self.args)\n        args.extend(f\"{k}={v!r}\" for (k, v) in self.keywords.items())\n        if type(self).__module__ == \"functools\":\n            return f\"functools.{qualname}({', '.join(args)})\"\n        return f\"{qualname}({', '.join(args)})\"\n\n    def __reduce__(self):\n        return type(self), (self.func,), (self.func, self.args,\n               self.keywords or None, self.__dict__ or None)\n\n    def __setstate__(self, state):\n        if not isinstance(state, tuple):\n            raise TypeError(\"argument to __setstate__ must be a tuple\")\n        if len(state) != 4:\n            raise TypeError(f\"expected 4 items in state, got {len(state)}\")\n        func, args, kwds, namespace = state\n        if (not callable(func) or not isinstance(args, tuple) or\n           (kwds is not None and not isinstance(kwds, dict)) or\n           (namespace is not None and not isinstance(namespace, dict))):\n            raise TypeError(\"invalid partial state\")\n\n        args = tuple(args) # just in case it's a subclass\n        if kwds is None:\n            kwds = {}\n        elif type(kwds) is not dict: # XXX does it need to be *exactly* dict?\n            kwds = dict(kwds)\n        if namespace is None:\n            namespace = {}\n\n        self.__dict__ = namespace\n        self.func = func\n        self.args = args\n        self.keywords = kwds\n\ntry:\n    from _functools import partial\nexcept ImportError:\n    pass\n\n# Descriptor version\nclass partialmethod(object):\n    \"\"\"Method descriptor with partial application of the given arguments\n    and keywords.\n\n    Supports wrapping existing descriptors and handles non-descriptor\n    callables as instance methods.\n    \"\"\"\n\n    def __init__(self, func, /, *args, **keywords):\n        if not callable(func) and not hasattr(func, \"__get__\"):\n            raise TypeError(\"{!r} is not callable or a descriptor\"\n                                 .format(func))\n\n        # func could be a descriptor like classmethod which isn't callable,\n        # so we can't inherit from partial (it verifies func is callable)\n        if isinstance(func, partialmethod):\n            # flattening is mandatory in order to place cls/self before all\n            # other arguments\n            # it's also more efficient since only one function will be called\n            self.func = func.func\n            self.args = func.args + args\n            self.keywords = {**func.keywords, **keywords}\n        else:\n            self.func = func\n            self.args = args\n            self.keywords = keywords\n\n    def __repr__(self):\n        args = \", \".join(map(repr, self.args))\n        keywords = \", \".join(\"{}={!r}\".format(k, v)\n                                 for k, v in self.keywords.items())\n        format_string = \"{module}.{cls}({func}, {args}, {keywords})\"\n        return format_string.format(module=self.__class__.__module__,\n                                    cls=self.__class__.__qualname__,\n                                    func=self.func,\n                                    args=args,\n                                    keywords=keywords)\n\n    def _make_unbound_method(self):\n        def _method(cls_or_self, /, *args, **keywords):\n            keywords = {**self.keywords, **keywords}\n            return self.func(cls_or_self, *self.args, *args, **keywords)\n        _method.__isabstractmethod__ = self.__isabstractmethod__\n        _method._partialmethod = self\n        return _method\n\n    def __get__(self, obj, cls=None):\n        get = getattr(self.func, \"__get__\", None)\n        result = None\n        if get is not None:\n            new_func = get(obj, cls)\n            if new_func is not self.func:\n                # Assume __get__ returning something new indicates the\n                # creation of an appropriate callable\n                result = partial(new_func, *self.args, **self.keywords)\n                try:\n                    result.__self__ = new_func.__self__\n                except AttributeError:\n                    pass\n        if result is None:\n            # If the underlying descriptor didn't do anything, treat this\n            # like an instance method\n            result = self._make_unbound_method().__get__(obj, cls)\n        return result\n\n    @property\n    def __isabstractmethod__(self):\n        return getattr(self.func, \"__isabstractmethod__\", False)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\n# Helper functions\n\ndef _unwrap_partial(func):\n    while isinstance(func, partial):\n        func = func.func\n    return func\n\n################################################################################\n### LRU Cache function decorator\n################################################################################\n\n_CacheInfo = namedtuple(\"CacheInfo\", [\"hits\", \"misses\", \"maxsize\", \"currsize\"])\n\nclass _HashedSeq(list):\n    \"\"\" This class guarantees that hash() will be called no more than once\n        per element.  This is important because the lru_cache() will hash\n        the key multiple times on a cache miss.\n\n    \"\"\"\n\n    __slots__ = 'hashvalue'\n\n    def __init__(self, tup, hash=hash):\n        self[:] = tup\n        self.hashvalue = hash(tup)\n\n    def __hash__(self):\n        return self.hashvalue\n\ndef _make_key(args, kwds, typed,\n             kwd_mark = (object(),),\n             fasttypes = {int, str},\n             tuple=tuple, type=type, len=len):\n    \"\"\"Make a cache key from optionally typed positional and keyword arguments\n\n    The key is constructed in a way that is flat as possible rather than\n    as a nested structure that would take more memory.\n\n    If there is only a single argument and its data type is known to cache\n    its hash value, then that argument is returned without a wrapper.  This\n    saves space and improves lookup speed.\n\n    \"\"\"\n    # All of code below relies on kwds preserving the order input by the user.\n    # Formerly, we sorted() the kwds before looping.  The new way is *much*\n    # faster; however, it means that f(x=1, y=2) will now be treated as a\n    # distinct call from f(y=2, x=1) which will be cached separately.\n    key = args\n    if kwds:\n        key += kwd_mark\n        for item in kwds.items():\n            key += item\n    if typed:\n        key += tuple(type(v) for v in args)\n        if kwds:\n            key += tuple(type(v) for v in kwds.values())\n    elif len(key) == 1 and type(key[0]) in fasttypes:\n        return key[0]\n    return _HashedSeq(key)\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"Least-recently-used cache decorator.\n\n    If *maxsize* is set to None, the LRU features are disabled and the cache\n    can grow without bound.\n\n    If *typed* is True, arguments of different types will be cached separately.\n    For example, f(3.0) and f(3) will be treated as distinct calls with\n    distinct results.\n\n    Arguments to the cached function must be hashable.\n\n    View the cache statistics named tuple (hits, misses, maxsize, currsize)\n    with f.cache_info().  Clear the cache and statistics with f.cache_clear().\n    Access the underlying function with f.__wrapped__.\n\n    See:  https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)\n\n    \"\"\"\n\n    # Users should only access the lru_cache through its public API:\n    #       cache_info, cache_clear, and f.__wrapped__\n    # The internals of the lru_cache are encapsulated for thread safety and\n    # to allow the implementation to change (including a possible C version).\n\n    if isinstance(maxsize, int):\n        # Negative maxsize is treated as 0\n        if maxsize < 0:\n            maxsize = 0\n    elif callable(maxsize) and isinstance(typed, bool):\n        # The user_function was passed in directly via the maxsize argument\n        user_function, maxsize = maxsize, 128\n        wrapper = _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo)\n        wrapper.cache_parameters = lambda : {'maxsize': maxsize, 'typed': typed}\n        return update_wrapper(wrapper, user_function)\n    elif maxsize is not None:\n        raise TypeError(\n            'Expected first argument to be an integer, a callable, or None')\n\n    def decorating_function(user_function):\n        wrapper = _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo)\n        wrapper.cache_parameters = lambda : {'maxsize': maxsize, 'typed': typed}\n        return update_wrapper(wrapper, user_function)\n\n    return decorating_function\n\ndef _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo):\n    # Constants shared by all lru cache instances:\n    sentinel = object()          # unique object used to signal cache misses\n    make_key = _make_key         # build a key from the function arguments\n    PREV, NEXT, KEY, RESULT = 0, 1, 2, 3   # names for the link fields\n\n    cache = {}\n    hits = misses = 0\n    full = False\n    cache_get = cache.get    # bound method to lookup a key or return None\n    cache_len = cache.__len__  # get cache size without calling len()\n    lock = RLock()           # because linkedlist updates aren't threadsafe\n    root = []                # root of the circular doubly linked list\n    root[:] = [root, root, None, None]     # initialize by pointing to self\n\n    if maxsize == 0:\n\n        def wrapper(*args, **kwds):\n            # No caching -- just a statistics update\n            nonlocal misses\n            misses += 1\n            result = user_function(*args, **kwds)\n            return result\n\n    elif maxsize is None:\n\n        def wrapper(*args, **kwds):\n            # Simple caching without ordering or size limit\n            nonlocal hits, misses\n            key = make_key(args, kwds, typed)\n            result = cache_get(key, sentinel)\n            if result is not sentinel:\n                hits += 1\n                return result\n            misses += 1\n            result = user_function(*args, **kwds)\n            cache[key] = result\n            return result\n\n    else:\n\n        def wrapper(*args, **kwds):\n            # Size limited caching that tracks accesses by recency\n            nonlocal root, hits, misses, full\n            key = make_key(args, kwds, typed)\n            with lock:\n                link = cache_get(key)\n                if link is not None:\n                    # Move the link to the front of the circular queue\n                    link_prev, link_next, _key, result = link\n                    link_prev[NEXT] = link_next\n                    link_next[PREV] = link_prev\n                    last = root[PREV]\n                    last[NEXT] = root[PREV] = link\n                    link[PREV] = last\n                    link[NEXT] = root\n                    hits += 1\n                    return result\n                misses += 1\n            result = user_function(*args, **kwds)\n            with lock:\n                if key in cache:\n                    # Getting here means that this same key was added to the\n                    # cache while the lock was released.  Since the link\n                    # update is already done, we need only return the\n                    # computed result and update the count of misses.\n                    pass\n                elif full:\n                    # Use the old root to store the new key and result.\n                    oldroot = root\n                    oldroot[KEY] = key\n                    oldroot[RESULT] = result\n                    # Empty the oldest link and make it the new root.\n                    # Keep a reference to the old key and old result to\n                    # prevent their ref counts from going to zero during the\n                    # update. That will prevent potentially arbitrary object\n                    # clean-up code (i.e. __del__) from running while we're\n                    # still adjusting the links.\n                    root = oldroot[NEXT]\n                    oldkey = root[KEY]\n                    oldresult = root[RESULT]\n                    root[KEY] = root[RESULT] = None\n                    # Now update the cache dictionary.\n                    del cache[oldkey]\n                    # Save the potentially reentrant cache[key] assignment\n                    # for last, after the root and links have been put in\n                    # a consistent state.\n                    cache[key] = oldroot\n                else:\n                    # Put result in a new link at the front of the queue.\n                    last = root[PREV]\n                    link = [last, root, key, result]\n                    last[NEXT] = root[PREV] = cache[key] = link\n                    # Use the cache_len bound method instead of the len() function\n                    # which could potentially be wrapped in an lru_cache itself.\n                    full = (cache_len() >= maxsize)\n            return result\n\n    def cache_info():\n        \"\"\"Report cache statistics\"\"\"\n        with lock:\n            return _CacheInfo(hits, misses, maxsize, cache_len())\n\n    def cache_clear():\n        \"\"\"Clear the cache and cache statistics\"\"\"\n        nonlocal hits, misses, full\n        with lock:\n            cache.clear()\n            root[:] = [root, root, None, None]\n            hits = misses = 0\n            full = False\n\n    wrapper.cache_info = cache_info\n    wrapper.cache_clear = cache_clear\n    return wrapper\n\ntry:\n    from _functools import _lru_cache_wrapper\nexcept ImportError:\n    pass\n\n\n################################################################################\n### cache -- simplified access to the infinity cache\n################################################################################\n\ndef cache(user_function, /):\n    'Simple lightweight unbounded cache.  Sometimes called \"memoize\".'\n    return lru_cache(maxsize=None)(user_function)\n\n\n################################################################################\n### singledispatch() - single-dispatch generic function decorator\n################################################################################\n\ndef _c3_merge(sequences):\n    \"\"\"Merges MROs in *sequences* to a single MRO using the C3 algorithm.\n\n    Adapted from https://www.python.org/download/releases/2.3/mro/.\n\n    \"\"\"\n    result = []\n    while True:\n        sequences = [s for s in sequences if s]   # purge empty sequences\n        if not sequences:\n            return result\n        for s1 in sequences:   # find merge candidates among seq heads\n            candidate = s1[0]\n            for s2 in sequences:\n                if candidate in s2[1:]:\n                    candidate = None\n                    break      # reject the current head, it appears later\n            else:\n                break\n        if candidate is None:\n            raise RuntimeError(\"Inconsistent hierarchy\")\n        result.append(candidate)\n        # remove the chosen candidate\n        for seq in sequences:\n            if seq[0] == candidate:\n                del seq[0]\n\ndef _c3_mro(cls, abcs=None):\n    \"\"\"Computes the method resolution order using extended C3 linearization.\n\n    If no *abcs* are given, the algorithm works exactly like the built-in C3\n    linearization used for method resolution.\n\n    If given, *abcs* is a list of abstract base classes that should be inserted\n    into the resulting MRO. Unrelated ABCs are ignored and don't end up in the\n    result. The algorithm inserts ABCs where their functionality is introduced,\n    i.e. issubclass(cls, abc) returns True for the class itself but returns\n    False for all its direct base classes. Implicit ABCs for a given class\n    (either registered or inferred from the presence of a special method like\n    __len__) are inserted directly after the last ABC explicitly listed in the\n    MRO of said class. If two implicit ABCs end up next to each other in the\n    resulting MRO, their ordering depends on the order of types in *abcs*.\n\n    \"\"\"\n    for i, base in enumerate(reversed(cls.__bases__)):\n        if hasattr(base, '__abstractmethods__'):\n            boundary = len(cls.__bases__) - i\n            break   # Bases up to the last explicit ABC are considered first.\n    else:\n        boundary = 0\n    abcs = list(abcs) if abcs else []\n    explicit_bases = list(cls.__bases__[:boundary])\n    abstract_bases = []\n    other_bases = list(cls.__bases__[boundary:])\n    for base in abcs:\n        if issubclass(cls, base) and not any(\n                issubclass(b, base) for b in cls.__bases__\n            ):\n            # If *cls* is the class that introduces behaviour described by\n            # an ABC *base*, insert said ABC to its MRO.\n            abstract_bases.append(base)\n    for base in abstract_bases:\n        abcs.remove(base)\n    explicit_c3_mros = [_c3_mro(base, abcs=abcs) for base in explicit_bases]\n    abstract_c3_mros = [_c3_mro(base, abcs=abcs) for base in abstract_bases]\n    other_c3_mros = [_c3_mro(base, abcs=abcs) for base in other_bases]\n    return _c3_merge(\n        [[cls]] +\n        explicit_c3_mros + abstract_c3_mros + other_c3_mros +\n        [explicit_bases] + [abstract_bases] + [other_bases]\n    )\n\ndef _compose_mro(cls, types):\n    \"\"\"Calculates the method resolution order for a given class *cls*.\n\n    Includes relevant abstract base classes (with their respective bases) from\n    the *types* iterable. Uses a modified C3 linearization algorithm.\n\n    \"\"\"\n    bases = set(cls.__mro__)\n    # Remove entries which are already present in the __mro__ or unrelated.\n    def is_related(typ):\n        return (typ not in bases and hasattr(typ, '__mro__')\n                                 and not isinstance(typ, GenericAlias)\n                                 and issubclass(cls, typ))\n    types = [n for n in types if is_related(n)]\n    # Remove entries which are strict bases of other entries (they will end up\n    # in the MRO anyway.\n    def is_strict_base(typ):\n        for other in types:\n            if typ != other and typ in other.__mro__:\n                return True\n        return False\n    types = [n for n in types if not is_strict_base(n)]\n    # Subclasses of the ABCs in *types* which are also implemented by\n    # *cls* can be used to stabilize ABC ordering.\n    type_set = set(types)\n    mro = []\n    for typ in types:\n        found = []\n        for sub in typ.__subclasses__():\n            if sub not in bases and issubclass(cls, sub):\n                found.append([s for s in sub.__mro__ if s in type_set])\n        if not found:\n            mro.append(typ)\n            continue\n        # Favor subclasses with the biggest number of useful bases\n        found.sort(key=len, reverse=True)\n        for sub in found:\n            for subcls in sub:\n                if subcls not in mro:\n                    mro.append(subcls)\n    return _c3_mro(cls, abcs=mro)\n\ndef _find_impl(cls, registry):\n    \"\"\"Returns the best matching implementation from *registry* for type *cls*.\n\n    Where there is no registered implementation for a specific type, its method\n    resolution order is used to find a more generic implementation.\n\n    Note: if *registry* does not contain an implementation for the base\n    *object* type, this function may return None.\n\n    \"\"\"\n    mro = _compose_mro(cls, registry.keys())\n    match = None\n    for t in mro:\n        if match is not None:\n            # If *match* is an implicit ABC but there is another unrelated,\n            # equally matching implicit ABC, refuse the temptation to guess.\n            if (t in registry and t not in cls.__mro__\n                              and match not in cls.__mro__\n                              and not issubclass(match, t)):\n                raise RuntimeError(\"Ambiguous dispatch: {} or {}\".format(\n                    match, t))\n            break\n        if t in registry:\n            match = t\n    return registry.get(match)\n\ndef singledispatch(func):\n    \"\"\"Single-dispatch generic function decorator.\n\n    Transforms a function into a generic function, which can have different\n    behaviours depending upon the type of its first argument. The decorated\n    function acts as the default implementation, and additional\n    implementations can be registered using the register() attribute of the\n    generic function.\n    \"\"\"\n    # There are many programs that use functools without singledispatch, so we\n    # trade-off making singledispatch marginally slower for the benefit of\n    # making start-up of such applications slightly faster.\n    import types, weakref\n\n    registry = {}\n    dispatch_cache = weakref.WeakKeyDictionary()\n    cache_token = None\n\n    def dispatch(cls):\n        \"\"\"generic_func.dispatch(cls) -> <function implementation>\n\n        Runs the dispatch algorithm to return the best available implementation\n        for the given *cls* registered on *generic_func*.\n\n        \"\"\"\n        nonlocal cache_token\n        if cache_token is not None:\n            current_token = get_cache_token()\n            if cache_token != current_token:\n                dispatch_cache.clear()\n                cache_token = current_token\n        try:\n            impl = dispatch_cache[cls]\n        except KeyError:\n            try:\n                impl = registry[cls]\n            except KeyError:\n                impl = _find_impl(cls, registry)\n            dispatch_cache[cls] = impl\n        return impl\n\n    def _is_union_type(cls):\n        from typing import get_origin, Union\n        return get_origin(cls) in {Union, types.UnionType}\n\n    def _is_valid_dispatch_type(cls):\n        if isinstance(cls, type):\n            return True\n        from typing import get_args\n        return (_is_union_type(cls) and\n                all(isinstance(arg, type) for arg in get_args(cls)))\n\n    def register(cls, func=None):\n        \"\"\"generic_func.register(cls, func) -> func\n\n        Registers a new implementation for the given *cls* on a *generic_func*.\n\n        \"\"\"\n        nonlocal cache_token\n        if _is_valid_dispatch_type(cls):\n            if func is None:\n                return lambda f: register(cls, f)\n        else:\n            if func is not None:\n                raise TypeError(\n                    f\"Invalid first argument to `register()`. \"\n                    f\"{cls!r} is not a class or union type.\"\n                )\n            ann = getattr(cls, '__annotations__', {})\n            if not ann:\n                raise TypeError(\n                    f\"Invalid first argument to `register()`: {cls!r}. \"\n                    f\"Use either `@register(some_class)` or plain `@register` \"\n                    f\"on an annotated function.\"\n                )\n            func = cls\n\n            # only import typing if annotation parsing is necessary\n            from typing import get_type_hints\n            argname, cls = next(iter(get_type_hints(func).items()))\n            if not _is_valid_dispatch_type(cls):\n                if _is_union_type(cls):\n                    raise TypeError(\n                        f\"Invalid annotation for {argname!r}. \"\n                        f\"{cls!r} not all arguments are classes.\"\n                    )\n                else:\n                    raise TypeError(\n                        f\"Invalid annotation for {argname!r}. \"\n                        f\"{cls!r} is not a class.\"\n                    )\n\n        if _is_union_type(cls):\n            from typing import get_args\n\n            for arg in get_args(cls):\n                registry[arg] = func\n        else:\n            registry[cls] = func\n        if cache_token is None and hasattr(cls, '__abstractmethods__'):\n            cache_token = get_cache_token()\n        dispatch_cache.clear()\n        return func\n\n    def wrapper(*args, **kw):\n        if not args:\n            raise TypeError(f'{funcname} requires at least '\n                            '1 positional argument')\n\n        return dispatch(args[0].__class__)(*args, **kw)\n\n    funcname = getattr(func, '__name__', 'singledispatch function')\n    registry[object] = func\n    wrapper.register = register\n    wrapper.dispatch = dispatch\n    wrapper.registry = types.MappingProxyType(registry)\n    wrapper._clear_cache = dispatch_cache.clear\n    update_wrapper(wrapper, func)\n    return wrapper\n\n\n# Descriptor version\nclass singledispatchmethod:\n    \"\"\"Single-dispatch generic method descriptor.\n\n    Supports wrapping existing descriptors and handles non-descriptor\n    callables as instance methods.\n    \"\"\"\n\n    def __init__(self, func):\n        if not callable(func) and not hasattr(func, \"__get__\"):\n            raise TypeError(f\"{func!r} is not callable or a descriptor\")\n\n        self.dispatcher = singledispatch(func)\n        self.func = func\n\n    def register(self, cls, method=None):\n        \"\"\"generic_method.register(cls, func) -> func\n\n        Registers a new implementation for the given *cls* on a *generic_method*.\n        \"\"\"\n        return self.dispatcher.register(cls, func=method)\n\n    def __get__(self, obj, cls=None):\n        def _method(*args, **kwargs):\n            method = self.dispatcher.dispatch(args[0].__class__)\n            return method.__get__(obj, cls)(*args, **kwargs)\n\n        _method.__isabstractmethod__ = self.__isabstractmethod__\n        _method.register = self.register\n        update_wrapper(_method, self.func)\n        return _method\n\n    @property\n    def __isabstractmethod__(self):\n        return getattr(self.func, '__isabstractmethod__', False)\n\n\n################################################################################\n### cached_property() - computed once per instance, cached as attribute\n################################################################################\n\n_NOT_FOUND = object()\n\n\nclass cached_property:\n    def __init__(self, func):\n        self.func = func\n        self.attrname = None\n        self.__doc__ = func.__doc__\n        self.lock = RLock()\n\n    def __set_name__(self, owner, name):\n        if self.attrname is None:\n            self.attrname = name\n        elif name != self.attrname:\n            raise TypeError(\n                \"Cannot assign the same cached_property to two different names \"\n                f\"({self.attrname!r} and {name!r}).\"\n            )\n\n    def __get__(self, instance, owner=None):\n        if instance is None:\n            return self\n        if self.attrname is None:\n            raise TypeError(\n                \"Cannot use cached_property instance without calling __set_name__ on it.\")\n        try:\n            cache = instance.__dict__\n        except AttributeError:  # not all objects have __dict__ (e.g. class defines slots)\n            msg = (\n                f\"No '__dict__' attribute on {type(instance).__name__!r} \"\n                f\"instance to cache {self.attrname!r} property.\"\n            )\n            raise TypeError(msg) from None\n        val = cache.get(self.attrname, _NOT_FOUND)\n        if val is _NOT_FOUND:\n            with self.lock:\n                # check if another thread filled cache while we awaited lock\n                val = cache.get(self.attrname, _NOT_FOUND)\n                if val is _NOT_FOUND:\n                    val = self.func(instance)\n                    try:\n                        cache[self.attrname] = val\n                    except TypeError:\n                        msg = (\n                            f\"The '__dict__' attribute on {type(instance).__name__!r} instance \"\n                            f\"does not support item assignment for caching {self.attrname!r} property.\"\n                        )\n                        raise TypeError(msg) from None\n        return val\n\n    __class_getitem__ = classmethod(GenericAlias)\n", 1012], "/usr/local/lib/python3.11/threading.py": ["\"\"\"Thread module emulating a subset of Java's threading model.\"\"\"\n\nimport os as _os\nimport sys as _sys\nimport _thread\nimport functools\n\nfrom time import monotonic as _time\nfrom _weakrefset import WeakSet\nfrom itertools import islice as _islice, count as _count\ntry:\n    from _collections import deque as _deque\nexcept ImportError:\n    from collections import deque as _deque\n\n# Note regarding PEP 8 compliant names\n#  This threading model was originally inspired by Java, and inherited\n# the convention of camelCase function and method names from that\n# language. Those original names are not in any imminent danger of\n# being deprecated (even for Py3k),so this module provides them as an\n# alias for the PEP 8 compliant names\n# Note that using the new PEP 8 compliant names facilitates substitution\n# with the multiprocessing module, which doesn't provide the old\n# Java inspired names.\n\n__all__ = ['get_ident', 'active_count', 'Condition', 'current_thread',\n           'enumerate', 'main_thread', 'TIMEOUT_MAX',\n           'Event', 'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore', 'Thread',\n           'Barrier', 'BrokenBarrierError', 'Timer', 'ThreadError',\n           'setprofile', 'settrace', 'local', 'stack_size',\n           'excepthook', 'ExceptHookArgs', 'gettrace', 'getprofile']\n\n# Rename some stuff so \"from threading import *\" is safe\n_start_new_thread = _thread.start_new_thread\n_allocate_lock = _thread.allocate_lock\n_set_sentinel = _thread._set_sentinel\nget_ident = _thread.get_ident\ntry:\n    get_native_id = _thread.get_native_id\n    _HAVE_THREAD_NATIVE_ID = True\n    __all__.append('get_native_id')\nexcept AttributeError:\n    _HAVE_THREAD_NATIVE_ID = False\nThreadError = _thread.error\ntry:\n    _CRLock = _thread.RLock\nexcept AttributeError:\n    _CRLock = None\nTIMEOUT_MAX = _thread.TIMEOUT_MAX\ndel _thread\n\n\n# Support for profile and trace hooks\n\n_profile_hook = None\n_trace_hook = None\n\ndef setprofile(func):\n    \"\"\"Set a profile function for all threads started from the threading module.\n\n    The func will be passed to sys.setprofile() for each thread, before its\n    run() method is called.\n\n    \"\"\"\n    global _profile_hook\n    _profile_hook = func\n\ndef getprofile():\n    \"\"\"Get the profiler function as set by threading.setprofile().\"\"\"\n    return _profile_hook\n\ndef settrace(func):\n    \"\"\"Set a trace function for all threads started from the threading module.\n\n    The func will be passed to sys.settrace() for each thread, before its run()\n    method is called.\n\n    \"\"\"\n    global _trace_hook\n    _trace_hook = func\n\ndef gettrace():\n    \"\"\"Get the trace function as set by threading.settrace().\"\"\"\n    return _trace_hook\n\n# Synchronization classes\n\nLock = _allocate_lock\n\ndef RLock(*args, **kwargs):\n    \"\"\"Factory function that returns a new reentrant lock.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it again\n    without blocking; the thread must release it once for each time it has\n    acquired it.\n\n    \"\"\"\n    if _CRLock is None:\n        return _PyRLock(*args, **kwargs)\n    return _CRLock(*args, **kwargs)\n\nclass _RLock:\n    \"\"\"This class implements reentrant lock objects.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it\n    again without blocking; the thread must release it once for each time it\n    has acquired it.\n\n    \"\"\"\n\n    def __init__(self):\n        self._block = _allocate_lock()\n        self._owner = None\n        self._count = 0\n\n    def __repr__(self):\n        owner = self._owner\n        try:\n            owner = _active[owner].name\n        except KeyError:\n            pass\n        return \"<%s %s.%s object owner=%r count=%d at %s>\" % (\n            \"locked\" if self._block.locked() else \"unlocked\",\n            self.__class__.__module__,\n            self.__class__.__qualname__,\n            owner,\n            self._count,\n            hex(id(self))\n        )\n\n    def _at_fork_reinit(self):\n        self._block._at_fork_reinit()\n        self._owner = None\n        self._count = 0\n\n    def acquire(self, blocking=True, timeout=-1):\n        \"\"\"Acquire a lock, blocking or non-blocking.\n\n        When invoked without arguments: if this thread already owns the lock,\n        increment the recursion level by one, and return immediately. Otherwise,\n        if another thread owns the lock, block until the lock is unlocked. Once\n        the lock is unlocked (not owned by any thread), then grab ownership, set\n        the recursion level to one, and return. If more than one thread is\n        blocked waiting until the lock is unlocked, only one at a time will be\n        able to grab ownership of the lock. There is no return value in this\n        case.\n\n        When invoked with the blocking argument set to true, do the same thing\n        as when called without arguments, and return true.\n\n        When invoked with the blocking argument set to false, do not block. If a\n        call without an argument would block, return false immediately;\n        otherwise, do the same thing as when called without arguments, and\n        return true.\n\n        When invoked with the floating-point timeout argument set to a positive\n        value, block for at most the number of seconds specified by timeout\n        and as long as the lock cannot be acquired.  Return true if the lock has\n        been acquired, false if the timeout has elapsed.\n\n        \"\"\"\n        me = get_ident()\n        if self._owner == me:\n            self._count += 1\n            return 1\n        rc = self._block.acquire(blocking, timeout)\n        if rc:\n            self._owner = me\n            self._count = 1\n        return rc\n\n    __enter__ = acquire\n\n    def release(self):\n        \"\"\"Release a lock, decrementing the recursion level.\n\n        If after the decrement it is zero, reset the lock to unlocked (not owned\n        by any thread), and if any other threads are blocked waiting for the\n        lock to become unlocked, allow exactly one of them to proceed. If after\n        the decrement the recursion level is still nonzero, the lock remains\n        locked and owned by the calling thread.\n\n        Only call this method when the calling thread owns the lock. A\n        RuntimeError is raised if this method is called when the lock is\n        unlocked.\n\n        There is no return value.\n\n        \"\"\"\n        if self._owner != get_ident():\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        self._count = count = self._count - 1\n        if not count:\n            self._owner = None\n            self._block.release()\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n    # Internal methods used by condition variables\n\n    def _acquire_restore(self, state):\n        self._block.acquire()\n        self._count, self._owner = state\n\n    def _release_save(self):\n        if self._count == 0:\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        count = self._count\n        self._count = 0\n        owner = self._owner\n        self._owner = None\n        self._block.release()\n        return (count, owner)\n\n    def _is_owned(self):\n        return self._owner == get_ident()\n\n    # Internal method used for reentrancy checks\n\n    def _recursion_count(self):\n        if self._owner != get_ident():\n            return 0\n        return self._count\n\n_PyRLock = _RLock\n\n\nclass Condition:\n    \"\"\"Class that implements a condition variable.\n\n    A condition variable allows one or more threads to wait until they are\n    notified by another thread.\n\n    If the lock argument is given and not None, it must be a Lock or RLock\n    object, and it is used as the underlying lock. Otherwise, a new RLock object\n    is created and used as the underlying lock.\n\n    \"\"\"\n\n    def __init__(self, lock=None):\n        if lock is None:\n            lock = RLock()\n        self._lock = lock\n        # Export the lock's acquire() and release() methods\n        self.acquire = lock.acquire\n        self.release = lock.release\n        # If the lock defines _release_save() and/or _acquire_restore(),\n        # these override the default implementations (which just call\n        # release() and acquire() on the lock).  Ditto for _is_owned().\n        try:\n            self._release_save = lock._release_save\n        except AttributeError:\n            pass\n        try:\n            self._acquire_restore = lock._acquire_restore\n        except AttributeError:\n            pass\n        try:\n            self._is_owned = lock._is_owned\n        except AttributeError:\n            pass\n        self._waiters = _deque()\n\n    def _at_fork_reinit(self):\n        self._lock._at_fork_reinit()\n        self._waiters.clear()\n\n    def __enter__(self):\n        return self._lock.__enter__()\n\n    def __exit__(self, *args):\n        return self._lock.__exit__(*args)\n\n    def __repr__(self):\n        return \"<Condition(%s, %d)>\" % (self._lock, len(self._waiters))\n\n    def _release_save(self):\n        self._lock.release()           # No state to save\n\n    def _acquire_restore(self, x):\n        self._lock.acquire()           # Ignore saved state\n\n    def _is_owned(self):\n        # Return True if lock is owned by current_thread.\n        # This method is called only if _lock doesn't have _is_owned().\n        if self._lock.acquire(False):\n            self._lock.release()\n            return False\n        else:\n            return True\n\n    def wait(self, timeout=None):\n        \"\"\"Wait until notified or until a timeout occurs.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method releases the underlying lock, and then blocks until it is\n        awakened by a notify() or notify_all() call for the same condition\n        variable in another thread, or until the optional timeout occurs. Once\n        awakened or timed out, it re-acquires the lock and returns.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        When the underlying lock is an RLock, it is not released using its\n        release() method, since this may not actually unlock the lock when it\n        was acquired multiple times recursively. Instead, an internal interface\n        of the RLock class is used, which really unlocks it even when it has\n        been recursively acquired several times. Another internal interface is\n        then used to restore the recursion level when the lock is reacquired.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot wait on un-acquired lock\")\n        waiter = _allocate_lock()\n        waiter.acquire()\n        self._waiters.append(waiter)\n        saved_state = self._release_save()\n        gotit = False\n        try:    # restore state no matter what (e.g., KeyboardInterrupt)\n            if timeout is None:\n                waiter.acquire()\n                gotit = True\n            else:\n                if timeout > 0:\n                    gotit = waiter.acquire(True, timeout)\n                else:\n                    gotit = waiter.acquire(False)\n            return gotit\n        finally:\n            self._acquire_restore(saved_state)\n            if not gotit:\n                try:\n                    self._waiters.remove(waiter)\n                except ValueError:\n                    pass\n\n    def wait_for(self, predicate, timeout=None):\n        \"\"\"Wait until a condition evaluates to True.\n\n        predicate should be a callable which result will be interpreted as a\n        boolean value.  A timeout may be provided giving the maximum time to\n        wait.\n\n        \"\"\"\n        endtime = None\n        waittime = timeout\n        result = predicate()\n        while not result:\n            if waittime is not None:\n                if endtime is None:\n                    endtime = _time() + waittime\n                else:\n                    waittime = endtime - _time()\n                    if waittime <= 0:\n                        break\n            self.wait(waittime)\n            result = predicate()\n        return result\n\n    def notify(self, n=1):\n        \"\"\"Wake up one or more threads waiting on this condition, if any.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method wakes up at most n of the threads waiting for the condition\n        variable; it is a no-op if no threads are waiting.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot notify on un-acquired lock\")\n        waiters = self._waiters\n        while waiters and n > 0:\n            waiter = waiters[0]\n            try:\n                waiter.release()\n            except RuntimeError:\n                # gh-92530: The previous call of notify() released the lock,\n                # but was interrupted before removing it from the queue.\n                # It can happen if a signal handler raises an exception,\n                # like CTRL+C which raises KeyboardInterrupt.\n                pass\n            else:\n                n -= 1\n            try:\n                waiters.remove(waiter)\n            except ValueError:\n                pass\n\n    def notify_all(self):\n        \"\"\"Wake up all threads waiting on this condition.\n\n        If the calling thread has not acquired the lock when this method\n        is called, a RuntimeError is raised.\n\n        \"\"\"\n        self.notify(len(self._waiters))\n\n    def notifyAll(self):\n        \"\"\"Wake up all threads waiting on this condition.\n\n        This method is deprecated, use notify_all() instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('notifyAll() is deprecated, use notify_all() instead',\n                      DeprecationWarning, stacklevel=2)\n        self.notify_all()\n\n\nclass Semaphore:\n    \"\"\"This class implements semaphore objects.\n\n    Semaphores manage a counter representing the number of release() calls minus\n    the number of acquire() calls, plus an initial value. The acquire() method\n    blocks if necessary until it can return without making the counter\n    negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    # After Tim Peters' semaphore class, but not quite the same (no maximum)\n\n    def __init__(self, value=1):\n        if value < 0:\n            raise ValueError(\"semaphore initial value must be >= 0\")\n        self._cond = Condition(Lock())\n        self._value = value\n\n    def __repr__(self):\n        cls = self.__class__\n        return (f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}:\"\n                f\" value={self._value}>\")\n\n    def acquire(self, blocking=True, timeout=None):\n        \"\"\"Acquire a semaphore, decrementing the internal counter by one.\n\n        When invoked without arguments: if the internal counter is larger than\n        zero on entry, decrement it by one and return immediately. If it is zero\n        on entry, block, waiting until some other thread has called release() to\n        make it larger than zero. This is done with proper interlocking so that\n        if multiple acquire() calls are blocked, release() will wake exactly one\n        of them up. The implementation may pick one at random, so the order in\n        which blocked threads are awakened should not be relied on. There is no\n        return value in this case.\n\n        When invoked with blocking set to true, do the same thing as when called\n        without arguments, and return true.\n\n        When invoked with blocking set to false, do not block. If a call without\n        an argument would block, return false immediately; otherwise, do the\n        same thing as when called without arguments, and return true.\n\n        When invoked with a timeout other than None, it will block for at\n        most timeout seconds.  If acquire does not complete successfully in\n        that interval, return false.  Return true otherwise.\n\n        \"\"\"\n        if not blocking and timeout is not None:\n            raise ValueError(\"can't specify timeout for non-blocking acquire\")\n        rc = False\n        endtime = None\n        with self._cond:\n            while self._value == 0:\n                if not blocking:\n                    break\n                if timeout is not None:\n                    if endtime is None:\n                        endtime = _time() + timeout\n                    else:\n                        timeout = endtime - _time()\n                        if timeout <= 0:\n                            break\n                self._cond.wait(timeout)\n            else:\n                self._value -= 1\n                rc = True\n        return rc\n\n    __enter__ = acquire\n\n    def release(self, n=1):\n        \"\"\"Release a semaphore, incrementing the internal counter by one or more.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        \"\"\"\n        if n < 1:\n            raise ValueError('n must be one or more')\n        with self._cond:\n            self._value += n\n            for i in range(n):\n                self._cond.notify()\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n\nclass BoundedSemaphore(Semaphore):\n    \"\"\"Implements a bounded semaphore.\n\n    A bounded semaphore checks to make sure its current value doesn't exceed its\n    initial value. If it does, ValueError is raised. In most situations\n    semaphores are used to guard resources with limited capacity.\n\n    If the semaphore is released too many times it's a sign of a bug. If not\n    given, value defaults to 1.\n\n    Like regular semaphores, bounded semaphores manage a counter representing\n    the number of release() calls minus the number of acquire() calls, plus an\n    initial value. The acquire() method blocks if necessary until it can return\n    without making the counter negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    def __init__(self, value=1):\n        Semaphore.__init__(self, value)\n        self._initial_value = value\n\n    def __repr__(self):\n        cls = self.__class__\n        return (f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}:\"\n                f\" value={self._value}/{self._initial_value}>\")\n\n    def release(self, n=1):\n        \"\"\"Release a semaphore, incrementing the internal counter by one or more.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        If the number of releases exceeds the number of acquires,\n        raise a ValueError.\n\n        \"\"\"\n        if n < 1:\n            raise ValueError('n must be one or more')\n        with self._cond:\n            if self._value + n > self._initial_value:\n                raise ValueError(\"Semaphore released too many times\")\n            self._value += n\n            for i in range(n):\n                self._cond.notify()\n\n\nclass Event:\n    \"\"\"Class implementing event objects.\n\n    Events manage a flag that can be set to true with the set() method and reset\n    to false with the clear() method. The wait() method blocks until the flag is\n    true.  The flag is initially false.\n\n    \"\"\"\n\n    # After Tim Peters' event class (without is_posted())\n\n    def __init__(self):\n        self._cond = Condition(Lock())\n        self._flag = False\n\n    def __repr__(self):\n        cls = self.__class__\n        status = 'set' if self._flag else 'unset'\n        return f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}: {status}>\"\n\n    def _at_fork_reinit(self):\n        # Private method called by Thread._reset_internal_locks()\n        self._cond._at_fork_reinit()\n\n    def is_set(self):\n        \"\"\"Return true if and only if the internal flag is true.\"\"\"\n        return self._flag\n\n    def isSet(self):\n        \"\"\"Return true if and only if the internal flag is true.\n\n        This method is deprecated, use is_set() instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('isSet() is deprecated, use is_set() instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.is_set()\n\n    def set(self):\n        \"\"\"Set the internal flag to true.\n\n        All threads waiting for it to become true are awakened. Threads\n        that call wait() once the flag is true will not block at all.\n\n        \"\"\"\n        with self._cond:\n            self._flag = True\n            self._cond.notify_all()\n\n    def clear(self):\n        \"\"\"Reset the internal flag to false.\n\n        Subsequently, threads calling wait() will block until set() is called to\n        set the internal flag to true again.\n\n        \"\"\"\n        with self._cond:\n            self._flag = False\n\n    def wait(self, timeout=None):\n        \"\"\"Block until the internal flag is true.\n\n        If the internal flag is true on entry, return immediately. Otherwise,\n        block until another thread calls set() to set the flag to true, or until\n        the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        This method returns the internal flag on exit, so it will always return\n        True except if a timeout is given and the operation times out.\n\n        \"\"\"\n        with self._cond:\n            signaled = self._flag\n            if not signaled:\n                signaled = self._cond.wait(timeout)\n            return signaled\n\n\n# A barrier class.  Inspired in part by the pthread_barrier_* api and\n# the CyclicBarrier class from Java.  See\n# http://sourceware.org/pthreads-win32/manual/pthread_barrier_init.html and\n# http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/\n#        CyclicBarrier.html\n# for information.\n# We maintain two main states, 'filling' and 'draining' enabling the barrier\n# to be cyclic.  Threads are not allowed into it until it has fully drained\n# since the previous cycle.  In addition, a 'resetting' state exists which is\n# similar to 'draining' except that threads leave with a BrokenBarrierError,\n# and a 'broken' state in which all threads get the exception.\nclass Barrier:\n    \"\"\"Implements a Barrier.\n\n    Useful for synchronizing a fixed number of threads at known synchronization\n    points.  Threads block on 'wait()' and are simultaneously awoken once they\n    have all made that call.\n\n    \"\"\"\n\n    def __init__(self, parties, action=None, timeout=None):\n        \"\"\"Create a barrier, initialised to 'parties' threads.\n\n        'action' is a callable which, when supplied, will be called by one of\n        the threads after they have all entered the barrier and just prior to\n        releasing them all. If a 'timeout' is provided, it is used as the\n        default for all subsequent 'wait()' calls.\n\n        \"\"\"\n        self._cond = Condition(Lock())\n        self._action = action\n        self._timeout = timeout\n        self._parties = parties\n        self._state = 0  # 0 filling, 1 draining, -1 resetting, -2 broken\n        self._count = 0\n\n    def __repr__(self):\n        cls = self.__class__\n        if self.broken:\n            return f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}: broken>\"\n        return (f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}:\"\n                f\" waiters={self.n_waiting}/{self.parties}>\")\n\n    def wait(self, timeout=None):\n        \"\"\"Wait for the barrier.\n\n        When the specified number of threads have started waiting, they are all\n        simultaneously awoken. If an 'action' was provided for the barrier, one\n        of the threads will have executed that callback prior to returning.\n        Returns an individual index number from 0 to 'parties-1'.\n\n        \"\"\"\n        if timeout is None:\n            timeout = self._timeout\n        with self._cond:\n            self._enter() # Block while the barrier drains.\n            index = self._count\n            self._count += 1\n            try:\n                if index + 1 == self._parties:\n                    # We release the barrier\n                    self._release()\n                else:\n                    # We wait until someone releases us\n                    self._wait(timeout)\n                return index\n            finally:\n                self._count -= 1\n                # Wake up any threads waiting for barrier to drain.\n                self._exit()\n\n    # Block until the barrier is ready for us, or raise an exception\n    # if it is broken.\n    def _enter(self):\n        while self._state in (-1, 1):\n            # It is draining or resetting, wait until done\n            self._cond.wait()\n        #see if the barrier is in a broken state\n        if self._state < 0:\n            raise BrokenBarrierError\n        assert self._state == 0\n\n    # Optionally run the 'action' and release the threads waiting\n    # in the barrier.\n    def _release(self):\n        try:\n            if self._action:\n                self._action()\n            # enter draining state\n            self._state = 1\n            self._cond.notify_all()\n        except:\n            #an exception during the _action handler.  Break and reraise\n            self._break()\n            raise\n\n    # Wait in the barrier until we are released.  Raise an exception\n    # if the barrier is reset or broken.\n    def _wait(self, timeout):\n        if not self._cond.wait_for(lambda : self._state != 0, timeout):\n            #timed out.  Break the barrier\n            self._break()\n            raise BrokenBarrierError\n        if self._state < 0:\n            raise BrokenBarrierError\n        assert self._state == 1\n\n    # If we are the last thread to exit the barrier, signal any threads\n    # waiting for the barrier to drain.\n    def _exit(self):\n        if self._count == 0:\n            if self._state in (-1, 1):\n                #resetting or draining\n                self._state = 0\n                self._cond.notify_all()\n\n    def reset(self):\n        \"\"\"Reset the barrier to the initial state.\n\n        Any threads currently waiting will get the BrokenBarrier exception\n        raised.\n\n        \"\"\"\n        with self._cond:\n            if self._count > 0:\n                if self._state == 0:\n                    #reset the barrier, waking up threads\n                    self._state = -1\n                elif self._state == -2:\n                    #was broken, set it to reset state\n                    #which clears when the last thread exits\n                    self._state = -1\n            else:\n                self._state = 0\n            self._cond.notify_all()\n\n    def abort(self):\n        \"\"\"Place the barrier into a 'broken' state.\n\n        Useful in case of error.  Any currently waiting threads and threads\n        attempting to 'wait()' will have BrokenBarrierError raised.\n\n        \"\"\"\n        with self._cond:\n            self._break()\n\n    def _break(self):\n        # An internal error was detected.  The barrier is set to\n        # a broken state all parties awakened.\n        self._state = -2\n        self._cond.notify_all()\n\n    @property\n    def parties(self):\n        \"\"\"Return the number of threads required to trip the barrier.\"\"\"\n        return self._parties\n\n    @property\n    def n_waiting(self):\n        \"\"\"Return the number of threads currently waiting at the barrier.\"\"\"\n        # We don't need synchronization here since this is an ephemeral result\n        # anyway.  It returns the correct value in the steady state.\n        if self._state == 0:\n            return self._count\n        return 0\n\n    @property\n    def broken(self):\n        \"\"\"Return True if the barrier is in a broken state.\"\"\"\n        return self._state == -2\n\n# exception raised by the Barrier class\nclass BrokenBarrierError(RuntimeError):\n    pass\n\n\n# Helper to generate new thread names\n_counter = _count(1).__next__\ndef _newname(name_template):\n    return name_template % _counter()\n\n# Active thread administration.\n#\n# bpo-44422: Use a reentrant lock to allow reentrant calls to functions like\n# threading.enumerate().\n_active_limbo_lock = RLock()\n_active = {}    # maps thread id to Thread object\n_limbo = {}\n_dangling = WeakSet()\n\n# Set of Thread._tstate_lock locks of non-daemon threads used by _shutdown()\n# to wait until all Python thread states get deleted:\n# see Thread._set_tstate_lock().\n_shutdown_locks_lock = _allocate_lock()\n_shutdown_locks = set()\n\ndef _maintain_shutdown_locks():\n    \"\"\"\n    Drop any shutdown locks that don't correspond to running threads anymore.\n\n    Calling this from time to time avoids an ever-growing _shutdown_locks\n    set when Thread objects are not joined explicitly. See bpo-37788.\n\n    This must be called with _shutdown_locks_lock acquired.\n    \"\"\"\n    # If a lock was released, the corresponding thread has exited\n    to_remove = [lock for lock in _shutdown_locks if not lock.locked()]\n    _shutdown_locks.difference_update(to_remove)\n\n\n# Main class for threads\n\nclass Thread:\n    \"\"\"A class that represents a thread of control.\n\n    This class can be safely subclassed in a limited fashion. There are two ways\n    to specify the activity: by passing a callable object to the constructor, or\n    by overriding the run() method in a subclass.\n\n    \"\"\"\n\n    _initialized = False\n\n    def __init__(self, group=None, target=None, name=None,\n                 args=(), kwargs=None, *, daemon=None):\n        \"\"\"This constructor should always be called with keyword arguments. Arguments are:\n\n        *group* should be None; reserved for future extension when a ThreadGroup\n        class is implemented.\n\n        *target* is the callable object to be invoked by the run()\n        method. Defaults to None, meaning nothing is called.\n\n        *name* is the thread name. By default, a unique name is constructed of\n        the form \"Thread-N\" where N is a small decimal number.\n\n        *args* is a list or tuple of arguments for the target invocation. Defaults to ().\n\n        *kwargs* is a dictionary of keyword arguments for the target\n        invocation. Defaults to {}.\n\n        If a subclass overrides the constructor, it must make sure to invoke\n        the base class constructor (Thread.__init__()) before doing anything\n        else to the thread.\n\n        \"\"\"\n        assert group is None, \"group argument must be None for now\"\n        if kwargs is None:\n            kwargs = {}\n        if name:\n            name = str(name)\n        else:\n            name = _newname(\"Thread-%d\")\n            if target is not None:\n                try:\n                    target_name = target.__name__\n                    name += f\" ({target_name})\"\n                except AttributeError:\n                    pass\n\n        self._target = target\n        self._name = name\n        self._args = args\n        self._kwargs = kwargs\n        if daemon is not None:\n            self._daemonic = daemon\n        else:\n            self._daemonic = current_thread().daemon\n        self._ident = None\n        if _HAVE_THREAD_NATIVE_ID:\n            self._native_id = None\n        self._tstate_lock = None\n        self._started = Event()\n        self._is_stopped = False\n        self._initialized = True\n        # Copy of sys.stderr used by self._invoke_excepthook()\n        self._stderr = _sys.stderr\n        self._invoke_excepthook = _make_invoke_excepthook()\n        # For debugging and _after_fork()\n        _dangling.add(self)\n\n    def _reset_internal_locks(self, is_alive):\n        # private!  Called by _after_fork() to reset our internal locks as\n        # they may be in an invalid state leading to a deadlock or crash.\n        self._started._at_fork_reinit()\n        if is_alive:\n            # bpo-42350: If the fork happens when the thread is already stopped\n            # (ex: after threading._shutdown() has been called), _tstate_lock\n            # is None. Do nothing in this case.\n            if self._tstate_lock is not None:\n                self._tstate_lock._at_fork_reinit()\n                self._tstate_lock.acquire()\n        else:\n            # The thread isn't alive after fork: it doesn't have a tstate\n            # anymore.\n            self._is_stopped = True\n            self._tstate_lock = None\n\n    def __repr__(self):\n        assert self._initialized, \"Thread.__init__() was not called\"\n        status = \"initial\"\n        if self._started.is_set():\n            status = \"started\"\n        self.is_alive() # easy way to get ._is_stopped set when appropriate\n        if self._is_stopped:\n            status = \"stopped\"\n        if self._daemonic:\n            status += \" daemon\"\n        if self._ident is not None:\n            status += \" %s\" % self._ident\n        return \"<%s(%s, %s)>\" % (self.__class__.__name__, self._name, status)\n\n    def start(self):\n        \"\"\"Start the thread's activity.\n\n        It must be called at most once per thread object. It arranges for the\n        object's run() method to be invoked in a separate thread of control.\n\n        This method will raise a RuntimeError if called more than once on the\n        same thread object.\n\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"thread.__init__() not called\")\n\n        if self._started.is_set():\n            raise RuntimeError(\"threads can only be started once\")\n\n        with _active_limbo_lock:\n            _limbo[self] = self\n        try:\n            _start_new_thread(self._bootstrap, ())\n        except Exception:\n            with _active_limbo_lock:\n                del _limbo[self]\n            raise\n        self._started.wait()\n\n    def run(self):\n        \"\"\"Method representing the thread's activity.\n\n        You may override this method in a subclass. The standard run() method\n        invokes the callable object passed to the object's constructor as the\n        target argument, if any, with sequential and keyword arguments taken\n        from the args and kwargs arguments, respectively.\n\n        \"\"\"\n        try:\n            if self._target is not None:\n                self._target(*self._args, **self._kwargs)\n        finally:\n            # Avoid a refcycle if the thread is running a function with\n            # an argument that has a member that points to the thread.\n            del self._target, self._args, self._kwargs\n\n    def _bootstrap(self):\n        # Wrapper around the real bootstrap code that ignores\n        # exceptions during interpreter cleanup.  Those typically\n        # happen when a daemon thread wakes up at an unfortunate\n        # moment, finds the world around it destroyed, and raises some\n        # random exception *** while trying to report the exception in\n        # _bootstrap_inner() below ***.  Those random exceptions\n        # don't help anybody, and they confuse users, so we suppress\n        # them.  We suppress them only when it appears that the world\n        # indeed has already been destroyed, so that exceptions in\n        # _bootstrap_inner() during normal business hours are properly\n        # reported.  Also, we only suppress them for daemonic threads;\n        # if a non-daemonic encounters this, something else is wrong.\n        try:\n            self._bootstrap_inner()\n        except:\n            if self._daemonic and _sys is None:\n                return\n            raise\n\n    def _set_ident(self):\n        self._ident = get_ident()\n\n    if _HAVE_THREAD_NATIVE_ID:\n        def _set_native_id(self):\n            self._native_id = get_native_id()\n\n    def _set_tstate_lock(self):\n        \"\"\"\n        Set a lock object which will be released by the interpreter when\n        the underlying thread state (see pystate.h) gets deleted.\n        \"\"\"\n        self._tstate_lock = _set_sentinel()\n        self._tstate_lock.acquire()\n\n        if not self.daemon:\n            with _shutdown_locks_lock:\n                _maintain_shutdown_locks()\n                _shutdown_locks.add(self._tstate_lock)\n\n    def _bootstrap_inner(self):\n        try:\n            self._set_ident()\n            self._set_tstate_lock()\n            if _HAVE_THREAD_NATIVE_ID:\n                self._set_native_id()\n            self._started.set()\n            with _active_limbo_lock:\n                _active[self._ident] = self\n                del _limbo[self]\n\n            if _trace_hook:\n                _sys.settrace(_trace_hook)\n            if _profile_hook:\n                _sys.setprofile(_profile_hook)\n\n            try:\n                self.run()\n            except:\n                self._invoke_excepthook(self)\n        finally:\n            self._delete()\n\n    def _stop(self):\n        # After calling ._stop(), .is_alive() returns False and .join() returns\n        # immediately.  ._tstate_lock must be released before calling ._stop().\n        #\n        # Normal case:  C code at the end of the thread's life\n        # (release_sentinel in _threadmodule.c) releases ._tstate_lock, and\n        # that's detected by our ._wait_for_tstate_lock(), called by .join()\n        # and .is_alive().  Any number of threads _may_ call ._stop()\n        # simultaneously (for example, if multiple threads are blocked in\n        # .join() calls), and they're not serialized.  That's harmless -\n        # they'll just make redundant rebindings of ._is_stopped and\n        # ._tstate_lock.  Obscure:  we rebind ._tstate_lock last so that the\n        # \"assert self._is_stopped\" in ._wait_for_tstate_lock() always works\n        # (the assert is executed only if ._tstate_lock is None).\n        #\n        # Special case:  _main_thread releases ._tstate_lock via this\n        # module's _shutdown() function.\n        lock = self._tstate_lock\n        if lock is not None:\n            assert not lock.locked()\n        self._is_stopped = True\n        self._tstate_lock = None\n        if not self.daemon:\n            with _shutdown_locks_lock:\n                # Remove our lock and other released locks from _shutdown_locks\n                _maintain_shutdown_locks()\n\n    def _delete(self):\n        \"Remove current thread from the dict of currently running threads.\"\n        with _active_limbo_lock:\n            del _active[get_ident()]\n            # There must not be any python code between the previous line\n            # and after the lock is released.  Otherwise a tracing function\n            # could try to acquire the lock again in the same thread, (in\n            # current_thread()), and would block.\n\n    def join(self, timeout=None):\n        \"\"\"Wait until the thread terminates.\n\n        This blocks the calling thread until the thread whose join() method is\n        called terminates -- either normally or through an unhandled exception\n        or until the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof). As join() always returns None, you must call\n        is_alive() after join() to decide whether a timeout happened -- if the\n        thread is still alive, the join() call timed out.\n\n        When the timeout argument is not present or None, the operation will\n        block until the thread terminates.\n\n        A thread can be join()ed many times.\n\n        join() raises a RuntimeError if an attempt is made to join the current\n        thread as that would cause a deadlock. It is also an error to join() a\n        thread before it has been started and attempts to do so raises the same\n        exception.\n\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if not self._started.is_set():\n            raise RuntimeError(\"cannot join thread before it is started\")\n        if self is current_thread():\n            raise RuntimeError(\"cannot join current thread\")\n\n        if timeout is None:\n            self._wait_for_tstate_lock()\n        else:\n            # the behavior of a negative timeout isn't documented, but\n            # historically .join(timeout=x) for x<0 has acted as if timeout=0\n            self._wait_for_tstate_lock(timeout=max(timeout, 0))\n\n    def _wait_for_tstate_lock(self, block=True, timeout=-1):\n        # Issue #18808: wait for the thread state to be gone.\n        # At the end of the thread's life, after all knowledge of the thread\n        # is removed from C data structures, C code releases our _tstate_lock.\n        # This method passes its arguments to _tstate_lock.acquire().\n        # If the lock is acquired, the C code is done, and self._stop() is\n        # called.  That sets ._is_stopped to True, and ._tstate_lock to None.\n        lock = self._tstate_lock\n        if lock is None:\n            # already determined that the C code is done\n            assert self._is_stopped\n            return\n\n        try:\n            if lock.acquire(block, timeout):\n                lock.release()\n                self._stop()\n        except:\n            if lock.locked():\n                # bpo-45274: lock.acquire() acquired the lock, but the function\n                # was interrupted with an exception before reaching the\n                # lock.release(). It can happen if a signal handler raises an\n                # exception, like CTRL+C which raises KeyboardInterrupt.\n                lock.release()\n                self._stop()\n            raise\n\n    @property\n    def name(self):\n        \"\"\"A string used for identification purposes only.\n\n        It has no semantics. Multiple threads may be given the same name. The\n        initial name is set by the constructor.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        assert self._initialized, \"Thread.__init__() not called\"\n        self._name = str(name)\n\n    @property\n    def ident(self):\n        \"\"\"Thread identifier of this thread or None if it has not been started.\n\n        This is a nonzero integer. See the get_ident() function. Thread\n        identifiers may be recycled when a thread exits and another thread is\n        created. The identifier is available even after the thread has exited.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._ident\n\n    if _HAVE_THREAD_NATIVE_ID:\n        @property\n        def native_id(self):\n            \"\"\"Native integral thread ID of this thread, or None if it has not been started.\n\n            This is a non-negative integer. See the get_native_id() function.\n            This represents the Thread ID as reported by the kernel.\n\n            \"\"\"\n            assert self._initialized, \"Thread.__init__() not called\"\n            return self._native_id\n\n    def is_alive(self):\n        \"\"\"Return whether the thread is alive.\n\n        This method returns True just before the run() method starts until just\n        after the run() method terminates. See also the module function\n        enumerate().\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        if self._is_stopped or not self._started.is_set():\n            return False\n        self._wait_for_tstate_lock(False)\n        return not self._is_stopped\n\n    @property\n    def daemon(self):\n        \"\"\"A boolean value indicating whether this thread is a daemon thread.\n\n        This must be set before start() is called, otherwise RuntimeError is\n        raised. Its initial value is inherited from the creating thread; the\n        main thread is not a daemon thread and therefore all threads created in\n        the main thread default to daemon = False.\n\n        The entire Python program exits when only daemon threads are left.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._daemonic\n\n    @daemon.setter\n    def daemon(self, daemonic):\n        if not self._initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if self._started.is_set():\n            raise RuntimeError(\"cannot set daemon status of active thread\")\n        self._daemonic = daemonic\n\n    def isDaemon(self):\n        \"\"\"Return whether this thread is a daemon.\n\n        This method is deprecated, use the daemon attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('isDaemon() is deprecated, get the daemon attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.daemon\n\n    def setDaemon(self, daemonic):\n        \"\"\"Set whether this thread is a daemon.\n\n        This method is deprecated, use the .daemon property instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('setDaemon() is deprecated, set the daemon attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        self.daemon = daemonic\n\n    def getName(self):\n        \"\"\"Return a string used for identification purposes only.\n\n        This method is deprecated, use the name attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('getName() is deprecated, get the name attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.name\n\n    def setName(self, name):\n        \"\"\"Set the name string for this thread.\n\n        This method is deprecated, use the name attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('setName() is deprecated, set the name attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        self.name = name\n\n\ntry:\n    from _thread import (_excepthook as excepthook,\n                         _ExceptHookArgs as ExceptHookArgs)\nexcept ImportError:\n    # Simple Python implementation if _thread._excepthook() is not available\n    from traceback import print_exception as _print_exception\n    from collections import namedtuple\n\n    _ExceptHookArgs = namedtuple(\n        'ExceptHookArgs',\n        'exc_type exc_value exc_traceback thread')\n\n    def ExceptHookArgs(args):\n        return _ExceptHookArgs(*args)\n\n    def excepthook(args, /):\n        \"\"\"\n        Handle uncaught Thread.run() exception.\n        \"\"\"\n        if args.exc_type == SystemExit:\n            # silently ignore SystemExit\n            return\n\n        if _sys is not None and _sys.stderr is not None:\n            stderr = _sys.stderr\n        elif args.thread is not None:\n            stderr = args.thread._stderr\n            if stderr is None:\n                # do nothing if sys.stderr is None and sys.stderr was None\n                # when the thread was created\n                return\n        else:\n            # do nothing if sys.stderr is None and args.thread is None\n            return\n\n        if args.thread is not None:\n            name = args.thread.name\n        else:\n            name = get_ident()\n        print(f\"Exception in thread {name}:\",\n              file=stderr, flush=True)\n        _print_exception(args.exc_type, args.exc_value, args.exc_traceback,\n                         file=stderr)\n        stderr.flush()\n\n\n# Original value of threading.excepthook\n__excepthook__ = excepthook\n\n\ndef _make_invoke_excepthook():\n    # Create a local namespace to ensure that variables remain alive\n    # when _invoke_excepthook() is called, even if it is called late during\n    # Python shutdown. It is mostly needed for daemon threads.\n\n    old_excepthook = excepthook\n    old_sys_excepthook = _sys.excepthook\n    if old_excepthook is None:\n        raise RuntimeError(\"threading.excepthook is None\")\n    if old_sys_excepthook is None:\n        raise RuntimeError(\"sys.excepthook is None\")\n\n    sys_exc_info = _sys.exc_info\n    local_print = print\n    local_sys = _sys\n\n    def invoke_excepthook(thread):\n        global excepthook\n        try:\n            hook = excepthook\n            if hook is None:\n                hook = old_excepthook\n\n            args = ExceptHookArgs([*sys_exc_info(), thread])\n\n            hook(args)\n        except Exception as exc:\n            exc.__suppress_context__ = True\n            del exc\n\n            if local_sys is not None and local_sys.stderr is not None:\n                stderr = local_sys.stderr\n            else:\n                stderr = thread._stderr\n\n            local_print(\"Exception in threading.excepthook:\",\n                        file=stderr, flush=True)\n\n            if local_sys is not None and local_sys.excepthook is not None:\n                sys_excepthook = local_sys.excepthook\n            else:\n                sys_excepthook = old_sys_excepthook\n\n            sys_excepthook(*sys_exc_info())\n        finally:\n            # Break reference cycle (exception stored in a variable)\n            args = None\n\n    return invoke_excepthook\n\n\n# The timer class was contributed by Itamar Shtull-Trauring\n\nclass Timer(Thread):\n    \"\"\"Call a function after a specified number of seconds:\n\n            t = Timer(30.0, f, args=None, kwargs=None)\n            t.start()\n            t.cancel()     # stop the timer's action if it's still waiting\n\n    \"\"\"\n\n    def __init__(self, interval, function, args=None, kwargs=None):\n        Thread.__init__(self)\n        self.interval = interval\n        self.function = function\n        self.args = args if args is not None else []\n        self.kwargs = kwargs if kwargs is not None else {}\n        self.finished = Event()\n\n    def cancel(self):\n        \"\"\"Stop the timer if it hasn't finished yet.\"\"\"\n        self.finished.set()\n\n    def run(self):\n        self.finished.wait(self.interval)\n        if not self.finished.is_set():\n            self.function(*self.args, **self.kwargs)\n        self.finished.set()\n\n\n# Special thread class to represent the main thread\n\nclass _MainThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=\"MainThread\", daemon=False)\n        self._set_tstate_lock()\n        self._started.set()\n        self._set_ident()\n        if _HAVE_THREAD_NATIVE_ID:\n            self._set_native_id()\n        with _active_limbo_lock:\n            _active[self._ident] = self\n\n\n# Dummy thread class to represent threads not started here.\n# These aren't garbage collected when they die, nor can they be waited for.\n# If they invoke anything in threading.py that calls current_thread(), they\n# leave an entry in the _active dict forever after.\n# Their purpose is to return *something* from current_thread().\n# They are marked as daemon threads so we won't wait for them\n# when we exit (conform previous semantics).\n\nclass _DummyThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=_newname(\"Dummy-%d\"), daemon=True)\n\n        self._started.set()\n        self._set_ident()\n        if _HAVE_THREAD_NATIVE_ID:\n            self._set_native_id()\n        with _active_limbo_lock:\n            _active[self._ident] = self\n\n    def _stop(self):\n        pass\n\n    def is_alive(self):\n        assert not self._is_stopped and self._started.is_set()\n        return True\n\n    def join(self, timeout=None):\n        assert False, \"cannot join a dummy thread\"\n\n\n# Global API functions\n\ndef current_thread():\n    \"\"\"Return the current Thread object, corresponding to the caller's thread of control.\n\n    If the caller's thread of control was not created through the threading\n    module, a dummy thread object with limited functionality is returned.\n\n    \"\"\"\n    try:\n        return _active[get_ident()]\n    except KeyError:\n        return _DummyThread()\n\ndef currentThread():\n    \"\"\"Return the current Thread object, corresponding to the caller's thread of control.\n\n    This function is deprecated, use current_thread() instead.\n\n    \"\"\"\n    import warnings\n    warnings.warn('currentThread() is deprecated, use current_thread() instead',\n                  DeprecationWarning, stacklevel=2)\n    return current_thread()\n\ndef active_count():\n    \"\"\"Return the number of Thread objects currently alive.\n\n    The returned count is equal to the length of the list returned by\n    enumerate().\n\n    \"\"\"\n    with _active_limbo_lock:\n        return len(_active) + len(_limbo)\n\ndef activeCount():\n    \"\"\"Return the number of Thread objects currently alive.\n\n    This function is deprecated, use active_count() instead.\n\n    \"\"\"\n    import warnings\n    warnings.warn('activeCount() is deprecated, use active_count() instead',\n                  DeprecationWarning, stacklevel=2)\n    return active_count()\n\ndef _enumerate():\n    # Same as enumerate(), but without the lock. Internal use only.\n    return list(_active.values()) + list(_limbo.values())\n\ndef enumerate():\n    \"\"\"Return a list of all Thread objects currently alive.\n\n    The list includes daemonic threads, dummy thread objects created by\n    current_thread(), and the main thread. It excludes terminated threads and\n    threads that have not yet been started.\n\n    \"\"\"\n    with _active_limbo_lock:\n        return list(_active.values()) + list(_limbo.values())\n\n\n_threading_atexits = []\n_SHUTTING_DOWN = False\n\ndef _register_atexit(func, *arg, **kwargs):\n    \"\"\"CPython internal: register *func* to be called before joining threads.\n\n    The registered *func* is called with its arguments just before all\n    non-daemon threads are joined in `_shutdown()`. It provides a similar\n    purpose to `atexit.register()`, but its functions are called prior to\n    threading shutdown instead of interpreter shutdown.\n\n    For similarity to atexit, the registered functions are called in reverse.\n    \"\"\"\n    if _SHUTTING_DOWN:\n        raise RuntimeError(\"can't register atexit after shutdown\")\n\n    call = functools.partial(func, *arg, **kwargs)\n    _threading_atexits.append(call)\n\n\nfrom _thread import stack_size\n\n# Create the main thread object,\n# and make it available for the interpreter\n# (Py_Main) as threading._shutdown.\n\n_main_thread = _MainThread()\n\ndef _shutdown():\n    \"\"\"\n    Wait until the Python thread state of all non-daemon threads get deleted.\n    \"\"\"\n    # Obscure:  other threads may be waiting to join _main_thread.  That's\n    # dubious, but some code does it.  We can't wait for C code to release\n    # the main thread's tstate_lock - that won't happen until the interpreter\n    # is nearly dead.  So we release it here.  Note that just calling _stop()\n    # isn't enough:  other threads may already be waiting on _tstate_lock.\n    if _main_thread._is_stopped:\n        # _shutdown() was already called\n        return\n\n    global _SHUTTING_DOWN\n    _SHUTTING_DOWN = True\n\n    # Call registered threading atexit functions before threads are joined.\n    # Order is reversed, similar to atexit.\n    for atexit_call in reversed(_threading_atexits):\n        atexit_call()\n\n    # Main thread\n    if _main_thread.ident == get_ident():\n        tlock = _main_thread._tstate_lock\n        # The main thread isn't finished yet, so its thread state lock can't\n        # have been released.\n        assert tlock is not None\n        assert tlock.locked()\n        tlock.release()\n        _main_thread._stop()\n    else:\n        # bpo-1596321: _shutdown() must be called in the main thread.\n        # If the threading module was not imported by the main thread,\n        # _main_thread is the thread which imported the threading module.\n        # In this case, ignore _main_thread, similar behavior than for threads\n        # spawned by C libraries or using _thread.start_new_thread().\n        pass\n\n    # Join all non-deamon threads\n    while True:\n        with _shutdown_locks_lock:\n            locks = list(_shutdown_locks)\n            _shutdown_locks.clear()\n\n        if not locks:\n            break\n\n        for lock in locks:\n            # mimic Thread.join()\n            lock.acquire()\n            lock.release()\n\n        # new threads can be spawned while we were waiting for the other\n        # threads to complete\n\n\ndef main_thread():\n    \"\"\"Return the main thread object.\n\n    In normal conditions, the main thread is the thread from which the\n    Python interpreter was started.\n    \"\"\"\n    return _main_thread\n\n# get thread-local implementation, either from the thread\n# module, or from the python fallback\n\ntry:\n    from _thread import _local as local\nexcept ImportError:\n    from _threading_local import local\n\n\ndef _after_fork():\n    \"\"\"\n    Cleanup threading module state that should not exist after a fork.\n    \"\"\"\n    # Reset _active_limbo_lock, in case we forked while the lock was held\n    # by another (non-forked) thread.  http://bugs.python.org/issue874900\n    global _active_limbo_lock, _main_thread\n    global _shutdown_locks_lock, _shutdown_locks\n    _active_limbo_lock = RLock()\n\n    # fork() only copied the current thread; clear references to others.\n    new_active = {}\n\n    try:\n        current = _active[get_ident()]\n    except KeyError:\n        # fork() was called in a thread which was not spawned\n        # by threading.Thread. For example, a thread spawned\n        # by thread.start_new_thread().\n        current = _MainThread()\n\n    _main_thread = current\n\n    # reset _shutdown() locks: threads re-register their _tstate_lock below\n    _shutdown_locks_lock = _allocate_lock()\n    _shutdown_locks = set()\n\n    with _active_limbo_lock:\n        # Dangling thread instances must still have their locks reset,\n        # because someone may join() them.\n        threads = set(_enumerate())\n        threads.update(_dangling)\n        for thread in threads:\n            # Any lock/condition variable may be currently locked or in an\n            # invalid state, so we reinitialize them.\n            if thread is current:\n                # There is only one active thread. We reset the ident to\n                # its new value since it can have changed.\n                thread._reset_internal_locks(True)\n                ident = get_ident()\n                if isinstance(thread, _DummyThread):\n                    thread.__class__ = _MainThread\n                    thread._name = 'MainThread'\n                    thread._daemonic = False\n                    thread._set_tstate_lock()\n                thread._ident = ident\n                new_active[ident] = thread\n            else:\n                # All the others are already stopped.\n                thread._reset_internal_locks(False)\n                thread._stop()\n\n        _limbo.clear()\n        _active.clear()\n        _active.update(new_active)\n        assert len(_active) == 1\n\n\nif hasattr(_os, \"register_at_fork\"):\n    _os.register_at_fork(after_in_child=_after_fork)\n", 1673], "/usr/local/lib/python3.11/concurrent/futures/_base.py": ["# Copyright 2009 Brian Quinlan. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n__author__ = 'Brian Quinlan (brian@sweetapp.com)'\n\nimport collections\nimport logging\nimport threading\nimport time\nimport types\n\nFIRST_COMPLETED = 'FIRST_COMPLETED'\nFIRST_EXCEPTION = 'FIRST_EXCEPTION'\nALL_COMPLETED = 'ALL_COMPLETED'\n_AS_COMPLETED = '_AS_COMPLETED'\n\n# Possible future states (for internal use by the futures package).\nPENDING = 'PENDING'\nRUNNING = 'RUNNING'\n# The future was cancelled by the user...\nCANCELLED = 'CANCELLED'\n# ...and _Waiter.add_cancelled() was called by a worker.\nCANCELLED_AND_NOTIFIED = 'CANCELLED_AND_NOTIFIED'\nFINISHED = 'FINISHED'\n\n_FUTURE_STATES = [\n    PENDING,\n    RUNNING,\n    CANCELLED,\n    CANCELLED_AND_NOTIFIED,\n    FINISHED\n]\n\n_STATE_TO_DESCRIPTION_MAP = {\n    PENDING: \"pending\",\n    RUNNING: \"running\",\n    CANCELLED: \"cancelled\",\n    CANCELLED_AND_NOTIFIED: \"cancelled\",\n    FINISHED: \"finished\"\n}\n\n# Logger for internal use by the futures package.\nLOGGER = logging.getLogger(\"concurrent.futures\")\n\nclass Error(Exception):\n    \"\"\"Base class for all future-related exceptions.\"\"\"\n    pass\n\nclass CancelledError(Error):\n    \"\"\"The Future was cancelled.\"\"\"\n    pass\n\nTimeoutError = TimeoutError  # make local alias for the standard exception\n\nclass InvalidStateError(Error):\n    \"\"\"The operation is not allowed in this state.\"\"\"\n    pass\n\nclass _Waiter(object):\n    \"\"\"Provides the event that wait() and as_completed() block on.\"\"\"\n    def __init__(self):\n        self.event = threading.Event()\n        self.finished_futures = []\n\n    def add_result(self, future):\n        self.finished_futures.append(future)\n\n    def add_exception(self, future):\n        self.finished_futures.append(future)\n\n    def add_cancelled(self, future):\n        self.finished_futures.append(future)\n\nclass _AsCompletedWaiter(_Waiter):\n    \"\"\"Used by as_completed().\"\"\"\n\n    def __init__(self):\n        super(_AsCompletedWaiter, self).__init__()\n        self.lock = threading.Lock()\n\n    def add_result(self, future):\n        with self.lock:\n            super(_AsCompletedWaiter, self).add_result(future)\n            self.event.set()\n\n    def add_exception(self, future):\n        with self.lock:\n            super(_AsCompletedWaiter, self).add_exception(future)\n            self.event.set()\n\n    def add_cancelled(self, future):\n        with self.lock:\n            super(_AsCompletedWaiter, self).add_cancelled(future)\n            self.event.set()\n\nclass _FirstCompletedWaiter(_Waiter):\n    \"\"\"Used by wait(return_when=FIRST_COMPLETED).\"\"\"\n\n    def add_result(self, future):\n        super().add_result(future)\n        self.event.set()\n\n    def add_exception(self, future):\n        super().add_exception(future)\n        self.event.set()\n\n    def add_cancelled(self, future):\n        super().add_cancelled(future)\n        self.event.set()\n\nclass _AllCompletedWaiter(_Waiter):\n    \"\"\"Used by wait(return_when=FIRST_EXCEPTION and ALL_COMPLETED).\"\"\"\n\n    def __init__(self, num_pending_calls, stop_on_exception):\n        self.num_pending_calls = num_pending_calls\n        self.stop_on_exception = stop_on_exception\n        self.lock = threading.Lock()\n        super().__init__()\n\n    def _decrement_pending_calls(self):\n        with self.lock:\n            self.num_pending_calls -= 1\n            if not self.num_pending_calls:\n                self.event.set()\n\n    def add_result(self, future):\n        super().add_result(future)\n        self._decrement_pending_calls()\n\n    def add_exception(self, future):\n        super().add_exception(future)\n        if self.stop_on_exception:\n            self.event.set()\n        else:\n            self._decrement_pending_calls()\n\n    def add_cancelled(self, future):\n        super().add_cancelled(future)\n        self._decrement_pending_calls()\n\nclass _AcquireFutures(object):\n    \"\"\"A context manager that does an ordered acquire of Future conditions.\"\"\"\n\n    def __init__(self, futures):\n        self.futures = sorted(futures, key=id)\n\n    def __enter__(self):\n        for future in self.futures:\n            future._condition.acquire()\n\n    def __exit__(self, *args):\n        for future in self.futures:\n            future._condition.release()\n\ndef _create_and_install_waiters(fs, return_when):\n    if return_when == _AS_COMPLETED:\n        waiter = _AsCompletedWaiter()\n    elif return_when == FIRST_COMPLETED:\n        waiter = _FirstCompletedWaiter()\n    else:\n        pending_count = sum(\n                f._state not in [CANCELLED_AND_NOTIFIED, FINISHED] for f in fs)\n\n        if return_when == FIRST_EXCEPTION:\n            waiter = _AllCompletedWaiter(pending_count, stop_on_exception=True)\n        elif return_when == ALL_COMPLETED:\n            waiter = _AllCompletedWaiter(pending_count, stop_on_exception=False)\n        else:\n            raise ValueError(\"Invalid return condition: %r\" % return_when)\n\n    for f in fs:\n        f._waiters.append(waiter)\n\n    return waiter\n\n\ndef _yield_finished_futures(fs, waiter, ref_collect):\n    \"\"\"\n    Iterate on the list *fs*, yielding finished futures one by one in\n    reverse order.\n    Before yielding a future, *waiter* is removed from its waiters\n    and the future is removed from each set in the collection of sets\n    *ref_collect*.\n\n    The aim of this function is to avoid keeping stale references after\n    the future is yielded and before the iterator resumes.\n    \"\"\"\n    while fs:\n        f = fs[-1]\n        for futures_set in ref_collect:\n            futures_set.remove(f)\n        with f._condition:\n            f._waiters.remove(waiter)\n        del f\n        # Careful not to keep a reference to the popped value\n        yield fs.pop()\n\n\ndef as_completed(fs, timeout=None):\n    \"\"\"An iterator over the given futures that yields each as it completes.\n\n    Args:\n        fs: The sequence of Futures (possibly created by different Executors) to\n            iterate over.\n        timeout: The maximum number of seconds to wait. If None, then there\n            is no limit on the wait time.\n\n    Returns:\n        An iterator that yields the given Futures as they complete (finished or\n        cancelled). If any given Futures are duplicated, they will be returned\n        once.\n\n    Raises:\n        TimeoutError: If the entire result iterator could not be generated\n            before the given timeout.\n    \"\"\"\n    if timeout is not None:\n        end_time = timeout + time.monotonic()\n\n    fs = set(fs)\n    total_futures = len(fs)\n    with _AcquireFutures(fs):\n        finished = set(\n                f for f in fs\n                if f._state in [CANCELLED_AND_NOTIFIED, FINISHED])\n        pending = fs - finished\n        waiter = _create_and_install_waiters(fs, _AS_COMPLETED)\n    finished = list(finished)\n    try:\n        yield from _yield_finished_futures(finished, waiter,\n                                           ref_collect=(fs,))\n\n        while pending:\n            if timeout is None:\n                wait_timeout = None\n            else:\n                wait_timeout = end_time - time.monotonic()\n                if wait_timeout < 0:\n                    raise TimeoutError(\n                            '%d (of %d) futures unfinished' % (\n                            len(pending), total_futures))\n\n            waiter.event.wait(wait_timeout)\n\n            with waiter.lock:\n                finished = waiter.finished_futures\n                waiter.finished_futures = []\n                waiter.event.clear()\n\n            # reverse to keep finishing order\n            finished.reverse()\n            yield from _yield_finished_futures(finished, waiter,\n                                               ref_collect=(fs, pending))\n\n    finally:\n        # Remove waiter from unfinished futures\n        for f in fs:\n            with f._condition:\n                f._waiters.remove(waiter)\n\nDoneAndNotDoneFutures = collections.namedtuple(\n        'DoneAndNotDoneFutures', 'done not_done')\ndef wait(fs, timeout=None, return_when=ALL_COMPLETED):\n    \"\"\"Wait for the futures in the given sequence to complete.\n\n    Args:\n        fs: The sequence of Futures (possibly created by different Executors) to\n            wait upon.\n        timeout: The maximum number of seconds to wait. If None, then there\n            is no limit on the wait time.\n        return_when: Indicates when this function should return. The options\n            are:\n\n            FIRST_COMPLETED - Return when any future finishes or is\n                              cancelled.\n            FIRST_EXCEPTION - Return when any future finishes by raising an\n                              exception. If no future raises an exception\n                              then it is equivalent to ALL_COMPLETED.\n            ALL_COMPLETED -   Return when all futures finish or are cancelled.\n\n    Returns:\n        A named 2-tuple of sets. The first set, named 'done', contains the\n        futures that completed (is finished or cancelled) before the wait\n        completed. The second set, named 'not_done', contains uncompleted\n        futures. Duplicate futures given to *fs* are removed and will be\n        returned only once.\n    \"\"\"\n    fs = set(fs)\n    with _AcquireFutures(fs):\n        done = {f for f in fs\n                   if f._state in [CANCELLED_AND_NOTIFIED, FINISHED]}\n        not_done = fs - done\n        if (return_when == FIRST_COMPLETED) and done:\n            return DoneAndNotDoneFutures(done, not_done)\n        elif (return_when == FIRST_EXCEPTION) and done:\n            if any(f for f in done\n                   if not f.cancelled() and f.exception() is not None):\n                return DoneAndNotDoneFutures(done, not_done)\n\n        if len(done) == len(fs):\n            return DoneAndNotDoneFutures(done, not_done)\n\n        waiter = _create_and_install_waiters(fs, return_when)\n\n    waiter.event.wait(timeout)\n    for f in fs:\n        with f._condition:\n            f._waiters.remove(waiter)\n\n    done.update(waiter.finished_futures)\n    return DoneAndNotDoneFutures(done, fs - done)\n\n\ndef _result_or_cancel(fut, timeout=None):\n    try:\n        try:\n            return fut.result(timeout)\n        finally:\n            fut.cancel()\n    finally:\n        # Break a reference cycle with the exception in self._exception\n        del fut\n\n\nclass Future(object):\n    \"\"\"Represents the result of an asynchronous computation.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes the future. Should not be called by clients.\"\"\"\n        self._condition = threading.Condition()\n        self._state = PENDING\n        self._result = None\n        self._exception = None\n        self._waiters = []\n        self._done_callbacks = []\n\n    def _invoke_callbacks(self):\n        for callback in self._done_callbacks:\n            try:\n                callback(self)\n            except Exception:\n                LOGGER.exception('exception calling callback for %r', self)\n\n    def __repr__(self):\n        with self._condition:\n            if self._state == FINISHED:\n                if self._exception:\n                    return '<%s at %#x state=%s raised %s>' % (\n                        self.__class__.__name__,\n                        id(self),\n                        _STATE_TO_DESCRIPTION_MAP[self._state],\n                        self._exception.__class__.__name__)\n                else:\n                    return '<%s at %#x state=%s returned %s>' % (\n                        self.__class__.__name__,\n                        id(self),\n                        _STATE_TO_DESCRIPTION_MAP[self._state],\n                        self._result.__class__.__name__)\n            return '<%s at %#x state=%s>' % (\n                    self.__class__.__name__,\n                    id(self),\n                   _STATE_TO_DESCRIPTION_MAP[self._state])\n\n    def cancel(self):\n        \"\"\"Cancel the future if possible.\n\n        Returns True if the future was cancelled, False otherwise. A future\n        cannot be cancelled if it is running or has already completed.\n        \"\"\"\n        with self._condition:\n            if self._state in [RUNNING, FINISHED]:\n                return False\n\n            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n                return True\n\n            self._state = CANCELLED\n            self._condition.notify_all()\n\n        self._invoke_callbacks()\n        return True\n\n    def cancelled(self):\n        \"\"\"Return True if the future was cancelled.\"\"\"\n        with self._condition:\n            return self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]\n\n    def running(self):\n        \"\"\"Return True if the future is currently executing.\"\"\"\n        with self._condition:\n            return self._state == RUNNING\n\n    def done(self):\n        \"\"\"Return True if the future was cancelled or finished executing.\"\"\"\n        with self._condition:\n            return self._state in [CANCELLED, CANCELLED_AND_NOTIFIED, FINISHED]\n\n    def __get_result(self):\n        if self._exception:\n            try:\n                raise self._exception\n            finally:\n                # Break a reference cycle with the exception in self._exception\n                self = None\n        else:\n            return self._result\n\n    def add_done_callback(self, fn):\n        \"\"\"Attaches a callable that will be called when the future finishes.\n\n        Args:\n            fn: A callable that will be called with this future as its only\n                argument when the future completes or is cancelled. The callable\n                will always be called by a thread in the same process in which\n                it was added. If the future has already completed or been\n                cancelled then the callable will be called immediately. These\n                callables are called in the order that they were added.\n        \"\"\"\n        with self._condition:\n            if self._state not in [CANCELLED, CANCELLED_AND_NOTIFIED, FINISHED]:\n                self._done_callbacks.append(fn)\n                return\n        try:\n            fn(self)\n        except Exception:\n            LOGGER.exception('exception calling callback for %r', self)\n\n    def result(self, timeout=None):\n        \"\"\"Return the result of the call that the future represents.\n\n        Args:\n            timeout: The number of seconds to wait for the result if the future\n                isn't done. If None, then there is no limit on the wait time.\n\n        Returns:\n            The result of the call that the future represents.\n\n        Raises:\n            CancelledError: If the future was cancelled.\n            TimeoutError: If the future didn't finish executing before the given\n                timeout.\n            Exception: If the call raised then that exception will be raised.\n        \"\"\"\n        try:\n            with self._condition:\n                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n                    raise CancelledError()\n                elif self._state == FINISHED:\n                    return self.__get_result()\n\n                self._condition.wait(timeout)\n\n                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n                    raise CancelledError()\n                elif self._state == FINISHED:\n                    return self.__get_result()\n                else:\n                    raise TimeoutError()\n        finally:\n            # Break a reference cycle with the exception in self._exception\n            self = None\n\n    def exception(self, timeout=None):\n        \"\"\"Return the exception raised by the call that the future represents.\n\n        Args:\n            timeout: The number of seconds to wait for the exception if the\n                future isn't done. If None, then there is no limit on the wait\n                time.\n\n        Returns:\n            The exception raised by the call that the future represents or None\n            if the call completed without raising.\n\n        Raises:\n            CancelledError: If the future was cancelled.\n            TimeoutError: If the future didn't finish executing before the given\n                timeout.\n        \"\"\"\n\n        with self._condition:\n            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n                raise CancelledError()\n            elif self._state == FINISHED:\n                return self._exception\n\n            self._condition.wait(timeout)\n\n            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n                raise CancelledError()\n            elif self._state == FINISHED:\n                return self._exception\n            else:\n                raise TimeoutError()\n\n    # The following methods should only be used by Executors and in tests.\n    def set_running_or_notify_cancel(self):\n        \"\"\"Mark the future as running or process any cancel notifications.\n\n        Should only be used by Executor implementations and unit tests.\n\n        If the future has been cancelled (cancel() was called and returned\n        True) then any threads waiting on the future completing (though calls\n        to as_completed() or wait()) are notified and False is returned.\n\n        If the future was not cancelled then it is put in the running state\n        (future calls to running() will return True) and True is returned.\n\n        This method should be called by Executor implementations before\n        executing the work associated with this future. If this method returns\n        False then the work should not be executed.\n\n        Returns:\n            False if the Future was cancelled, True otherwise.\n\n        Raises:\n            RuntimeError: if this method was already called or if set_result()\n                or set_exception() was called.\n        \"\"\"\n        with self._condition:\n            if self._state == CANCELLED:\n                self._state = CANCELLED_AND_NOTIFIED\n                for waiter in self._waiters:\n                    waiter.add_cancelled(self)\n                # self._condition.notify_all() is not necessary because\n                # self.cancel() triggers a notification.\n                return False\n            elif self._state == PENDING:\n                self._state = RUNNING\n                return True\n            else:\n                LOGGER.critical('Future %s in unexpected state: %s',\n                                id(self),\n                                self._state)\n                raise RuntimeError('Future in unexpected state')\n\n    def set_result(self, result):\n        \"\"\"Sets the return value of work associated with the future.\n\n        Should only be used by Executor implementations and unit tests.\n        \"\"\"\n        with self._condition:\n            if self._state in {CANCELLED, CANCELLED_AND_NOTIFIED, FINISHED}:\n                raise InvalidStateError('{}: {!r}'.format(self._state, self))\n            self._result = result\n            self._state = FINISHED\n            for waiter in self._waiters:\n                waiter.add_result(self)\n            self._condition.notify_all()\n        self._invoke_callbacks()\n\n    def set_exception(self, exception):\n        \"\"\"Sets the result of the future as being the given exception.\n\n        Should only be used by Executor implementations and unit tests.\n        \"\"\"\n        with self._condition:\n            if self._state in {CANCELLED, CANCELLED_AND_NOTIFIED, FINISHED}:\n                raise InvalidStateError('{}: {!r}'.format(self._state, self))\n            self._exception = exception\n            self._state = FINISHED\n            for waiter in self._waiters:\n                waiter.add_exception(self)\n            self._condition.notify_all()\n        self._invoke_callbacks()\n\n    __class_getitem__ = classmethod(types.GenericAlias)\n\nclass Executor(object):\n    \"\"\"This is an abstract base class for concrete asynchronous executors.\"\"\"\n\n    def submit(self, fn, /, *args, **kwargs):\n        \"\"\"Submits a callable to be executed with the given arguments.\n\n        Schedules the callable to be executed as fn(*args, **kwargs) and returns\n        a Future instance representing the execution of the callable.\n\n        Returns:\n            A Future representing the given call.\n        \"\"\"\n        raise NotImplementedError()\n\n    def map(self, fn, *iterables, timeout=None, chunksize=1):\n        \"\"\"Returns an iterator equivalent to map(fn, iter).\n\n        Args:\n            fn: A callable that will take as many arguments as there are\n                passed iterables.\n            timeout: The maximum number of seconds to wait. If None, then there\n                is no limit on the wait time.\n            chunksize: The size of the chunks the iterable will be broken into\n                before being passed to a child process. This argument is only\n                used by ProcessPoolExecutor; it is ignored by\n                ThreadPoolExecutor.\n\n        Returns:\n            An iterator equivalent to: map(func, *iterables) but the calls may\n            be evaluated out-of-order.\n\n        Raises:\n            TimeoutError: If the entire result iterator could not be generated\n                before the given timeout.\n            Exception: If fn(*args) raises for any values.\n        \"\"\"\n        if timeout is not None:\n            end_time = timeout + time.monotonic()\n\n        fs = [self.submit(fn, *args) for args in zip(*iterables)]\n\n        # Yield must be hidden in closure so that the futures are submitted\n        # before the first iterator value is required.\n        def result_iterator():\n            try:\n                # reverse to keep finishing order\n                fs.reverse()\n                while fs:\n                    # Careful not to keep a reference to the popped future\n                    if timeout is None:\n                        yield _result_or_cancel(fs.pop())\n                    else:\n                        yield _result_or_cancel(fs.pop(), end_time - time.monotonic())\n            finally:\n                for future in fs:\n                    future.cancel()\n        return result_iterator()\n\n    def shutdown(self, wait=True, *, cancel_futures=False):\n        \"\"\"Clean-up the resources associated with the Executor.\n\n        It is safe to call this method several times. Otherwise, no other\n        methods can be called after this one.\n\n        Args:\n            wait: If True then shutdown will not return until all running\n                futures have finished executing and the resources used by the\n                executor have been reclaimed.\n            cancel_futures: If True then shutdown will cancel all pending\n                futures. Futures that are completed or running will not be\n                cancelled.\n        \"\"\"\n        pass\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.shutdown(wait=True)\n        return False\n\n\nclass BrokenExecutor(RuntimeError):\n    \"\"\"\n    Raised when a executor has become non-functional after a severe failure.\n    \"\"\"\n", 654], "/usr/local/lib/python3.11/concurrent/futures/thread.py": ["# Copyright 2009 Brian Quinlan. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Implements ThreadPoolExecutor.\"\"\"\n\n__author__ = 'Brian Quinlan (brian@sweetapp.com)'\n\nfrom concurrent.futures import _base\nimport itertools\nimport queue\nimport threading\nimport types\nimport weakref\nimport os\n\n\n_threads_queues = weakref.WeakKeyDictionary()\n_shutdown = False\n# Lock that ensures that new workers are not created while the interpreter is\n# shutting down. Must be held while mutating _threads_queues and _shutdown.\n_global_shutdown_lock = threading.Lock()\n\ndef _python_exit():\n    global _shutdown\n    with _global_shutdown_lock:\n        _shutdown = True\n    items = list(_threads_queues.items())\n    for t, q in items:\n        q.put(None)\n    for t, q in items:\n        t.join()\n\n# Register for `_python_exit()` to be called just before joining all\n# non-daemon threads. This is used instead of `atexit.register()` for\n# compatibility with subinterpreters, which no longer support daemon threads.\n# See bpo-39812 for context.\nthreading._register_atexit(_python_exit)\n\n# At fork, reinitialize the `_global_shutdown_lock` lock in the child process\nif hasattr(os, 'register_at_fork'):\n    os.register_at_fork(before=_global_shutdown_lock.acquire,\n                        after_in_child=_global_shutdown_lock._at_fork_reinit,\n                        after_in_parent=_global_shutdown_lock.release)\n\n\nclass _WorkItem(object):\n    def __init__(self, future, fn, args, kwargs):\n        self.future = future\n        self.fn = fn\n        self.args = args\n        self.kwargs = kwargs\n\n    def run(self):\n        if not self.future.set_running_or_notify_cancel():\n            return\n\n        try:\n            result = self.fn(*self.args, **self.kwargs)\n        except BaseException as exc:\n            self.future.set_exception(exc)\n            # Break a reference cycle with the exception 'exc'\n            self = None\n        else:\n            self.future.set_result(result)\n\n    __class_getitem__ = classmethod(types.GenericAlias)\n\n\ndef _worker(executor_reference, work_queue, initializer, initargs):\n    if initializer is not None:\n        try:\n            initializer(*initargs)\n        except BaseException:\n            _base.LOGGER.critical('Exception in initializer:', exc_info=True)\n            executor = executor_reference()\n            if executor is not None:\n                executor._initializer_failed()\n            return\n    try:\n        while True:\n            work_item = work_queue.get(block=True)\n            if work_item is not None:\n                work_item.run()\n                # Delete references to object. See issue16284\n                del work_item\n\n                # attempt to increment idle count\n                executor = executor_reference()\n                if executor is not None:\n                    executor._idle_semaphore.release()\n                del executor\n                continue\n\n            executor = executor_reference()\n            # Exit if:\n            #   - The interpreter is shutting down OR\n            #   - The executor that owns the worker has been collected OR\n            #   - The executor that owns the worker has been shutdown.\n            if _shutdown or executor is None or executor._shutdown:\n                # Flag the executor as shutting down as early as possible if it\n                # is not gc-ed yet.\n                if executor is not None:\n                    executor._shutdown = True\n                # Notice other workers\n                work_queue.put(None)\n                return\n            del executor\n    except BaseException:\n        _base.LOGGER.critical('Exception in worker', exc_info=True)\n\n\nclass BrokenThreadPool(_base.BrokenExecutor):\n    \"\"\"\n    Raised when a worker thread in a ThreadPoolExecutor failed initializing.\n    \"\"\"\n\n\nclass ThreadPoolExecutor(_base.Executor):\n\n    # Used to assign unique thread names when thread_name_prefix is not supplied.\n    _counter = itertools.count().__next__\n\n    def __init__(self, max_workers=None, thread_name_prefix='',\n                 initializer=None, initargs=()):\n        \"\"\"Initializes a new ThreadPoolExecutor instance.\n\n        Args:\n            max_workers: The maximum number of threads that can be used to\n                execute the given calls.\n            thread_name_prefix: An optional name prefix to give our threads.\n            initializer: A callable used to initialize worker threads.\n            initargs: A tuple of arguments to pass to the initializer.\n        \"\"\"\n        if max_workers is None:\n            # ThreadPoolExecutor is often used to:\n            # * CPU bound task which releases GIL\n            # * I/O bound task (which releases GIL, of course)\n            #\n            # We use cpu_count + 4 for both types of tasks.\n            # But we limit it to 32 to avoid consuming surprisingly large resource\n            # on many core machine.\n            max_workers = min(32, (os.cpu_count() or 1) + 4)\n        if max_workers <= 0:\n            raise ValueError(\"max_workers must be greater than 0\")\n\n        if initializer is not None and not callable(initializer):\n            raise TypeError(\"initializer must be a callable\")\n\n        self._max_workers = max_workers\n        self._work_queue = queue.SimpleQueue()\n        self._idle_semaphore = threading.Semaphore(0)\n        self._threads = set()\n        self._broken = False\n        self._shutdown = False\n        self._shutdown_lock = threading.Lock()\n        self._thread_name_prefix = (thread_name_prefix or\n                                    (\"ThreadPoolExecutor-%d\" % self._counter()))\n        self._initializer = initializer\n        self._initargs = initargs\n\n    def submit(self, fn, /, *args, **kwargs):\n        with self._shutdown_lock, _global_shutdown_lock:\n            if self._broken:\n                raise BrokenThreadPool(self._broken)\n\n            if self._shutdown:\n                raise RuntimeError('cannot schedule new futures after shutdown')\n            if _shutdown:\n                raise RuntimeError('cannot schedule new futures after '\n                                   'interpreter shutdown')\n\n            f = _base.Future()\n            w = _WorkItem(f, fn, args, kwargs)\n\n            self._work_queue.put(w)\n            self._adjust_thread_count()\n            return f\n    submit.__doc__ = _base.Executor.submit.__doc__\n\n    def _adjust_thread_count(self):\n        # if idle threads are available, don't spin new threads\n        if self._idle_semaphore.acquire(timeout=0):\n            return\n\n        # When the executor gets lost, the weakref callback will wake up\n        # the worker threads.\n        def weakref_cb(_, q=self._work_queue):\n            q.put(None)\n\n        num_threads = len(self._threads)\n        if num_threads < self._max_workers:\n            thread_name = '%s_%d' % (self._thread_name_prefix or self,\n                                     num_threads)\n            t = threading.Thread(name=thread_name, target=_worker,\n                                 args=(weakref.ref(self, weakref_cb),\n                                       self._work_queue,\n                                       self._initializer,\n                                       self._initargs))\n            t.start()\n            self._threads.add(t)\n            _threads_queues[t] = self._work_queue\n\n    def _initializer_failed(self):\n        with self._shutdown_lock:\n            self._broken = ('A thread initializer failed, the thread pool '\n                            'is not usable anymore')\n            # Drain work queue and mark pending futures failed\n            while True:\n                try:\n                    work_item = self._work_queue.get_nowait()\n                except queue.Empty:\n                    break\n                if work_item is not None:\n                    work_item.future.set_exception(BrokenThreadPool(self._broken))\n\n    def shutdown(self, wait=True, *, cancel_futures=False):\n        with self._shutdown_lock:\n            self._shutdown = True\n            if cancel_futures:\n                # Drain all work items from the queue, and then cancel their\n                # associated futures.\n                while True:\n                    try:\n                        work_item = self._work_queue.get_nowait()\n                    except queue.Empty:\n                        break\n                    if work_item is not None:\n                        work_item.future.cancel()\n\n            # Send a wake-up to prevent threads calling\n            # _work_queue.get(block=True) from permanently blocking.\n            self._work_queue.put(None)\n        if wait:\n            for t in self._threads:\n                t.join()\n    shutdown.__doc__ = _base.Executor.shutdown.__doc__\n", 236], "/usr/local/lib/python3.11/asyncio/base_futures.py": ["__all__ = ()\n\nimport reprlib\nfrom _thread import get_ident\n\nfrom . import format_helpers\n\n# States for Future.\n_PENDING = 'PENDING'\n_CANCELLED = 'CANCELLED'\n_FINISHED = 'FINISHED'\n\n\ndef isfuture(obj):\n    \"\"\"Check for a Future.\n\n    This returns True when obj is a Future instance or is advertising\n    itself as duck-type compatible by setting _asyncio_future_blocking.\n    See comment in Future for more details.\n    \"\"\"\n    return (hasattr(obj.__class__, '_asyncio_future_blocking') and\n            obj._asyncio_future_blocking is not None)\n\n\ndef _format_callbacks(cb):\n    \"\"\"helper function for Future.__repr__\"\"\"\n    size = len(cb)\n    if not size:\n        cb = ''\n\n    def format_cb(callback):\n        return format_helpers._format_callback_source(callback, ())\n\n    if size == 1:\n        cb = format_cb(cb[0][0])\n    elif size == 2:\n        cb = '{}, {}'.format(format_cb(cb[0][0]), format_cb(cb[1][0]))\n    elif size > 2:\n        cb = '{}, <{} more>, {}'.format(format_cb(cb[0][0]),\n                                        size - 2,\n                                        format_cb(cb[-1][0]))\n    return f'cb=[{cb}]'\n\n\ndef _future_repr_info(future):\n    # (Future) -> str\n    \"\"\"helper function for Future.__repr__\"\"\"\n    info = [future._state.lower()]\n    if future._state == _FINISHED:\n        if future._exception is not None:\n            info.append(f'exception={future._exception!r}')\n        else:\n            # use reprlib to limit the length of the output, especially\n            # for very long strings\n            result = reprlib.repr(future._result)\n            info.append(f'result={result}')\n    if future._callbacks:\n        info.append(_format_callbacks(future._callbacks))\n    if future._source_traceback:\n        frame = future._source_traceback[-1]\n        info.append(f'created at {frame[0]}:{frame[1]}')\n    return info\n\n\n@reprlib.recursive_repr()\ndef _future_repr(future):\n    info = ' '.join(_future_repr_info(future))\n    return f'<{future.__class__.__name__} {info}>'\n", 68], "/usr/local/lib/python3.11/asyncio/threads.py": ["\"\"\"High-level support for working with threads in asyncio\"\"\"\n\nimport functools\nimport contextvars\n\nfrom . import events\n\n\n__all__ = \"to_thread\",\n\n\nasync def to_thread(func, /, *args, **kwargs):\n    \"\"\"Asynchronously run function *func* in a separate thread.\n\n    Any *args and **kwargs supplied for this function are directly passed\n    to *func*. Also, the current :class:`contextvars.Context` is propagated,\n    allowing context variables from the main thread to be accessed in the\n    separate thread.\n\n    Return a coroutine that can be awaited to get the eventual result of *func*.\n    \"\"\"\n    loop = events.get_running_loop()\n    ctx = contextvars.copy_context()\n    func_call = functools.partial(ctx.run, func, *args, **kwargs)\n    return await loop.run_in_executor(None, func_call)\n", 25], "/app/app/main.py": ["import asyncio\nimport contextlib\nimport json\nimport logging\nimport os\nfrom contextlib import asynccontextmanager\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom fastapi import FastAPI, HTTPException, Query, WebSocket, WebSocketDisconnect\nfrom fastapi.encoders import jsonable_encoder\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom prometheus_client import make_asgi_app\nfrom dotenv import load_dotenv\n\nfrom .core.config import Settings, get_settings, hot_reload, validate_settings\nfrom .services import MarketDataService\nfrom .services.options_service import OptionContract, OptionsChain, options_service\nfrom .observability.trace import maybe_trace\n\n# Load environment variables\nload_dotenv()\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Global service instance\nmarket_service = MarketDataService()\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup\n    await market_service.start_background_tasks()\n    logger.info(\"Market Data Service started\")\n    yield\n    # Shutdown\n    market_service.background_tasks_running = False\n    await market_service.data_collector.stop()\n    logger.info(\"Market Data Service stopped\")\n\napp = FastAPI(\n    title=\"Market Data Service\",\n    description=\"Reliable real-time and historical stock data\",\n    version=\"1.0.0\",\n    lifespan=lifespan\n)\n\nmetrics_app = make_asgi_app()\napp.mount(\"/metrics\", metrics_app)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.get(\"/\")\nasync def root():\n    stats = await market_service.get_stats()\n    settings = get_settings()\n    return {\n        \"service\": \"Market Data Service\",\n        \"status\": \"running\",\n        \"timestamp\": datetime.now().isoformat(),\n        \"env\": settings.env,\n        \"policy_version\": settings.policy_version,\n        \"stats\": stats,\n        \"endpoints\": {\n            \"stock_price\": \"/stocks/{symbol}/price\",\n            \"historical\": \"/stocks/{symbol}/history\",\n            \"company_profile\": \"/stocks/{symbol}/profile\",\n            \"news_sentiment\": \"/stocks/{symbol}/sentiment\",\n            \"options_metrics\": \"/options/{symbol}/metrics\",\n            \"options_metrics_history\": \"/options/{symbol}/metrics/history\",\n            \"unusual_options_activity\": \"/options/{symbol}/unusual\",\n            \"options_flow_analysis\": \"/options/{symbol}/flow\",\n            \"macro_snapshot\": \"/factors/macro\",\n            \"macro_history\": \"/factors/macro/{factor_key}/history\",\n            \"macro_refresh\": \"/admin/macro/refresh\",\n            \"batch_quotes\": \"/stocks/batch\",\n            \"health\": \"/health\",\n            \"websocket\": \"/ws/{symbol}\"\n        }\n    }\n\n@app.get(\"/health\")\nasync def health_check():\n    stats = await market_service.get_stats()\n    settings = get_settings()\n    return {\n        \"status\": \"healthy\",\n        \"service\": \"market-data-service\",\n        \"timestamp\": datetime.now().isoformat(),\n        \"env\": settings.env,\n        \"policy_version\": settings.policy_version,\n        \"providers_status\": {\n            provider[\"name\"]: provider[\"available\"]\n            for provider in stats[\"providers\"]\n        },\n        \"macro\": stats.get(\"macro\", {}),\n        \"options_metrics\": stats.get(\"options_metrics\", {})\n    }\n\n\n@app.post(\"/ops/validate\")\ndef ops_validate():\n    candidate = Settings()\n    ok, reason = validate_settings(candidate)\n    return {\"ok\": ok, \"reason\": reason, \"policy_version\": candidate.policy_version}\n\n\n@app.post(\"/ops/reload\")\nasync def ops_reload():\n    result = hot_reload()\n    if result.get(\"ok\"):\n        market_service.reload_configuration()\n        result[\"policy_version\"] = get_settings().policy_version\n    return result\n\n\n@app.get(\"/stats/providers\")\nasync def stats_providers():\n    registry = market_service.registry\n    breakers = market_service.breakers\n    settings = get_settings()\n\n    providers = []\n    for name, entry in registry.providers.items():\n        providers.append(\n            {\n                \"provider\": name,\n                \"state\": breakers.get_state(name),\n                \"capabilities\": sorted(entry.capabilities),\n                \"latency_p95_ms\": entry.stats.latency_p95_ms,\n                \"error_ewma\": entry.stats.error_ewma,\n                \"completeness_deficit\": entry.stats.completeness_deficit,\n                \"health_score\": registry.health_score(name),\n                \"history\": [{\"t\": t, \"h\": h} for (t, h) in entry.h_history],\n            }\n        )\n\n    return {\n        \"policy_version\": settings.policy_version,\n        \"policy\": {\n            \"bars_1m\": settings.POLICY_BARS_1M,\n            \"eod\": settings.POLICY_EOD,\n            \"quotes_l1\": settings.POLICY_QUOTES_L1,\n            \"options_chain\": settings.POLICY_OPTIONS_CHAIN,\n        },\n        \"registered_capabilities\": registry.capabilities_map(),\n        \"providers\": providers,\n    }\n\n\n@app.get(\"/stats/cadence\")\nasync def stats_cadence():\n    \"\"\"Return universe tiering and cadence policy.\"\"\"\n    settings = get_settings()\n    return {\n        \"tiers\": {\"max\": settings.TIER_MAXS},\n        \"cadence\": {\n            \"T0\": settings.CADENCE_T0,\n            \"T1\": settings.CADENCE_T1,\n            \"T2\": settings.CADENCE_T2,\n        },\n        \"use_rlc\": settings.USE_RLC,\n        \"local_sweep\": settings.LOCAL_SWEEP_ENABLED,\n        \"policy_version\": settings.policy_version,\n    }\n\n\n@app.get(\"/ops/cursor/{symbol}/{interval}/{source}\")\nasync def read_cursor(symbol: str, interval: str, source: str):\n    \"\"\"Read ingestion cursor for a symbol/interval/source.\"\"\"\n    from .services.database import db_service\n    ts = await db_service.get_cursor(symbol, interval, source)\n    return {\n        \"symbol\": symbol,\n        \"interval\": interval,\n        \"source\": source,\n        \"last_ts\": ts.isoformat() + \"Z\" if ts else None,\n    }\n\n\n@app.post(\"/ops/backfill\")\nasync def trigger_backfill(payload: dict):\n    \"\"\"\n    Manually trigger a backfill.\n    Payload: {symbol, interval, start, end, priority}\n    \"\"\"\n    from .services.database import db_service\n    req = {**payload}\n    with maybe_trace(f\"backfill_{req.get('symbol', 'unknown')}\"):\n        try:\n            start = datetime.fromisoformat(req[\"start\"].replace(\"Z\", \"\"))\n            end = datetime.fromisoformat(req[\"end\"].replace(\"Z\", \"\"))\n        except Exception:\n            raise HTTPException(400, \"Invalid start/end ISO8601\")\n\n        await db_service.enqueue_backfill(\n            req[\"symbol\"], req.get(\"interval\", \"1m\"), start, end, req.get(\"priority\", \"T2\")\n        )\n        return {\"enqueued\": True}\n\n@app.get(\"/factors/macro\")\nasync def get_macro_factors(factors: Optional[List[str]] = Query(None)):\n    \"\"\"Return snapshot of configured macro factors.\"\"\"\n    return await market_service.get_macro_snapshot(factors)\n\n\n@app.get(\"/factors/macro/{factor_key}/history\")\nasync def get_macro_history_endpoint(factor_key: str, lookback_days: int = 90):\n    \"\"\"Return macro factor history for the specified key.\"\"\"\n    try:\n        return await market_service.get_macro_history(factor_key, lookback_days)\n    except ValueError:\n        raise HTTPException(status_code=404, detail=f\"Unknown macro factor {factor_key.upper()}\")\n\n\n@app.post(\"/admin/macro/refresh\")\nasync def refresh_macro_endpoint(factor: Optional[str] = Query(None)):\n    \"\"\"Trigger a manual macro factor refresh.\"\"\"\n    return await market_service.refresh_macro_factors(factor)\n\n\n@app.get(\"/stocks/{symbol}/price\")\nasync def get_stock_price(symbol: str):\n    \"\"\"Get current stock price\"\"\"\n    with maybe_trace(f\"get_price_{symbol}\"):\n        return await market_service.get_stock_price(symbol.upper())\n\n@app.get(\"/stocks/{symbol}/history\")\nasync def get_historical_data(symbol: str, period: str = \"1mo\"):\n    \"\"\"Get historical stock data\"\"\"\n    with maybe_trace(f\"get_history_{symbol}_{period}\"):\n        return await market_service.get_historical_data(symbol.upper(), period)\n\n\n@app.get(\"/stocks/{symbol}/intraday\")\nasync def get_intraday_data(symbol: str, interval: str = \"1m\"):\n    \"\"\"Get intraday stock data (1-minute bars).\"\"\"\n    return await market_service.get_intraday_data(symbol.upper(), interval)\n\n\n@app.post(\"/stocks/batch\")\nasync def get_multiple_stocks(symbols: List[str]):\n    \"\"\"Get prices for multiple stocks\"\"\"\n    results = []\n    for symbol in symbols:\n        try:\n            data = await market_service.get_stock_price(symbol.upper())\n            results.append({\"status\": \"success\", **data})\n        except Exception as e:\n            results.append({\n                \"status\": \"error\",\n                \"symbol\": symbol.upper(),\n                \"error\": str(e)\n            })\n    return {\"results\": results}\n\n@app.websocket(\"/ws/{symbol}\")\nasync def websocket_endpoint(websocket: WebSocket, symbol: str):\n    \"\"\"WebSocket endpoint for real-time price updates\"\"\"\n    await market_service.connection_manager.connect(websocket, symbol.upper())\n    try:\n        # Send initial data\n        try:\n            initial_data = await market_service.get_stock_price(symbol.upper())\n            await websocket.send_text(json.dumps(initial_data))\n        except:\n            pass\n        \n        # Keep connection alive and handle client messages\n        while True:\n            try:\n                # Wait for client message (ping/pong or subscription changes)\n                message = await asyncio.wait_for(websocket.receive_text(), timeout=30)\n                # Echo back for now (can add subscription management later)\n                await websocket.send_text(json.dumps({\"status\": \"received\", \"message\": message}))\n            except asyncio.TimeoutError:\n                # Send ping to keep connection alive\n                await websocket.send_text(json.dumps({\"type\": \"ping\", \"timestamp\": datetime.now().isoformat()}))\n            except Exception as e:\n                logger.error(f\"WebSocket error: {e}\")\n                break\n    except WebSocketDisconnect:\n        logger.info(f\"Client disconnected from {symbol}\")\n    finally:\n        market_service.connection_manager.disconnect(websocket)\n\n\ndef _find_contract(contracts: List[OptionContract], target_symbol: Optional[str]) -> Optional[OptionContract]:\n    if not target_symbol:\n        return None\n    for contract in contracts:\n        if contract.symbol == target_symbol:\n            return contract\n    return None\n\n\nasync def _options_chain_payload(symbol: str) -> Tuple[Dict[str, Any], OptionsChain]:\n    \"\"\"Fetch the latest options chain and return a summary payload.\"\"\"\n    chain = await options_service.fetch_options_chain(symbol)\n    total_calls = len(chain.calls)\n    total_puts = len(chain.puts)\n    expiries = [exp.isoformat() for exp in chain.expiries]\n\n    return {\n        \"type\": \"options_chain\",\n        \"symbol\": symbol,\n        \"underlying_price\": chain.underlying_price,\n        \"timestamp\": datetime.now().isoformat(),\n        \"calls\": total_calls,\n        \"puts\": total_puts,\n        \"expiries\": expiries,\n    }, chain\n\n\ndef _options_update_payload(symbol: str, chain: OptionsChain, metrics) -> Dict[str, Any]:\n    \"\"\"Build a payload for incremental options metrics updates.\"\"\"\n    call_volume = metrics.call_volume or 0\n    put_volume = metrics.put_volume or 0\n    total_volume = call_volume + put_volume\n    volume_ratio = (call_volume / put_volume) if put_volume else None\n    iv_rank = metrics.metadata.get(\"iv_rank\")\n\n    atm_call_symbol = metrics.metadata.get(\"atm_call\")\n    atm_put_symbol = metrics.metadata.get(\"atm_put\")\n    atm_call = _find_contract(chain.calls, atm_call_symbol)\n    atm_put = _find_contract(chain.puts, atm_put_symbol)\n\n    if iv_rank is None and metrics.atm_iv is not None:\n        iv_rank = max(0.0, min(100.0, metrics.atm_iv * 100))\n\n    return {\n        \"type\": \"options_update\",\n        \"symbol\": symbol,\n        \"underlying_price\": chain.underlying_price,\n        \"timestamp\": datetime.now().isoformat(),\n        \"iv_rank\": iv_rank,\n        \"atm_call_iv\": (atm_call.implied_volatility if atm_call else metrics.atm_iv),\n        \"atm_put_iv\": (atm_put.implied_volatility if atm_put else metrics.atm_iv),\n        \"volume_ratio\": volume_ratio,\n        \"total_volume\": total_volume,\n    }\n\n\nasync def _options_consumer(websocket: WebSocket):\n    \"\"\"Listen for client messages to keep the WebSocket alive.\"\"\"\n    try:\n        while True:\n            message = await websocket.receive_text()\n            if message.strip().lower() == \"ping\":\n                await websocket.send_text(json.dumps({\"type\": \"pong\", \"timestamp\": datetime.now().isoformat()}))\n    except WebSocketDisconnect:\n        raise\n    except Exception as exc:\n        logger.error(\"Options WebSocket consumer error: %s\", exc)\n\n\nasync def _options_producer(websocket: WebSocket, symbol: str, interval: int):\n    \"\"\"Continuously push options updates to the client.\"\"\"\n    try:\n        chain_payload, chain = await _options_chain_payload(symbol)\n    except Exception as exc:\n        logger.error(\"Error fetching initial options chain for %s: %s\", symbol, exc)\n        await websocket.send_text(\n            json.dumps(\n                {\n                    \"type\": \"options_error\",\n                    \"symbol\": symbol,\n                    \"timestamp\": datetime.now().isoformat(),\n                    \"error\": \"Failed to load options chain\",\n                }\n            )\n        )\n        raise\n\n    await websocket.send_text(json.dumps(chain_payload))\n\n    async def send_update(current_chain: OptionsChain):\n        try:\n            metrics = options_service.calculate_chain_metrics(current_chain)\n            update = _options_update_payload(symbol, current_chain, metrics)\n            await websocket.send_text(json.dumps(update))\n        except Exception as exc:\n            logger.error(\"Error generating options update for %s: %s\", symbol, exc)\n            await websocket.send_text(\n                json.dumps(\n                    {\n                        \"type\": \"options_error\",\n                        \"symbol\": symbol,\n                        \"timestamp\": datetime.now().isoformat(),\n                        \"error\": \"Failed to generate options update\",\n                    }\n                )\n            )\n\n    # Send initial metrics right away\n    await send_update(chain)\n\n    refresh_interval = max(1, interval)\n    while True:\n        await asyncio.sleep(refresh_interval)\n        try:\n            chain = await options_service.fetch_options_chain(symbol)\n        except Exception as exc:\n            logger.error(\"Error refreshing options chain for %s: %s\", symbol, exc)\n            await websocket.send_text(\n                json.dumps(\n                    {\n                        \"type\": \"options_error\",\n                        \"symbol\": symbol,\n                        \"timestamp\": datetime.now().isoformat(),\n                        \"error\": \"Failed to refresh options chain\",\n                    }\n                )\n            )\n            continue\n\n        await send_update(chain)\n\n\nasync def _handle_options_websocket(websocket: WebSocket, symbol: Optional[str]):\n    \"\"\"Shared handler for options WebSocket connections with symbol from path or query.\"\"\"\n    if not symbol:\n        symbol = websocket.query_params.get(\"symbol\")\n    if not symbol:\n        await websocket.accept()\n        await websocket.send_text(json.dumps({\"type\": \"error\", \"error\": \"symbol query parameter is required\"}))\n        await websocket.close(code=4400)\n        return\n\n    symbol = symbol.upper()\n    await websocket.accept()\n\n    interval = getattr(market_service, \"update_interval\", 5)\n    consumer_task = asyncio.create_task(_options_consumer(websocket))\n    producer_task = asyncio.create_task(_options_producer(websocket, symbol, interval))\n\n    try:\n        done, pending = await asyncio.wait(\n            [consumer_task, producer_task],\n            return_when=asyncio.FIRST_EXCEPTION,\n        )\n        for task in done:\n            exc = task.exception()\n            if exc and not isinstance(exc, WebSocketDisconnect):\n                logger.error(\"Options WebSocket task error for %s: %s\", symbol, exc)\n    except WebSocketDisconnect:\n        logger.info(\"Options WebSocket client disconnected for %s\", symbol)\n    finally:\n        for task in (consumer_task, producer_task):\n            task.cancel()\n            with contextlib.suppress(asyncio.CancelledError):\n                await task\n        with contextlib.suppress(Exception):\n            await websocket.close()\n\n\n@app.websocket(\"/ws/options\")\nasync def options_websocket_query(websocket: WebSocket):\n    \"\"\"Options WebSocket endpoint using symbol query parameter.\"\"\"\n    await _handle_options_websocket(websocket, None)\n\n\n@app.websocket(\"/ws/options/{symbol}\")\nasync def options_websocket_path(websocket: WebSocket, symbol: str):\n    \"\"\"Options WebSocket endpoint with symbol in the path.\"\"\"\n    await _handle_options_websocket(websocket, symbol)\n\n\n@app.get(\"/options/{symbol}/chain\")\nasync def get_options_chain(symbol: str):\n    \"\"\"Return the full options chain for a symbol.\"\"\"\n    with maybe_trace(f\"get_options_chain_{symbol}\"):\n        try:\n            chain = await options_service.fetch_options_chain(symbol.upper())\n        except Exception as exc:\n            logger.error(\"Error fetching options chain for %s: %s\", symbol, exc)\n            raise HTTPException(status_code=502, detail=f\"Unable to fetch options chain for {symbol.upper()}\") from exc\n\n        return jsonable_encoder(chain)\n\n\ndef _validate_sentiment(sentiment: str, allowed: List[str]) -> str:\n    normalized = (sentiment or \"all\").lower()\n    if normalized not in allowed:\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Invalid sentiment '{sentiment}'. Expected one of {', '.join(allowed)}.\",\n        )\n    return normalized\n\n\ndef _validate_complexity(complexity: str, allowed: List[str]) -> str:\n    normalized = (complexity or \"all\").lower()\n    if normalized not in allowed:\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Invalid complexity '{complexity}'. Expected one of {', '.join(allowed)}.\",\n        )\n    return normalized\n\n\n@app.get(\"/options/{symbol}/strategies\")\nasync def get_options_strategies(\n    symbol: str,\n    sentiment: str = Query(\"all\", description=\"Filter by sentiment: bullish, bearish, neutral, or all\"),\n    complexity: str = Query(\"all\", description=\"Filter by complexity: beginner, intermediate, advanced, or all\"),\n):\n    \"\"\"Return advanced options strategies derived from the options chain.\"\"\"\n    sentiment_normalized = _validate_sentiment(sentiment, [\"bullish\", \"bearish\", \"neutral\", \"all\"])\n    complexity_normalized = _validate_complexity(complexity, [\"beginner\", \"intermediate\", \"advanced\", \"all\"])\n\n    try:\n        strategies = await options_service.get_advanced_strategies(\n            symbol.upper(), sentiment=sentiment_normalized, complexity=complexity_normalized\n        )\n    except Exception as exc:\n        logger.error(\"Error generating strategies for %s: %s\", symbol, exc)\n        raise HTTPException(status_code=502, detail=f\"Unable to generate strategies for {symbol.upper()}\") from exc\n\n    payload = {\"symbol\": symbol.upper(), \"count\": len(strategies), \"strategies\": strategies}\n    return jsonable_encoder(payload)\n\n\n@app.get(\"/options/{symbol}/suggestions\")\nasync def get_options_suggestions(\n    symbol: str,\n    sentiment: str = Query(\"bullish\", description=\"Trading bias for suggestions\"),\n    target_delta: float = Query(0.3, ge=0, le=1, description=\"Approximate option delta to target\"),\n    max_dte: int = Query(7, ge=1, le=60, description=\"Maximum days to expiration\"),\n    min_liquidity: float = Query(50, ge=0, le=100, description=\"Minimum liquidity score threshold\"),\n):\n    \"\"\"Return tailored day-trading suggestions built from the options chain.\"\"\"\n    sentiment_normalized = _validate_sentiment(sentiment, [\"bullish\", \"bearish\", \"neutral\"])\n\n    try:\n        chain = await options_service.fetch_options_chain(symbol.upper())\n        suggestions = options_service.suggest_day_trade(\n            symbol.upper(),\n            sentiment_normalized,\n            chain.underlying_price,\n            target_delta=target_delta,\n            max_dte=max_dte,\n            min_liquidity=min_liquidity,\n        )\n    except Exception as exc:\n        logger.error(\"Error generating suggestions for %s: %s\", symbol, exc)\n        raise HTTPException(status_code=502, detail=f\"Unable to generate suggestions for {symbol.upper()}\") from exc\n\n    if not suggestions:\n        raise HTTPException(\n            status_code=404,\n            detail=f\"No trade suggestions available for {symbol.upper()} with current filters\",\n        )\n\n    payload = {\n        \"symbol\": symbol.upper(),\n        \"underlying_price\": chain.underlying_price,\n        \"suggestions\": suggestions,\n    }\n    return jsonable_encoder(payload)\n\n\n@app.get(\"/options/{symbol}/metrics\")\nasync def get_options_metrics_endpoint(symbol: str):\n    \"\"\"Return latest ATM IV, skew, and implied move metrics.\"\"\"\n    with maybe_trace(f\"get_options_metrics_{symbol}\"):\n        return await market_service.get_options_metrics(symbol.upper())\n\n\n@app.get(\"/options/{symbol}/metrics/history\")\nasync def get_options_metrics_history_endpoint(symbol: str, limit: int = 50):\n    \"\"\"Return historical options metrics records.\"\"\"\n    history = await market_service.get_options_metrics_history(symbol.upper(), limit)\n    return {\"symbol\": symbol.upper(), \"metrics\": history}\n\n@app.get(\"/options/{symbol}/unusual\")\nasync def get_unusual_options_activity_endpoint(\n    symbol: str, \n    lookback_days: int = Query(20, ge=1, le=90, description=\"Days to look back for unusual activity\")\n):\n    \"\"\"Detect and return unusual options activity for a symbol\"\"\"\n    return await market_service.get_unusual_options_activity(symbol.upper(), lookback_days)\n\n@app.get(\"/options/{symbol}/flow\")\nasync def get_options_flow_analysis_endpoint(symbol: str):\n    \"\"\"Get comprehensive options flow analysis including unusual activity and smart money signals\"\"\"\n    return await market_service.get_options_flow_analysis(symbol.upper())\n\n@app.get(\"/stocks/{symbol}/profile\")\nasync def get_company_profile(symbol: str):\n    \"\"\"Get company profile data\"\"\"\n    with maybe_trace(f\"get_profile_{symbol}\"):\n        return await market_service.get_company_profile(symbol.upper())\n\n@app.get(\"/stocks/{symbol}/sentiment\")\nasync def get_news_sentiment(symbol: str):\n    \"\"\"Get news sentiment data\"\"\"\n    return await market_service.get_news_sentiment(symbol.upper())\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(\n        app,\n        host=\"0.0.0.0\",\n        port=8001,\n        log_level=\"info\",\n        reload=True\n    )\n\n", 616], "/usr/local/lib/python3.11/site-packages/fastapi/routing.py": ["import dataclasses\nimport email.message\nimport functools\nimport inspect\nimport json\nimport sys\nfrom contextlib import AsyncExitStack, asynccontextmanager\nfrom enum import Enum, IntEnum\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Awaitable,\n    Callable,\n    Collection,\n    Coroutine,\n    Dict,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Type,\n    Union,\n)\n\nfrom fastapi import params\nfrom fastapi._compat import (\n    ModelField,\n    Undefined,\n    _get_model_config,\n    _model_dump,\n    _normalize_errors,\n    lenient_issubclass,\n)\nfrom fastapi.datastructures import Default, DefaultPlaceholder\nfrom fastapi.dependencies.models import Dependant\nfrom fastapi.dependencies.utils import (\n    _should_embed_body_fields,\n    get_body_field,\n    get_dependant,\n    get_flat_dependant,\n    get_parameterless_sub_dependant,\n    get_typed_return_annotation,\n    solve_dependencies,\n)\nfrom fastapi.encoders import jsonable_encoder\nfrom fastapi.exceptions import (\n    FastAPIError,\n    RequestValidationError,\n    ResponseValidationError,\n    WebSocketRequestValidationError,\n)\nfrom fastapi.types import DecoratedCallable, IncEx\nfrom fastapi.utils import (\n    create_cloned_field,\n    create_model_field,\n    generate_unique_id,\n    get_value_or_default,\n    is_body_allowed_for_status_code,\n)\nfrom pydantic import BaseModel\nfrom starlette import routing\nfrom starlette._exception_handler import wrap_app_handling_exceptions\nfrom starlette._utils import is_async_callable\nfrom starlette.concurrency import run_in_threadpool\nfrom starlette.exceptions import HTTPException\nfrom starlette.requests import Request\nfrom starlette.responses import JSONResponse, Response\nfrom starlette.routing import (\n    BaseRoute,\n    Match,\n    compile_path,\n    get_name,\n)\nfrom starlette.routing import Mount as Mount  # noqa\nfrom starlette.types import AppType, ASGIApp, Lifespan, Receive, Scope, Send\nfrom starlette.websockets import WebSocket\nfrom typing_extensions import Annotated, Doc, deprecated\n\nif sys.version_info >= (3, 13):  # pragma: no cover\n    from inspect import iscoroutinefunction\nelse:  # pragma: no cover\n    from asyncio import iscoroutinefunction\n\n\n# Copy of starlette.routing.request_response modified to include the\n# dependencies' AsyncExitStack\ndef request_response(\n    func: Callable[[Request], Union[Awaitable[Response], Response]],\n) -> ASGIApp:\n    \"\"\"\n    Takes a function or coroutine `func(request) -> response`,\n    and returns an ASGI application.\n    \"\"\"\n    f: Callable[[Request], Awaitable[Response]] = (\n        func if is_async_callable(func) else functools.partial(run_in_threadpool, func)  # type:ignore\n    )\n\n    async def app(scope: Scope, receive: Receive, send: Send) -> None:\n        request = Request(scope, receive, send)\n\n        async def app(scope: Scope, receive: Receive, send: Send) -> None:\n            # Starts customization\n            response_awaited = False\n            async with AsyncExitStack() as stack:\n                scope[\"fastapi_inner_astack\"] = stack\n                # Same as in Starlette\n                response = await f(request)\n                await response(scope, receive, send)\n                # Continues customization\n                response_awaited = True\n            if not response_awaited:\n                raise FastAPIError(\n                    \"Response not awaited. There's a high chance that the \"\n                    \"application code is raising an exception and a dependency with yield \"\n                    \"has a block with a bare except, or a block with except Exception, \"\n                    \"and is not raising the exception again. Read more about it in the \"\n                    \"docs: https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-with-yield/#dependencies-with-yield-and-except\"\n                )\n\n        # Same as in Starlette\n        await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n\n    return app\n\n\n# Copy of starlette.routing.websocket_session modified to include the\n# dependencies' AsyncExitStack\ndef websocket_session(\n    func: Callable[[WebSocket], Awaitable[None]],\n) -> ASGIApp:\n    \"\"\"\n    Takes a coroutine `func(session)`, and returns an ASGI application.\n    \"\"\"\n    # assert asyncio.iscoroutinefunction(func), \"WebSocket endpoints must be async\"\n\n    async def app(scope: Scope, receive: Receive, send: Send) -> None:\n        session = WebSocket(scope, receive=receive, send=send)\n\n        async def app(scope: Scope, receive: Receive, send: Send) -> None:\n            # Starts customization\n            async with AsyncExitStack() as stack:\n                scope[\"fastapi_inner_astack\"] = stack\n                # Same as in Starlette\n                await func(session)\n\n        # Same as in Starlette\n        await wrap_app_handling_exceptions(app, session)(scope, receive, send)\n\n    return app\n\n\ndef _prepare_response_content(\n    res: Any,\n    *,\n    exclude_unset: bool,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n) -> Any:\n    if isinstance(res, BaseModel):\n        read_with_orm_mode = getattr(_get_model_config(res), \"read_with_orm_mode\", None)\n        if read_with_orm_mode:\n            # Let from_orm extract the data from this model instead of converting\n            # it now to a dict.\n            # Otherwise, there's no way to extract lazy data that requires attribute\n            # access instead of dict iteration, e.g. lazy relationships.\n            return res\n        return _model_dump(\n            res,\n            by_alias=True,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n        )\n    elif isinstance(res, list):\n        return [\n            _prepare_response_content(\n                item,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n            for item in res\n        ]\n    elif isinstance(res, dict):\n        return {\n            k: _prepare_response_content(\n                v,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n            for k, v in res.items()\n        }\n    elif dataclasses.is_dataclass(res):\n        assert not isinstance(res, type)\n        return dataclasses.asdict(res)\n    return res\n\n\ndef _merge_lifespan_context(\n    original_context: Lifespan[Any], nested_context: Lifespan[Any]\n) -> Lifespan[Any]:\n    @asynccontextmanager\n    async def merged_lifespan(\n        app: AppType,\n    ) -> AsyncIterator[Optional[Mapping[str, Any]]]:\n        async with original_context(app) as maybe_original_state:\n            async with nested_context(app) as maybe_nested_state:\n                if maybe_nested_state is None and maybe_original_state is None:\n                    yield None  # old ASGI compatibility\n                else:\n                    yield {**(maybe_nested_state or {}), **(maybe_original_state or {})}\n\n    return merged_lifespan  # type: ignore[return-value]\n\n\nasync def serialize_response(\n    *,\n    field: Optional[ModelField] = None,\n    response_content: Any,\n    include: Optional[IncEx] = None,\n    exclude: Optional[IncEx] = None,\n    by_alias: bool = True,\n    exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n    is_coroutine: bool = True,\n) -> Any:\n    if field:\n        errors = []\n        if not hasattr(field, \"serialize\"):\n            # pydantic v1\n            response_content = _prepare_response_content(\n                response_content,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n        if is_coroutine:\n            value, errors_ = field.validate(response_content, {}, loc=(\"response\",))\n        else:\n            value, errors_ = await run_in_threadpool(\n                field.validate, response_content, {}, loc=(\"response\",)\n            )\n        if isinstance(errors_, list):\n            errors.extend(errors_)\n        elif errors_:\n            errors.append(errors_)\n        if errors:\n            raise ResponseValidationError(\n                errors=_normalize_errors(errors), body=response_content\n            )\n\n        if hasattr(field, \"serialize\"):\n            return field.serialize(\n                value,\n                include=include,\n                exclude=exclude,\n                by_alias=by_alias,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n\n        return jsonable_encoder(\n            value,\n            include=include,\n            exclude=exclude,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n        )\n    else:\n        return jsonable_encoder(response_content)\n\n\nasync def run_endpoint_function(\n    *, dependant: Dependant, values: Dict[str, Any], is_coroutine: bool\n) -> Any:\n    # Only called by get_request_handler. Has been split into its own function to\n    # facilitate profiling endpoints, since inner functions are harder to profile.\n    assert dependant.call is not None, \"dependant.call must be a function\"\n\n    if is_coroutine:\n        return await dependant.call(**values)\n    else:\n        return await run_in_threadpool(dependant.call, **values)\n\n\ndef get_request_handler(\n    dependant: Dependant,\n    body_field: Optional[ModelField] = None,\n    status_code: Optional[int] = None,\n    response_class: Union[Type[Response], DefaultPlaceholder] = Default(JSONResponse),\n    response_field: Optional[ModelField] = None,\n    response_model_include: Optional[IncEx] = None,\n    response_model_exclude: Optional[IncEx] = None,\n    response_model_by_alias: bool = True,\n    response_model_exclude_unset: bool = False,\n    response_model_exclude_defaults: bool = False,\n    response_model_exclude_none: bool = False,\n    dependency_overrides_provider: Optional[Any] = None,\n    embed_body_fields: bool = False,\n) -> Callable[[Request], Coroutine[Any, Any, Response]]:\n    assert dependant.call is not None, \"dependant.call must be a function\"\n    is_coroutine = iscoroutinefunction(dependant.call)\n    is_body_form = body_field and isinstance(body_field.field_info, params.Form)\n    if isinstance(response_class, DefaultPlaceholder):\n        actual_response_class: Type[Response] = response_class.value\n    else:\n        actual_response_class = response_class\n\n    async def app(request: Request) -> Response:\n        response: Union[Response, None] = None\n        file_stack = request.scope.get(\"fastapi_middleware_astack\")\n        assert isinstance(file_stack, AsyncExitStack), (\n            \"fastapi_middleware_astack not found in request scope\"\n        )\n\n        # Read body and auto-close files\n        try:\n            body: Any = None\n            if body_field:\n                if is_body_form:\n                    body = await request.form()\n                    file_stack.push_async_callback(body.close)\n                else:\n                    body_bytes = await request.body()\n                    if body_bytes:\n                        json_body: Any = Undefined\n                        content_type_value = request.headers.get(\"content-type\")\n                        if not content_type_value:\n                            json_body = await request.json()\n                        else:\n                            message = email.message.Message()\n                            message[\"content-type\"] = content_type_value\n                            if message.get_content_maintype() == \"application\":\n                                subtype = message.get_content_subtype()\n                                if subtype == \"json\" or subtype.endswith(\"+json\"):\n                                    json_body = await request.json()\n                        if json_body != Undefined:\n                            body = json_body\n                        else:\n                            body = body_bytes\n        except json.JSONDecodeError as e:\n            validation_error = RequestValidationError(\n                [\n                    {\n                        \"type\": \"json_invalid\",\n                        \"loc\": (\"body\", e.pos),\n                        \"msg\": \"JSON decode error\",\n                        \"input\": {},\n                        \"ctx\": {\"error\": e.msg},\n                    }\n                ],\n                body=e.doc,\n            )\n            raise validation_error from e\n        except HTTPException:\n            # If a middleware raises an HTTPException, it should be raised again\n            raise\n        except Exception as e:\n            http_error = HTTPException(\n                status_code=400, detail=\"There was an error parsing the body\"\n            )\n            raise http_error from e\n\n        # Solve dependencies and run path operation function, auto-closing dependencies\n        errors: List[Any] = []\n        async_exit_stack = request.scope.get(\"fastapi_inner_astack\")\n        assert isinstance(async_exit_stack, AsyncExitStack), (\n            \"fastapi_inner_astack not found in request scope\"\n        )\n        solved_result = await solve_dependencies(\n            request=request,\n            dependant=dependant,\n            body=body,\n            dependency_overrides_provider=dependency_overrides_provider,\n            async_exit_stack=async_exit_stack,\n            embed_body_fields=embed_body_fields,\n        )\n        errors = solved_result.errors\n        if not errors:\n            raw_response = await run_endpoint_function(\n                dependant=dependant,\n                values=solved_result.values,\n                is_coroutine=is_coroutine,\n            )\n            if isinstance(raw_response, Response):\n                if raw_response.background is None:\n                    raw_response.background = solved_result.background_tasks\n                response = raw_response\n            else:\n                response_args: Dict[str, Any] = {\n                    \"background\": solved_result.background_tasks\n                }\n                # If status_code was set, use it, otherwise use the default from the\n                # response class, in the case of redirect it's 307\n                current_status_code = (\n                    status_code if status_code else solved_result.response.status_code\n                )\n                if current_status_code is not None:\n                    response_args[\"status_code\"] = current_status_code\n                if solved_result.response.status_code:\n                    response_args[\"status_code\"] = solved_result.response.status_code\n                content = await serialize_response(\n                    field=response_field,\n                    response_content=raw_response,\n                    include=response_model_include,\n                    exclude=response_model_exclude,\n                    by_alias=response_model_by_alias,\n                    exclude_unset=response_model_exclude_unset,\n                    exclude_defaults=response_model_exclude_defaults,\n                    exclude_none=response_model_exclude_none,\n                    is_coroutine=is_coroutine,\n                )\n                response = actual_response_class(content, **response_args)\n                if not is_body_allowed_for_status_code(response.status_code):\n                    response.body = b\"\"\n                response.headers.raw.extend(solved_result.response.headers.raw)\n        if errors:\n            validation_error = RequestValidationError(\n                _normalize_errors(errors), body=body\n            )\n            raise validation_error\n\n        # Return response\n        assert response\n        return response\n\n    return app\n\n\ndef get_websocket_app(\n    dependant: Dependant,\n    dependency_overrides_provider: Optional[Any] = None,\n    embed_body_fields: bool = False,\n) -> Callable[[WebSocket], Coroutine[Any, Any, Any]]:\n    async def app(websocket: WebSocket) -> None:\n        async_exit_stack = websocket.scope.get(\"fastapi_inner_astack\")\n        assert isinstance(async_exit_stack, AsyncExitStack), (\n            \"fastapi_inner_astack not found in request scope\"\n        )\n        solved_result = await solve_dependencies(\n            request=websocket,\n            dependant=dependant,\n            dependency_overrides_provider=dependency_overrides_provider,\n            async_exit_stack=async_exit_stack,\n            embed_body_fields=embed_body_fields,\n        )\n        if solved_result.errors:\n            raise WebSocketRequestValidationError(\n                _normalize_errors(solved_result.errors)\n            )\n        assert dependant.call is not None, \"dependant.call must be a function\"\n        await dependant.call(**solved_result.values)\n\n    return app\n\n\nclass APIWebSocketRoute(routing.WebSocketRoute):\n    def __init__(\n        self,\n        path: str,\n        endpoint: Callable[..., Any],\n        *,\n        name: Optional[str] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        dependency_overrides_provider: Optional[Any] = None,\n    ) -> None:\n        self.path = path\n        self.endpoint = endpoint\n        self.name = get_name(endpoint) if name is None else name\n        self.dependencies = list(dependencies or [])\n        self.path_regex, self.path_format, self.param_convertors = compile_path(path)\n        self.dependant = get_dependant(path=self.path_format, call=self.endpoint)\n        for depends in self.dependencies[::-1]:\n            self.dependant.dependencies.insert(\n                0,\n                get_parameterless_sub_dependant(depends=depends, path=self.path_format),\n            )\n        self._flat_dependant = get_flat_dependant(self.dependant)\n        self._embed_body_fields = _should_embed_body_fields(\n            self._flat_dependant.body_params\n        )\n        self.app = websocket_session(\n            get_websocket_app(\n                dependant=self.dependant,\n                dependency_overrides_provider=dependency_overrides_provider,\n                embed_body_fields=self._embed_body_fields,\n            )\n        )\n\n    def matches(self, scope: Scope) -> Tuple[Match, Scope]:\n        match, child_scope = super().matches(scope)\n        if match != Match.NONE:\n            child_scope[\"route\"] = self\n        return match, child_scope\n\n\nclass APIRoute(routing.Route):\n    def __init__(\n        self,\n        path: str,\n        endpoint: Callable[..., Any],\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        name: Optional[str] = None,\n        methods: Optional[Union[Set[str], List[str]]] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Union[Type[Response], DefaultPlaceholder] = Default(\n            JSONResponse\n        ),\n        dependency_overrides_provider: Optional[Any] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Union[\n            Callable[[\"APIRoute\"], str], DefaultPlaceholder\n        ] = Default(generate_unique_id),\n    ) -> None:\n        self.path = path\n        self.endpoint = endpoint\n        if isinstance(response_model, DefaultPlaceholder):\n            return_annotation = get_typed_return_annotation(endpoint)\n            if lenient_issubclass(return_annotation, Response):\n                response_model = None\n            else:\n                response_model = return_annotation\n        self.response_model = response_model\n        self.summary = summary\n        self.response_description = response_description\n        self.deprecated = deprecated\n        self.operation_id = operation_id\n        self.response_model_include = response_model_include\n        self.response_model_exclude = response_model_exclude\n        self.response_model_by_alias = response_model_by_alias\n        self.response_model_exclude_unset = response_model_exclude_unset\n        self.response_model_exclude_defaults = response_model_exclude_defaults\n        self.response_model_exclude_none = response_model_exclude_none\n        self.include_in_schema = include_in_schema\n        self.response_class = response_class\n        self.dependency_overrides_provider = dependency_overrides_provider\n        self.callbacks = callbacks\n        self.openapi_extra = openapi_extra\n        self.generate_unique_id_function = generate_unique_id_function\n        self.tags = tags or []\n        self.responses = responses or {}\n        self.name = get_name(endpoint) if name is None else name\n        self.path_regex, self.path_format, self.param_convertors = compile_path(path)\n        if methods is None:\n            methods = [\"GET\"]\n        self.methods: Set[str] = {method.upper() for method in methods}\n        if isinstance(generate_unique_id_function, DefaultPlaceholder):\n            current_generate_unique_id: Callable[[APIRoute], str] = (\n                generate_unique_id_function.value\n            )\n        else:\n            current_generate_unique_id = generate_unique_id_function\n        self.unique_id = self.operation_id or current_generate_unique_id(self)\n        # normalize enums e.g. http.HTTPStatus\n        if isinstance(status_code, IntEnum):\n            status_code = int(status_code)\n        self.status_code = status_code\n        if self.response_model:\n            assert is_body_allowed_for_status_code(status_code), (\n                f\"Status code {status_code} must not have a response body\"\n            )\n            response_name = \"Response_\" + self.unique_id\n            self.response_field = create_model_field(\n                name=response_name,\n                type_=self.response_model,\n                mode=\"serialization\",\n            )\n            # Create a clone of the field, so that a Pydantic submodel is not returned\n            # as is just because it's an instance of a subclass of a more limited class\n            # e.g. UserInDB (containing hashed_password) could be a subclass of User\n            # that doesn't have the hashed_password. But because it's a subclass, it\n            # would pass the validation and be returned as is.\n            # By being a new field, no inheritance will be passed as is. A new model\n            # will always be created.\n            # TODO: remove when deprecating Pydantic v1\n            self.secure_cloned_response_field: Optional[ModelField] = (\n                create_cloned_field(self.response_field)\n            )\n        else:\n            self.response_field = None  # type: ignore\n            self.secure_cloned_response_field = None\n        self.dependencies = list(dependencies or [])\n        self.description = description or inspect.cleandoc(self.endpoint.__doc__ or \"\")\n        # if a \"form feed\" character (page break) is found in the description text,\n        # truncate description text to the content preceding the first \"form feed\"\n        self.description = self.description.split(\"\\f\")[0].strip()\n        response_fields = {}\n        for additional_status_code, response in self.responses.items():\n            assert isinstance(response, dict), \"An additional response must be a dict\"\n            model = response.get(\"model\")\n            if model:\n                assert is_body_allowed_for_status_code(additional_status_code), (\n                    f\"Status code {additional_status_code} must not have a response body\"\n                )\n                response_name = f\"Response_{additional_status_code}_{self.unique_id}\"\n                response_field = create_model_field(\n                    name=response_name, type_=model, mode=\"serialization\"\n                )\n                response_fields[additional_status_code] = response_field\n        if response_fields:\n            self.response_fields: Dict[Union[int, str], ModelField] = response_fields\n        else:\n            self.response_fields = {}\n\n        assert callable(endpoint), \"An endpoint must be a callable\"\n        self.dependant = get_dependant(path=self.path_format, call=self.endpoint)\n        for depends in self.dependencies[::-1]:\n            self.dependant.dependencies.insert(\n                0,\n                get_parameterless_sub_dependant(depends=depends, path=self.path_format),\n            )\n        self._flat_dependant = get_flat_dependant(self.dependant)\n        self._embed_body_fields = _should_embed_body_fields(\n            self._flat_dependant.body_params\n        )\n        self.body_field = get_body_field(\n            flat_dependant=self._flat_dependant,\n            name=self.unique_id,\n            embed_body_fields=self._embed_body_fields,\n        )\n        self.app = request_response(self.get_route_handler())\n\n    def get_route_handler(self) -> Callable[[Request], Coroutine[Any, Any, Response]]:\n        return get_request_handler(\n            dependant=self.dependant,\n            body_field=self.body_field,\n            status_code=self.status_code,\n            response_class=self.response_class,\n            response_field=self.secure_cloned_response_field,\n            response_model_include=self.response_model_include,\n            response_model_exclude=self.response_model_exclude,\n            response_model_by_alias=self.response_model_by_alias,\n            response_model_exclude_unset=self.response_model_exclude_unset,\n            response_model_exclude_defaults=self.response_model_exclude_defaults,\n            response_model_exclude_none=self.response_model_exclude_none,\n            dependency_overrides_provider=self.dependency_overrides_provider,\n            embed_body_fields=self._embed_body_fields,\n        )\n\n    def matches(self, scope: Scope) -> Tuple[Match, Scope]:\n        match, child_scope = super().matches(scope)\n        if match != Match.NONE:\n            child_scope[\"route\"] = self\n        return match, child_scope\n\n\nclass APIRouter(routing.Router):\n    \"\"\"\n    `APIRouter` class, used to group *path operations*, for example to structure\n    an app in multiple files. It would then be included in the `FastAPI` app, or\n    in another `APIRouter` (ultimately included in the app).\n\n    Read more about it in the\n    [FastAPI docs for Bigger Applications - Multiple Files](https://fastapi.tiangolo.com/tutorial/bigger-applications/).\n\n    ## Example\n\n    ```python\n    from fastapi import APIRouter, FastAPI\n\n    app = FastAPI()\n    router = APIRouter()\n\n\n    @router.get(\"/users/\", tags=[\"users\"])\n    async def read_users():\n        return [{\"username\": \"Rick\"}, {\"username\": \"Morty\"}]\n\n\n    app.include_router(router)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        prefix: Annotated[str, Doc(\"An optional path prefix for the router.\")] = \"\",\n        tags: Annotated[\n            Optional[List[Union[str, Enum]]],\n            Doc(\n                \"\"\"\n                A list of tags to be applied to all the *path operations* in this\n                router.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        dependencies: Annotated[\n            Optional[Sequence[params.Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be applied to all the\n                *path operations* in this router.\n\n                Read more about it in the\n                [FastAPI docs for Bigger Applications - Multiple Files](https://fastapi.tiangolo.com/tutorial/bigger-applications/#include-an-apirouter-with-a-custom-prefix-tags-responses-and-dependencies).\n                \"\"\"\n            ),\n        ] = None,\n        default_response_class: Annotated[\n            Type[Response],\n            Doc(\n                \"\"\"\n                The default response class to be used.\n\n                Read more in the\n                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#default-response-class).\n                \"\"\"\n            ),\n        ] = Default(JSONResponse),\n        responses: Annotated[\n            Optional[Dict[Union[int, str], Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                Additional responses to be shown in OpenAPI.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Additional Responses in OpenAPI](https://fastapi.tiangolo.com/advanced/additional-responses/).\n\n                And in the\n                [FastAPI docs for Bigger Applications](https://fastapi.tiangolo.com/tutorial/bigger-applications/#include-an-apirouter-with-a-custom-prefix-tags-responses-and-dependencies).\n                \"\"\"\n            ),\n        ] = None,\n        callbacks: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                OpenAPI callbacks that should apply to all *path operations* in this\n                router.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).\n                \"\"\"\n            ),\n        ] = None,\n        routes: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                **Note**: you probably shouldn't use this parameter, it is inherited\n                from Starlette and supported for compatibility.\n\n                ---\n\n                A list of routes to serve incoming HTTP and WebSocket requests.\n                \"\"\"\n            ),\n            deprecated(\n                \"\"\"\n                You normally wouldn't use this parameter with FastAPI, it is inherited\n                from Starlette and supported for compatibility.\n\n                In FastAPI, you normally would use the *path operation methods*,\n                like `router.get()`, `router.post()`, etc.\n                \"\"\"\n            ),\n        ] = None,\n        redirect_slashes: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Whether to detect and redirect slashes in URLs when the client doesn't\n                use the same format.\n                \"\"\"\n            ),\n        ] = True,\n        default: Annotated[\n            Optional[ASGIApp],\n            Doc(\n                \"\"\"\n                Default function handler for this router. Used to handle\n                404 Not Found errors.\n                \"\"\"\n            ),\n        ] = None,\n        dependency_overrides_provider: Annotated[\n            Optional[Any],\n            Doc(\n                \"\"\"\n                Only used internally by FastAPI to handle dependency overrides.\n\n                You shouldn't need to use it. It normally points to the `FastAPI` app\n                object.\n                \"\"\"\n            ),\n        ] = None,\n        route_class: Annotated[\n            Type[APIRoute],\n            Doc(\n                \"\"\"\n                Custom route (*path operation*) class to be used by this router.\n\n                Read more about it in the\n                [FastAPI docs for Custom Request and APIRoute class](https://fastapi.tiangolo.com/how-to/custom-request-and-route/#custom-apiroute-class-in-a-router).\n                \"\"\"\n            ),\n        ] = APIRoute,\n        on_startup: Annotated[\n            Optional[Sequence[Callable[[], Any]]],\n            Doc(\n                \"\"\"\n                A list of startup event handler functions.\n\n                You should instead use the `lifespan` handlers.\n\n                Read more in the [FastAPI docs for `lifespan`](https://fastapi.tiangolo.com/advanced/events/).\n                \"\"\"\n            ),\n        ] = None,\n        on_shutdown: Annotated[\n            Optional[Sequence[Callable[[], Any]]],\n            Doc(\n                \"\"\"\n                A list of shutdown event handler functions.\n\n                You should instead use the `lifespan` handlers.\n\n                Read more in the\n                [FastAPI docs for `lifespan`](https://fastapi.tiangolo.com/advanced/events/).\n                \"\"\"\n            ),\n        ] = None,\n        # the generic to Lifespan[AppType] is the type of the top level application\n        # which the router cannot know statically, so we use typing.Any\n        lifespan: Annotated[\n            Optional[Lifespan[Any]],\n            Doc(\n                \"\"\"\n                A `Lifespan` context manager handler. This replaces `startup` and\n                `shutdown` functions with a single context manager.\n\n                Read more in the\n                [FastAPI docs for `lifespan`](https://fastapi.tiangolo.com/advanced/events/).\n                \"\"\"\n            ),\n        ] = None,\n        deprecated: Annotated[\n            Optional[bool],\n            Doc(\n                \"\"\"\n                Mark all *path operations* in this router as deprecated.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        include_in_schema: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                To include (or not) all the *path operations* in this router in the\n                generated OpenAPI.\n\n                This affects the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).\n                \"\"\"\n            ),\n        ] = True,\n        generate_unique_id_function: Annotated[\n            Callable[[APIRoute], str],\n            Doc(\n                \"\"\"\n                Customize the function used to generate unique IDs for the *path\n                operations* shown in the generated OpenAPI.\n\n                This is particularly useful when automatically generating clients or\n                SDKs for your API.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = Default(generate_unique_id),\n    ) -> None:\n        super().__init__(\n            routes=routes,\n            redirect_slashes=redirect_slashes,\n            default=default,\n            on_startup=on_startup,\n            on_shutdown=on_shutdown,\n            lifespan=lifespan,\n        )\n        if prefix:\n            assert prefix.startswith(\"/\"), \"A path prefix must start with '/'\"\n            assert not prefix.endswith(\"/\"), (\n                \"A path prefix must not end with '/', as the routes will start with '/'\"\n            )\n        self.prefix = prefix\n        self.tags: List[Union[str, Enum]] = tags or []\n        self.dependencies = list(dependencies or [])\n        self.deprecated = deprecated\n        self.include_in_schema = include_in_schema\n        self.responses = responses or {}\n        self.callbacks = callbacks or []\n        self.dependency_overrides_provider = dependency_overrides_provider\n        self.route_class = route_class\n        self.default_response_class = default_response_class\n        self.generate_unique_id_function = generate_unique_id_function\n\n    def route(\n        self,\n        path: str,\n        methods: Optional[Collection[str]] = None,\n        name: Optional[str] = None,\n        include_in_schema: bool = True,\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_route(\n                path,\n                func,\n                methods=methods,\n                name=name,\n                include_in_schema=include_in_schema,\n            )\n            return func\n\n        return decorator\n\n    def add_api_route(\n        self,\n        path: str,\n        endpoint: Callable[..., Any],\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        methods: Optional[Union[Set[str], List[str]]] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Union[Type[Response], DefaultPlaceholder] = Default(\n            JSONResponse\n        ),\n        name: Optional[str] = None,\n        route_class_override: Optional[Type[APIRoute]] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Union[\n            Callable[[APIRoute], str], DefaultPlaceholder\n        ] = Default(generate_unique_id),\n    ) -> None:\n        route_class = route_class_override or self.route_class\n        responses = responses or {}\n        combined_responses = {**self.responses, **responses}\n        current_response_class = get_value_or_default(\n            response_class, self.default_response_class\n        )\n        current_tags = self.tags.copy()\n        if tags:\n            current_tags.extend(tags)\n        current_dependencies = self.dependencies.copy()\n        if dependencies:\n            current_dependencies.extend(dependencies)\n        current_callbacks = self.callbacks.copy()\n        if callbacks:\n            current_callbacks.extend(callbacks)\n        current_generate_unique_id = get_value_or_default(\n            generate_unique_id_function, self.generate_unique_id_function\n        )\n        route = route_class(\n            self.prefix + path,\n            endpoint=endpoint,\n            response_model=response_model,\n            status_code=status_code,\n            tags=current_tags,\n            dependencies=current_dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=combined_responses,\n            deprecated=deprecated or self.deprecated,\n            methods=methods,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema and self.include_in_schema,\n            response_class=current_response_class,\n            name=name,\n            dependency_overrides_provider=self.dependency_overrides_provider,\n            callbacks=current_callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=current_generate_unique_id,\n        )\n        self.routes.append(route)\n\n    def api_route(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        methods: Optional[List[str]] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        callbacks: Optional[List[BaseRoute]] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_api_route(\n                path,\n                func,\n                response_model=response_model,\n                status_code=status_code,\n                tags=tags,\n                dependencies=dependencies,\n                summary=summary,\n                description=description,\n                response_description=response_description,\n                responses=responses,\n                deprecated=deprecated,\n                methods=methods,\n                operation_id=operation_id,\n                response_model_include=response_model_include,\n                response_model_exclude=response_model_exclude,\n                response_model_by_alias=response_model_by_alias,\n                response_model_exclude_unset=response_model_exclude_unset,\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n                include_in_schema=include_in_schema,\n                response_class=response_class,\n                name=name,\n                callbacks=callbacks,\n                openapi_extra=openapi_extra,\n                generate_unique_id_function=generate_unique_id_function,\n            )\n            return func\n\n        return decorator\n\n    def add_api_websocket_route(\n        self,\n        path: str,\n        endpoint: Callable[..., Any],\n        name: Optional[str] = None,\n        *,\n        dependencies: Optional[Sequence[params.Depends]] = None,\n    ) -> None:\n        current_dependencies = self.dependencies.copy()\n        if dependencies:\n            current_dependencies.extend(dependencies)\n\n        route = APIWebSocketRoute(\n            self.prefix + path,\n            endpoint=endpoint,\n            name=name,\n            dependencies=current_dependencies,\n            dependency_overrides_provider=self.dependency_overrides_provider,\n        )\n        self.routes.append(route)\n\n    def websocket(\n        self,\n        path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                WebSocket path.\n                \"\"\"\n            ),\n        ],\n        name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A name for the WebSocket. Only used internally.\n                \"\"\"\n            ),\n        ] = None,\n        *,\n        dependencies: Annotated[\n            Optional[Sequence[params.Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be used for this\n                WebSocket.\n\n                Read more about it in the\n                [FastAPI docs for WebSockets](https://fastapi.tiangolo.com/advanced/websockets/).\n                \"\"\"\n            ),\n        ] = None,\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Decorate a WebSocket function.\n\n        Read more about it in the\n        [FastAPI docs for WebSockets](https://fastapi.tiangolo.com/advanced/websockets/).\n\n        **Example**\n\n        ## Example\n\n        ```python\n        from fastapi import APIRouter, FastAPI, WebSocket\n\n        app = FastAPI()\n        router = APIRouter()\n\n        @router.websocket(\"/ws\")\n        async def websocket_endpoint(websocket: WebSocket):\n            await websocket.accept()\n            while True:\n                data = await websocket.receive_text()\n                await websocket.send_text(f\"Message text was: {data}\")\n\n        app.include_router(router)\n        ```\n        \"\"\"\n\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_api_websocket_route(\n                path, func, name=name, dependencies=dependencies\n            )\n            return func\n\n        return decorator\n\n    def websocket_route(\n        self, path: str, name: Union[str, None] = None\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_websocket_route(path, func, name=name)\n            return func\n\n        return decorator\n\n    def include_router(\n        self,\n        router: Annotated[\"APIRouter\", Doc(\"The `APIRouter` to include.\")],\n        *,\n        prefix: Annotated[str, Doc(\"An optional path prefix for the router.\")] = \"\",\n        tags: Annotated[\n            Optional[List[Union[str, Enum]]],\n            Doc(\n                \"\"\"\n                A list of tags to be applied to all the *path operations* in this\n                router.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        dependencies: Annotated[\n            Optional[Sequence[params.Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be applied to all the\n                *path operations* in this router.\n\n                Read more about it in the\n                [FastAPI docs for Bigger Applications - Multiple Files](https://fastapi.tiangolo.com/tutorial/bigger-applications/#include-an-apirouter-with-a-custom-prefix-tags-responses-and-dependencies).\n                \"\"\"\n            ),\n        ] = None,\n        default_response_class: Annotated[\n            Type[Response],\n            Doc(\n                \"\"\"\n                The default response class to be used.\n\n                Read more in the\n                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#default-response-class).\n                \"\"\"\n            ),\n        ] = Default(JSONResponse),\n        responses: Annotated[\n            Optional[Dict[Union[int, str], Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                Additional responses to be shown in OpenAPI.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Additional Responses in OpenAPI](https://fastapi.tiangolo.com/advanced/additional-responses/).\n\n                And in the\n                [FastAPI docs for Bigger Applications](https://fastapi.tiangolo.com/tutorial/bigger-applications/#include-an-apirouter-with-a-custom-prefix-tags-responses-and-dependencies).\n                \"\"\"\n            ),\n        ] = None,\n        callbacks: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                OpenAPI callbacks that should apply to all *path operations* in this\n                router.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).\n                \"\"\"\n            ),\n        ] = None,\n        deprecated: Annotated[\n            Optional[bool],\n            Doc(\n                \"\"\"\n                Mark all *path operations* in this router as deprecated.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        include_in_schema: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Include (or not) all the *path operations* in this router in the\n                generated OpenAPI schema.\n\n                This affects the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = True,\n        generate_unique_id_function: Annotated[\n            Callable[[APIRoute], str],\n            Doc(\n                \"\"\"\n                Customize the function used to generate unique IDs for the *path\n                operations* shown in the generated OpenAPI.\n\n                This is particularly useful when automatically generating clients or\n                SDKs for your API.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = Default(generate_unique_id),\n    ) -> None:\n        \"\"\"\n        Include another `APIRouter` in the same current `APIRouter`.\n\n        Read more about it in the\n        [FastAPI docs for Bigger Applications](https://fastapi.tiangolo.com/tutorial/bigger-applications/).\n\n        ## Example\n\n        ```python\n        from fastapi import APIRouter, FastAPI\n\n        app = FastAPI()\n        internal_router = APIRouter()\n        users_router = APIRouter()\n\n        @users_router.get(\"/users/\")\n        def read_users():\n            return [{\"name\": \"Rick\"}, {\"name\": \"Morty\"}]\n\n        internal_router.include_router(users_router)\n        app.include_router(internal_router)\n        ```\n        \"\"\"\n        if prefix:\n            assert prefix.startswith(\"/\"), \"A path prefix must start with '/'\"\n            assert not prefix.endswith(\"/\"), (\n                \"A path prefix must not end with '/', as the routes will start with '/'\"\n            )\n        else:\n            for r in router.routes:\n                path = getattr(r, \"path\")  # noqa: B009\n                name = getattr(r, \"name\", \"unknown\")\n                if path is not None and not path:\n                    raise FastAPIError(\n                        f\"Prefix and path cannot be both empty (path operation: {name})\"\n                    )\n        if responses is None:\n            responses = {}\n        for route in router.routes:\n            if isinstance(route, APIRoute):\n                combined_responses = {**responses, **route.responses}\n                use_response_class = get_value_or_default(\n                    route.response_class,\n                    router.default_response_class,\n                    default_response_class,\n                    self.default_response_class,\n                )\n                current_tags = []\n                if tags:\n                    current_tags.extend(tags)\n                if route.tags:\n                    current_tags.extend(route.tags)\n                current_dependencies: List[params.Depends] = []\n                if dependencies:\n                    current_dependencies.extend(dependencies)\n                if route.dependencies:\n                    current_dependencies.extend(route.dependencies)\n                current_callbacks = []\n                if callbacks:\n                    current_callbacks.extend(callbacks)\n                if route.callbacks:\n                    current_callbacks.extend(route.callbacks)\n                current_generate_unique_id = get_value_or_default(\n                    route.generate_unique_id_function,\n                    router.generate_unique_id_function,\n                    generate_unique_id_function,\n                    self.generate_unique_id_function,\n                )\n                self.add_api_route(\n                    prefix + route.path,\n                    route.endpoint,\n                    response_model=route.response_model,\n                    status_code=route.status_code,\n                    tags=current_tags,\n                    dependencies=current_dependencies,\n                    summary=route.summary,\n                    description=route.description,\n                    response_description=route.response_description,\n                    responses=combined_responses,\n                    deprecated=route.deprecated or deprecated or self.deprecated,\n                    methods=route.methods,\n                    operation_id=route.operation_id,\n                    response_model_include=route.response_model_include,\n                    response_model_exclude=route.response_model_exclude,\n                    response_model_by_alias=route.response_model_by_alias,\n                    response_model_exclude_unset=route.response_model_exclude_unset,\n                    response_model_exclude_defaults=route.response_model_exclude_defaults,\n                    response_model_exclude_none=route.response_model_exclude_none,\n                    include_in_schema=route.include_in_schema\n                    and self.include_in_schema\n                    and include_in_schema,\n                    response_class=use_response_class,\n                    name=route.name,\n                    route_class_override=type(route),\n                    callbacks=current_callbacks,\n                    openapi_extra=route.openapi_extra,\n                    generate_unique_id_function=current_generate_unique_id,\n                )\n            elif isinstance(route, routing.Route):\n                methods = list(route.methods or [])\n                self.add_route(\n                    prefix + route.path,\n                    route.endpoint,\n                    methods=methods,\n                    include_in_schema=route.include_in_schema,\n                    name=route.name,\n                )\n            elif isinstance(route, APIWebSocketRoute):\n                current_dependencies = []\n                if dependencies:\n                    current_dependencies.extend(dependencies)\n                if route.dependencies:\n                    current_dependencies.extend(route.dependencies)\n                self.add_api_websocket_route(\n                    prefix + route.path,\n                    route.endpoint,\n                    dependencies=current_dependencies,\n                    name=route.name,\n                )\n            elif isinstance(route, routing.WebSocketRoute):\n                self.add_websocket_route(\n                    prefix + route.path, route.endpoint, name=route.name\n                )\n        for handler in router.on_startup:\n            self.add_event_handler(\"startup\", handler)\n        for handler in router.on_shutdown:\n            self.add_event_handler(\"shutdown\", handler)\n        self.lifespan_context = _merge_lifespan_context(\n            self.lifespan_context,\n            router.lifespan_context,\n        )\n\n    def get(\n        self,\n        path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The URL path to be used for this *path operation*.\n\n                For example, in `http://example.com/items`, the path is `/items`.\n                \"\"\"\n            ),\n        ],\n        *,\n        response_model: Annotated[\n            Any,\n            Doc(\n                \"\"\"\n                The type to use for the response.\n\n                It could be any valid Pydantic *field* type. So, it doesn't have to\n                be a Pydantic model, it could be other things, like a `list`, `dict`,\n                etc.\n\n                It will be used for:\n\n                * Documentation: the generated OpenAPI (and the UI at `/docs`) will\n                    show it as the response (JSON Schema).\n                * Serialization: you could return an arbitrary object and the\n                    `response_model` would be used to serialize that object into the\n                    corresponding JSON.\n                * Filtering: the JSON sent to the client will only contain the data\n                    (fields) defined in the `response_model`. If you returned an object\n                    that contains an attribute `password` but the `response_model` does\n                    not include that field, the JSON sent to the client would not have\n                    that `password`.\n                * Validation: whatever you return will be serialized with the\n                    `response_model`, converting any data as necessary to generate the\n                    corresponding JSON. But if the data in the object returned is not\n                    valid, that would mean a violation of the contract with the client,\n                    so it's an error from the API developer. So, FastAPI will raise an\n                    error and return a 500 error code (Internal Server Error).\n\n                Read more about it in the\n                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).\n                \"\"\"\n            ),\n        ] = Default(None),\n        status_code: Annotated[\n            Optional[int],\n            Doc(\n                \"\"\"\n                The default status code to be used for the response.\n\n                You could override the status code by returning a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).\n                \"\"\"\n            ),\n        ] = None,\n        tags: Annotated[\n            Optional[List[Union[str, Enum]]],\n            Doc(\n                \"\"\"\n                A list of tags to be applied to the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).\n                \"\"\"\n            ),\n        ] = None,\n        dependencies: Annotated[\n            Optional[Sequence[params.Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be applied to the\n                *path operation*.\n\n                Read more about it in the\n                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).\n                \"\"\"\n            ),\n        ] = None,\n        summary: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A summary for the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A description for the *path operation*.\n\n                If not provided, it will be extracted automatically from the docstring\n                of the *path operation function*.\n\n                It can contain Markdown.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        response_description: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The description for the default response.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = \"Successful Response\",\n        responses: Annotated[\n            Optional[Dict[Union[int, str], Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                Additional responses that could be returned by this *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        deprecated: Annotated[\n            Optional[bool],\n            Doc(\n                \"\"\"\n                Mark this *path operation* as deprecated.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        operation_id: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Custom operation ID to be used by this *path operation*.\n\n                By default, it is generated automatically.\n\n                If you provide a custom operation ID, you need to make sure it is\n                unique for the whole API.\n\n                You can customize the\n                operation ID generation with the parameter\n                `generate_unique_id_function` in the `FastAPI` class.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_include: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to include only certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_exclude: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to exclude certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_by_alias: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response model\n                should be serialized by alias when an alias is used.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = True,\n        response_model_exclude_unset: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that were not set and\n                have their default values. This is different from\n                `response_model_exclude_defaults` in that if the fields are set,\n                they will be included in the response, even if the value is the same\n                as the default.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_defaults: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that have the same value\n                as the default. This is different from `response_model_exclude_unset`\n                in that if the fields are set but contain the same default values,\n                they will be excluded from the response.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_none: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data should\n                exclude fields set to `None`.\n\n                This is much simpler (less smart) than `response_model_exclude_unset`\n                and `response_model_exclude_defaults`. You probably want to use one of\n                those two instead of this one, as those allow returning `None` values\n                when it makes sense.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).\n                \"\"\"\n            ),\n        ] = False,\n        include_in_schema: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Include this *path operation* in the generated OpenAPI schema.\n\n                This affects the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).\n                \"\"\"\n            ),\n        ] = True,\n        response_class: Annotated[\n            Type[Response],\n            Doc(\n                \"\"\"\n                Response class to be used for this *path operation*.\n\n                This will not be used if you return a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).\n                \"\"\"\n            ),\n        ] = Default(JSONResponse),\n        name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Name for this *path operation*. Only used internally.\n                \"\"\"\n            ),\n        ] = None,\n        callbacks: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                List of *path operations* that will be used as OpenAPI callbacks.\n\n                This is only for OpenAPI documentation, the callbacks won't be used\n                directly.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).\n                \"\"\"\n            ),\n        ] = None,\n        openapi_extra: Annotated[\n            Optional[Dict[str, Any]],\n            Doc(\n                \"\"\"\n                Extra metadata to be included in the OpenAPI schema for this *path\n                operation*.\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).\n                \"\"\"\n            ),\n        ] = None,\n        generate_unique_id_function: Annotated[\n            Callable[[APIRoute], str],\n            Doc(\n                \"\"\"\n                Customize the function used to generate unique IDs for the *path\n                operations* shown in the generated OpenAPI.\n\n                This is particularly useful when automatically generating clients or\n                SDKs for your API.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = Default(generate_unique_id),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add a *path operation* using an HTTP GET operation.\n\n        ## Example\n\n        ```python\n        from fastapi import APIRouter, FastAPI\n\n        app = FastAPI()\n        router = APIRouter()\n\n        @router.get(\"/items/\")\n        def read_items():\n            return [{\"name\": \"Empanada\"}, {\"name\": \"Arepa\"}]\n\n        app.include_router(router)\n        ```\n        \"\"\"\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"GET\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def put(\n        self,\n        path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The URL path to be used for this *path operation*.\n\n                For example, in `http://example.com/items`, the path is `/items`.\n                \"\"\"\n            ),\n        ],\n        *,\n        response_model: Annotated[\n            Any,\n            Doc(\n                \"\"\"\n                The type to use for the response.\n\n                It could be any valid Pydantic *field* type. So, it doesn't have to\n                be a Pydantic model, it could be other things, like a `list`, `dict`,\n                etc.\n\n                It will be used for:\n\n                * Documentation: the generated OpenAPI (and the UI at `/docs`) will\n                    show it as the response (JSON Schema).\n                * Serialization: you could return an arbitrary object and the\n                    `response_model` would be used to serialize that object into the\n                    corresponding JSON.\n                * Filtering: the JSON sent to the client will only contain the data\n                    (fields) defined in the `response_model`. If you returned an object\n                    that contains an attribute `password` but the `response_model` does\n                    not include that field, the JSON sent to the client would not have\n                    that `password`.\n                * Validation: whatever you return will be serialized with the\n                    `response_model`, converting any data as necessary to generate the\n                    corresponding JSON. But if the data in the object returned is not\n                    valid, that would mean a violation of the contract with the client,\n                    so it's an error from the API developer. So, FastAPI will raise an\n                    error and return a 500 error code (Internal Server Error).\n\n                Read more about it in the\n                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).\n                \"\"\"\n            ),\n        ] = Default(None),\n        status_code: Annotated[\n            Optional[int],\n            Doc(\n                \"\"\"\n                The default status code to be used for the response.\n\n                You could override the status code by returning a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).\n                \"\"\"\n            ),\n        ] = None,\n        tags: Annotated[\n            Optional[List[Union[str, Enum]]],\n            Doc(\n                \"\"\"\n                A list of tags to be applied to the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).\n                \"\"\"\n            ),\n        ] = None,\n        dependencies: Annotated[\n            Optional[Sequence[params.Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be applied to the\n                *path operation*.\n\n                Read more about it in the\n                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).\n                \"\"\"\n            ),\n        ] = None,\n        summary: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A summary for the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A description for the *path operation*.\n\n                If not provided, it will be extracted automatically from the docstring\n                of the *path operation function*.\n\n                It can contain Markdown.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        response_description: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The description for the default response.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = \"Successful Response\",\n        responses: Annotated[\n            Optional[Dict[Union[int, str], Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                Additional responses that could be returned by this *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        deprecated: Annotated[\n            Optional[bool],\n            Doc(\n                \"\"\"\n                Mark this *path operation* as deprecated.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        operation_id: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Custom operation ID to be used by this *path operation*.\n\n                By default, it is generated automatically.\n\n                If you provide a custom operation ID, you need to make sure it is\n                unique for the whole API.\n\n                You can customize the\n                operation ID generation with the parameter\n                `generate_unique_id_function` in the `FastAPI` class.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_include: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to include only certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_exclude: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to exclude certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_by_alias: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response model\n                should be serialized by alias when an alias is used.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = True,\n        response_model_exclude_unset: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that were not set and\n                have their default values. This is different from\n                `response_model_exclude_defaults` in that if the fields are set,\n                they will be included in the response, even if the value is the same\n                as the default.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_defaults: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that have the same value\n                as the default. This is different from `response_model_exclude_unset`\n                in that if the fields are set but contain the same default values,\n                they will be excluded from the response.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_none: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data should\n                exclude fields set to `None`.\n\n                This is much simpler (less smart) than `response_model_exclude_unset`\n                and `response_model_exclude_defaults`. You probably want to use one of\n                those two instead of this one, as those allow returning `None` values\n                when it makes sense.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).\n                \"\"\"\n            ),\n        ] = False,\n        include_in_schema: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Include this *path operation* in the generated OpenAPI schema.\n\n                This affects the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).\n                \"\"\"\n            ),\n        ] = True,\n        response_class: Annotated[\n            Type[Response],\n            Doc(\n                \"\"\"\n                Response class to be used for this *path operation*.\n\n                This will not be used if you return a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).\n                \"\"\"\n            ),\n        ] = Default(JSONResponse),\n        name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Name for this *path operation*. Only used internally.\n                \"\"\"\n            ),\n        ] = None,\n        callbacks: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                List of *path operations* that will be used as OpenAPI callbacks.\n\n                This is only for OpenAPI documentation, the callbacks won't be used\n                directly.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).\n                \"\"\"\n            ),\n        ] = None,\n        openapi_extra: Annotated[\n            Optional[Dict[str, Any]],\n            Doc(\n                \"\"\"\n                Extra metadata to be included in the OpenAPI schema for this *path\n                operation*.\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).\n                \"\"\"\n            ),\n        ] = None,\n        generate_unique_id_function: Annotated[\n            Callable[[APIRoute], str],\n            Doc(\n                \"\"\"\n                Customize the function used to generate unique IDs for the *path\n                operations* shown in the generated OpenAPI.\n\n                This is particularly useful when automatically generating clients or\n                SDKs for your API.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = Default(generate_unique_id),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add a *path operation* using an HTTP PUT operation.\n\n        ## Example\n\n        ```python\n        from fastapi import APIRouter, FastAPI\n        from pydantic import BaseModel\n\n        class Item(BaseModel):\n            name: str\n            description: str | None = None\n\n        app = FastAPI()\n        router = APIRouter()\n\n        @router.put(\"/items/{item_id}\")\n        def replace_item(item_id: str, item: Item):\n            return {\"message\": \"Item replaced\", \"id\": item_id}\n\n        app.include_router(router)\n        ```\n        \"\"\"\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"PUT\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def post(\n        self,\n        path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The URL path to be used for this *path operation*.\n\n                For example, in `http://example.com/items`, the path is `/items`.\n                \"\"\"\n            ),\n        ],\n        *,\n        response_model: Annotated[\n            Any,\n            Doc(\n                \"\"\"\n                The type to use for the response.\n\n                It could be any valid Pydantic *field* type. So, it doesn't have to\n                be a Pydantic model, it could be other things, like a `list`, `dict`,\n                etc.\n\n                It will be used for:\n\n                * Documentation: the generated OpenAPI (and the UI at `/docs`) will\n                    show it as the response (JSON Schema).\n                * Serialization: you could return an arbitrary object and the\n                    `response_model` would be used to serialize that object into the\n                    corresponding JSON.\n                * Filtering: the JSON sent to the client will only contain the data\n                    (fields) defined in the `response_model`. If you returned an object\n                    that contains an attribute `password` but the `response_model` does\n                    not include that field, the JSON sent to the client would not have\n                    that `password`.\n                * Validation: whatever you return will be serialized with the\n                    `response_model`, converting any data as necessary to generate the\n                    corresponding JSON. But if the data in the object returned is not\n                    valid, that would mean a violation of the contract with the client,\n                    so it's an error from the API developer. So, FastAPI will raise an\n                    error and return a 500 error code (Internal Server Error).\n\n                Read more about it in the\n                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).\n                \"\"\"\n            ),\n        ] = Default(None),\n        status_code: Annotated[\n            Optional[int],\n            Doc(\n                \"\"\"\n                The default status code to be used for the response.\n\n                You could override the status code by returning a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).\n                \"\"\"\n            ),\n        ] = None,\n        tags: Annotated[\n            Optional[List[Union[str, Enum]]],\n            Doc(\n                \"\"\"\n                A list of tags to be applied to the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).\n                \"\"\"\n            ),\n        ] = None,\n        dependencies: Annotated[\n            Optional[Sequence[params.Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be applied to the\n                *path operation*.\n\n                Read more about it in the\n                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).\n                \"\"\"\n            ),\n        ] = None,\n        summary: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A summary for the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A description for the *path operation*.\n\n                If not provided, it will be extracted automatically from the docstring\n                of the *path operation function*.\n\n                It can contain Markdown.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        response_description: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The description for the default response.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = \"Successful Response\",\n        responses: Annotated[\n            Optional[Dict[Union[int, str], Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                Additional responses that could be returned by this *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        deprecated: Annotated[\n            Optional[bool],\n            Doc(\n                \"\"\"\n                Mark this *path operation* as deprecated.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        operation_id: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Custom operation ID to be used by this *path operation*.\n\n                By default, it is generated automatically.\n\n                If you provide a custom operation ID, you need to make sure it is\n                unique for the whole API.\n\n                You can customize the\n                operation ID generation with the parameter\n                `generate_unique_id_function` in the `FastAPI` class.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_include: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to include only certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_exclude: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to exclude certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_by_alias: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response model\n                should be serialized by alias when an alias is used.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = True,\n        response_model_exclude_unset: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that were not set and\n                have their default values. This is different from\n                `response_model_exclude_defaults` in that if the fields are set,\n                they will be included in the response, even if the value is the same\n                as the default.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_defaults: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that have the same value\n                as the default. This is different from `response_model_exclude_unset`\n                in that if the fields are set but contain the same default values,\n                they will be excluded from the response.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_none: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data should\n                exclude fields set to `None`.\n\n                This is much simpler (less smart) than `response_model_exclude_unset`\n                and `response_model_exclude_defaults`. You probably want to use one of\n                those two instead of this one, as those allow returning `None` values\n                when it makes sense.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).\n                \"\"\"\n            ),\n        ] = False,\n        include_in_schema: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Include this *path operation* in the generated OpenAPI schema.\n\n                This affects the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).\n                \"\"\"\n            ),\n        ] = True,\n        response_class: Annotated[\n            Type[Response],\n            Doc(\n                \"\"\"\n                Response class to be used for this *path operation*.\n\n                This will not be used if you return a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).\n                \"\"\"\n            ),\n        ] = Default(JSONResponse),\n        name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Name for this *path operation*. Only used internally.\n                \"\"\"\n            ),\n        ] = None,\n        callbacks: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                List of *path operations* that will be used as OpenAPI callbacks.\n\n                This is only for OpenAPI documentation, the callbacks won't be used\n                directly.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).\n                \"\"\"\n            ),\n        ] = None,\n        openapi_extra: Annotated[\n            Optional[Dict[str, Any]],\n            Doc(\n                \"\"\"\n                Extra metadata to be included in the OpenAPI schema for this *path\n                operation*.\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).\n                \"\"\"\n            ),\n        ] = None,\n        generate_unique_id_function: Annotated[\n            Callable[[APIRoute], str],\n            Doc(\n                \"\"\"\n                Customize the function used to generate unique IDs for the *path\n                operations* shown in the generated OpenAPI.\n\n                This is particularly useful when automatically generating clients or\n                SDKs for your API.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = Default(generate_unique_id),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add a *path operation* using an HTTP POST operation.\n\n        ## Example\n\n        ```python\n        from fastapi import APIRouter, FastAPI\n        from pydantic import BaseModel\n\n        class Item(BaseModel):\n            name: str\n            description: str | None = None\n\n        app = FastAPI()\n        router = APIRouter()\n\n        @router.post(\"/items/\")\n        def create_item(item: Item):\n            return {\"message\": \"Item created\"}\n\n        app.include_router(router)\n        ```\n        \"\"\"\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"POST\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def delete(\n        self,\n        path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The URL path to be used for this *path operation*.\n\n                For example, in `http://example.com/items`, the path is `/items`.\n                \"\"\"\n            ),\n        ],\n        *,\n        response_model: Annotated[\n            Any,\n            Doc(\n                \"\"\"\n                The type to use for the response.\n\n                It could be any valid Pydantic *field* type. So, it doesn't have to\n                be a Pydantic model, it could be other things, like a `list`, `dict`,\n                etc.\n\n                It will be used for:\n\n                * Documentation: the generated OpenAPI (and the UI at `/docs`) will\n                    show it as the response (JSON Schema).\n                * Serialization: you could return an arbitrary object and the\n                    `response_model` would be used to serialize that object into the\n                    corresponding JSON.\n                * Filtering: the JSON sent to the client will only contain the data\n                    (fields) defined in the `response_model`. If you returned an object\n                    that contains an attribute `password` but the `response_model` does\n                    not include that field, the JSON sent to the client would not have\n                    that `password`.\n                * Validation: whatever you return will be serialized with the\n                    `response_model`, converting any data as necessary to generate the\n                    corresponding JSON. But if the data in the object returned is not\n                    valid, that would mean a violation of the contract with the client,\n                    so it's an error from the API developer. So, FastAPI will raise an\n                    error and return a 500 error code (Internal Server Error).\n\n                Read more about it in the\n                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).\n                \"\"\"\n            ),\n        ] = Default(None),\n        status_code: Annotated[\n            Optional[int],\n            Doc(\n                \"\"\"\n                The default status code to be used for the response.\n\n                You could override the status code by returning a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).\n                \"\"\"\n            ),\n        ] = None,\n        tags: Annotated[\n            Optional[List[Union[str, Enum]]],\n            Doc(\n                \"\"\"\n                A list of tags to be applied to the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).\n                \"\"\"\n            ),\n        ] = None,\n        dependencies: Annotated[\n            Optional[Sequence[params.Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be applied to the\n                *path operation*.\n\n                Read more about it in the\n                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).\n                \"\"\"\n            ),\n        ] = None,\n        summary: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A summary for the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A description for the *path operation*.\n\n                If not provided, it will be extracted automatically from the docstring\n                of the *path operation function*.\n\n                It can contain Markdown.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        response_description: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The description for the default response.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = \"Successful Response\",\n        responses: Annotated[\n            Optional[Dict[Union[int, str], Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                Additional responses that could be returned by this *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        deprecated: Annotated[\n            Optional[bool],\n            Doc(\n                \"\"\"\n                Mark this *path operation* as deprecated.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        operation_id: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Custom operation ID to be used by this *path operation*.\n\n                By default, it is generated automatically.\n\n                If you provide a custom operation ID, you need to make sure it is\n                unique for the whole API.\n\n                You can customize the\n                operation ID generation with the parameter\n                `generate_unique_id_function` in the `FastAPI` class.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_include: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to include only certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_exclude: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to exclude certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_by_alias: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response model\n                should be serialized by alias when an alias is used.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = True,\n        response_model_exclude_unset: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that were not set and\n                have their default values. This is different from\n                `response_model_exclude_defaults` in that if the fields are set,\n                they will be included in the response, even if the value is the same\n                as the default.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_defaults: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that have the same value\n                as the default. This is different from `response_model_exclude_unset`\n                in that if the fields are set but contain the same default values,\n                they will be excluded from the response.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_none: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data should\n                exclude fields set to `None`.\n\n                This is much simpler (less smart) than `response_model_exclude_unset`\n                and `response_model_exclude_defaults`. You probably want to use one of\n                those two instead of this one, as those allow returning `None` values\n                when it makes sense.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).\n                \"\"\"\n            ),\n        ] = False,\n        include_in_schema: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Include this *path operation* in the generated OpenAPI schema.\n\n                This affects the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).\n                \"\"\"\n            ),\n        ] = True,\n        response_class: Annotated[\n            Type[Response],\n            Doc(\n                \"\"\"\n                Response class to be used for this *path operation*.\n\n                This will not be used if you return a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).\n                \"\"\"\n            ),\n        ] = Default(JSONResponse),\n        name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Name for this *path operation*. Only used internally.\n                \"\"\"\n            ),\n        ] = None,\n        callbacks: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                List of *path operations* that will be used as OpenAPI callbacks.\n\n                This is only for OpenAPI documentation, the callbacks won't be used\n                directly.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).\n                \"\"\"\n            ),\n        ] = None,\n        openapi_extra: Annotated[\n            Optional[Dict[str, Any]],\n            Doc(\n                \"\"\"\n                Extra metadata to be included in the OpenAPI schema for this *path\n                operation*.\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).\n                \"\"\"\n            ),\n        ] = None,\n        generate_unique_id_function: Annotated[\n            Callable[[APIRoute], str],\n            Doc(\n                \"\"\"\n                Customize the function used to generate unique IDs for the *path\n                operations* shown in the generated OpenAPI.\n\n                This is particularly useful when automatically generating clients or\n                SDKs for your API.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = Default(generate_unique_id),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add a *path operation* using an HTTP DELETE operation.\n\n        ## Example\n\n        ```python\n        from fastapi import APIRouter, FastAPI\n\n        app = FastAPI()\n        router = APIRouter()\n\n        @router.delete(\"/items/{item_id}\")\n        def delete_item(item_id: str):\n            return {\"message\": \"Item deleted\"}\n\n        app.include_router(router)\n        ```\n        \"\"\"\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"DELETE\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def options(\n        self,\n        path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The URL path to be used for this *path operation*.\n\n                For example, in `http://example.com/items`, the path is `/items`.\n                \"\"\"\n            ),\n        ],\n        *,\n        response_model: Annotated[\n            Any,\n            Doc(\n                \"\"\"\n                The type to use for the response.\n\n                It could be any valid Pydantic *field* type. So, it doesn't have to\n                be a Pydantic model, it could be other things, like a `list`, `dict`,\n                etc.\n\n                It will be used for:\n\n                * Documentation: the generated OpenAPI (and the UI at `/docs`) will\n                    show it as the response (JSON Schema).\n                * Serialization: you could return an arbitrary object and the\n                    `response_model` would be used to serialize that object into the\n                    corresponding JSON.\n                * Filtering: the JSON sent to the client will only contain the data\n                    (fields) defined in the `response_model`. If you returned an object\n                    that contains an attribute `password` but the `response_model` does\n                    not include that field, the JSON sent to the client would not have\n                    that `password`.\n                * Validation: whatever you return will be serialized with the\n                    `response_model`, converting any data as necessary to generate the\n                    corresponding JSON. But if the data in the object returned is not\n                    valid, that would mean a violation of the contract with the client,\n                    so it's an error from the API developer. So, FastAPI will raise an\n                    error and return a 500 error code (Internal Server Error).\n\n                Read more about it in the\n                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).\n                \"\"\"\n            ),\n        ] = Default(None),\n        status_code: Annotated[\n            Optional[int],\n            Doc(\n                \"\"\"\n                The default status code to be used for the response.\n\n                You could override the status code by returning a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).\n                \"\"\"\n            ),\n        ] = None,\n        tags: Annotated[\n            Optional[List[Union[str, Enum]]],\n            Doc(\n                \"\"\"\n                A list of tags to be applied to the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).\n                \"\"\"\n            ),\n        ] = None,\n        dependencies: Annotated[\n            Optional[Sequence[params.Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be applied to the\n                *path operation*.\n\n                Read more about it in the\n                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).\n                \"\"\"\n            ),\n        ] = None,\n        summary: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A summary for the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A description for the *path operation*.\n\n                If not provided, it will be extracted automatically from the docstring\n                of the *path operation function*.\n\n                It can contain Markdown.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        response_description: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The description for the default response.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = \"Successful Response\",\n        responses: Annotated[\n            Optional[Dict[Union[int, str], Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                Additional responses that could be returned by this *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        deprecated: Annotated[\n            Optional[bool],\n            Doc(\n                \"\"\"\n                Mark this *path operation* as deprecated.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        operation_id: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Custom operation ID to be used by this *path operation*.\n\n                By default, it is generated automatically.\n\n                If you provide a custom operation ID, you need to make sure it is\n                unique for the whole API.\n\n                You can customize the\n                operation ID generation with the parameter\n                `generate_unique_id_function` in the `FastAPI` class.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_include: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to include only certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_exclude: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to exclude certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_by_alias: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response model\n                should be serialized by alias when an alias is used.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = True,\n        response_model_exclude_unset: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that were not set and\n                have their default values. This is different from\n                `response_model_exclude_defaults` in that if the fields are set,\n                they will be included in the response, even if the value is the same\n                as the default.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_defaults: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that have the same value\n                as the default. This is different from `response_model_exclude_unset`\n                in that if the fields are set but contain the same default values,\n                they will be excluded from the response.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_none: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data should\n                exclude fields set to `None`.\n\n                This is much simpler (less smart) than `response_model_exclude_unset`\n                and `response_model_exclude_defaults`. You probably want to use one of\n                those two instead of this one, as those allow returning `None` values\n                when it makes sense.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).\n                \"\"\"\n            ),\n        ] = False,\n        include_in_schema: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Include this *path operation* in the generated OpenAPI schema.\n\n                This affects the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).\n                \"\"\"\n            ),\n        ] = True,\n        response_class: Annotated[\n            Type[Response],\n            Doc(\n                \"\"\"\n                Response class to be used for this *path operation*.\n\n                This will not be used if you return a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).\n                \"\"\"\n            ),\n        ] = Default(JSONResponse),\n        name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Name for this *path operation*. Only used internally.\n                \"\"\"\n            ),\n        ] = None,\n        callbacks: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                List of *path operations* that will be used as OpenAPI callbacks.\n\n                This is only for OpenAPI documentation, the callbacks won't be used\n                directly.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).\n                \"\"\"\n            ),\n        ] = None,\n        openapi_extra: Annotated[\n            Optional[Dict[str, Any]],\n            Doc(\n                \"\"\"\n                Extra metadata to be included in the OpenAPI schema for this *path\n                operation*.\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).\n                \"\"\"\n            ),\n        ] = None,\n        generate_unique_id_function: Annotated[\n            Callable[[APIRoute], str],\n            Doc(\n                \"\"\"\n                Customize the function used to generate unique IDs for the *path\n                operations* shown in the generated OpenAPI.\n\n                This is particularly useful when automatically generating clients or\n                SDKs for your API.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = Default(generate_unique_id),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add a *path operation* using an HTTP OPTIONS operation.\n\n        ## Example\n\n        ```python\n        from fastapi import APIRouter, FastAPI\n\n        app = FastAPI()\n        router = APIRouter()\n\n        @router.options(\"/items/\")\n        def get_item_options():\n            return {\"additions\": [\"Aji\", \"Guacamole\"]}\n\n        app.include_router(router)\n        ```\n        \"\"\"\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"OPTIONS\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def head(\n        self,\n        path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The URL path to be used for this *path operation*.\n\n                For example, in `http://example.com/items`, the path is `/items`.\n                \"\"\"\n            ),\n        ],\n        *,\n        response_model: Annotated[\n            Any,\n            Doc(\n                \"\"\"\n                The type to use for the response.\n\n                It could be any valid Pydantic *field* type. So, it doesn't have to\n                be a Pydantic model, it could be other things, like a `list`, `dict`,\n                etc.\n\n                It will be used for:\n\n                * Documentation: the generated OpenAPI (and the UI at `/docs`) will\n                    show it as the response (JSON Schema).\n                * Serialization: you could return an arbitrary object and the\n                    `response_model` would be used to serialize that object into the\n                    corresponding JSON.\n                * Filtering: the JSON sent to the client will only contain the data\n                    (fields) defined in the `response_model`. If you returned an object\n                    that contains an attribute `password` but the `response_model` does\n                    not include that field, the JSON sent to the client would not have\n                    that `password`.\n                * Validation: whatever you return will be serialized with the\n                    `response_model`, converting any data as necessary to generate the\n                    corresponding JSON. But if the data in the object returned is not\n                    valid, that would mean a violation of the contract with the client,\n                    so it's an error from the API developer. So, FastAPI will raise an\n                    error and return a 500 error code (Internal Server Error).\n\n                Read more about it in the\n                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).\n                \"\"\"\n            ),\n        ] = Default(None),\n        status_code: Annotated[\n            Optional[int],\n            Doc(\n                \"\"\"\n                The default status code to be used for the response.\n\n                You could override the status code by returning a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).\n                \"\"\"\n            ),\n        ] = None,\n        tags: Annotated[\n            Optional[List[Union[str, Enum]]],\n            Doc(\n                \"\"\"\n                A list of tags to be applied to the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).\n                \"\"\"\n            ),\n        ] = None,\n        dependencies: Annotated[\n            Optional[Sequence[params.Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be applied to the\n                *path operation*.\n\n                Read more about it in the\n                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).\n                \"\"\"\n            ),\n        ] = None,\n        summary: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A summary for the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A description for the *path operation*.\n\n                If not provided, it will be extracted automatically from the docstring\n                of the *path operation function*.\n\n                It can contain Markdown.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        response_description: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The description for the default response.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = \"Successful Response\",\n        responses: Annotated[\n            Optional[Dict[Union[int, str], Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                Additional responses that could be returned by this *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        deprecated: Annotated[\n            Optional[bool],\n            Doc(\n                \"\"\"\n                Mark this *path operation* as deprecated.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        operation_id: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Custom operation ID to be used by this *path operation*.\n\n                By default, it is generated automatically.\n\n                If you provide a custom operation ID, you need to make sure it is\n                unique for the whole API.\n\n                You can customize the\n                operation ID generation with the parameter\n                `generate_unique_id_function` in the `FastAPI` class.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_include: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to include only certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_exclude: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to exclude certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_by_alias: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response model\n                should be serialized by alias when an alias is used.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = True,\n        response_model_exclude_unset: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that were not set and\n                have their default values. This is different from\n                `response_model_exclude_defaults` in that if the fields are set,\n                they will be included in the response, even if the value is the same\n                as the default.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_defaults: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that have the same value\n                as the default. This is different from `response_model_exclude_unset`\n                in that if the fields are set but contain the same default values,\n                they will be excluded from the response.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_none: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data should\n                exclude fields set to `None`.\n\n                This is much simpler (less smart) than `response_model_exclude_unset`\n                and `response_model_exclude_defaults`. You probably want to use one of\n                those two instead of this one, as those allow returning `None` values\n                when it makes sense.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).\n                \"\"\"\n            ),\n        ] = False,\n        include_in_schema: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Include this *path operation* in the generated OpenAPI schema.\n\n                This affects the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).\n                \"\"\"\n            ),\n        ] = True,\n        response_class: Annotated[\n            Type[Response],\n            Doc(\n                \"\"\"\n                Response class to be used for this *path operation*.\n\n                This will not be used if you return a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).\n                \"\"\"\n            ),\n        ] = Default(JSONResponse),\n        name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Name for this *path operation*. Only used internally.\n                \"\"\"\n            ),\n        ] = None,\n        callbacks: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                List of *path operations* that will be used as OpenAPI callbacks.\n\n                This is only for OpenAPI documentation, the callbacks won't be used\n                directly.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).\n                \"\"\"\n            ),\n        ] = None,\n        openapi_extra: Annotated[\n            Optional[Dict[str, Any]],\n            Doc(\n                \"\"\"\n                Extra metadata to be included in the OpenAPI schema for this *path\n                operation*.\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).\n                \"\"\"\n            ),\n        ] = None,\n        generate_unique_id_function: Annotated[\n            Callable[[APIRoute], str],\n            Doc(\n                \"\"\"\n                Customize the function used to generate unique IDs for the *path\n                operations* shown in the generated OpenAPI.\n\n                This is particularly useful when automatically generating clients or\n                SDKs for your API.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = Default(generate_unique_id),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add a *path operation* using an HTTP HEAD operation.\n\n        ## Example\n\n        ```python\n        from fastapi import APIRouter, FastAPI\n        from pydantic import BaseModel\n\n        class Item(BaseModel):\n            name: str\n            description: str | None = None\n\n        app = FastAPI()\n        router = APIRouter()\n\n        @router.head(\"/items/\", status_code=204)\n        def get_items_headers(response: Response):\n            response.headers[\"X-Cat-Dog\"] = \"Alone in the world\"\n\n        app.include_router(router)\n        ```\n        \"\"\"\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"HEAD\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def patch(\n        self,\n        path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The URL path to be used for this *path operation*.\n\n                For example, in `http://example.com/items`, the path is `/items`.\n                \"\"\"\n            ),\n        ],\n        *,\n        response_model: Annotated[\n            Any,\n            Doc(\n                \"\"\"\n                The type to use for the response.\n\n                It could be any valid Pydantic *field* type. So, it doesn't have to\n                be a Pydantic model, it could be other things, like a `list`, `dict`,\n                etc.\n\n                It will be used for:\n\n                * Documentation: the generated OpenAPI (and the UI at `/docs`) will\n                    show it as the response (JSON Schema).\n                * Serialization: you could return an arbitrary object and the\n                    `response_model` would be used to serialize that object into the\n                    corresponding JSON.\n                * Filtering: the JSON sent to the client will only contain the data\n                    (fields) defined in the `response_model`. If you returned an object\n                    that contains an attribute `password` but the `response_model` does\n                    not include that field, the JSON sent to the client would not have\n                    that `password`.\n                * Validation: whatever you return will be serialized with the\n                    `response_model`, converting any data as necessary to generate the\n                    corresponding JSON. But if the data in the object returned is not\n                    valid, that would mean a violation of the contract with the client,\n                    so it's an error from the API developer. So, FastAPI will raise an\n                    error and return a 500 error code (Internal Server Error).\n\n                Read more about it in the\n                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).\n                \"\"\"\n            ),\n        ] = Default(None),\n        status_code: Annotated[\n            Optional[int],\n            Doc(\n                \"\"\"\n                The default status code to be used for the response.\n\n                You could override the status code by returning a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).\n                \"\"\"\n            ),\n        ] = None,\n        tags: Annotated[\n            Optional[List[Union[str, Enum]]],\n            Doc(\n                \"\"\"\n                A list of tags to be applied to the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).\n                \"\"\"\n            ),\n        ] = None,\n        dependencies: Annotated[\n            Optional[Sequence[params.Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be applied to the\n                *path operation*.\n\n                Read more about it in the\n                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).\n                \"\"\"\n            ),\n        ] = None,\n        summary: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A summary for the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A description for the *path operation*.\n\n                If not provided, it will be extracted automatically from the docstring\n                of the *path operation function*.\n\n                It can contain Markdown.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        response_description: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The description for the default response.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = \"Successful Response\",\n        responses: Annotated[\n            Optional[Dict[Union[int, str], Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                Additional responses that could be returned by this *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        deprecated: Annotated[\n            Optional[bool],\n            Doc(\n                \"\"\"\n                Mark this *path operation* as deprecated.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        operation_id: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Custom operation ID to be used by this *path operation*.\n\n                By default, it is generated automatically.\n\n                If you provide a custom operation ID, you need to make sure it is\n                unique for the whole API.\n\n                You can customize the\n                operation ID generation with the parameter\n                `generate_unique_id_function` in the `FastAPI` class.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_include: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to include only certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_exclude: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to exclude certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_by_alias: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response model\n                should be serialized by alias when an alias is used.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = True,\n        response_model_exclude_unset: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that were not set and\n                have their default values. This is different from\n                `response_model_exclude_defaults` in that if the fields are set,\n                they will be included in the response, even if the value is the same\n                as the default.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_defaults: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that have the same value\n                as the default. This is different from `response_model_exclude_unset`\n                in that if the fields are set but contain the same default values,\n                they will be excluded from the response.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_none: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data should\n                exclude fields set to `None`.\n\n                This is much simpler (less smart) than `response_model_exclude_unset`\n                and `response_model_exclude_defaults`. You probably want to use one of\n                those two instead of this one, as those allow returning `None` values\n                when it makes sense.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).\n                \"\"\"\n            ),\n        ] = False,\n        include_in_schema: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Include this *path operation* in the generated OpenAPI schema.\n\n                This affects the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).\n                \"\"\"\n            ),\n        ] = True,\n        response_class: Annotated[\n            Type[Response],\n            Doc(\n                \"\"\"\n                Response class to be used for this *path operation*.\n\n                This will not be used if you return a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).\n                \"\"\"\n            ),\n        ] = Default(JSONResponse),\n        name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Name for this *path operation*. Only used internally.\n                \"\"\"\n            ),\n        ] = None,\n        callbacks: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                List of *path operations* that will be used as OpenAPI callbacks.\n\n                This is only for OpenAPI documentation, the callbacks won't be used\n                directly.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).\n                \"\"\"\n            ),\n        ] = None,\n        openapi_extra: Annotated[\n            Optional[Dict[str, Any]],\n            Doc(\n                \"\"\"\n                Extra metadata to be included in the OpenAPI schema for this *path\n                operation*.\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).\n                \"\"\"\n            ),\n        ] = None,\n        generate_unique_id_function: Annotated[\n            Callable[[APIRoute], str],\n            Doc(\n                \"\"\"\n                Customize the function used to generate unique IDs for the *path\n                operations* shown in the generated OpenAPI.\n\n                This is particularly useful when automatically generating clients or\n                SDKs for your API.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = Default(generate_unique_id),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add a *path operation* using an HTTP PATCH operation.\n\n        ## Example\n\n        ```python\n        from fastapi import APIRouter, FastAPI\n        from pydantic import BaseModel\n\n        class Item(BaseModel):\n            name: str\n            description: str | None = None\n\n        app = FastAPI()\n        router = APIRouter()\n\n        @router.patch(\"/items/\")\n        def update_item(item: Item):\n            return {\"message\": \"Item updated in place\"}\n\n        app.include_router(router)\n        ```\n        \"\"\"\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"PATCH\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def trace(\n        self,\n        path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The URL path to be used for this *path operation*.\n\n                For example, in `http://example.com/items`, the path is `/items`.\n                \"\"\"\n            ),\n        ],\n        *,\n        response_model: Annotated[\n            Any,\n            Doc(\n                \"\"\"\n                The type to use for the response.\n\n                It could be any valid Pydantic *field* type. So, it doesn't have to\n                be a Pydantic model, it could be other things, like a `list`, `dict`,\n                etc.\n\n                It will be used for:\n\n                * Documentation: the generated OpenAPI (and the UI at `/docs`) will\n                    show it as the response (JSON Schema).\n                * Serialization: you could return an arbitrary object and the\n                    `response_model` would be used to serialize that object into the\n                    corresponding JSON.\n                * Filtering: the JSON sent to the client will only contain the data\n                    (fields) defined in the `response_model`. If you returned an object\n                    that contains an attribute `password` but the `response_model` does\n                    not include that field, the JSON sent to the client would not have\n                    that `password`.\n                * Validation: whatever you return will be serialized with the\n                    `response_model`, converting any data as necessary to generate the\n                    corresponding JSON. But if the data in the object returned is not\n                    valid, that would mean a violation of the contract with the client,\n                    so it's an error from the API developer. So, FastAPI will raise an\n                    error and return a 500 error code (Internal Server Error).\n\n                Read more about it in the\n                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).\n                \"\"\"\n            ),\n        ] = Default(None),\n        status_code: Annotated[\n            Optional[int],\n            Doc(\n                \"\"\"\n                The default status code to be used for the response.\n\n                You could override the status code by returning a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).\n                \"\"\"\n            ),\n        ] = None,\n        tags: Annotated[\n            Optional[List[Union[str, Enum]]],\n            Doc(\n                \"\"\"\n                A list of tags to be applied to the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).\n                \"\"\"\n            ),\n        ] = None,\n        dependencies: Annotated[\n            Optional[Sequence[params.Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be applied to the\n                *path operation*.\n\n                Read more about it in the\n                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).\n                \"\"\"\n            ),\n        ] = None,\n        summary: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A summary for the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A description for the *path operation*.\n\n                If not provided, it will be extracted automatically from the docstring\n                of the *path operation function*.\n\n                It can contain Markdown.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        response_description: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The description for the default response.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = \"Successful Response\",\n        responses: Annotated[\n            Optional[Dict[Union[int, str], Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                Additional responses that could be returned by this *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        deprecated: Annotated[\n            Optional[bool],\n            Doc(\n                \"\"\"\n                Mark this *path operation* as deprecated.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        operation_id: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Custom operation ID to be used by this *path operation*.\n\n                By default, it is generated automatically.\n\n                If you provide a custom operation ID, you need to make sure it is\n                unique for the whole API.\n\n                You can customize the\n                operation ID generation with the parameter\n                `generate_unique_id_function` in the `FastAPI` class.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_include: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to include only certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_exclude: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to exclude certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_by_alias: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response model\n                should be serialized by alias when an alias is used.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = True,\n        response_model_exclude_unset: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that were not set and\n                have their default values. This is different from\n                `response_model_exclude_defaults` in that if the fields are set,\n                they will be included in the response, even if the value is the same\n                as the default.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_defaults: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that have the same value\n                as the default. This is different from `response_model_exclude_unset`\n                in that if the fields are set but contain the same default values,\n                they will be excluded from the response.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_none: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data should\n                exclude fields set to `None`.\n\n                This is much simpler (less smart) than `response_model_exclude_unset`\n                and `response_model_exclude_defaults`. You probably want to use one of\n                those two instead of this one, as those allow returning `None` values\n                when it makes sense.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).\n                \"\"\"\n            ),\n        ] = False,\n        include_in_schema: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Include this *path operation* in the generated OpenAPI schema.\n\n                This affects the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).\n                \"\"\"\n            ),\n        ] = True,\n        response_class: Annotated[\n            Type[Response],\n            Doc(\n                \"\"\"\n                Response class to be used for this *path operation*.\n\n                This will not be used if you return a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).\n                \"\"\"\n            ),\n        ] = Default(JSONResponse),\n        name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Name for this *path operation*. Only used internally.\n                \"\"\"\n            ),\n        ] = None,\n        callbacks: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                List of *path operations* that will be used as OpenAPI callbacks.\n\n                This is only for OpenAPI documentation, the callbacks won't be used\n                directly.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).\n                \"\"\"\n            ),\n        ] = None,\n        openapi_extra: Annotated[\n            Optional[Dict[str, Any]],\n            Doc(\n                \"\"\"\n                Extra metadata to be included in the OpenAPI schema for this *path\n                operation*.\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).\n                \"\"\"\n            ),\n        ] = None,\n        generate_unique_id_function: Annotated[\n            Callable[[APIRoute], str],\n            Doc(\n                \"\"\"\n                Customize the function used to generate unique IDs for the *path\n                operations* shown in the generated OpenAPI.\n\n                This is particularly useful when automatically generating clients or\n                SDKs for your API.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = Default(generate_unique_id),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add a *path operation* using an HTTP TRACE operation.\n\n        ## Example\n\n        ```python\n        from fastapi import APIRouter, FastAPI\n        from pydantic import BaseModel\n\n        class Item(BaseModel):\n            name: str\n            description: str | None = None\n\n        app = FastAPI()\n        router = APIRouter()\n\n        @router.trace(\"/items/{item_id}\")\n        def trace_item(item_id: str):\n            return None\n\n        app.include_router(router)\n        ```\n        \"\"\"\n        return self.api_route(\n            path=path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=[\"TRACE\"],\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    @deprecated(\n        \"\"\"\n        on_event is deprecated, use lifespan event handlers instead.\n\n        Read more about it in the\n        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n        \"\"\"\n    )\n    def on_event(\n        self,\n        event_type: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The type of event. `startup` or `shutdown`.\n                \"\"\"\n            ),\n        ],\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add an event handler for the router.\n\n        `on_event` is deprecated, use `lifespan` event handlers instead.\n\n        Read more about it in the\n        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/#alternative-events-deprecated).\n        \"\"\"\n\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_event_handler(event_type, func)\n            return func\n\n        return decorator\n", 4515], "/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py": ["from __future__ import annotations\n\nfrom typing import Any\n\nfrom starlette._utils import is_async_callable\nfrom starlette.concurrency import run_in_threadpool\nfrom starlette.exceptions import HTTPException\nfrom starlette.requests import Request\nfrom starlette.types import ASGIApp, ExceptionHandler, Message, Receive, Scope, Send\nfrom starlette.websockets import WebSocket\n\nExceptionHandlers = dict[Any, ExceptionHandler]\nStatusHandlers = dict[int, ExceptionHandler]\n\n\ndef _lookup_exception_handler(exc_handlers: ExceptionHandlers, exc: Exception) -> ExceptionHandler | None:\n    for cls in type(exc).__mro__:\n        if cls in exc_handlers:\n            return exc_handlers[cls]\n    return None\n\n\ndef wrap_app_handling_exceptions(app: ASGIApp, conn: Request | WebSocket) -> ASGIApp:\n    exception_handlers: ExceptionHandlers\n    status_handlers: StatusHandlers\n    try:\n        exception_handlers, status_handlers = conn.scope[\"starlette.exception_handlers\"]\n    except KeyError:\n        exception_handlers, status_handlers = {}, {}\n\n    async def wrapped_app(scope: Scope, receive: Receive, send: Send) -> None:\n        response_started = False\n\n        async def sender(message: Message) -> None:\n            nonlocal response_started\n\n            if message[\"type\"] == \"http.response.start\":\n                response_started = True\n            await send(message)\n\n        try:\n            await app(scope, receive, sender)\n        except Exception as exc:\n            handler = None\n\n            if isinstance(exc, HTTPException):\n                handler = status_handlers.get(exc.status_code)\n\n            if handler is None:\n                handler = _lookup_exception_handler(exception_handlers, exc)\n\n            if handler is None:\n                raise exc\n\n            if response_started:\n                raise RuntimeError(\"Caught handled exception, but response already started.\") from exc\n\n            if is_async_callable(handler):\n                response = await handler(conn, exc)\n            else:\n                response = await run_in_threadpool(handler, conn, exc)\n            if response is not None:\n                await response(scope, receive, sender)\n\n    return wrapped_app\n", 65], "/usr/local/lib/python3.11/site-packages/starlette/routing.py": ["from __future__ import annotations\n\nimport contextlib\nimport functools\nimport inspect\nimport re\nimport traceback\nimport types\nimport warnings\nfrom collections.abc import Awaitable, Collection, Generator, Sequence\nfrom contextlib import AbstractAsyncContextManager, AbstractContextManager, asynccontextmanager\nfrom enum import Enum\nfrom re import Pattern\nfrom typing import Any, Callable, TypeVar\n\nfrom starlette._exception_handler import wrap_app_handling_exceptions\nfrom starlette._utils import get_route_path, is_async_callable\nfrom starlette.concurrency import run_in_threadpool\nfrom starlette.convertors import CONVERTOR_TYPES, Convertor\nfrom starlette.datastructures import URL, Headers, URLPath\nfrom starlette.exceptions import HTTPException\nfrom starlette.middleware import Middleware\nfrom starlette.requests import Request\nfrom starlette.responses import PlainTextResponse, RedirectResponse, Response\nfrom starlette.types import ASGIApp, Lifespan, Receive, Scope, Send\nfrom starlette.websockets import WebSocket, WebSocketClose\n\n\nclass NoMatchFound(Exception):\n    \"\"\"\n    Raised by `.url_for(name, **path_params)` and `.url_path_for(name, **path_params)`\n    if no matching route exists.\n    \"\"\"\n\n    def __init__(self, name: str, path_params: dict[str, Any]) -> None:\n        params = \", \".join(list(path_params.keys()))\n        super().__init__(f'No route exists for name \"{name}\" and params \"{params}\".')\n\n\nclass Match(Enum):\n    NONE = 0\n    PARTIAL = 1\n    FULL = 2\n\n\ndef iscoroutinefunction_or_partial(obj: Any) -> bool:  # pragma: no cover\n    \"\"\"\n    Correctly determines if an object is a coroutine function,\n    including those wrapped in functools.partial objects.\n    \"\"\"\n    warnings.warn(\n        \"iscoroutinefunction_or_partial is deprecated, and will be removed in a future release.\",\n        DeprecationWarning,\n    )\n    while isinstance(obj, functools.partial):\n        obj = obj.func\n    return inspect.iscoroutinefunction(obj)\n\n\ndef request_response(\n    func: Callable[[Request], Awaitable[Response] | Response],\n) -> ASGIApp:\n    \"\"\"\n    Takes a function or coroutine `func(request) -> response`,\n    and returns an ASGI application.\n    \"\"\"\n    f: Callable[[Request], Awaitable[Response]] = (\n        func if is_async_callable(func) else functools.partial(run_in_threadpool, func)\n    )\n\n    async def app(scope: Scope, receive: Receive, send: Send) -> None:\n        request = Request(scope, receive, send)\n\n        async def app(scope: Scope, receive: Receive, send: Send) -> None:\n            response = await f(request)\n            await response(scope, receive, send)\n\n        await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n\n    return app\n\n\ndef websocket_session(\n    func: Callable[[WebSocket], Awaitable[None]],\n) -> ASGIApp:\n    \"\"\"\n    Takes a coroutine `func(session)`, and returns an ASGI application.\n    \"\"\"\n    # assert asyncio.iscoroutinefunction(func), \"WebSocket endpoints must be async\"\n\n    async def app(scope: Scope, receive: Receive, send: Send) -> None:\n        session = WebSocket(scope, receive=receive, send=send)\n\n        async def app(scope: Scope, receive: Receive, send: Send) -> None:\n            await func(session)\n\n        await wrap_app_handling_exceptions(app, session)(scope, receive, send)\n\n    return app\n\n\ndef get_name(endpoint: Callable[..., Any]) -> str:\n    return getattr(endpoint, \"__name__\", endpoint.__class__.__name__)\n\n\ndef replace_params(\n    path: str,\n    param_convertors: dict[str, Convertor[Any]],\n    path_params: dict[str, str],\n) -> tuple[str, dict[str, str]]:\n    for key, value in list(path_params.items()):\n        if \"{\" + key + \"}\" in path:\n            convertor = param_convertors[key]\n            value = convertor.to_string(value)\n            path = path.replace(\"{\" + key + \"}\", value)\n            path_params.pop(key)\n    return path, path_params\n\n\n# Match parameters in URL paths, eg. '{param}', and '{param:int}'\nPARAM_REGEX = re.compile(\"{([a-zA-Z_][a-zA-Z0-9_]*)(:[a-zA-Z_][a-zA-Z0-9_]*)?}\")\n\n\ndef compile_path(\n    path: str,\n) -> tuple[Pattern[str], str, dict[str, Convertor[Any]]]:\n    \"\"\"\n    Given a path string, like: \"/{username:str}\",\n    or a host string, like: \"{subdomain}.mydomain.org\", return a three-tuple\n    of (regex, format, {param_name:convertor}).\n\n    regex:      \"/(?P<username>[^/]+)\"\n    format:     \"/{username}\"\n    convertors: {\"username\": StringConvertor()}\n    \"\"\"\n    is_host = not path.startswith(\"/\")\n\n    path_regex = \"^\"\n    path_format = \"\"\n    duplicated_params = set()\n\n    idx = 0\n    param_convertors = {}\n    for match in PARAM_REGEX.finditer(path):\n        param_name, convertor_type = match.groups(\"str\")\n        convertor_type = convertor_type.lstrip(\":\")\n        assert convertor_type in CONVERTOR_TYPES, f\"Unknown path convertor '{convertor_type}'\"\n        convertor = CONVERTOR_TYPES[convertor_type]\n\n        path_regex += re.escape(path[idx : match.start()])\n        path_regex += f\"(?P<{param_name}>{convertor.regex})\"\n\n        path_format += path[idx : match.start()]\n        path_format += \"{%s}\" % param_name\n\n        if param_name in param_convertors:\n            duplicated_params.add(param_name)\n\n        param_convertors[param_name] = convertor\n\n        idx = match.end()\n\n    if duplicated_params:\n        names = \", \".join(sorted(duplicated_params))\n        ending = \"s\" if len(duplicated_params) > 1 else \"\"\n        raise ValueError(f\"Duplicated param name{ending} {names} at path {path}\")\n\n    if is_host:\n        # Align with `Host.matches()` behavior, which ignores port.\n        hostname = path[idx:].split(\":\")[0]\n        path_regex += re.escape(hostname) + \"$\"\n    else:\n        path_regex += re.escape(path[idx:]) + \"$\"\n\n    path_format += path[idx:]\n\n    return re.compile(path_regex), path_format, param_convertors\n\n\nclass BaseRoute:\n    def matches(self, scope: Scope) -> tuple[Match, Scope]:\n        raise NotImplementedError()  # pragma: no cover\n\n    def url_path_for(self, name: str, /, **path_params: Any) -> URLPath:\n        raise NotImplementedError()  # pragma: no cover\n\n    async def handle(self, scope: Scope, receive: Receive, send: Send) -> None:\n        raise NotImplementedError()  # pragma: no cover\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        \"\"\"\n        A route may be used in isolation as a stand-alone ASGI app.\n        This is a somewhat contrived case, as they'll almost always be used\n        within a Router, but could be useful for some tooling and minimal apps.\n        \"\"\"\n        match, child_scope = self.matches(scope)\n        if match == Match.NONE:\n            if scope[\"type\"] == \"http\":\n                response = PlainTextResponse(\"Not Found\", status_code=404)\n                await response(scope, receive, send)\n            elif scope[\"type\"] == \"websocket\":  # pragma: no branch\n                websocket_close = WebSocketClose()\n                await websocket_close(scope, receive, send)\n            return\n\n        scope.update(child_scope)\n        await self.handle(scope, receive, send)\n\n\nclass Route(BaseRoute):\n    def __init__(\n        self,\n        path: str,\n        endpoint: Callable[..., Any],\n        *,\n        methods: Collection[str] | None = None,\n        name: str | None = None,\n        include_in_schema: bool = True,\n        middleware: Sequence[Middleware] | None = None,\n    ) -> None:\n        assert path.startswith(\"/\"), \"Routed paths must start with '/'\"\n        self.path = path\n        self.endpoint = endpoint\n        self.name = get_name(endpoint) if name is None else name\n        self.include_in_schema = include_in_schema\n\n        endpoint_handler = endpoint\n        while isinstance(endpoint_handler, functools.partial):\n            endpoint_handler = endpoint_handler.func\n        if inspect.isfunction(endpoint_handler) or inspect.ismethod(endpoint_handler):\n            # Endpoint is function or method. Treat it as `func(request) -> response`.\n            self.app = request_response(endpoint)\n            if methods is None:\n                methods = [\"GET\"]\n        else:\n            # Endpoint is a class. Treat it as ASGI.\n            self.app = endpoint\n\n        if middleware is not None:\n            for cls, args, kwargs in reversed(middleware):\n                self.app = cls(self.app, *args, **kwargs)\n\n        if methods is None:\n            self.methods = None\n        else:\n            self.methods = {method.upper() for method in methods}\n            if \"GET\" in self.methods:\n                self.methods.add(\"HEAD\")\n\n        self.path_regex, self.path_format, self.param_convertors = compile_path(path)\n\n    def matches(self, scope: Scope) -> tuple[Match, Scope]:\n        path_params: dict[str, Any]\n        if scope[\"type\"] == \"http\":\n            route_path = get_route_path(scope)\n            match = self.path_regex.match(route_path)\n            if match:\n                matched_params = match.groupdict()\n                for key, value in matched_params.items():\n                    matched_params[key] = self.param_convertors[key].convert(value)\n                path_params = dict(scope.get(\"path_params\", {}))\n                path_params.update(matched_params)\n                child_scope = {\"endpoint\": self.endpoint, \"path_params\": path_params}\n                if self.methods and scope[\"method\"] not in self.methods:\n                    return Match.PARTIAL, child_scope\n                else:\n                    return Match.FULL, child_scope\n        return Match.NONE, {}\n\n    def url_path_for(self, name: str, /, **path_params: Any) -> URLPath:\n        seen_params = set(path_params.keys())\n        expected_params = set(self.param_convertors.keys())\n\n        if name != self.name or seen_params != expected_params:\n            raise NoMatchFound(name, path_params)\n\n        path, remaining_params = replace_params(self.path_format, self.param_convertors, path_params)\n        assert not remaining_params\n        return URLPath(path=path, protocol=\"http\")\n\n    async def handle(self, scope: Scope, receive: Receive, send: Send) -> None:\n        if self.methods and scope[\"method\"] not in self.methods:\n            headers = {\"Allow\": \", \".join(self.methods)}\n            if \"app\" in scope:\n                raise HTTPException(status_code=405, headers=headers)\n            else:\n                response = PlainTextResponse(\"Method Not Allowed\", status_code=405, headers=headers)\n            await response(scope, receive, send)\n        else:\n            await self.app(scope, receive, send)\n\n    def __eq__(self, other: Any) -> bool:\n        return (\n            isinstance(other, Route)\n            and self.path == other.path\n            and self.endpoint == other.endpoint\n            and self.methods == other.methods\n        )\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        methods = sorted(self.methods or [])\n        path, name = self.path, self.name\n        return f\"{class_name}(path={path!r}, name={name!r}, methods={methods!r})\"\n\n\nclass WebSocketRoute(BaseRoute):\n    def __init__(\n        self,\n        path: str,\n        endpoint: Callable[..., Any],\n        *,\n        name: str | None = None,\n        middleware: Sequence[Middleware] | None = None,\n    ) -> None:\n        assert path.startswith(\"/\"), \"Routed paths must start with '/'\"\n        self.path = path\n        self.endpoint = endpoint\n        self.name = get_name(endpoint) if name is None else name\n\n        endpoint_handler = endpoint\n        while isinstance(endpoint_handler, functools.partial):\n            endpoint_handler = endpoint_handler.func\n        if inspect.isfunction(endpoint_handler) or inspect.ismethod(endpoint_handler):\n            # Endpoint is function or method. Treat it as `func(websocket)`.\n            self.app = websocket_session(endpoint)\n        else:\n            # Endpoint is a class. Treat it as ASGI.\n            self.app = endpoint\n\n        if middleware is not None:\n            for cls, args, kwargs in reversed(middleware):\n                self.app = cls(self.app, *args, **kwargs)\n\n        self.path_regex, self.path_format, self.param_convertors = compile_path(path)\n\n    def matches(self, scope: Scope) -> tuple[Match, Scope]:\n        path_params: dict[str, Any]\n        if scope[\"type\"] == \"websocket\":\n            route_path = get_route_path(scope)\n            match = self.path_regex.match(route_path)\n            if match:\n                matched_params = match.groupdict()\n                for key, value in matched_params.items():\n                    matched_params[key] = self.param_convertors[key].convert(value)\n                path_params = dict(scope.get(\"path_params\", {}))\n                path_params.update(matched_params)\n                child_scope = {\"endpoint\": self.endpoint, \"path_params\": path_params}\n                return Match.FULL, child_scope\n        return Match.NONE, {}\n\n    def url_path_for(self, name: str, /, **path_params: Any) -> URLPath:\n        seen_params = set(path_params.keys())\n        expected_params = set(self.param_convertors.keys())\n\n        if name != self.name or seen_params != expected_params:\n            raise NoMatchFound(name, path_params)\n\n        path, remaining_params = replace_params(self.path_format, self.param_convertors, path_params)\n        assert not remaining_params\n        return URLPath(path=path, protocol=\"websocket\")\n\n    async def handle(self, scope: Scope, receive: Receive, send: Send) -> None:\n        await self.app(scope, receive, send)\n\n    def __eq__(self, other: Any) -> bool:\n        return isinstance(other, WebSocketRoute) and self.path == other.path and self.endpoint == other.endpoint\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}(path={self.path!r}, name={self.name!r})\"\n\n\nclass Mount(BaseRoute):\n    def __init__(\n        self,\n        path: str,\n        app: ASGIApp | None = None,\n        routes: Sequence[BaseRoute] | None = None,\n        name: str | None = None,\n        *,\n        middleware: Sequence[Middleware] | None = None,\n    ) -> None:\n        assert path == \"\" or path.startswith(\"/\"), \"Routed paths must start with '/'\"\n        assert app is not None or routes is not None, \"Either 'app=...', or 'routes=' must be specified\"\n        self.path = path.rstrip(\"/\")\n        if app is not None:\n            self._base_app: ASGIApp = app\n        else:\n            self._base_app = Router(routes=routes)\n        self.app = self._base_app\n        if middleware is not None:\n            for cls, args, kwargs in reversed(middleware):\n                self.app = cls(self.app, *args, **kwargs)\n        self.name = name\n        self.path_regex, self.path_format, self.param_convertors = compile_path(self.path + \"/{path:path}\")\n\n    @property\n    def routes(self) -> list[BaseRoute]:\n        return getattr(self._base_app, \"routes\", [])\n\n    def matches(self, scope: Scope) -> tuple[Match, Scope]:\n        path_params: dict[str, Any]\n        if scope[\"type\"] in (\"http\", \"websocket\"):  # pragma: no branch\n            root_path = scope.get(\"root_path\", \"\")\n            route_path = get_route_path(scope)\n            match = self.path_regex.match(route_path)\n            if match:\n                matched_params = match.groupdict()\n                for key, value in matched_params.items():\n                    matched_params[key] = self.param_convertors[key].convert(value)\n                remaining_path = \"/\" + matched_params.pop(\"path\")\n                matched_path = route_path[: -len(remaining_path)]\n                path_params = dict(scope.get(\"path_params\", {}))\n                path_params.update(matched_params)\n                child_scope = {\n                    \"path_params\": path_params,\n                    # app_root_path will only be set at the top level scope,\n                    # initialized with the (optional) value of a root_path\n                    # set above/before Starlette. And even though any\n                    # mount will have its own child scope with its own respective\n                    # root_path, the app_root_path will always be available in all\n                    # the child scopes with the same top level value because it's\n                    # set only once here with a default, any other child scope will\n                    # just inherit that app_root_path default value stored in the\n                    # scope. All this is needed to support Request.url_for(), as it\n                    # uses the app_root_path to build the URL path.\n                    \"app_root_path\": scope.get(\"app_root_path\", root_path),\n                    \"root_path\": root_path + matched_path,\n                    \"endpoint\": self.app,\n                }\n                return Match.FULL, child_scope\n        return Match.NONE, {}\n\n    def url_path_for(self, name: str, /, **path_params: Any) -> URLPath:\n        if self.name is not None and name == self.name and \"path\" in path_params:\n            # 'name' matches \"<mount_name>\".\n            path_params[\"path\"] = path_params[\"path\"].lstrip(\"/\")\n            path, remaining_params = replace_params(self.path_format, self.param_convertors, path_params)\n            if not remaining_params:\n                return URLPath(path=path)\n        elif self.name is None or name.startswith(self.name + \":\"):\n            if self.name is None:\n                # No mount name.\n                remaining_name = name\n            else:\n                # 'name' matches \"<mount_name>:<child_name>\".\n                remaining_name = name[len(self.name) + 1 :]\n            path_kwarg = path_params.get(\"path\")\n            path_params[\"path\"] = \"\"\n            path_prefix, remaining_params = replace_params(self.path_format, self.param_convertors, path_params)\n            if path_kwarg is not None:\n                remaining_params[\"path\"] = path_kwarg\n            for route in self.routes or []:\n                try:\n                    url = route.url_path_for(remaining_name, **remaining_params)\n                    return URLPath(path=path_prefix.rstrip(\"/\") + str(url), protocol=url.protocol)\n                except NoMatchFound:\n                    pass\n        raise NoMatchFound(name, path_params)\n\n    async def handle(self, scope: Scope, receive: Receive, send: Send) -> None:\n        await self.app(scope, receive, send)\n\n    def __eq__(self, other: Any) -> bool:\n        return isinstance(other, Mount) and self.path == other.path and self.app == other.app\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        name = self.name or \"\"\n        return f\"{class_name}(path={self.path!r}, name={name!r}, app={self.app!r})\"\n\n\nclass Host(BaseRoute):\n    def __init__(self, host: str, app: ASGIApp, name: str | None = None) -> None:\n        assert not host.startswith(\"/\"), \"Host must not start with '/'\"\n        self.host = host\n        self.app = app\n        self.name = name\n        self.host_regex, self.host_format, self.param_convertors = compile_path(host)\n\n    @property\n    def routes(self) -> list[BaseRoute]:\n        return getattr(self.app, \"routes\", [])\n\n    def matches(self, scope: Scope) -> tuple[Match, Scope]:\n        if scope[\"type\"] in (\"http\", \"websocket\"):  # pragma:no branch\n            headers = Headers(scope=scope)\n            host = headers.get(\"host\", \"\").split(\":\")[0]\n            match = self.host_regex.match(host)\n            if match:\n                matched_params = match.groupdict()\n                for key, value in matched_params.items():\n                    matched_params[key] = self.param_convertors[key].convert(value)\n                path_params = dict(scope.get(\"path_params\", {}))\n                path_params.update(matched_params)\n                child_scope = {\"path_params\": path_params, \"endpoint\": self.app}\n                return Match.FULL, child_scope\n        return Match.NONE, {}\n\n    def url_path_for(self, name: str, /, **path_params: Any) -> URLPath:\n        if self.name is not None and name == self.name and \"path\" in path_params:\n            # 'name' matches \"<mount_name>\".\n            path = path_params.pop(\"path\")\n            host, remaining_params = replace_params(self.host_format, self.param_convertors, path_params)\n            if not remaining_params:\n                return URLPath(path=path, host=host)\n        elif self.name is None or name.startswith(self.name + \":\"):\n            if self.name is None:\n                # No mount name.\n                remaining_name = name\n            else:\n                # 'name' matches \"<mount_name>:<child_name>\".\n                remaining_name = name[len(self.name) + 1 :]\n            host, remaining_params = replace_params(self.host_format, self.param_convertors, path_params)\n            for route in self.routes or []:\n                try:\n                    url = route.url_path_for(remaining_name, **remaining_params)\n                    return URLPath(path=str(url), protocol=url.protocol, host=host)\n                except NoMatchFound:\n                    pass\n        raise NoMatchFound(name, path_params)\n\n    async def handle(self, scope: Scope, receive: Receive, send: Send) -> None:\n        await self.app(scope, receive, send)\n\n    def __eq__(self, other: Any) -> bool:\n        return isinstance(other, Host) and self.host == other.host and self.app == other.app\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        name = self.name or \"\"\n        return f\"{class_name}(host={self.host!r}, name={name!r}, app={self.app!r})\"\n\n\n_T = TypeVar(\"_T\")\n\n\nclass _AsyncLiftContextManager(AbstractAsyncContextManager[_T]):\n    def __init__(self, cm: AbstractContextManager[_T]):\n        self._cm = cm\n\n    async def __aenter__(self) -> _T:\n        return self._cm.__enter__()\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_value: BaseException | None,\n        traceback: types.TracebackType | None,\n    ) -> bool | None:\n        return self._cm.__exit__(exc_type, exc_value, traceback)\n\n\ndef _wrap_gen_lifespan_context(\n    lifespan_context: Callable[[Any], Generator[Any, Any, Any]],\n) -> Callable[[Any], AbstractAsyncContextManager[Any]]:\n    cmgr = contextlib.contextmanager(lifespan_context)\n\n    @functools.wraps(cmgr)\n    def wrapper(app: Any) -> _AsyncLiftContextManager[Any]:\n        return _AsyncLiftContextManager(cmgr(app))\n\n    return wrapper\n\n\nclass _DefaultLifespan:\n    def __init__(self, router: Router):\n        self._router = router\n\n    async def __aenter__(self) -> None:\n        await self._router.startup()\n\n    async def __aexit__(self, *exc_info: object) -> None:\n        await self._router.shutdown()\n\n    def __call__(self: _T, app: object) -> _T:\n        return self\n\n\nclass Router:\n    def __init__(\n        self,\n        routes: Sequence[BaseRoute] | None = None,\n        redirect_slashes: bool = True,\n        default: ASGIApp | None = None,\n        on_startup: Sequence[Callable[[], Any]] | None = None,\n        on_shutdown: Sequence[Callable[[], Any]] | None = None,\n        # the generic to Lifespan[AppType] is the type of the top level application\n        # which the router cannot know statically, so we use Any\n        lifespan: Lifespan[Any] | None = None,\n        *,\n        middleware: Sequence[Middleware] | None = None,\n    ) -> None:\n        self.routes = [] if routes is None else list(routes)\n        self.redirect_slashes = redirect_slashes\n        self.default = self.not_found if default is None else default\n        self.on_startup = [] if on_startup is None else list(on_startup)\n        self.on_shutdown = [] if on_shutdown is None else list(on_shutdown)\n\n        if on_startup or on_shutdown:\n            warnings.warn(\n                \"The on_startup and on_shutdown parameters are deprecated, and they \"\n                \"will be removed on version 1.0. Use the lifespan parameter instead. \"\n                \"See more about it on https://www.starlette.io/lifespan/.\",\n                DeprecationWarning,\n            )\n            if lifespan:\n                warnings.warn(\n                    \"The `lifespan` parameter cannot be used with `on_startup` or \"\n                    \"`on_shutdown`. Both `on_startup` and `on_shutdown` will be \"\n                    \"ignored.\"\n                )\n\n        if lifespan is None:\n            self.lifespan_context: Lifespan[Any] = _DefaultLifespan(self)\n\n        elif inspect.isasyncgenfunction(lifespan):\n            warnings.warn(\n                \"async generator function lifespans are deprecated, \"\n                \"use an @contextlib.asynccontextmanager function instead\",\n                DeprecationWarning,\n            )\n            self.lifespan_context = asynccontextmanager(\n                lifespan,\n            )\n        elif inspect.isgeneratorfunction(lifespan):\n            warnings.warn(\n                \"generator function lifespans are deprecated, use an @contextlib.asynccontextmanager function instead\",\n                DeprecationWarning,\n            )\n            self.lifespan_context = _wrap_gen_lifespan_context(\n                lifespan,\n            )\n        else:\n            self.lifespan_context = lifespan\n\n        self.middleware_stack = self.app\n        if middleware:\n            for cls, args, kwargs in reversed(middleware):\n                self.middleware_stack = cls(self.middleware_stack, *args, **kwargs)\n\n    async def not_found(self, scope: Scope, receive: Receive, send: Send) -> None:\n        if scope[\"type\"] == \"websocket\":\n            websocket_close = WebSocketClose()\n            await websocket_close(scope, receive, send)\n            return\n\n        # If we're running inside a starlette application then raise an\n        # exception, so that the configurable exception handler can deal with\n        # returning the response. For plain ASGI apps, just return the response.\n        if \"app\" in scope:\n            raise HTTPException(status_code=404)\n        else:\n            response = PlainTextResponse(\"Not Found\", status_code=404)\n        await response(scope, receive, send)\n\n    def url_path_for(self, name: str, /, **path_params: Any) -> URLPath:\n        for route in self.routes:\n            try:\n                return route.url_path_for(name, **path_params)\n            except NoMatchFound:\n                pass\n        raise NoMatchFound(name, path_params)\n\n    async def startup(self) -> None:\n        \"\"\"\n        Run any `.on_startup` event handlers.\n        \"\"\"\n        for handler in self.on_startup:\n            if is_async_callable(handler):\n                await handler()\n            else:\n                handler()\n\n    async def shutdown(self) -> None:\n        \"\"\"\n        Run any `.on_shutdown` event handlers.\n        \"\"\"\n        for handler in self.on_shutdown:\n            if is_async_callable(handler):\n                await handler()\n            else:\n                handler()\n\n    async def lifespan(self, scope: Scope, receive: Receive, send: Send) -> None:\n        \"\"\"\n        Handle ASGI lifespan messages, which allows us to manage application\n        startup and shutdown events.\n        \"\"\"\n        started = False\n        app: Any = scope.get(\"app\")\n        await receive()\n        try:\n            async with self.lifespan_context(app) as maybe_state:\n                if maybe_state is not None:\n                    if \"state\" not in scope:\n                        raise RuntimeError('The server does not support \"state\" in the lifespan scope.')\n                    scope[\"state\"].update(maybe_state)\n                await send({\"type\": \"lifespan.startup.complete\"})\n                started = True\n                await receive()\n        except BaseException:\n            exc_text = traceback.format_exc()\n            if started:\n                await send({\"type\": \"lifespan.shutdown.failed\", \"message\": exc_text})\n            else:\n                await send({\"type\": \"lifespan.startup.failed\", \"message\": exc_text})\n            raise\n        else:\n            await send({\"type\": \"lifespan.shutdown.complete\"})\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        \"\"\"\n        The main entry point to the Router class.\n        \"\"\"\n        await self.middleware_stack(scope, receive, send)\n\n    async def app(self, scope: Scope, receive: Receive, send: Send) -> None:\n        assert scope[\"type\"] in (\"http\", \"websocket\", \"lifespan\")\n\n        if \"router\" not in scope:\n            scope[\"router\"] = self\n\n        if scope[\"type\"] == \"lifespan\":\n            await self.lifespan(scope, receive, send)\n            return\n\n        partial = None\n\n        for route in self.routes:\n            # Determine if any route matches the incoming scope,\n            # and hand over to the matching route if found.\n            match, child_scope = route.matches(scope)\n            if match == Match.FULL:\n                scope.update(child_scope)\n                await route.handle(scope, receive, send)\n                return\n            elif match == Match.PARTIAL and partial is None:\n                partial = route\n                partial_scope = child_scope\n\n        if partial is not None:\n            # \u00a0Handle partial matches. These are cases where an endpoint is\n            # able to handle the request, but is not a preferred option.\n            # We use this in particular to deal with \"405 Method Not Allowed\".\n            scope.update(partial_scope)\n            await partial.handle(scope, receive, send)\n            return\n\n        route_path = get_route_path(scope)\n        if scope[\"type\"] == \"http\" and self.redirect_slashes and route_path != \"/\":\n            redirect_scope = dict(scope)\n            if route_path.endswith(\"/\"):\n                redirect_scope[\"path\"] = redirect_scope[\"path\"].rstrip(\"/\")\n            else:\n                redirect_scope[\"path\"] = redirect_scope[\"path\"] + \"/\"\n\n            for route in self.routes:\n                match, child_scope = route.matches(redirect_scope)\n                if match != Match.NONE:\n                    redirect_url = URL(scope=redirect_scope)\n                    response = RedirectResponse(url=str(redirect_url))\n                    await response(scope, receive, send)\n                    return\n\n        await self.default(scope, receive, send)\n\n    def __eq__(self, other: Any) -> bool:\n        return isinstance(other, Router) and self.routes == other.routes\n\n    def mount(self, path: str, app: ASGIApp, name: str | None = None) -> None:  # pragma: no cover\n        route = Mount(path, app=app, name=name)\n        self.routes.append(route)\n\n    def host(self, host: str, app: ASGIApp, name: str | None = None) -> None:  # pragma: no cover\n        route = Host(host, app=app, name=name)\n        self.routes.append(route)\n\n    def add_route(\n        self,\n        path: str,\n        endpoint: Callable[[Request], Awaitable[Response] | Response],\n        methods: Collection[str] | None = None,\n        name: str | None = None,\n        include_in_schema: bool = True,\n    ) -> None:  # pragma: no cover\n        route = Route(\n            path,\n            endpoint=endpoint,\n            methods=methods,\n            name=name,\n            include_in_schema=include_in_schema,\n        )\n        self.routes.append(route)\n\n    def add_websocket_route(\n        self,\n        path: str,\n        endpoint: Callable[[WebSocket], Awaitable[None]],\n        name: str | None = None,\n    ) -> None:  # pragma: no cover\n        route = WebSocketRoute(path, endpoint=endpoint, name=name)\n        self.routes.append(route)\n\n    def route(\n        self,\n        path: str,\n        methods: Collection[str] | None = None,\n        name: str | None = None,\n        include_in_schema: bool = True,\n    ) -> Callable:  # type: ignore[type-arg]\n        \"\"\"\n        We no longer document this decorator style API, and its usage is discouraged.\n        Instead you should use the following approach:\n\n        >>> routes = [Route(path, endpoint=...), ...]\n        >>> app = Starlette(routes=routes)\n        \"\"\"\n        warnings.warn(\n            \"The `route` decorator is deprecated, and will be removed in version 1.0.0.\"\n            \"Refer to https://www.starlette.io/routing/#http-routing for the recommended approach.\",\n            DeprecationWarning,\n        )\n\n        def decorator(func: Callable) -> Callable:  # type: ignore[type-arg]\n            self.add_route(\n                path,\n                func,\n                methods=methods,\n                name=name,\n                include_in_schema=include_in_schema,\n            )\n            return func\n\n        return decorator\n\n    def websocket_route(self, path: str, name: str | None = None) -> Callable:  # type: ignore[type-arg]\n        \"\"\"\n        We no longer document this decorator style API, and its usage is discouraged.\n        Instead you should use the following approach:\n\n        >>> routes = [WebSocketRoute(path, endpoint=...), ...]\n        >>> app = Starlette(routes=routes)\n        \"\"\"\n        warnings.warn(\n            \"The `websocket_route` decorator is deprecated, and will be removed in version 1.0.0. Refer to \"\n            \"https://www.starlette.io/routing/#websocket-routing for the recommended approach.\",\n            DeprecationWarning,\n        )\n\n        def decorator(func: Callable) -> Callable:  # type: ignore[type-arg]\n            self.add_websocket_route(path, func, name=name)\n            return func\n\n        return decorator\n\n    def add_event_handler(self, event_type: str, func: Callable[[], Any]) -> None:  # pragma: no cover\n        assert event_type in (\"startup\", \"shutdown\")\n\n        if event_type == \"startup\":\n            self.on_startup.append(func)\n        else:\n            self.on_shutdown.append(func)\n\n    def on_event(self, event_type: str) -> Callable:  # type: ignore[type-arg]\n        warnings.warn(\n            \"The `on_event` decorator is deprecated, and will be removed in version 1.0.0. \"\n            \"Refer to https://www.starlette.io/lifespan/ for recommended approach.\",\n            DeprecationWarning,\n        )\n\n        def decorator(func: Callable) -> Callable:  # type: ignore[type-arg]\n            self.add_event_handler(event_type, func)\n            return func\n\n        return decorator\n", 876], "/usr/local/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py": ["from contextlib import AsyncExitStack\n\nfrom starlette.types import ASGIApp, Receive, Scope, Send\n\n\n# Used mainly to close files after the request is done, dependencies are closed\n# in their own AsyncExitStack\nclass AsyncExitStackMiddleware:\n    def __init__(\n        self, app: ASGIApp, context_name: str = \"fastapi_middleware_astack\"\n    ) -> None:\n        self.app = app\n        self.context_name = context_name\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        async with AsyncExitStack() as stack:\n            scope[self.context_name] = stack\n            await self.app(scope, receive, send)\n", 18], "/usr/local/lib/python3.11/site-packages/starlette/middleware/exceptions.py": ["from __future__ import annotations\n\nfrom collections.abc import Mapping\nfrom typing import Any\n\nfrom starlette._exception_handler import (\n    ExceptionHandlers,\n    StatusHandlers,\n    wrap_app_handling_exceptions,\n)\nfrom starlette.exceptions import HTTPException, WebSocketException\nfrom starlette.requests import Request\nfrom starlette.responses import PlainTextResponse, Response\nfrom starlette.types import ASGIApp, ExceptionHandler, Receive, Scope, Send\nfrom starlette.websockets import WebSocket\n\n\nclass ExceptionMiddleware:\n    def __init__(\n        self,\n        app: ASGIApp,\n        handlers: Mapping[Any, ExceptionHandler] | None = None,\n        debug: bool = False,\n    ) -> None:\n        self.app = app\n        self.debug = debug  # TODO: We ought to handle 404 cases if debug is set.\n        self._status_handlers: StatusHandlers = {}\n        self._exception_handlers: ExceptionHandlers = {\n            HTTPException: self.http_exception,\n            WebSocketException: self.websocket_exception,\n        }\n        if handlers is not None:  # pragma: no branch\n            for key, value in handlers.items():\n                self.add_exception_handler(key, value)\n\n    def add_exception_handler(\n        self,\n        exc_class_or_status_code: int | type[Exception],\n        handler: ExceptionHandler,\n    ) -> None:\n        if isinstance(exc_class_or_status_code, int):\n            self._status_handlers[exc_class_or_status_code] = handler\n        else:\n            assert issubclass(exc_class_or_status_code, Exception)\n            self._exception_handlers[exc_class_or_status_code] = handler\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        if scope[\"type\"] not in (\"http\", \"websocket\"):\n            await self.app(scope, receive, send)\n            return\n\n        scope[\"starlette.exception_handlers\"] = (\n            self._exception_handlers,\n            self._status_handlers,\n        )\n\n        conn: Request | WebSocket\n        if scope[\"type\"] == \"http\":\n            conn = Request(scope, receive, send)\n        else:\n            conn = WebSocket(scope, receive, send)\n\n        await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n\n    async def http_exception(self, request: Request, exc: Exception) -> Response:\n        assert isinstance(exc, HTTPException)\n        if exc.status_code in {204, 304}:\n            return Response(status_code=exc.status_code, headers=exc.headers)\n        return PlainTextResponse(exc.detail, status_code=exc.status_code, headers=exc.headers)\n\n    async def websocket_exception(self, websocket: WebSocket, exc: Exception) -> None:\n        assert isinstance(exc, WebSocketException)\n        await websocket.close(code=exc.code, reason=exc.reason)  # pragma: no cover\n", 73], "/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py": ["from __future__ import annotations\n\nimport functools\nimport re\nfrom collections.abc import Sequence\n\nfrom starlette.datastructures import Headers, MutableHeaders\nfrom starlette.responses import PlainTextResponse, Response\nfrom starlette.types import ASGIApp, Message, Receive, Scope, Send\n\nALL_METHODS = (\"DELETE\", \"GET\", \"HEAD\", \"OPTIONS\", \"PATCH\", \"POST\", \"PUT\")\nSAFELISTED_HEADERS = {\"Accept\", \"Accept-Language\", \"Content-Language\", \"Content-Type\"}\n\n\nclass CORSMiddleware:\n    def __init__(\n        self,\n        app: ASGIApp,\n        allow_origins: Sequence[str] = (),\n        allow_methods: Sequence[str] = (\"GET\",),\n        allow_headers: Sequence[str] = (),\n        allow_credentials: bool = False,\n        allow_origin_regex: str | None = None,\n        expose_headers: Sequence[str] = (),\n        max_age: int = 600,\n    ) -> None:\n        if \"*\" in allow_methods:\n            allow_methods = ALL_METHODS\n\n        compiled_allow_origin_regex = None\n        if allow_origin_regex is not None:\n            compiled_allow_origin_regex = re.compile(allow_origin_regex)\n\n        allow_all_origins = \"*\" in allow_origins\n        allow_all_headers = \"*\" in allow_headers\n        preflight_explicit_allow_origin = not allow_all_origins or allow_credentials\n\n        simple_headers = {}\n        if allow_all_origins:\n            simple_headers[\"Access-Control-Allow-Origin\"] = \"*\"\n        if allow_credentials:\n            simple_headers[\"Access-Control-Allow-Credentials\"] = \"true\"\n        if expose_headers:\n            simple_headers[\"Access-Control-Expose-Headers\"] = \", \".join(expose_headers)\n\n        preflight_headers = {}\n        if preflight_explicit_allow_origin:\n            # The origin value will be set in preflight_response() if it is allowed.\n            preflight_headers[\"Vary\"] = \"Origin\"\n        else:\n            preflight_headers[\"Access-Control-Allow-Origin\"] = \"*\"\n        preflight_headers.update(\n            {\n                \"Access-Control-Allow-Methods\": \", \".join(allow_methods),\n                \"Access-Control-Max-Age\": str(max_age),\n            }\n        )\n        allow_headers = sorted(SAFELISTED_HEADERS | set(allow_headers))\n        if allow_headers and not allow_all_headers:\n            preflight_headers[\"Access-Control-Allow-Headers\"] = \", \".join(allow_headers)\n        if allow_credentials:\n            preflight_headers[\"Access-Control-Allow-Credentials\"] = \"true\"\n\n        self.app = app\n        self.allow_origins = allow_origins\n        self.allow_methods = allow_methods\n        self.allow_headers = [h.lower() for h in allow_headers]\n        self.allow_all_origins = allow_all_origins\n        self.allow_all_headers = allow_all_headers\n        self.preflight_explicit_allow_origin = preflight_explicit_allow_origin\n        self.allow_origin_regex = compiled_allow_origin_regex\n        self.simple_headers = simple_headers\n        self.preflight_headers = preflight_headers\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        if scope[\"type\"] != \"http\":  # pragma: no cover\n            await self.app(scope, receive, send)\n            return\n\n        method = scope[\"method\"]\n        headers = Headers(scope=scope)\n        origin = headers.get(\"origin\")\n\n        if origin is None:\n            await self.app(scope, receive, send)\n            return\n\n        if method == \"OPTIONS\" and \"access-control-request-method\" in headers:\n            response = self.preflight_response(request_headers=headers)\n            await response(scope, receive, send)\n            return\n\n        await self.simple_response(scope, receive, send, request_headers=headers)\n\n    def is_allowed_origin(self, origin: str) -> bool:\n        if self.allow_all_origins:\n            return True\n\n        if self.allow_origin_regex is not None and self.allow_origin_regex.fullmatch(origin):\n            return True\n\n        return origin in self.allow_origins\n\n    def preflight_response(self, request_headers: Headers) -> Response:\n        requested_origin = request_headers[\"origin\"]\n        requested_method = request_headers[\"access-control-request-method\"]\n        requested_headers = request_headers.get(\"access-control-request-headers\")\n\n        headers = dict(self.preflight_headers)\n        failures = []\n\n        if self.is_allowed_origin(origin=requested_origin):\n            if self.preflight_explicit_allow_origin:\n                # The \"else\" case is already accounted for in self.preflight_headers\n                # and the value would be \"*\".\n                headers[\"Access-Control-Allow-Origin\"] = requested_origin\n        else:\n            failures.append(\"origin\")\n\n        if requested_method not in self.allow_methods:\n            failures.append(\"method\")\n\n        # If we allow all headers, then we have to mirror back any requested\n        # headers in the response.\n        if self.allow_all_headers and requested_headers is not None:\n            headers[\"Access-Control-Allow-Headers\"] = requested_headers\n        elif requested_headers is not None:\n            for header in [h.lower() for h in requested_headers.split(\",\")]:\n                if header.strip() not in self.allow_headers:\n                    failures.append(\"headers\")\n                    break\n\n        # We don't strictly need to use 400 responses here, since its up to\n        # the browser to enforce the CORS policy, but its more informative\n        # if we do.\n        if failures:\n            failure_text = \"Disallowed CORS \" + \", \".join(failures)\n            return PlainTextResponse(failure_text, status_code=400, headers=headers)\n\n        return PlainTextResponse(\"OK\", status_code=200, headers=headers)\n\n    async def simple_response(self, scope: Scope, receive: Receive, send: Send, request_headers: Headers) -> None:\n        send = functools.partial(self.send, send=send, request_headers=request_headers)\n        await self.app(scope, receive, send)\n\n    async def send(self, message: Message, send: Send, request_headers: Headers) -> None:\n        if message[\"type\"] != \"http.response.start\":\n            await send(message)\n            return\n\n        message.setdefault(\"headers\", [])\n        headers = MutableHeaders(scope=message)\n        headers.update(self.simple_headers)\n        origin = request_headers[\"Origin\"]\n        has_cookie = \"cookie\" in request_headers\n\n        # If request includes any cookie headers, then we must respond\n        # with the specific origin instead of '*'.\n        if self.allow_all_origins and has_cookie:\n            self.allow_explicit_origin(headers, origin)\n\n        # If we only allow specific origins, then we have to mirror back\n        # the Origin header in the response.\n        elif not self.allow_all_origins and self.is_allowed_origin(origin=origin):\n            self.allow_explicit_origin(headers, origin)\n\n        await send(message)\n\n    @staticmethod\n    def allow_explicit_origin(headers: MutableHeaders, origin: str) -> None:\n        headers[\"Access-Control-Allow-Origin\"] = origin\n        headers.add_vary_header(\"Origin\")\n", 172], "/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py": ["from __future__ import annotations\n\nimport html\nimport inspect\nimport sys\nimport traceback\n\nfrom starlette._utils import is_async_callable\nfrom starlette.concurrency import run_in_threadpool\nfrom starlette.requests import Request\nfrom starlette.responses import HTMLResponse, PlainTextResponse, Response\nfrom starlette.types import ASGIApp, ExceptionHandler, Message, Receive, Scope, Send\n\nSTYLES = \"\"\"\np {\n    color: #211c1c;\n}\n.traceback-container {\n    border: 1px solid #038BB8;\n}\n.traceback-title {\n    background-color: #038BB8;\n    color: lemonchiffon;\n    padding: 12px;\n    font-size: 20px;\n    margin-top: 0px;\n}\n.frame-line {\n    padding-left: 10px;\n    font-family: monospace;\n}\n.frame-filename {\n    font-family: monospace;\n}\n.center-line {\n    background-color: #038BB8;\n    color: #f9f6e1;\n    padding: 5px 0px 5px 5px;\n}\n.lineno {\n    margin-right: 5px;\n}\n.frame-title {\n    font-weight: unset;\n    padding: 10px 10px 10px 10px;\n    background-color: #E4F4FD;\n    margin-right: 10px;\n    color: #191f21;\n    font-size: 17px;\n    border: 1px solid #c7dce8;\n}\n.collapse-btn {\n    float: right;\n    padding: 0px 5px 1px 5px;\n    border: solid 1px #96aebb;\n    cursor: pointer;\n}\n.collapsed {\n  display: none;\n}\n.source-code {\n  font-family: courier;\n  font-size: small;\n  padding-bottom: 10px;\n}\n\"\"\"\n\nJS = \"\"\"\n<script type=\"text/javascript\">\n    function collapse(element){\n        const frameId = element.getAttribute(\"data-frame-id\");\n        const frame = document.getElementById(frameId);\n\n        if (frame.classList.contains(\"collapsed\")){\n            element.innerHTML = \"&#8210;\";\n            frame.classList.remove(\"collapsed\");\n        } else {\n            element.innerHTML = \"+\";\n            frame.classList.add(\"collapsed\");\n        }\n    }\n</script>\n\"\"\"\n\nTEMPLATE = \"\"\"\n<html>\n    <head>\n        <style type='text/css'>\n            {styles}\n        </style>\n        <title>Starlette Debugger</title>\n    </head>\n    <body>\n        <h1>500 Server Error</h1>\n        <h2>{error}</h2>\n        <div class=\"traceback-container\">\n            <p class=\"traceback-title\">Traceback</p>\n            <div>{exc_html}</div>\n        </div>\n        {js}\n    </body>\n</html>\n\"\"\"\n\nFRAME_TEMPLATE = \"\"\"\n<div>\n    <p class=\"frame-title\">File <span class=\"frame-filename\">{frame_filename}</span>,\n    line <i>{frame_lineno}</i>,\n    in <b>{frame_name}</b>\n    <span class=\"collapse-btn\" data-frame-id=\"{frame_filename}-{frame_lineno}\" onclick=\"collapse(this)\">{collapse_button}</span>\n    </p>\n    <div id=\"{frame_filename}-{frame_lineno}\" class=\"source-code {collapsed}\">{code_context}</div>\n</div>\n\"\"\"  # noqa: E501\n\nLINE = \"\"\"\n<p><span class=\"frame-line\">\n<span class=\"lineno\">{lineno}.</span> {line}</span></p>\n\"\"\"\n\nCENTER_LINE = \"\"\"\n<p class=\"center-line\"><span class=\"frame-line center-line\">\n<span class=\"lineno\">{lineno}.</span> {line}</span></p>\n\"\"\"\n\n\nclass ServerErrorMiddleware:\n    \"\"\"\n    Handles returning 500 responses when a server error occurs.\n\n    If 'debug' is set, then traceback responses will be returned,\n    otherwise the designated 'handler' will be called.\n\n    This middleware class should generally be used to wrap *everything*\n    else up, so that unhandled exceptions anywhere in the stack\n    always result in an appropriate 500 response.\n    \"\"\"\n\n    def __init__(\n        self,\n        app: ASGIApp,\n        handler: ExceptionHandler | None = None,\n        debug: bool = False,\n    ) -> None:\n        self.app = app\n        self.handler = handler\n        self.debug = debug\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        if scope[\"type\"] != \"http\":\n            await self.app(scope, receive, send)\n            return\n\n        response_started = False\n\n        async def _send(message: Message) -> None:\n            nonlocal response_started, send\n\n            if message[\"type\"] == \"http.response.start\":\n                response_started = True\n            await send(message)\n\n        try:\n            await self.app(scope, receive, _send)\n        except Exception as exc:\n            request = Request(scope)\n            if self.debug:\n                # In debug mode, return traceback responses.\n                response = self.debug_response(request, exc)\n            elif self.handler is None:\n                # Use our default 500 error handler.\n                response = self.error_response(request, exc)\n            else:\n                # Use an installed 500 error handler.\n                if is_async_callable(self.handler):\n                    response = await self.handler(request, exc)\n                else:\n                    response = await run_in_threadpool(self.handler, request, exc)\n\n            if not response_started:\n                await response(scope, receive, send)\n\n            # We always continue to raise the exception.\n            # This allows servers to log the error, or allows test clients\n            # to optionally raise the error within the test case.\n            raise exc\n\n    def format_line(self, index: int, line: str, frame_lineno: int, frame_index: int) -> str:\n        values = {\n            # HTML escape - line could contain < or >\n            \"line\": html.escape(line).replace(\" \", \"&nbsp\"),\n            \"lineno\": (frame_lineno - frame_index) + index,\n        }\n\n        if index != frame_index:\n            return LINE.format(**values)\n        return CENTER_LINE.format(**values)\n\n    def generate_frame_html(self, frame: inspect.FrameInfo, is_collapsed: bool) -> str:\n        code_context = \"\".join(\n            self.format_line(\n                index,\n                line,\n                frame.lineno,\n                frame.index,  # type: ignore[arg-type]\n            )\n            for index, line in enumerate(frame.code_context or [])\n        )\n\n        values = {\n            # HTML escape - filename could contain < or >, especially if it's a virtual\n            # file e.g. <stdin> in the REPL\n            \"frame_filename\": html.escape(frame.filename),\n            \"frame_lineno\": frame.lineno,\n            # HTML escape - if you try very hard it's possible to name a function with <\n            # or >\n            \"frame_name\": html.escape(frame.function),\n            \"code_context\": code_context,\n            \"collapsed\": \"collapsed\" if is_collapsed else \"\",\n            \"collapse_button\": \"+\" if is_collapsed else \"&#8210;\",\n        }\n        return FRAME_TEMPLATE.format(**values)\n\n    def generate_html(self, exc: Exception, limit: int = 7) -> str:\n        traceback_obj = traceback.TracebackException.from_exception(exc, capture_locals=True)\n\n        exc_html = \"\"\n        is_collapsed = False\n        exc_traceback = exc.__traceback__\n        if exc_traceback is not None:\n            frames = inspect.getinnerframes(exc_traceback, limit)\n            for frame in reversed(frames):\n                exc_html += self.generate_frame_html(frame, is_collapsed)\n                is_collapsed = True\n\n        if sys.version_info >= (3, 13):  # pragma: no cover\n            exc_type_str = traceback_obj.exc_type_str\n        else:  # pragma: no cover\n            exc_type_str = traceback_obj.exc_type.__name__\n\n        # escape error class and text\n        error = f\"{html.escape(exc_type_str)}: {html.escape(str(traceback_obj))}\"\n\n        return TEMPLATE.format(styles=STYLES, js=JS, error=error, exc_html=exc_html)\n\n    def generate_plain_text(self, exc: Exception) -> str:\n        return \"\".join(traceback.format_exception(type(exc), exc, exc.__traceback__))\n\n    def debug_response(self, request: Request, exc: Exception) -> Response:\n        accept = request.headers.get(\"accept\", \"\")\n\n        if \"text/html\" in accept:\n            content = self.generate_html(exc)\n            return HTMLResponse(content, status_code=500)\n        content = self.generate_plain_text(exc)\n        return PlainTextResponse(content, status_code=500)\n\n    def error_response(self, request: Request, exc: Exception) -> Response:\n        return PlainTextResponse(\"Internal Server Error\", status_code=500)\n", 259], "/usr/local/lib/python3.11/site-packages/starlette/applications.py": ["from __future__ import annotations\n\nimport sys\nimport warnings\nfrom collections.abc import Awaitable, Mapping, Sequence\nfrom typing import Any, Callable, TypeVar\n\nif sys.version_info >= (3, 10):  # pragma: no cover\n    from typing import ParamSpec\nelse:  # pragma: no cover\n    from typing_extensions import ParamSpec\n\nfrom starlette.datastructures import State, URLPath\nfrom starlette.middleware import Middleware, _MiddlewareFactory\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.middleware.errors import ServerErrorMiddleware\nfrom starlette.middleware.exceptions import ExceptionMiddleware\nfrom starlette.requests import Request\nfrom starlette.responses import Response\nfrom starlette.routing import BaseRoute, Router\nfrom starlette.types import ASGIApp, ExceptionHandler, Lifespan, Receive, Scope, Send\nfrom starlette.websockets import WebSocket\n\nAppType = TypeVar(\"AppType\", bound=\"Starlette\")\nP = ParamSpec(\"P\")\n\n\nclass Starlette:\n    \"\"\"Creates an Starlette application.\"\"\"\n\n    def __init__(\n        self: AppType,\n        debug: bool = False,\n        routes: Sequence[BaseRoute] | None = None,\n        middleware: Sequence[Middleware] | None = None,\n        exception_handlers: Mapping[Any, ExceptionHandler] | None = None,\n        on_startup: Sequence[Callable[[], Any]] | None = None,\n        on_shutdown: Sequence[Callable[[], Any]] | None = None,\n        lifespan: Lifespan[AppType] | None = None,\n    ) -> None:\n        \"\"\"Initializes the application.\n\n        Parameters:\n            debug: Boolean indicating if debug tracebacks should be returned on errors.\n            routes: A list of routes to serve incoming HTTP and WebSocket requests.\n            middleware: A list of middleware to run for every request. A starlette\n                application will always automatically include two middleware classes.\n                `ServerErrorMiddleware` is added as the very outermost middleware, to handle\n                any uncaught errors occurring anywhere in the entire stack.\n                `ExceptionMiddleware` is added as the very innermost middleware, to deal\n                with handled exception cases occurring in the routing or endpoints.\n            exception_handlers: A mapping of either integer status codes,\n                or exception class types onto callables which handle the exceptions.\n                Exception handler callables should be of the form\n                `handler(request, exc) -> response` and may be either standard functions, or\n                async functions.\n            on_startup: A list of callables to run on application startup.\n                Startup handler callables do not take any arguments, and may be either\n                standard functions, or async functions.\n            on_shutdown: A list of callables to run on application shutdown.\n                Shutdown handler callables do not take any arguments, and may be either\n                standard functions, or async functions.\n            lifespan: A lifespan context function, which can be used to perform\n                startup and shutdown tasks. This is a newer style that replaces the\n                `on_startup` and `on_shutdown` handlers. Use one or the other, not both.\n        \"\"\"\n        # The lifespan context function is a newer style that replaces\n        # on_startup / on_shutdown handlers. Use one or the other, not both.\n        assert lifespan is None or (on_startup is None and on_shutdown is None), (\n            \"Use either 'lifespan' or 'on_startup'/'on_shutdown', not both.\"\n        )\n\n        self.debug = debug\n        self.state = State()\n        self.router = Router(routes, on_startup=on_startup, on_shutdown=on_shutdown, lifespan=lifespan)\n        self.exception_handlers = {} if exception_handlers is None else dict(exception_handlers)\n        self.user_middleware = [] if middleware is None else list(middleware)\n        self.middleware_stack: ASGIApp | None = None\n\n    def build_middleware_stack(self) -> ASGIApp:\n        debug = self.debug\n        error_handler = None\n        exception_handlers: dict[Any, ExceptionHandler] = {}\n\n        for key, value in self.exception_handlers.items():\n            if key in (500, Exception):\n                error_handler = value\n            else:\n                exception_handlers[key] = value\n\n        middleware = (\n            [Middleware(ServerErrorMiddleware, handler=error_handler, debug=debug)]\n            + self.user_middleware\n            + [Middleware(ExceptionMiddleware, handlers=exception_handlers, debug=debug)]\n        )\n\n        app = self.router\n        for cls, args, kwargs in reversed(middleware):\n            app = cls(app, *args, **kwargs)\n        return app\n\n    @property\n    def routes(self) -> list[BaseRoute]:\n        return self.router.routes\n\n    def url_path_for(self, name: str, /, **path_params: Any) -> URLPath:\n        return self.router.url_path_for(name, **path_params)\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        scope[\"app\"] = self\n        if self.middleware_stack is None:\n            self.middleware_stack = self.build_middleware_stack()\n        await self.middleware_stack(scope, receive, send)\n\n    def on_event(self, event_type: str) -> Callable:  # type: ignore[type-arg]\n        return self.router.on_event(event_type)  # pragma: no cover\n\n    def mount(self, path: str, app: ASGIApp, name: str | None = None) -> None:\n        self.router.mount(path, app=app, name=name)  # pragma: no cover\n\n    def host(self, host: str, app: ASGIApp, name: str | None = None) -> None:\n        self.router.host(host, app=app, name=name)  # pragma: no cover\n\n    def add_middleware(\n        self,\n        middleware_class: _MiddlewareFactory[P],\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -> None:\n        if self.middleware_stack is not None:  # pragma: no cover\n            raise RuntimeError(\"Cannot add middleware after an application has started\")\n        self.user_middleware.insert(0, Middleware(middleware_class, *args, **kwargs))\n\n    def add_exception_handler(\n        self,\n        exc_class_or_status_code: int | type[Exception],\n        handler: ExceptionHandler,\n    ) -> None:  # pragma: no cover\n        self.exception_handlers[exc_class_or_status_code] = handler\n\n    def add_event_handler(\n        self,\n        event_type: str,\n        func: Callable,  # type: ignore[type-arg]\n    ) -> None:  # pragma: no cover\n        self.router.add_event_handler(event_type, func)\n\n    def add_route(\n        self,\n        path: str,\n        route: Callable[[Request], Awaitable[Response] | Response],\n        methods: list[str] | None = None,\n        name: str | None = None,\n        include_in_schema: bool = True,\n    ) -> None:  # pragma: no cover\n        self.router.add_route(path, route, methods=methods, name=name, include_in_schema=include_in_schema)\n\n    def add_websocket_route(\n        self,\n        path: str,\n        route: Callable[[WebSocket], Awaitable[None]],\n        name: str | None = None,\n    ) -> None:  # pragma: no cover\n        self.router.add_websocket_route(path, route, name=name)\n\n    def exception_handler(self, exc_class_or_status_code: int | type[Exception]) -> Callable:  # type: ignore[type-arg]\n        warnings.warn(\n            \"The `exception_handler` decorator is deprecated, and will be removed in version 1.0.0. \"\n            \"Refer to https://www.starlette.io/exceptions/ for the recommended approach.\",\n            DeprecationWarning,\n        )\n\n        def decorator(func: Callable) -> Callable:  # type: ignore[type-arg]\n            self.add_exception_handler(exc_class_or_status_code, func)\n            return func\n\n        return decorator\n\n    def route(\n        self,\n        path: str,\n        methods: list[str] | None = None,\n        name: str | None = None,\n        include_in_schema: bool = True,\n    ) -> Callable:  # type: ignore[type-arg]\n        \"\"\"\n        We no longer document this decorator style API, and its usage is discouraged.\n        Instead you should use the following approach:\n\n        >>> routes = [Route(path, endpoint=...), ...]\n        >>> app = Starlette(routes=routes)\n        \"\"\"\n        warnings.warn(\n            \"The `route` decorator is deprecated, and will be removed in version 1.0.0. \"\n            \"Refer to https://www.starlette.io/routing/ for the recommended approach.\",\n            DeprecationWarning,\n        )\n\n        def decorator(func: Callable) -> Callable:  # type: ignore[type-arg]\n            self.router.add_route(\n                path,\n                func,\n                methods=methods,\n                name=name,\n                include_in_schema=include_in_schema,\n            )\n            return func\n\n        return decorator\n\n    def websocket_route(self, path: str, name: str | None = None) -> Callable:  # type: ignore[type-arg]\n        \"\"\"\n        We no longer document this decorator style API, and its usage is discouraged.\n        Instead you should use the following approach:\n\n        >>> routes = [WebSocketRoute(path, endpoint=...), ...]\n        >>> app = Starlette(routes=routes)\n        \"\"\"\n        warnings.warn(\n            \"The `websocket_route` decorator is deprecated, and will be removed in version 1.0.0. \"\n            \"Refer to https://www.starlette.io/routing/#websocket-routing for the recommended approach.\",\n            DeprecationWarning,\n        )\n\n        def decorator(func: Callable) -> Callable:  # type: ignore[type-arg]\n            self.router.add_websocket_route(path, func, name=name)\n            return func\n\n        return decorator\n\n    def middleware(self, middleware_type: str) -> Callable:  # type: ignore[type-arg]\n        \"\"\"\n        We no longer document this decorator style API, and its usage is discouraged.\n        Instead you should use the following approach:\n\n        >>> middleware = [Middleware(...), ...]\n        >>> app = Starlette(middleware=middleware)\n        \"\"\"\n        warnings.warn(\n            \"The `middleware` decorator is deprecated, and will be removed in version 1.0.0. \"\n            \"Refer to https://www.starlette.io/middleware/#using-middleware for recommended approach.\",\n            DeprecationWarning,\n        )\n        assert middleware_type == \"http\", 'Currently only middleware(\"http\") is supported.'\n\n        def decorator(func: Callable) -> Callable:  # type: ignore[type-arg]\n            self.add_middleware(BaseHTTPMiddleware, dispatch=func)\n            return func\n\n        return decorator\n", 250], "/usr/local/lib/python3.11/site-packages/fastapi/applications.py": ["from enum import Enum\nfrom typing import (\n    Any,\n    Awaitable,\n    Callable,\n    Coroutine,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Type,\n    TypeVar,\n    Union,\n)\n\nfrom fastapi import routing\nfrom fastapi.datastructures import Default, DefaultPlaceholder\nfrom fastapi.exception_handlers import (\n    http_exception_handler,\n    request_validation_exception_handler,\n    websocket_request_validation_exception_handler,\n)\nfrom fastapi.exceptions import RequestValidationError, WebSocketRequestValidationError\nfrom fastapi.logger import logger\nfrom fastapi.middleware.asyncexitstack import AsyncExitStackMiddleware\nfrom fastapi.openapi.docs import (\n    get_redoc_html,\n    get_swagger_ui_html,\n    get_swagger_ui_oauth2_redirect_html,\n)\nfrom fastapi.openapi.utils import get_openapi\nfrom fastapi.params import Depends\nfrom fastapi.types import DecoratedCallable, IncEx\nfrom fastapi.utils import generate_unique_id\nfrom starlette.applications import Starlette\nfrom starlette.datastructures import State\nfrom starlette.exceptions import HTTPException\nfrom starlette.middleware import Middleware\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.middleware.errors import ServerErrorMiddleware\nfrom starlette.middleware.exceptions import ExceptionMiddleware\nfrom starlette.requests import Request\nfrom starlette.responses import HTMLResponse, JSONResponse, Response\nfrom starlette.routing import BaseRoute\nfrom starlette.types import ASGIApp, ExceptionHandler, Lifespan, Receive, Scope, Send\nfrom typing_extensions import Annotated, Doc, deprecated\n\nAppType = TypeVar(\"AppType\", bound=\"FastAPI\")\n\n\nclass FastAPI(Starlette):\n    \"\"\"\n    `FastAPI` app class, the main entrypoint to use FastAPI.\n\n    Read more in the\n    [FastAPI docs for First Steps](https://fastapi.tiangolo.com/tutorial/first-steps/).\n\n    ## Example\n\n    ```python\n    from fastapi import FastAPI\n\n    app = FastAPI()\n    ```\n    \"\"\"\n\n    def __init__(\n        self: AppType,\n        *,\n        debug: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Boolean indicating if debug tracebacks should be returned on server\n                errors.\n\n                Read more in the\n                [Starlette docs for Applications](https://www.starlette.io/applications/#instantiating-the-application).\n                \"\"\"\n            ),\n        ] = False,\n        routes: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                **Note**: you probably shouldn't use this parameter, it is inherited\n                from Starlette and supported for compatibility.\n\n                ---\n\n                A list of routes to serve incoming HTTP and WebSocket requests.\n                \"\"\"\n            ),\n            deprecated(\n                \"\"\"\n                You normally wouldn't use this parameter with FastAPI, it is inherited\n                from Starlette and supported for compatibility.\n\n                In FastAPI, you normally would use the *path operation methods*,\n                like `app.get()`, `app.post()`, etc.\n                \"\"\"\n            ),\n        ] = None,\n        title: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The title of the API.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more in the\n                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#metadata-for-api).\n\n                **Example**\n\n                ```python\n                from fastapi import FastAPI\n\n                app = FastAPI(title=\"ChimichangApp\")\n                ```\n                \"\"\"\n            ),\n        ] = \"FastAPI\",\n        summary: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A short summary of the API.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more in the\n                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#metadata-for-api).\n\n                **Example**\n\n                ```python\n                from fastapi import FastAPI\n\n                app = FastAPI(summary=\"Deadpond's favorite app. Nuff said.\")\n                ```\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            str,\n            Doc(\n                '''\n                A description of the API. Supports Markdown (using\n                [CommonMark syntax](https://commonmark.org/)).\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more in the\n                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#metadata-for-api).\n\n                **Example**\n\n                ```python\n                from fastapi import FastAPI\n\n                app = FastAPI(\n                    description=\"\"\"\n                                ChimichangApp API helps you do awesome stuff. \ud83d\ude80\n\n                                ## Items\n\n                                You can **read items**.\n\n                                ## Users\n\n                                You will be able to:\n\n                                * **Create users** (_not implemented_).\n                                * **Read users** (_not implemented_).\n\n                                \"\"\"\n                )\n                ```\n                '''\n            ),\n        ] = \"\",\n        version: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The version of the API.\n\n                **Note** This is the version of your application, not the version of\n                the OpenAPI specification nor the version of FastAPI being used.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more in the\n                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#metadata-for-api).\n\n                **Example**\n\n                ```python\n                from fastapi import FastAPI\n\n                app = FastAPI(version=\"0.0.1\")\n                ```\n                \"\"\"\n            ),\n        ] = \"0.1.0\",\n        openapi_url: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                The URL where the OpenAPI schema will be served from.\n\n                If you set it to `None`, no OpenAPI schema will be served publicly, and\n                the default automatic endpoints `/docs` and `/redoc` will also be\n                disabled.\n\n                Read more in the\n                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#openapi-url).\n\n                **Example**\n\n                ```python\n                from fastapi import FastAPI\n\n                app = FastAPI(openapi_url=\"/api/v1/openapi.json\")\n                ```\n                \"\"\"\n            ),\n        ] = \"/openapi.json\",\n        openapi_tags: Annotated[\n            Optional[List[Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                A list of tags used by OpenAPI, these are the same `tags` you can set\n                in the *path operations*, like:\n\n                * `@app.get(\"/users/\", tags=[\"users\"])`\n                * `@app.get(\"/items/\", tags=[\"items\"])`\n\n                The order of the tags can be used to specify the order shown in\n                tools like Swagger UI, used in the automatic path `/docs`.\n\n                It's not required to specify all the tags used.\n\n                The tags that are not declared MAY be organized randomly or based\n                on the tools' logic. Each tag name in the list MUST be unique.\n\n                The value of each item is a `dict` containing:\n\n                * `name`: The name of the tag.\n                * `description`: A short description of the tag.\n                    [CommonMark syntax](https://commonmark.org/) MAY be used for rich\n                    text representation.\n                * `externalDocs`: Additional external documentation for this tag. If\n                    provided, it would contain a `dict` with:\n                    * `description`: A short description of the target documentation.\n                        [CommonMark syntax](https://commonmark.org/) MAY be used for\n                        rich text representation.\n                    * `url`: The URL for the target documentation. Value MUST be in\n                        the form of a URL.\n\n                Read more in the\n                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#metadata-for-tags).\n\n                **Example**\n\n                ```python\n                from fastapi import FastAPI\n\n                tags_metadata = [\n                    {\n                        \"name\": \"users\",\n                        \"description\": \"Operations with users. The **login** logic is also here.\",\n                    },\n                    {\n                        \"name\": \"items\",\n                        \"description\": \"Manage items. So _fancy_ they have their own docs.\",\n                        \"externalDocs\": {\n                            \"description\": \"Items external docs\",\n                            \"url\": \"https://fastapi.tiangolo.com/\",\n                        },\n                    },\n                ]\n\n                app = FastAPI(openapi_tags=tags_metadata)\n                ```\n                \"\"\"\n            ),\n        ] = None,\n        servers: Annotated[\n            Optional[List[Dict[str, Union[str, Any]]]],\n            Doc(\n                \"\"\"\n                A `list` of `dict`s with connectivity information to a target server.\n\n                You would use it, for example, if your application is served from\n                different domains and you want to use the same Swagger UI in the\n                browser to interact with each of them (instead of having multiple\n                browser tabs open). Or if you want to leave fixed the possible URLs.\n\n                If the servers `list` is not provided, or is an empty `list`, the\n                default value would be a `dict` with a `url` value of `/`.\n\n                Each item in the `list` is a `dict` containing:\n\n                * `url`: A URL to the target host. This URL supports Server Variables\n                and MAY be relative, to indicate that the host location is relative\n                to the location where the OpenAPI document is being served. Variable\n                substitutions will be made when a variable is named in `{`brackets`}`.\n                * `description`: An optional string describing the host designated by\n                the URL. [CommonMark syntax](https://commonmark.org/) MAY be used for\n                rich text representation.\n                * `variables`: A `dict` between a variable name and its value. The value\n                    is used for substitution in the server's URL template.\n\n                Read more in the\n                [FastAPI docs for Behind a Proxy](https://fastapi.tiangolo.com/advanced/behind-a-proxy/#additional-servers).\n\n                **Example**\n\n                ```python\n                from fastapi import FastAPI\n\n                app = FastAPI(\n                    servers=[\n                        {\"url\": \"https://stag.example.com\", \"description\": \"Staging environment\"},\n                        {\"url\": \"https://prod.example.com\", \"description\": \"Production environment\"},\n                    ]\n                )\n                ```\n                \"\"\"\n            ),\n        ] = None,\n        dependencies: Annotated[\n            Optional[Sequence[Depends]],\n            Doc(\n                \"\"\"\n                A list of global dependencies, they will be applied to each\n                *path operation*, including in sub-routers.\n\n                Read more about it in the\n                [FastAPI docs for Global Dependencies](https://fastapi.tiangolo.com/tutorial/dependencies/global-dependencies/).\n\n                **Example**\n\n                ```python\n                from fastapi import Depends, FastAPI\n\n                from .dependencies import func_dep_1, func_dep_2\n\n                app = FastAPI(dependencies=[Depends(func_dep_1), Depends(func_dep_2)])\n                ```\n                \"\"\"\n            ),\n        ] = None,\n        default_response_class: Annotated[\n            Type[Response],\n            Doc(\n                \"\"\"\n                The default response class to be used.\n\n                Read more in the\n                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#default-response-class).\n\n                **Example**\n\n                ```python\n                from fastapi import FastAPI\n                from fastapi.responses import ORJSONResponse\n\n                app = FastAPI(default_response_class=ORJSONResponse)\n                ```\n                \"\"\"\n            ),\n        ] = Default(JSONResponse),\n        redirect_slashes: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Whether to detect and redirect slashes in URLs when the client doesn't\n                use the same format.\n\n                **Example**\n\n                ```python\n                from fastapi import FastAPI\n\n                app = FastAPI(redirect_slashes=True)  # the default\n\n                @app.get(\"/items/\")\n                async def read_items():\n                    return [{\"item_id\": \"Foo\"}]\n                ```\n\n                With this app, if a client goes to `/items` (without a trailing slash),\n                they will be automatically redirected with an HTTP status code of 307\n                to `/items/`.\n                \"\"\"\n            ),\n        ] = True,\n        docs_url: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                The path to the automatic interactive API documentation.\n                It is handled in the browser by Swagger UI.\n\n                The default URL is `/docs`. You can disable it by setting it to `None`.\n\n                If `openapi_url` is set to `None`, this will be automatically disabled.\n\n                Read more in the\n                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#docs-urls).\n\n                **Example**\n\n                ```python\n                from fastapi import FastAPI\n\n                app = FastAPI(docs_url=\"/documentation\", redoc_url=None)\n                ```\n                \"\"\"\n            ),\n        ] = \"/docs\",\n        redoc_url: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                The path to the alternative automatic interactive API documentation\n                provided by ReDoc.\n\n                The default URL is `/redoc`. You can disable it by setting it to `None`.\n\n                If `openapi_url` is set to `None`, this will be automatically disabled.\n\n                Read more in the\n                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#docs-urls).\n\n                **Example**\n\n                ```python\n                from fastapi import FastAPI\n\n                app = FastAPI(docs_url=\"/documentation\", redoc_url=\"redocumentation\")\n                ```\n                \"\"\"\n            ),\n        ] = \"/redoc\",\n        swagger_ui_oauth2_redirect_url: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                The OAuth2 redirect endpoint for the Swagger UI.\n\n                By default it is `/docs/oauth2-redirect`.\n\n                This is only used if you use OAuth2 (with the \"Authorize\" button)\n                with Swagger UI.\n                \"\"\"\n            ),\n        ] = \"/docs/oauth2-redirect\",\n        swagger_ui_init_oauth: Annotated[\n            Optional[Dict[str, Any]],\n            Doc(\n                \"\"\"\n                OAuth2 configuration for the Swagger UI, by default shown at `/docs`.\n\n                Read more about the available configuration options in the\n                [Swagger UI docs](https://swagger.io/docs/open-source-tools/swagger-ui/usage/oauth2/).\n                \"\"\"\n            ),\n        ] = None,\n        middleware: Annotated[\n            Optional[Sequence[Middleware]],\n            Doc(\n                \"\"\"\n                List of middleware to be added when creating the application.\n\n                In FastAPI you would normally do this with `app.add_middleware()`\n                instead.\n\n                Read more in the\n                [FastAPI docs for Middleware](https://fastapi.tiangolo.com/tutorial/middleware/).\n                \"\"\"\n            ),\n        ] = None,\n        exception_handlers: Annotated[\n            Optional[\n                Dict[\n                    Union[int, Type[Exception]],\n                    Callable[[Request, Any], Coroutine[Any, Any, Response]],\n                ]\n            ],\n            Doc(\n                \"\"\"\n                A dictionary with handlers for exceptions.\n\n                In FastAPI, you would normally use the decorator\n                `@app.exception_handler()`.\n\n                Read more in the\n                [FastAPI docs for Handling Errors](https://fastapi.tiangolo.com/tutorial/handling-errors/).\n                \"\"\"\n            ),\n        ] = None,\n        on_startup: Annotated[\n            Optional[Sequence[Callable[[], Any]]],\n            Doc(\n                \"\"\"\n                A list of startup event handler functions.\n\n                You should instead use the `lifespan` handlers.\n\n                Read more in the [FastAPI docs for `lifespan`](https://fastapi.tiangolo.com/advanced/events/).\n                \"\"\"\n            ),\n        ] = None,\n        on_shutdown: Annotated[\n            Optional[Sequence[Callable[[], Any]]],\n            Doc(\n                \"\"\"\n                A list of shutdown event handler functions.\n\n                You should instead use the `lifespan` handlers.\n\n                Read more in the\n                [FastAPI docs for `lifespan`](https://fastapi.tiangolo.com/advanced/events/).\n                \"\"\"\n            ),\n        ] = None,\n        lifespan: Annotated[\n            Optional[Lifespan[AppType]],\n            Doc(\n                \"\"\"\n                A `Lifespan` context manager handler. This replaces `startup` and\n                `shutdown` functions with a single context manager.\n\n                Read more in the\n                [FastAPI docs for `lifespan`](https://fastapi.tiangolo.com/advanced/events/).\n                \"\"\"\n            ),\n        ] = None,\n        terms_of_service: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A URL to the Terms of Service for your API.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more at the\n                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#metadata-for-api).\n\n                **Example**\n\n                ```python\n                app = FastAPI(terms_of_service=\"http://example.com/terms/\")\n                ```\n                \"\"\"\n            ),\n        ] = None,\n        contact: Annotated[\n            Optional[Dict[str, Union[str, Any]]],\n            Doc(\n                \"\"\"\n                A dictionary with the contact information for the exposed API.\n\n                It can contain several fields.\n\n                * `name`: (`str`) The name of the contact person/organization.\n                * `url`: (`str`) A URL pointing to the contact information. MUST be in\n                    the format of a URL.\n                * `email`: (`str`) The email address of the contact person/organization.\n                    MUST be in the format of an email address.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more at the\n                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#metadata-for-api).\n\n                **Example**\n\n                ```python\n                app = FastAPI(\n                    contact={\n                        \"name\": \"Deadpoolio the Amazing\",\n                        \"url\": \"http://x-force.example.com/contact/\",\n                        \"email\": \"dp@x-force.example.com\",\n                    }\n                )\n                ```\n                \"\"\"\n            ),\n        ] = None,\n        license_info: Annotated[\n            Optional[Dict[str, Union[str, Any]]],\n            Doc(\n                \"\"\"\n                A dictionary with the license information for the exposed API.\n\n                It can contain several fields.\n\n                * `name`: (`str`) **REQUIRED** (if a `license_info` is set). The\n                    license name used for the API.\n                * `identifier`: (`str`) An [SPDX](https://spdx.dev/) license expression\n                    for the API. The `identifier` field is mutually exclusive of the `url`\n                    field. Available since OpenAPI 3.1.0, FastAPI 0.99.0.\n                * `url`: (`str`) A URL to the license used for the API. This MUST be\n                    the format of a URL.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more at the\n                [FastAPI docs for Metadata and Docs URLs](https://fastapi.tiangolo.com/tutorial/metadata/#metadata-for-api).\n\n                **Example**\n\n                ```python\n                app = FastAPI(\n                    license_info={\n                        \"name\": \"Apache 2.0\",\n                        \"url\": \"https://www.apache.org/licenses/LICENSE-2.0.html\",\n                    }\n                )\n                ```\n                \"\"\"\n            ),\n        ] = None,\n        openapi_prefix: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                A URL prefix for the OpenAPI URL.\n                \"\"\"\n            ),\n            deprecated(\n                \"\"\"\n                \"openapi_prefix\" has been deprecated in favor of \"root_path\", which\n                follows more closely the ASGI standard, is simpler, and more\n                automatic.\n                \"\"\"\n            ),\n        ] = \"\",\n        root_path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                A path prefix handled by a proxy that is not seen by the application\n                but is seen by external clients, which affects things like Swagger UI.\n\n                Read more about it at the\n                [FastAPI docs for Behind a Proxy](https://fastapi.tiangolo.com/advanced/behind-a-proxy/).\n\n                **Example**\n\n                ```python\n                from fastapi import FastAPI\n\n                app = FastAPI(root_path=\"/api/v1\")\n                ```\n                \"\"\"\n            ),\n        ] = \"\",\n        root_path_in_servers: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                To disable automatically generating the URLs in the `servers` field\n                in the autogenerated OpenAPI using the `root_path`.\n\n                Read more about it in the\n                [FastAPI docs for Behind a Proxy](https://fastapi.tiangolo.com/advanced/behind-a-proxy/#disable-automatic-server-from-root_path).\n\n                **Example**\n\n                ```python\n                from fastapi import FastAPI\n\n                app = FastAPI(root_path_in_servers=False)\n                ```\n                \"\"\"\n            ),\n        ] = True,\n        responses: Annotated[\n            Optional[Dict[Union[int, str], Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                Additional responses to be shown in OpenAPI.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Additional Responses in OpenAPI](https://fastapi.tiangolo.com/advanced/additional-responses/).\n\n                And in the\n                [FastAPI docs for Bigger Applications](https://fastapi.tiangolo.com/tutorial/bigger-applications/#include-an-apirouter-with-a-custom-prefix-tags-responses-and-dependencies).\n                \"\"\"\n            ),\n        ] = None,\n        callbacks: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                OpenAPI callbacks that should apply to all *path operations*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).\n                \"\"\"\n            ),\n        ] = None,\n        webhooks: Annotated[\n            Optional[routing.APIRouter],\n            Doc(\n                \"\"\"\n                Add OpenAPI webhooks. This is similar to `callbacks` but it doesn't\n                depend on specific *path operations*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                **Note**: This is available since OpenAPI 3.1.0, FastAPI 0.99.0.\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Webhooks](https://fastapi.tiangolo.com/advanced/openapi-webhooks/).\n                \"\"\"\n            ),\n        ] = None,\n        deprecated: Annotated[\n            Optional[bool],\n            Doc(\n                \"\"\"\n                Mark all *path operations* as deprecated. You probably don't need it,\n                but it's available.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        include_in_schema: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                To include (or not) all the *path operations* in the generated OpenAPI.\n                You probably don't need it, but it's available.\n\n                This affects the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).\n                \"\"\"\n            ),\n        ] = True,\n        swagger_ui_parameters: Annotated[\n            Optional[Dict[str, Any]],\n            Doc(\n                \"\"\"\n                Parameters to configure Swagger UI, the autogenerated interactive API\n                documentation (by default at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs about how to Configure Swagger UI](https://fastapi.tiangolo.com/how-to/configure-swagger-ui/).\n                \"\"\"\n            ),\n        ] = None,\n        generate_unique_id_function: Annotated[\n            Callable[[routing.APIRoute], str],\n            Doc(\n                \"\"\"\n                Customize the function used to generate unique IDs for the *path\n                operations* shown in the generated OpenAPI.\n\n                This is particularly useful when automatically generating clients or\n                SDKs for your API.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = Default(generate_unique_id),\n        separate_input_output_schemas: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Whether to generate separate OpenAPI schemas for request body and\n                response body when the results would be more precise.\n\n                This is particularly useful when automatically generating clients.\n\n                For example, if you have a model like:\n\n                ```python\n                from pydantic import BaseModel\n\n                class Item(BaseModel):\n                    name: str\n                    tags: list[str] = []\n                ```\n\n                When `Item` is used for input, a request body, `tags` is not required,\n                the client doesn't have to provide it.\n\n                But when using `Item` for output, for a response body, `tags` is always\n                available because it has a default value, even if it's just an empty\n                list. So, the client should be able to always expect it.\n\n                In this case, there would be two different schemas, one for input and\n                another one for output.\n                \"\"\"\n            ),\n        ] = True,\n        openapi_external_docs: Annotated[\n            Optional[Dict[str, Any]],\n            Doc(\n                \"\"\"\n                This field allows you to provide additional external documentation links.\n                If provided, it must be a dictionary containing:\n\n                * `description`: A brief description of the external documentation.\n                * `url`: The URL pointing to the external documentation. The value **MUST**\n                be a valid URL format.\n\n                **Example**:\n\n                ```python\n                from fastapi import FastAPI\n\n                external_docs = {\n                    \"description\": \"Detailed API Reference\",\n                    \"url\": \"https://example.com/api-docs\",\n                }\n\n                app = FastAPI(openapi_external_docs=external_docs)\n                ```\n                \"\"\"\n            ),\n        ] = None,\n        **extra: Annotated[\n            Any,\n            Doc(\n                \"\"\"\n                Extra keyword arguments to be stored in the app, not used by FastAPI\n                anywhere.\n                \"\"\"\n            ),\n        ],\n    ) -> None:\n        self.debug = debug\n        self.title = title\n        self.summary = summary\n        self.description = description\n        self.version = version\n        self.terms_of_service = terms_of_service\n        self.contact = contact\n        self.license_info = license_info\n        self.openapi_url = openapi_url\n        self.openapi_tags = openapi_tags\n        self.root_path_in_servers = root_path_in_servers\n        self.docs_url = docs_url\n        self.redoc_url = redoc_url\n        self.swagger_ui_oauth2_redirect_url = swagger_ui_oauth2_redirect_url\n        self.swagger_ui_init_oauth = swagger_ui_init_oauth\n        self.swagger_ui_parameters = swagger_ui_parameters\n        self.servers = servers or []\n        self.separate_input_output_schemas = separate_input_output_schemas\n        self.openapi_external_docs = openapi_external_docs\n        self.extra = extra\n        self.openapi_version: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The version string of OpenAPI.\n\n                FastAPI will generate OpenAPI version 3.1.0, and will output that as\n                the OpenAPI version. But some tools, even though they might be\n                compatible with OpenAPI 3.1.0, might not recognize it as a valid.\n\n                So you could override this value to trick those tools into using\n                the generated OpenAPI. Have in mind that this is a hack. But if you\n                avoid using features added in OpenAPI 3.1.0, it might work for your\n                use case.\n\n                This is not passed as a parameter to the `FastAPI` class to avoid\n                giving the false idea that FastAPI would generate a different OpenAPI\n                schema. It is only available as an attribute.\n\n                **Example**\n\n                ```python\n                from fastapi import FastAPI\n\n                app = FastAPI()\n\n                app.openapi_version = \"3.0.2\"\n                ```\n                \"\"\"\n            ),\n        ] = \"3.1.0\"\n        self.openapi_schema: Optional[Dict[str, Any]] = None\n        if self.openapi_url:\n            assert self.title, \"A title must be provided for OpenAPI, e.g.: 'My API'\"\n            assert self.version, \"A version must be provided for OpenAPI, e.g.: '2.1.0'\"\n        # TODO: remove when discarding the openapi_prefix parameter\n        if openapi_prefix:\n            logger.warning(\n                '\"openapi_prefix\" has been deprecated in favor of \"root_path\", which '\n                \"follows more closely the ASGI standard, is simpler, and more \"\n                \"automatic. Check the docs at \"\n                \"https://fastapi.tiangolo.com/advanced/sub-applications/\"\n            )\n        self.webhooks: Annotated[\n            routing.APIRouter,\n            Doc(\n                \"\"\"\n                The `app.webhooks` attribute is an `APIRouter` with the *path\n                operations* that will be used just for documentation of webhooks.\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Webhooks](https://fastapi.tiangolo.com/advanced/openapi-webhooks/).\n                \"\"\"\n            ),\n        ] = webhooks or routing.APIRouter()\n        self.root_path = root_path or openapi_prefix\n        self.state: Annotated[\n            State,\n            Doc(\n                \"\"\"\n                A state object for the application. This is the same object for the\n                entire application, it doesn't change from request to request.\n\n                You normally wouldn't use this in FastAPI, for most of the cases you\n                would instead use FastAPI dependencies.\n\n                This is simply inherited from Starlette.\n\n                Read more about it in the\n                [Starlette docs for Applications](https://www.starlette.io/applications/#storing-state-on-the-app-instance).\n                \"\"\"\n            ),\n        ] = State()\n        self.dependency_overrides: Annotated[\n            Dict[Callable[..., Any], Callable[..., Any]],\n            Doc(\n                \"\"\"\n                A dictionary with overrides for the dependencies.\n\n                Each key is the original dependency callable, and the value is the\n                actual dependency that should be called.\n\n                This is for testing, to replace expensive dependencies with testing\n                versions.\n\n                Read more about it in the\n                [FastAPI docs for Testing Dependencies with Overrides](https://fastapi.tiangolo.com/advanced/testing-dependencies/).\n                \"\"\"\n            ),\n        ] = {}\n        self.router: routing.APIRouter = routing.APIRouter(\n            routes=routes,\n            redirect_slashes=redirect_slashes,\n            dependency_overrides_provider=self,\n            on_startup=on_startup,\n            on_shutdown=on_shutdown,\n            lifespan=lifespan,\n            default_response_class=default_response_class,\n            dependencies=dependencies,\n            callbacks=callbacks,\n            deprecated=deprecated,\n            include_in_schema=include_in_schema,\n            responses=responses,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n        self.exception_handlers: Dict[\n            Any, Callable[[Request, Any], Union[Response, Awaitable[Response]]]\n        ] = {} if exception_handlers is None else dict(exception_handlers)\n        self.exception_handlers.setdefault(HTTPException, http_exception_handler)\n        self.exception_handlers.setdefault(\n            RequestValidationError, request_validation_exception_handler\n        )\n        self.exception_handlers.setdefault(\n            WebSocketRequestValidationError,\n            # Starlette still has incorrect type specification for the handlers\n            websocket_request_validation_exception_handler,  # type: ignore\n        )\n\n        self.user_middleware: List[Middleware] = (\n            [] if middleware is None else list(middleware)\n        )\n        self.middleware_stack: Union[ASGIApp, None] = None\n        self.setup()\n\n    def build_middleware_stack(self) -> ASGIApp:\n        # Duplicate/override from Starlette to add AsyncExitStackMiddleware\n        # inside of ExceptionMiddleware, inside of custom user middlewares\n        debug = self.debug\n        error_handler = None\n        exception_handlers: dict[Any, ExceptionHandler] = {}\n\n        for key, value in self.exception_handlers.items():\n            if key in (500, Exception):\n                error_handler = value\n            else:\n                exception_handlers[key] = value\n\n        middleware = (\n            [Middleware(ServerErrorMiddleware, handler=error_handler, debug=debug)]\n            + self.user_middleware\n            + [\n                Middleware(\n                    ExceptionMiddleware, handlers=exception_handlers, debug=debug\n                ),\n                # Add FastAPI-specific AsyncExitStackMiddleware for closing files.\n                # Before this was also used for closing dependencies with yield but\n                # those now have their own AsyncExitStack, to properly support\n                # streaming responses while keeping compatibility with the previous\n                # versions (as of writing 0.117.1) that allowed doing\n                # except HTTPException inside a dependency with yield.\n                # This needs to happen after user middlewares because those create a\n                # new contextvars context copy by using a new AnyIO task group.\n                # This AsyncExitStack preserves the context for contextvars, not\n                # strictly necessary for closing files but it was one of the original\n                # intentions.\n                # If the AsyncExitStack lived outside of the custom middlewares and\n                # contextvars were set, for example in a dependency with 'yield'\n                # in that internal contextvars context, the values would not be\n                # available in the outer context of the AsyncExitStack.\n                # By placing the middleware and the AsyncExitStack here, inside all\n                # user middlewares, the same context is used.\n                # This is currently not needed, only for closing files, but used to be\n                # important when dependencies with yield were closed here.\n                Middleware(AsyncExitStackMiddleware),\n            ]\n        )\n\n        app = self.router\n        for cls, args, kwargs in reversed(middleware):\n            app = cls(app, *args, **kwargs)\n        return app\n\n    def openapi(self) -> Dict[str, Any]:\n        \"\"\"\n        Generate the OpenAPI schema of the application. This is called by FastAPI\n        internally.\n\n        The first time it is called it stores the result in the attribute\n        `app.openapi_schema`, and next times it is called, it just returns that same\n        result. To avoid the cost of generating the schema every time.\n\n        If you need to modify the generated OpenAPI schema, you could modify it.\n\n        Read more in the\n        [FastAPI docs for OpenAPI](https://fastapi.tiangolo.com/how-to/extending-openapi/).\n        \"\"\"\n        if not self.openapi_schema:\n            self.openapi_schema = get_openapi(\n                title=self.title,\n                version=self.version,\n                openapi_version=self.openapi_version,\n                summary=self.summary,\n                description=self.description,\n                terms_of_service=self.terms_of_service,\n                contact=self.contact,\n                license_info=self.license_info,\n                routes=self.routes,\n                webhooks=self.webhooks.routes,\n                tags=self.openapi_tags,\n                servers=self.servers,\n                separate_input_output_schemas=self.separate_input_output_schemas,\n                external_docs=self.openapi_external_docs,\n            )\n        return self.openapi_schema\n\n    def setup(self) -> None:\n        if self.openapi_url:\n            urls = (server_data.get(\"url\") for server_data in self.servers)\n            server_urls = {url for url in urls if url}\n\n            async def openapi(req: Request) -> JSONResponse:\n                root_path = req.scope.get(\"root_path\", \"\").rstrip(\"/\")\n                if root_path not in server_urls:\n                    if root_path and self.root_path_in_servers:\n                        self.servers.insert(0, {\"url\": root_path})\n                        server_urls.add(root_path)\n                return JSONResponse(self.openapi())\n\n            self.add_route(self.openapi_url, openapi, include_in_schema=False)\n        if self.openapi_url and self.docs_url:\n\n            async def swagger_ui_html(req: Request) -> HTMLResponse:\n                root_path = req.scope.get(\"root_path\", \"\").rstrip(\"/\")\n                openapi_url = root_path + self.openapi_url\n                oauth2_redirect_url = self.swagger_ui_oauth2_redirect_url\n                if oauth2_redirect_url:\n                    oauth2_redirect_url = root_path + oauth2_redirect_url\n                return get_swagger_ui_html(\n                    openapi_url=openapi_url,\n                    title=f\"{self.title} - Swagger UI\",\n                    oauth2_redirect_url=oauth2_redirect_url,\n                    init_oauth=self.swagger_ui_init_oauth,\n                    swagger_ui_parameters=self.swagger_ui_parameters,\n                )\n\n            self.add_route(self.docs_url, swagger_ui_html, include_in_schema=False)\n\n            if self.swagger_ui_oauth2_redirect_url:\n\n                async def swagger_ui_redirect(req: Request) -> HTMLResponse:\n                    return get_swagger_ui_oauth2_redirect_html()\n\n                self.add_route(\n                    self.swagger_ui_oauth2_redirect_url,\n                    swagger_ui_redirect,\n                    include_in_schema=False,\n                )\n        if self.openapi_url and self.redoc_url:\n\n            async def redoc_html(req: Request) -> HTMLResponse:\n                root_path = req.scope.get(\"root_path\", \"\").rstrip(\"/\")\n                openapi_url = root_path + self.openapi_url\n                return get_redoc_html(\n                    openapi_url=openapi_url, title=f\"{self.title} - ReDoc\"\n                )\n\n            self.add_route(self.redoc_url, redoc_html, include_in_schema=False)\n\n    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n        if self.root_path:\n            scope[\"root_path\"] = self.root_path\n        await super().__call__(scope, receive, send)\n\n    def add_api_route(\n        self,\n        path: str,\n        endpoint: Callable[..., Any],\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        methods: Optional[List[str]] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Union[Type[Response], DefaultPlaceholder] = Default(\n            JSONResponse\n        ),\n        name: Optional[str] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> None:\n        self.router.add_api_route(\n            path,\n            endpoint=endpoint,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            methods=methods,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def api_route(\n        self,\n        path: str,\n        *,\n        response_model: Any = Default(None),\n        status_code: Optional[int] = None,\n        tags: Optional[List[Union[str, Enum]]] = None,\n        dependencies: Optional[Sequence[Depends]] = None,\n        summary: Optional[str] = None,\n        description: Optional[str] = None,\n        response_description: str = \"Successful Response\",\n        responses: Optional[Dict[Union[int, str], Dict[str, Any]]] = None,\n        deprecated: Optional[bool] = None,\n        methods: Optional[List[str]] = None,\n        operation_id: Optional[str] = None,\n        response_model_include: Optional[IncEx] = None,\n        response_model_exclude: Optional[IncEx] = None,\n        response_model_by_alias: bool = True,\n        response_model_exclude_unset: bool = False,\n        response_model_exclude_defaults: bool = False,\n        response_model_exclude_none: bool = False,\n        include_in_schema: bool = True,\n        response_class: Type[Response] = Default(JSONResponse),\n        name: Optional[str] = None,\n        openapi_extra: Optional[Dict[str, Any]] = None,\n        generate_unique_id_function: Callable[[routing.APIRoute], str] = Default(\n            generate_unique_id\n        ),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.router.add_api_route(\n                path,\n                func,\n                response_model=response_model,\n                status_code=status_code,\n                tags=tags,\n                dependencies=dependencies,\n                summary=summary,\n                description=description,\n                response_description=response_description,\n                responses=responses,\n                deprecated=deprecated,\n                methods=methods,\n                operation_id=operation_id,\n                response_model_include=response_model_include,\n                response_model_exclude=response_model_exclude,\n                response_model_by_alias=response_model_by_alias,\n                response_model_exclude_unset=response_model_exclude_unset,\n                response_model_exclude_defaults=response_model_exclude_defaults,\n                response_model_exclude_none=response_model_exclude_none,\n                include_in_schema=include_in_schema,\n                response_class=response_class,\n                name=name,\n                openapi_extra=openapi_extra,\n                generate_unique_id_function=generate_unique_id_function,\n            )\n            return func\n\n        return decorator\n\n    def add_api_websocket_route(\n        self,\n        path: str,\n        endpoint: Callable[..., Any],\n        name: Optional[str] = None,\n        *,\n        dependencies: Optional[Sequence[Depends]] = None,\n    ) -> None:\n        self.router.add_api_websocket_route(\n            path,\n            endpoint,\n            name=name,\n            dependencies=dependencies,\n        )\n\n    def websocket(\n        self,\n        path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                WebSocket path.\n                \"\"\"\n            ),\n        ],\n        name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A name for the WebSocket. Only used internally.\n                \"\"\"\n            ),\n        ] = None,\n        *,\n        dependencies: Annotated[\n            Optional[Sequence[Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be used for this\n                WebSocket.\n\n                Read more about it in the\n                [FastAPI docs for WebSockets](https://fastapi.tiangolo.com/advanced/websockets/).\n                \"\"\"\n            ),\n        ] = None,\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Decorate a WebSocket function.\n\n        Read more about it in the\n        [FastAPI docs for WebSockets](https://fastapi.tiangolo.com/advanced/websockets/).\n\n        **Example**\n\n        ```python\n        from fastapi import FastAPI, WebSocket\n\n        app = FastAPI()\n\n        @app.websocket(\"/ws\")\n        async def websocket_endpoint(websocket: WebSocket):\n            await websocket.accept()\n            while True:\n                data = await websocket.receive_text()\n                await websocket.send_text(f\"Message text was: {data}\")\n        ```\n        \"\"\"\n\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_api_websocket_route(\n                path,\n                func,\n                name=name,\n                dependencies=dependencies,\n            )\n            return func\n\n        return decorator\n\n    def include_router(\n        self,\n        router: Annotated[routing.APIRouter, Doc(\"The `APIRouter` to include.\")],\n        *,\n        prefix: Annotated[str, Doc(\"An optional path prefix for the router.\")] = \"\",\n        tags: Annotated[\n            Optional[List[Union[str, Enum]]],\n            Doc(\n                \"\"\"\n                A list of tags to be applied to all the *path operations* in this\n                router.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        dependencies: Annotated[\n            Optional[Sequence[Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be applied to all the\n                *path operations* in this router.\n\n                Read more about it in the\n                [FastAPI docs for Bigger Applications - Multiple Files](https://fastapi.tiangolo.com/tutorial/bigger-applications/#include-an-apirouter-with-a-custom-prefix-tags-responses-and-dependencies).\n\n                **Example**\n\n                ```python\n                from fastapi import Depends, FastAPI\n\n                from .dependencies import get_token_header\n                from .internal import admin\n\n                app = FastAPI()\n\n                app.include_router(\n                    admin.router,\n                    dependencies=[Depends(get_token_header)],\n                )\n                ```\n                \"\"\"\n            ),\n        ] = None,\n        responses: Annotated[\n            Optional[Dict[Union[int, str], Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                Additional responses to be shown in OpenAPI.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Additional Responses in OpenAPI](https://fastapi.tiangolo.com/advanced/additional-responses/).\n\n                And in the\n                [FastAPI docs for Bigger Applications](https://fastapi.tiangolo.com/tutorial/bigger-applications/#include-an-apirouter-with-a-custom-prefix-tags-responses-and-dependencies).\n                \"\"\"\n            ),\n        ] = None,\n        deprecated: Annotated[\n            Optional[bool],\n            Doc(\n                \"\"\"\n                Mark all the *path operations* in this router as deprecated.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                **Example**\n\n                ```python\n                from fastapi import FastAPI\n\n                from .internal import old_api\n\n                app = FastAPI()\n\n                app.include_router(\n                    old_api.router,\n                    deprecated=True,\n                )\n                ```\n                \"\"\"\n            ),\n        ] = None,\n        include_in_schema: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Include (or not) all the *path operations* in this router in the\n                generated OpenAPI schema.\n\n                This affects the generated OpenAPI (e.g. visible at `/docs`).\n\n                **Example**\n\n                ```python\n                from fastapi import FastAPI\n\n                from .internal import old_api\n\n                app = FastAPI()\n\n                app.include_router(\n                    old_api.router,\n                    include_in_schema=False,\n                )\n                ```\n                \"\"\"\n            ),\n        ] = True,\n        default_response_class: Annotated[\n            Type[Response],\n            Doc(\n                \"\"\"\n                Default response class to be used for the *path operations* in this\n                router.\n\n                Read more in the\n                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#default-response-class).\n\n                **Example**\n\n                ```python\n                from fastapi import FastAPI\n                from fastapi.responses import ORJSONResponse\n\n                from .internal import old_api\n\n                app = FastAPI()\n\n                app.include_router(\n                    old_api.router,\n                    default_response_class=ORJSONResponse,\n                )\n                ```\n                \"\"\"\n            ),\n        ] = Default(JSONResponse),\n        callbacks: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                List of *path operations* that will be used as OpenAPI callbacks.\n\n                This is only for OpenAPI documentation, the callbacks won't be used\n                directly.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).\n                \"\"\"\n            ),\n        ] = None,\n        generate_unique_id_function: Annotated[\n            Callable[[routing.APIRoute], str],\n            Doc(\n                \"\"\"\n                Customize the function used to generate unique IDs for the *path\n                operations* shown in the generated OpenAPI.\n\n                This is particularly useful when automatically generating clients or\n                SDKs for your API.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = Default(generate_unique_id),\n    ) -> None:\n        \"\"\"\n        Include an `APIRouter` in the same app.\n\n        Read more about it in the\n        [FastAPI docs for Bigger Applications](https://fastapi.tiangolo.com/tutorial/bigger-applications/).\n\n        ## Example\n\n        ```python\n        from fastapi import FastAPI\n\n        from .users import users_router\n\n        app = FastAPI()\n\n        app.include_router(users_router)\n        ```\n        \"\"\"\n        self.router.include_router(\n            router,\n            prefix=prefix,\n            tags=tags,\n            dependencies=dependencies,\n            responses=responses,\n            deprecated=deprecated,\n            include_in_schema=include_in_schema,\n            default_response_class=default_response_class,\n            callbacks=callbacks,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def get(\n        self,\n        path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The URL path to be used for this *path operation*.\n\n                For example, in `http://example.com/items`, the path is `/items`.\n                \"\"\"\n            ),\n        ],\n        *,\n        response_model: Annotated[\n            Any,\n            Doc(\n                \"\"\"\n                The type to use for the response.\n\n                It could be any valid Pydantic *field* type. So, it doesn't have to\n                be a Pydantic model, it could be other things, like a `list`, `dict`,\n                etc.\n\n                It will be used for:\n\n                * Documentation: the generated OpenAPI (and the UI at `/docs`) will\n                    show it as the response (JSON Schema).\n                * Serialization: you could return an arbitrary object and the\n                    `response_model` would be used to serialize that object into the\n                    corresponding JSON.\n                * Filtering: the JSON sent to the client will only contain the data\n                    (fields) defined in the `response_model`. If you returned an object\n                    that contains an attribute `password` but the `response_model` does\n                    not include that field, the JSON sent to the client would not have\n                    that `password`.\n                * Validation: whatever you return will be serialized with the\n                    `response_model`, converting any data as necessary to generate the\n                    corresponding JSON. But if the data in the object returned is not\n                    valid, that would mean a violation of the contract with the client,\n                    so it's an error from the API developer. So, FastAPI will raise an\n                    error and return a 500 error code (Internal Server Error).\n\n                Read more about it in the\n                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).\n                \"\"\"\n            ),\n        ] = Default(None),\n        status_code: Annotated[\n            Optional[int],\n            Doc(\n                \"\"\"\n                The default status code to be used for the response.\n\n                You could override the status code by returning a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).\n                \"\"\"\n            ),\n        ] = None,\n        tags: Annotated[\n            Optional[List[Union[str, Enum]]],\n            Doc(\n                \"\"\"\n                A list of tags to be applied to the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).\n                \"\"\"\n            ),\n        ] = None,\n        dependencies: Annotated[\n            Optional[Sequence[Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be applied to the\n                *path operation*.\n\n                Read more about it in the\n                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).\n                \"\"\"\n            ),\n        ] = None,\n        summary: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A summary for the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A description for the *path operation*.\n\n                If not provided, it will be extracted automatically from the docstring\n                of the *path operation function*.\n\n                It can contain Markdown.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        response_description: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The description for the default response.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = \"Successful Response\",\n        responses: Annotated[\n            Optional[Dict[Union[int, str], Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                Additional responses that could be returned by this *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        deprecated: Annotated[\n            Optional[bool],\n            Doc(\n                \"\"\"\n                Mark this *path operation* as deprecated.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        operation_id: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Custom operation ID to be used by this *path operation*.\n\n                By default, it is generated automatically.\n\n                If you provide a custom operation ID, you need to make sure it is\n                unique for the whole API.\n\n                You can customize the\n                operation ID generation with the parameter\n                `generate_unique_id_function` in the `FastAPI` class.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_include: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to include only certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_exclude: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to exclude certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_by_alias: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response model\n                should be serialized by alias when an alias is used.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = True,\n        response_model_exclude_unset: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that were not set and\n                have their default values. This is different from\n                `response_model_exclude_defaults` in that if the fields are set,\n                they will be included in the response, even if the value is the same\n                as the default.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_defaults: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that have the same value\n                as the default. This is different from `response_model_exclude_unset`\n                in that if the fields are set but contain the same default values,\n                they will be excluded from the response.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_none: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data should\n                exclude fields set to `None`.\n\n                This is much simpler (less smart) than `response_model_exclude_unset`\n                and `response_model_exclude_defaults`. You probably want to use one of\n                those two instead of this one, as those allow returning `None` values\n                when it makes sense.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).\n                \"\"\"\n            ),\n        ] = False,\n        include_in_schema: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Include this *path operation* in the generated OpenAPI schema.\n\n                This affects the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).\n                \"\"\"\n            ),\n        ] = True,\n        response_class: Annotated[\n            Type[Response],\n            Doc(\n                \"\"\"\n                Response class to be used for this *path operation*.\n\n                This will not be used if you return a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).\n                \"\"\"\n            ),\n        ] = Default(JSONResponse),\n        name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Name for this *path operation*. Only used internally.\n                \"\"\"\n            ),\n        ] = None,\n        callbacks: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                List of *path operations* that will be used as OpenAPI callbacks.\n\n                This is only for OpenAPI documentation, the callbacks won't be used\n                directly.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).\n                \"\"\"\n            ),\n        ] = None,\n        openapi_extra: Annotated[\n            Optional[Dict[str, Any]],\n            Doc(\n                \"\"\"\n                Extra metadata to be included in the OpenAPI schema for this *path\n                operation*.\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).\n                \"\"\"\n            ),\n        ] = None,\n        generate_unique_id_function: Annotated[\n            Callable[[routing.APIRoute], str],\n            Doc(\n                \"\"\"\n                Customize the function used to generate unique IDs for the *path\n                operations* shown in the generated OpenAPI.\n\n                This is particularly useful when automatically generating clients or\n                SDKs for your API.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = Default(generate_unique_id),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add a *path operation* using an HTTP GET operation.\n\n        ## Example\n\n        ```python\n        from fastapi import FastAPI\n\n        app = FastAPI()\n\n        @app.get(\"/items/\")\n        def read_items():\n            return [{\"name\": \"Empanada\"}, {\"name\": \"Arepa\"}]\n        ```\n        \"\"\"\n        return self.router.get(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def put(\n        self,\n        path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The URL path to be used for this *path operation*.\n\n                For example, in `http://example.com/items`, the path is `/items`.\n                \"\"\"\n            ),\n        ],\n        *,\n        response_model: Annotated[\n            Any,\n            Doc(\n                \"\"\"\n                The type to use for the response.\n\n                It could be any valid Pydantic *field* type. So, it doesn't have to\n                be a Pydantic model, it could be other things, like a `list`, `dict`,\n                etc.\n\n                It will be used for:\n\n                * Documentation: the generated OpenAPI (and the UI at `/docs`) will\n                    show it as the response (JSON Schema).\n                * Serialization: you could return an arbitrary object and the\n                    `response_model` would be used to serialize that object into the\n                    corresponding JSON.\n                * Filtering: the JSON sent to the client will only contain the data\n                    (fields) defined in the `response_model`. If you returned an object\n                    that contains an attribute `password` but the `response_model` does\n                    not include that field, the JSON sent to the client would not have\n                    that `password`.\n                * Validation: whatever you return will be serialized with the\n                    `response_model`, converting any data as necessary to generate the\n                    corresponding JSON. But if the data in the object returned is not\n                    valid, that would mean a violation of the contract with the client,\n                    so it's an error from the API developer. So, FastAPI will raise an\n                    error and return a 500 error code (Internal Server Error).\n\n                Read more about it in the\n                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).\n                \"\"\"\n            ),\n        ] = Default(None),\n        status_code: Annotated[\n            Optional[int],\n            Doc(\n                \"\"\"\n                The default status code to be used for the response.\n\n                You could override the status code by returning a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).\n                \"\"\"\n            ),\n        ] = None,\n        tags: Annotated[\n            Optional[List[Union[str, Enum]]],\n            Doc(\n                \"\"\"\n                A list of tags to be applied to the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).\n                \"\"\"\n            ),\n        ] = None,\n        dependencies: Annotated[\n            Optional[Sequence[Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be applied to the\n                *path operation*.\n\n                Read more about it in the\n                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).\n                \"\"\"\n            ),\n        ] = None,\n        summary: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A summary for the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A description for the *path operation*.\n\n                If not provided, it will be extracted automatically from the docstring\n                of the *path operation function*.\n\n                It can contain Markdown.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        response_description: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The description for the default response.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = \"Successful Response\",\n        responses: Annotated[\n            Optional[Dict[Union[int, str], Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                Additional responses that could be returned by this *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        deprecated: Annotated[\n            Optional[bool],\n            Doc(\n                \"\"\"\n                Mark this *path operation* as deprecated.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        operation_id: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Custom operation ID to be used by this *path operation*.\n\n                By default, it is generated automatically.\n\n                If you provide a custom operation ID, you need to make sure it is\n                unique for the whole API.\n\n                You can customize the\n                operation ID generation with the parameter\n                `generate_unique_id_function` in the `FastAPI` class.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_include: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to include only certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_exclude: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to exclude certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_by_alias: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response model\n                should be serialized by alias when an alias is used.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = True,\n        response_model_exclude_unset: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that were not set and\n                have their default values. This is different from\n                `response_model_exclude_defaults` in that if the fields are set,\n                they will be included in the response, even if the value is the same\n                as the default.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_defaults: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that have the same value\n                as the default. This is different from `response_model_exclude_unset`\n                in that if the fields are set but contain the same default values,\n                they will be excluded from the response.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_none: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data should\n                exclude fields set to `None`.\n\n                This is much simpler (less smart) than `response_model_exclude_unset`\n                and `response_model_exclude_defaults`. You probably want to use one of\n                those two instead of this one, as those allow returning `None` values\n                when it makes sense.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).\n                \"\"\"\n            ),\n        ] = False,\n        include_in_schema: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Include this *path operation* in the generated OpenAPI schema.\n\n                This affects the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).\n                \"\"\"\n            ),\n        ] = True,\n        response_class: Annotated[\n            Type[Response],\n            Doc(\n                \"\"\"\n                Response class to be used for this *path operation*.\n\n                This will not be used if you return a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).\n                \"\"\"\n            ),\n        ] = Default(JSONResponse),\n        name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Name for this *path operation*. Only used internally.\n                \"\"\"\n            ),\n        ] = None,\n        callbacks: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                List of *path operations* that will be used as OpenAPI callbacks.\n\n                This is only for OpenAPI documentation, the callbacks won't be used\n                directly.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).\n                \"\"\"\n            ),\n        ] = None,\n        openapi_extra: Annotated[\n            Optional[Dict[str, Any]],\n            Doc(\n                \"\"\"\n                Extra metadata to be included in the OpenAPI schema for this *path\n                operation*.\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).\n                \"\"\"\n            ),\n        ] = None,\n        generate_unique_id_function: Annotated[\n            Callable[[routing.APIRoute], str],\n            Doc(\n                \"\"\"\n                Customize the function used to generate unique IDs for the *path\n                operations* shown in the generated OpenAPI.\n\n                This is particularly useful when automatically generating clients or\n                SDKs for your API.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = Default(generate_unique_id),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add a *path operation* using an HTTP PUT operation.\n\n        ## Example\n\n        ```python\n        from fastapi import FastAPI\n        from pydantic import BaseModel\n\n        class Item(BaseModel):\n            name: str\n            description: str | None = None\n\n        app = FastAPI()\n\n        @app.put(\"/items/{item_id}\")\n        def replace_item(item_id: str, item: Item):\n            return {\"message\": \"Item replaced\", \"id\": item_id}\n        ```\n        \"\"\"\n        return self.router.put(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def post(\n        self,\n        path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The URL path to be used for this *path operation*.\n\n                For example, in `http://example.com/items`, the path is `/items`.\n                \"\"\"\n            ),\n        ],\n        *,\n        response_model: Annotated[\n            Any,\n            Doc(\n                \"\"\"\n                The type to use for the response.\n\n                It could be any valid Pydantic *field* type. So, it doesn't have to\n                be a Pydantic model, it could be other things, like a `list`, `dict`,\n                etc.\n\n                It will be used for:\n\n                * Documentation: the generated OpenAPI (and the UI at `/docs`) will\n                    show it as the response (JSON Schema).\n                * Serialization: you could return an arbitrary object and the\n                    `response_model` would be used to serialize that object into the\n                    corresponding JSON.\n                * Filtering: the JSON sent to the client will only contain the data\n                    (fields) defined in the `response_model`. If you returned an object\n                    that contains an attribute `password` but the `response_model` does\n                    not include that field, the JSON sent to the client would not have\n                    that `password`.\n                * Validation: whatever you return will be serialized with the\n                    `response_model`, converting any data as necessary to generate the\n                    corresponding JSON. But if the data in the object returned is not\n                    valid, that would mean a violation of the contract with the client,\n                    so it's an error from the API developer. So, FastAPI will raise an\n                    error and return a 500 error code (Internal Server Error).\n\n                Read more about it in the\n                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).\n                \"\"\"\n            ),\n        ] = Default(None),\n        status_code: Annotated[\n            Optional[int],\n            Doc(\n                \"\"\"\n                The default status code to be used for the response.\n\n                You could override the status code by returning a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).\n                \"\"\"\n            ),\n        ] = None,\n        tags: Annotated[\n            Optional[List[Union[str, Enum]]],\n            Doc(\n                \"\"\"\n                A list of tags to be applied to the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).\n                \"\"\"\n            ),\n        ] = None,\n        dependencies: Annotated[\n            Optional[Sequence[Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be applied to the\n                *path operation*.\n\n                Read more about it in the\n                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).\n                \"\"\"\n            ),\n        ] = None,\n        summary: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A summary for the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A description for the *path operation*.\n\n                If not provided, it will be extracted automatically from the docstring\n                of the *path operation function*.\n\n                It can contain Markdown.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        response_description: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The description for the default response.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = \"Successful Response\",\n        responses: Annotated[\n            Optional[Dict[Union[int, str], Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                Additional responses that could be returned by this *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        deprecated: Annotated[\n            Optional[bool],\n            Doc(\n                \"\"\"\n                Mark this *path operation* as deprecated.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        operation_id: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Custom operation ID to be used by this *path operation*.\n\n                By default, it is generated automatically.\n\n                If you provide a custom operation ID, you need to make sure it is\n                unique for the whole API.\n\n                You can customize the\n                operation ID generation with the parameter\n                `generate_unique_id_function` in the `FastAPI` class.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_include: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to include only certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_exclude: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to exclude certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_by_alias: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response model\n                should be serialized by alias when an alias is used.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = True,\n        response_model_exclude_unset: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that were not set and\n                have their default values. This is different from\n                `response_model_exclude_defaults` in that if the fields are set,\n                they will be included in the response, even if the value is the same\n                as the default.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_defaults: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that have the same value\n                as the default. This is different from `response_model_exclude_unset`\n                in that if the fields are set but contain the same default values,\n                they will be excluded from the response.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_none: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data should\n                exclude fields set to `None`.\n\n                This is much simpler (less smart) than `response_model_exclude_unset`\n                and `response_model_exclude_defaults`. You probably want to use one of\n                those two instead of this one, as those allow returning `None` values\n                when it makes sense.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).\n                \"\"\"\n            ),\n        ] = False,\n        include_in_schema: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Include this *path operation* in the generated OpenAPI schema.\n\n                This affects the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).\n                \"\"\"\n            ),\n        ] = True,\n        response_class: Annotated[\n            Type[Response],\n            Doc(\n                \"\"\"\n                Response class to be used for this *path operation*.\n\n                This will not be used if you return a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).\n                \"\"\"\n            ),\n        ] = Default(JSONResponse),\n        name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Name for this *path operation*. Only used internally.\n                \"\"\"\n            ),\n        ] = None,\n        callbacks: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                List of *path operations* that will be used as OpenAPI callbacks.\n\n                This is only for OpenAPI documentation, the callbacks won't be used\n                directly.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).\n                \"\"\"\n            ),\n        ] = None,\n        openapi_extra: Annotated[\n            Optional[Dict[str, Any]],\n            Doc(\n                \"\"\"\n                Extra metadata to be included in the OpenAPI schema for this *path\n                operation*.\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).\n                \"\"\"\n            ),\n        ] = None,\n        generate_unique_id_function: Annotated[\n            Callable[[routing.APIRoute], str],\n            Doc(\n                \"\"\"\n                Customize the function used to generate unique IDs for the *path\n                operations* shown in the generated OpenAPI.\n\n                This is particularly useful when automatically generating clients or\n                SDKs for your API.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = Default(generate_unique_id),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add a *path operation* using an HTTP POST operation.\n\n        ## Example\n\n        ```python\n        from fastapi import FastAPI\n        from pydantic import BaseModel\n\n        class Item(BaseModel):\n            name: str\n            description: str | None = None\n\n        app = FastAPI()\n\n        @app.post(\"/items/\")\n        def create_item(item: Item):\n            return {\"message\": \"Item created\"}\n        ```\n        \"\"\"\n        return self.router.post(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def delete(\n        self,\n        path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The URL path to be used for this *path operation*.\n\n                For example, in `http://example.com/items`, the path is `/items`.\n                \"\"\"\n            ),\n        ],\n        *,\n        response_model: Annotated[\n            Any,\n            Doc(\n                \"\"\"\n                The type to use for the response.\n\n                It could be any valid Pydantic *field* type. So, it doesn't have to\n                be a Pydantic model, it could be other things, like a `list`, `dict`,\n                etc.\n\n                It will be used for:\n\n                * Documentation: the generated OpenAPI (and the UI at `/docs`) will\n                    show it as the response (JSON Schema).\n                * Serialization: you could return an arbitrary object and the\n                    `response_model` would be used to serialize that object into the\n                    corresponding JSON.\n                * Filtering: the JSON sent to the client will only contain the data\n                    (fields) defined in the `response_model`. If you returned an object\n                    that contains an attribute `password` but the `response_model` does\n                    not include that field, the JSON sent to the client would not have\n                    that `password`.\n                * Validation: whatever you return will be serialized with the\n                    `response_model`, converting any data as necessary to generate the\n                    corresponding JSON. But if the data in the object returned is not\n                    valid, that would mean a violation of the contract with the client,\n                    so it's an error from the API developer. So, FastAPI will raise an\n                    error and return a 500 error code (Internal Server Error).\n\n                Read more about it in the\n                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).\n                \"\"\"\n            ),\n        ] = Default(None),\n        status_code: Annotated[\n            Optional[int],\n            Doc(\n                \"\"\"\n                The default status code to be used for the response.\n\n                You could override the status code by returning a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).\n                \"\"\"\n            ),\n        ] = None,\n        tags: Annotated[\n            Optional[List[Union[str, Enum]]],\n            Doc(\n                \"\"\"\n                A list of tags to be applied to the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).\n                \"\"\"\n            ),\n        ] = None,\n        dependencies: Annotated[\n            Optional[Sequence[Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be applied to the\n                *path operation*.\n\n                Read more about it in the\n                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).\n                \"\"\"\n            ),\n        ] = None,\n        summary: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A summary for the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A description for the *path operation*.\n\n                If not provided, it will be extracted automatically from the docstring\n                of the *path operation function*.\n\n                It can contain Markdown.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        response_description: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The description for the default response.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = \"Successful Response\",\n        responses: Annotated[\n            Optional[Dict[Union[int, str], Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                Additional responses that could be returned by this *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        deprecated: Annotated[\n            Optional[bool],\n            Doc(\n                \"\"\"\n                Mark this *path operation* as deprecated.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        operation_id: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Custom operation ID to be used by this *path operation*.\n\n                By default, it is generated automatically.\n\n                If you provide a custom operation ID, you need to make sure it is\n                unique for the whole API.\n\n                You can customize the\n                operation ID generation with the parameter\n                `generate_unique_id_function` in the `FastAPI` class.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_include: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to include only certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_exclude: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to exclude certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_by_alias: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response model\n                should be serialized by alias when an alias is used.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = True,\n        response_model_exclude_unset: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that were not set and\n                have their default values. This is different from\n                `response_model_exclude_defaults` in that if the fields are set,\n                they will be included in the response, even if the value is the same\n                as the default.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_defaults: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that have the same value\n                as the default. This is different from `response_model_exclude_unset`\n                in that if the fields are set but contain the same default values,\n                they will be excluded from the response.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_none: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data should\n                exclude fields set to `None`.\n\n                This is much simpler (less smart) than `response_model_exclude_unset`\n                and `response_model_exclude_defaults`. You probably want to use one of\n                those two instead of this one, as those allow returning `None` values\n                when it makes sense.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).\n                \"\"\"\n            ),\n        ] = False,\n        include_in_schema: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Include this *path operation* in the generated OpenAPI schema.\n\n                This affects the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).\n                \"\"\"\n            ),\n        ] = True,\n        response_class: Annotated[\n            Type[Response],\n            Doc(\n                \"\"\"\n                Response class to be used for this *path operation*.\n\n                This will not be used if you return a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).\n                \"\"\"\n            ),\n        ] = Default(JSONResponse),\n        name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Name for this *path operation*. Only used internally.\n                \"\"\"\n            ),\n        ] = None,\n        callbacks: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                List of *path operations* that will be used as OpenAPI callbacks.\n\n                This is only for OpenAPI documentation, the callbacks won't be used\n                directly.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).\n                \"\"\"\n            ),\n        ] = None,\n        openapi_extra: Annotated[\n            Optional[Dict[str, Any]],\n            Doc(\n                \"\"\"\n                Extra metadata to be included in the OpenAPI schema for this *path\n                operation*.\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).\n                \"\"\"\n            ),\n        ] = None,\n        generate_unique_id_function: Annotated[\n            Callable[[routing.APIRoute], str],\n            Doc(\n                \"\"\"\n                Customize the function used to generate unique IDs for the *path\n                operations* shown in the generated OpenAPI.\n\n                This is particularly useful when automatically generating clients or\n                SDKs for your API.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = Default(generate_unique_id),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add a *path operation* using an HTTP DELETE operation.\n\n        ## Example\n\n        ```python\n        from fastapi import FastAPI\n\n        app = FastAPI()\n\n        @app.delete(\"/items/{item_id}\")\n        def delete_item(item_id: str):\n            return {\"message\": \"Item deleted\"}\n        ```\n        \"\"\"\n        return self.router.delete(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def options(\n        self,\n        path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The URL path to be used for this *path operation*.\n\n                For example, in `http://example.com/items`, the path is `/items`.\n                \"\"\"\n            ),\n        ],\n        *,\n        response_model: Annotated[\n            Any,\n            Doc(\n                \"\"\"\n                The type to use for the response.\n\n                It could be any valid Pydantic *field* type. So, it doesn't have to\n                be a Pydantic model, it could be other things, like a `list`, `dict`,\n                etc.\n\n                It will be used for:\n\n                * Documentation: the generated OpenAPI (and the UI at `/docs`) will\n                    show it as the response (JSON Schema).\n                * Serialization: you could return an arbitrary object and the\n                    `response_model` would be used to serialize that object into the\n                    corresponding JSON.\n                * Filtering: the JSON sent to the client will only contain the data\n                    (fields) defined in the `response_model`. If you returned an object\n                    that contains an attribute `password` but the `response_model` does\n                    not include that field, the JSON sent to the client would not have\n                    that `password`.\n                * Validation: whatever you return will be serialized with the\n                    `response_model`, converting any data as necessary to generate the\n                    corresponding JSON. But if the data in the object returned is not\n                    valid, that would mean a violation of the contract with the client,\n                    so it's an error from the API developer. So, FastAPI will raise an\n                    error and return a 500 error code (Internal Server Error).\n\n                Read more about it in the\n                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).\n                \"\"\"\n            ),\n        ] = Default(None),\n        status_code: Annotated[\n            Optional[int],\n            Doc(\n                \"\"\"\n                The default status code to be used for the response.\n\n                You could override the status code by returning a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).\n                \"\"\"\n            ),\n        ] = None,\n        tags: Annotated[\n            Optional[List[Union[str, Enum]]],\n            Doc(\n                \"\"\"\n                A list of tags to be applied to the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).\n                \"\"\"\n            ),\n        ] = None,\n        dependencies: Annotated[\n            Optional[Sequence[Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be applied to the\n                *path operation*.\n\n                Read more about it in the\n                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).\n                \"\"\"\n            ),\n        ] = None,\n        summary: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A summary for the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A description for the *path operation*.\n\n                If not provided, it will be extracted automatically from the docstring\n                of the *path operation function*.\n\n                It can contain Markdown.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        response_description: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The description for the default response.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = \"Successful Response\",\n        responses: Annotated[\n            Optional[Dict[Union[int, str], Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                Additional responses that could be returned by this *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        deprecated: Annotated[\n            Optional[bool],\n            Doc(\n                \"\"\"\n                Mark this *path operation* as deprecated.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        operation_id: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Custom operation ID to be used by this *path operation*.\n\n                By default, it is generated automatically.\n\n                If you provide a custom operation ID, you need to make sure it is\n                unique for the whole API.\n\n                You can customize the\n                operation ID generation with the parameter\n                `generate_unique_id_function` in the `FastAPI` class.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_include: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to include only certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_exclude: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to exclude certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_by_alias: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response model\n                should be serialized by alias when an alias is used.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = True,\n        response_model_exclude_unset: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that were not set and\n                have their default values. This is different from\n                `response_model_exclude_defaults` in that if the fields are set,\n                they will be included in the response, even if the value is the same\n                as the default.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_defaults: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that have the same value\n                as the default. This is different from `response_model_exclude_unset`\n                in that if the fields are set but contain the same default values,\n                they will be excluded from the response.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_none: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data should\n                exclude fields set to `None`.\n\n                This is much simpler (less smart) than `response_model_exclude_unset`\n                and `response_model_exclude_defaults`. You probably want to use one of\n                those two instead of this one, as those allow returning `None` values\n                when it makes sense.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).\n                \"\"\"\n            ),\n        ] = False,\n        include_in_schema: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Include this *path operation* in the generated OpenAPI schema.\n\n                This affects the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).\n                \"\"\"\n            ),\n        ] = True,\n        response_class: Annotated[\n            Type[Response],\n            Doc(\n                \"\"\"\n                Response class to be used for this *path operation*.\n\n                This will not be used if you return a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).\n                \"\"\"\n            ),\n        ] = Default(JSONResponse),\n        name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Name for this *path operation*. Only used internally.\n                \"\"\"\n            ),\n        ] = None,\n        callbacks: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                List of *path operations* that will be used as OpenAPI callbacks.\n\n                This is only for OpenAPI documentation, the callbacks won't be used\n                directly.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).\n                \"\"\"\n            ),\n        ] = None,\n        openapi_extra: Annotated[\n            Optional[Dict[str, Any]],\n            Doc(\n                \"\"\"\n                Extra metadata to be included in the OpenAPI schema for this *path\n                operation*.\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).\n                \"\"\"\n            ),\n        ] = None,\n        generate_unique_id_function: Annotated[\n            Callable[[routing.APIRoute], str],\n            Doc(\n                \"\"\"\n                Customize the function used to generate unique IDs for the *path\n                operations* shown in the generated OpenAPI.\n\n                This is particularly useful when automatically generating clients or\n                SDKs for your API.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = Default(generate_unique_id),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add a *path operation* using an HTTP OPTIONS operation.\n\n        ## Example\n\n        ```python\n        from fastapi import FastAPI\n\n        app = FastAPI()\n\n        @app.options(\"/items/\")\n        def get_item_options():\n            return {\"additions\": [\"Aji\", \"Guacamole\"]}\n        ```\n        \"\"\"\n        return self.router.options(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def head(\n        self,\n        path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The URL path to be used for this *path operation*.\n\n                For example, in `http://example.com/items`, the path is `/items`.\n                \"\"\"\n            ),\n        ],\n        *,\n        response_model: Annotated[\n            Any,\n            Doc(\n                \"\"\"\n                The type to use for the response.\n\n                It could be any valid Pydantic *field* type. So, it doesn't have to\n                be a Pydantic model, it could be other things, like a `list`, `dict`,\n                etc.\n\n                It will be used for:\n\n                * Documentation: the generated OpenAPI (and the UI at `/docs`) will\n                    show it as the response (JSON Schema).\n                * Serialization: you could return an arbitrary object and the\n                    `response_model` would be used to serialize that object into the\n                    corresponding JSON.\n                * Filtering: the JSON sent to the client will only contain the data\n                    (fields) defined in the `response_model`. If you returned an object\n                    that contains an attribute `password` but the `response_model` does\n                    not include that field, the JSON sent to the client would not have\n                    that `password`.\n                * Validation: whatever you return will be serialized with the\n                    `response_model`, converting any data as necessary to generate the\n                    corresponding JSON. But if the data in the object returned is not\n                    valid, that would mean a violation of the contract with the client,\n                    so it's an error from the API developer. So, FastAPI will raise an\n                    error and return a 500 error code (Internal Server Error).\n\n                Read more about it in the\n                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).\n                \"\"\"\n            ),\n        ] = Default(None),\n        status_code: Annotated[\n            Optional[int],\n            Doc(\n                \"\"\"\n                The default status code to be used for the response.\n\n                You could override the status code by returning a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).\n                \"\"\"\n            ),\n        ] = None,\n        tags: Annotated[\n            Optional[List[Union[str, Enum]]],\n            Doc(\n                \"\"\"\n                A list of tags to be applied to the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).\n                \"\"\"\n            ),\n        ] = None,\n        dependencies: Annotated[\n            Optional[Sequence[Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be applied to the\n                *path operation*.\n\n                Read more about it in the\n                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).\n                \"\"\"\n            ),\n        ] = None,\n        summary: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A summary for the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A description for the *path operation*.\n\n                If not provided, it will be extracted automatically from the docstring\n                of the *path operation function*.\n\n                It can contain Markdown.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        response_description: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The description for the default response.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = \"Successful Response\",\n        responses: Annotated[\n            Optional[Dict[Union[int, str], Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                Additional responses that could be returned by this *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        deprecated: Annotated[\n            Optional[bool],\n            Doc(\n                \"\"\"\n                Mark this *path operation* as deprecated.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        operation_id: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Custom operation ID to be used by this *path operation*.\n\n                By default, it is generated automatically.\n\n                If you provide a custom operation ID, you need to make sure it is\n                unique for the whole API.\n\n                You can customize the\n                operation ID generation with the parameter\n                `generate_unique_id_function` in the `FastAPI` class.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_include: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to include only certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_exclude: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to exclude certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_by_alias: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response model\n                should be serialized by alias when an alias is used.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = True,\n        response_model_exclude_unset: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that were not set and\n                have their default values. This is different from\n                `response_model_exclude_defaults` in that if the fields are set,\n                they will be included in the response, even if the value is the same\n                as the default.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_defaults: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that have the same value\n                as the default. This is different from `response_model_exclude_unset`\n                in that if the fields are set but contain the same default values,\n                they will be excluded from the response.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_none: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data should\n                exclude fields set to `None`.\n\n                This is much simpler (less smart) than `response_model_exclude_unset`\n                and `response_model_exclude_defaults`. You probably want to use one of\n                those two instead of this one, as those allow returning `None` values\n                when it makes sense.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).\n                \"\"\"\n            ),\n        ] = False,\n        include_in_schema: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Include this *path operation* in the generated OpenAPI schema.\n\n                This affects the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).\n                \"\"\"\n            ),\n        ] = True,\n        response_class: Annotated[\n            Type[Response],\n            Doc(\n                \"\"\"\n                Response class to be used for this *path operation*.\n\n                This will not be used if you return a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).\n                \"\"\"\n            ),\n        ] = Default(JSONResponse),\n        name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Name for this *path operation*. Only used internally.\n                \"\"\"\n            ),\n        ] = None,\n        callbacks: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                List of *path operations* that will be used as OpenAPI callbacks.\n\n                This is only for OpenAPI documentation, the callbacks won't be used\n                directly.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).\n                \"\"\"\n            ),\n        ] = None,\n        openapi_extra: Annotated[\n            Optional[Dict[str, Any]],\n            Doc(\n                \"\"\"\n                Extra metadata to be included in the OpenAPI schema for this *path\n                operation*.\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).\n                \"\"\"\n            ),\n        ] = None,\n        generate_unique_id_function: Annotated[\n            Callable[[routing.APIRoute], str],\n            Doc(\n                \"\"\"\n                Customize the function used to generate unique IDs for the *path\n                operations* shown in the generated OpenAPI.\n\n                This is particularly useful when automatically generating clients or\n                SDKs for your API.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = Default(generate_unique_id),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add a *path operation* using an HTTP HEAD operation.\n\n        ## Example\n\n        ```python\n        from fastapi import FastAPI, Response\n\n        app = FastAPI()\n\n        @app.head(\"/items/\", status_code=204)\n        def get_items_headers(response: Response):\n            response.headers[\"X-Cat-Dog\"] = \"Alone in the world\"\n        ```\n        \"\"\"\n        return self.router.head(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def patch(\n        self,\n        path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The URL path to be used for this *path operation*.\n\n                For example, in `http://example.com/items`, the path is `/items`.\n                \"\"\"\n            ),\n        ],\n        *,\n        response_model: Annotated[\n            Any,\n            Doc(\n                \"\"\"\n                The type to use for the response.\n\n                It could be any valid Pydantic *field* type. So, it doesn't have to\n                be a Pydantic model, it could be other things, like a `list`, `dict`,\n                etc.\n\n                It will be used for:\n\n                * Documentation: the generated OpenAPI (and the UI at `/docs`) will\n                    show it as the response (JSON Schema).\n                * Serialization: you could return an arbitrary object and the\n                    `response_model` would be used to serialize that object into the\n                    corresponding JSON.\n                * Filtering: the JSON sent to the client will only contain the data\n                    (fields) defined in the `response_model`. If you returned an object\n                    that contains an attribute `password` but the `response_model` does\n                    not include that field, the JSON sent to the client would not have\n                    that `password`.\n                * Validation: whatever you return will be serialized with the\n                    `response_model`, converting any data as necessary to generate the\n                    corresponding JSON. But if the data in the object returned is not\n                    valid, that would mean a violation of the contract with the client,\n                    so it's an error from the API developer. So, FastAPI will raise an\n                    error and return a 500 error code (Internal Server Error).\n\n                Read more about it in the\n                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).\n                \"\"\"\n            ),\n        ] = Default(None),\n        status_code: Annotated[\n            Optional[int],\n            Doc(\n                \"\"\"\n                The default status code to be used for the response.\n\n                You could override the status code by returning a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).\n                \"\"\"\n            ),\n        ] = None,\n        tags: Annotated[\n            Optional[List[Union[str, Enum]]],\n            Doc(\n                \"\"\"\n                A list of tags to be applied to the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).\n                \"\"\"\n            ),\n        ] = None,\n        dependencies: Annotated[\n            Optional[Sequence[Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be applied to the\n                *path operation*.\n\n                Read more about it in the\n                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).\n                \"\"\"\n            ),\n        ] = None,\n        summary: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A summary for the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A description for the *path operation*.\n\n                If not provided, it will be extracted automatically from the docstring\n                of the *path operation function*.\n\n                It can contain Markdown.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        response_description: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The description for the default response.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = \"Successful Response\",\n        responses: Annotated[\n            Optional[Dict[Union[int, str], Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                Additional responses that could be returned by this *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        deprecated: Annotated[\n            Optional[bool],\n            Doc(\n                \"\"\"\n                Mark this *path operation* as deprecated.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        operation_id: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Custom operation ID to be used by this *path operation*.\n\n                By default, it is generated automatically.\n\n                If you provide a custom operation ID, you need to make sure it is\n                unique for the whole API.\n\n                You can customize the\n                operation ID generation with the parameter\n                `generate_unique_id_function` in the `FastAPI` class.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_include: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to include only certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_exclude: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to exclude certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_by_alias: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response model\n                should be serialized by alias when an alias is used.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = True,\n        response_model_exclude_unset: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that were not set and\n                have their default values. This is different from\n                `response_model_exclude_defaults` in that if the fields are set,\n                they will be included in the response, even if the value is the same\n                as the default.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_defaults: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that have the same value\n                as the default. This is different from `response_model_exclude_unset`\n                in that if the fields are set but contain the same default values,\n                they will be excluded from the response.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_none: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data should\n                exclude fields set to `None`.\n\n                This is much simpler (less smart) than `response_model_exclude_unset`\n                and `response_model_exclude_defaults`. You probably want to use one of\n                those two instead of this one, as those allow returning `None` values\n                when it makes sense.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).\n                \"\"\"\n            ),\n        ] = False,\n        include_in_schema: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Include this *path operation* in the generated OpenAPI schema.\n\n                This affects the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).\n                \"\"\"\n            ),\n        ] = True,\n        response_class: Annotated[\n            Type[Response],\n            Doc(\n                \"\"\"\n                Response class to be used for this *path operation*.\n\n                This will not be used if you return a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).\n                \"\"\"\n            ),\n        ] = Default(JSONResponse),\n        name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Name for this *path operation*. Only used internally.\n                \"\"\"\n            ),\n        ] = None,\n        callbacks: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                List of *path operations* that will be used as OpenAPI callbacks.\n\n                This is only for OpenAPI documentation, the callbacks won't be used\n                directly.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).\n                \"\"\"\n            ),\n        ] = None,\n        openapi_extra: Annotated[\n            Optional[Dict[str, Any]],\n            Doc(\n                \"\"\"\n                Extra metadata to be included in the OpenAPI schema for this *path\n                operation*.\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).\n                \"\"\"\n            ),\n        ] = None,\n        generate_unique_id_function: Annotated[\n            Callable[[routing.APIRoute], str],\n            Doc(\n                \"\"\"\n                Customize the function used to generate unique IDs for the *path\n                operations* shown in the generated OpenAPI.\n\n                This is particularly useful when automatically generating clients or\n                SDKs for your API.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = Default(generate_unique_id),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add a *path operation* using an HTTP PATCH operation.\n\n        ## Example\n\n        ```python\n        from fastapi import FastAPI\n        from pydantic import BaseModel\n\n        class Item(BaseModel):\n            name: str\n            description: str | None = None\n\n        app = FastAPI()\n\n        @app.patch(\"/items/\")\n        def update_item(item: Item):\n            return {\"message\": \"Item updated in place\"}\n        ```\n        \"\"\"\n        return self.router.patch(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def trace(\n        self,\n        path: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The URL path to be used for this *path operation*.\n\n                For example, in `http://example.com/items`, the path is `/items`.\n                \"\"\"\n            ),\n        ],\n        *,\n        response_model: Annotated[\n            Any,\n            Doc(\n                \"\"\"\n                The type to use for the response.\n\n                It could be any valid Pydantic *field* type. So, it doesn't have to\n                be a Pydantic model, it could be other things, like a `list`, `dict`,\n                etc.\n\n                It will be used for:\n\n                * Documentation: the generated OpenAPI (and the UI at `/docs`) will\n                    show it as the response (JSON Schema).\n                * Serialization: you could return an arbitrary object and the\n                    `response_model` would be used to serialize that object into the\n                    corresponding JSON.\n                * Filtering: the JSON sent to the client will only contain the data\n                    (fields) defined in the `response_model`. If you returned an object\n                    that contains an attribute `password` but the `response_model` does\n                    not include that field, the JSON sent to the client would not have\n                    that `password`.\n                * Validation: whatever you return will be serialized with the\n                    `response_model`, converting any data as necessary to generate the\n                    corresponding JSON. But if the data in the object returned is not\n                    valid, that would mean a violation of the contract with the client,\n                    so it's an error from the API developer. So, FastAPI will raise an\n                    error and return a 500 error code (Internal Server Error).\n\n                Read more about it in the\n                [FastAPI docs for Response Model](https://fastapi.tiangolo.com/tutorial/response-model/).\n                \"\"\"\n            ),\n        ] = Default(None),\n        status_code: Annotated[\n            Optional[int],\n            Doc(\n                \"\"\"\n                The default status code to be used for the response.\n\n                You could override the status code by returning a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Response Status Code](https://fastapi.tiangolo.com/tutorial/response-status-code/).\n                \"\"\"\n            ),\n        ] = None,\n        tags: Annotated[\n            Optional[List[Union[str, Enum]]],\n            Doc(\n                \"\"\"\n                A list of tags to be applied to the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/#tags).\n                \"\"\"\n            ),\n        ] = None,\n        dependencies: Annotated[\n            Optional[Sequence[Depends]],\n            Doc(\n                \"\"\"\n                A list of dependencies (using `Depends()`) to be applied to the\n                *path operation*.\n\n                Read more about it in the\n                [FastAPI docs for Dependencies in path operation decorators](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/).\n                \"\"\"\n            ),\n        ] = None,\n        summary: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A summary for the *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        description: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                A description for the *path operation*.\n\n                If not provided, it will be extracted automatically from the docstring\n                of the *path operation function*.\n\n                It can contain Markdown.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Configuration](https://fastapi.tiangolo.com/tutorial/path-operation-configuration/).\n                \"\"\"\n            ),\n        ] = None,\n        response_description: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The description for the default response.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = \"Successful Response\",\n        responses: Annotated[\n            Optional[Dict[Union[int, str], Dict[str, Any]]],\n            Doc(\n                \"\"\"\n                Additional responses that could be returned by this *path operation*.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        deprecated: Annotated[\n            Optional[bool],\n            Doc(\n                \"\"\"\n                Mark this *path operation* as deprecated.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n                \"\"\"\n            ),\n        ] = None,\n        operation_id: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Custom operation ID to be used by this *path operation*.\n\n                By default, it is generated automatically.\n\n                If you provide a custom operation ID, you need to make sure it is\n                unique for the whole API.\n\n                You can customize the\n                operation ID generation with the parameter\n                `generate_unique_id_function` in the `FastAPI` class.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_include: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to include only certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_exclude: Annotated[\n            Optional[IncEx],\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to exclude certain fields in the\n                response data.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = None,\n        response_model_by_alias: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response model\n                should be serialized by alias when an alias is used.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_include-and-response_model_exclude).\n                \"\"\"\n            ),\n        ] = True,\n        response_model_exclude_unset: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that were not set and\n                have their default values. This is different from\n                `response_model_exclude_defaults` in that if the fields are set,\n                they will be included in the response, even if the value is the same\n                as the default.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_defaults: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data\n                should have all the fields, including the ones that have the same value\n                as the default. This is different from `response_model_exclude_unset`\n                in that if the fields are set but contain the same default values,\n                they will be excluded from the response.\n\n                When `True`, default values are omitted from the response.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#use-the-response_model_exclude_unset-parameter).\n                \"\"\"\n            ),\n        ] = False,\n        response_model_exclude_none: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Configuration passed to Pydantic to define if the response data should\n                exclude fields set to `None`.\n\n                This is much simpler (less smart) than `response_model_exclude_unset`\n                and `response_model_exclude_defaults`. You probably want to use one of\n                those two instead of this one, as those allow returning `None` values\n                when it makes sense.\n\n                Read more about it in the\n                [FastAPI docs for Response Model - Return Type](https://fastapi.tiangolo.com/tutorial/response-model/#response_model_exclude_none).\n                \"\"\"\n            ),\n        ] = False,\n        include_in_schema: Annotated[\n            bool,\n            Doc(\n                \"\"\"\n                Include this *path operation* in the generated OpenAPI schema.\n\n                This affects the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for Query Parameters and String Validations](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/#exclude-parameters-from-openapi).\n                \"\"\"\n            ),\n        ] = True,\n        response_class: Annotated[\n            Type[Response],\n            Doc(\n                \"\"\"\n                Response class to be used for this *path operation*.\n\n                This will not be used if you return a response directly.\n\n                Read more about it in the\n                [FastAPI docs for Custom Response - HTML, Stream, File, others](https://fastapi.tiangolo.com/advanced/custom-response/#redirectresponse).\n                \"\"\"\n            ),\n        ] = Default(JSONResponse),\n        name: Annotated[\n            Optional[str],\n            Doc(\n                \"\"\"\n                Name for this *path operation*. Only used internally.\n                \"\"\"\n            ),\n        ] = None,\n        callbacks: Annotated[\n            Optional[List[BaseRoute]],\n            Doc(\n                \"\"\"\n                List of *path operations* that will be used as OpenAPI callbacks.\n\n                This is only for OpenAPI documentation, the callbacks won't be used\n                directly.\n\n                It will be added to the generated OpenAPI (e.g. visible at `/docs`).\n\n                Read more about it in the\n                [FastAPI docs for OpenAPI Callbacks](https://fastapi.tiangolo.com/advanced/openapi-callbacks/).\n                \"\"\"\n            ),\n        ] = None,\n        openapi_extra: Annotated[\n            Optional[Dict[str, Any]],\n            Doc(\n                \"\"\"\n                Extra metadata to be included in the OpenAPI schema for this *path\n                operation*.\n\n                Read more about it in the\n                [FastAPI docs for Path Operation Advanced Configuration](https://fastapi.tiangolo.com/advanced/path-operation-advanced-configuration/#custom-openapi-path-operation-schema).\n                \"\"\"\n            ),\n        ] = None,\n        generate_unique_id_function: Annotated[\n            Callable[[routing.APIRoute], str],\n            Doc(\n                \"\"\"\n                Customize the function used to generate unique IDs for the *path\n                operations* shown in the generated OpenAPI.\n\n                This is particularly useful when automatically generating clients or\n                SDKs for your API.\n\n                Read more about it in the\n                [FastAPI docs about how to Generate Clients](https://fastapi.tiangolo.com/advanced/generate-clients/#custom-generate-unique-id-function).\n                \"\"\"\n            ),\n        ] = Default(generate_unique_id),\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add a *path operation* using an HTTP TRACE operation.\n\n        ## Example\n\n        ```python\n        from fastapi import FastAPI\n\n        app = FastAPI()\n\n        @app.trace(\"/items/{item_id}\")\n        def trace_item(item_id: str):\n            return None\n        ```\n        \"\"\"\n        return self.router.trace(\n            path,\n            response_model=response_model,\n            status_code=status_code,\n            tags=tags,\n            dependencies=dependencies,\n            summary=summary,\n            description=description,\n            response_description=response_description,\n            responses=responses,\n            deprecated=deprecated,\n            operation_id=operation_id,\n            response_model_include=response_model_include,\n            response_model_exclude=response_model_exclude,\n            response_model_by_alias=response_model_by_alias,\n            response_model_exclude_unset=response_model_exclude_unset,\n            response_model_exclude_defaults=response_model_exclude_defaults,\n            response_model_exclude_none=response_model_exclude_none,\n            include_in_schema=include_in_schema,\n            response_class=response_class,\n            name=name,\n            callbacks=callbacks,\n            openapi_extra=openapi_extra,\n            generate_unique_id_function=generate_unique_id_function,\n        )\n\n    def websocket_route(\n        self, path: str, name: Union[str, None] = None\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.router.add_websocket_route(path, func, name=name)\n            return func\n\n        return decorator\n\n    @deprecated(\n        \"\"\"\n        on_event is deprecated, use lifespan event handlers instead.\n\n        Read more about it in the\n        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n        \"\"\"\n    )\n    def on_event(\n        self,\n        event_type: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The type of event. `startup` or `shutdown`.\n                \"\"\"\n            ),\n        ],\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add an event handler for the application.\n\n        `on_event` is deprecated, use `lifespan` event handlers instead.\n\n        Read more about it in the\n        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/#alternative-events-deprecated).\n        \"\"\"\n        return self.router.on_event(event_type)\n\n    def middleware(\n        self,\n        middleware_type: Annotated[\n            str,\n            Doc(\n                \"\"\"\n                The type of middleware. Currently only supports `http`.\n                \"\"\"\n            ),\n        ],\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add a middleware to the application.\n\n        Read more about it in the\n        [FastAPI docs for Middleware](https://fastapi.tiangolo.com/tutorial/middleware/).\n\n        ## Example\n\n        ```python\n        import time\n        from typing import Awaitable, Callable\n\n        from fastapi import FastAPI, Request, Response\n\n        app = FastAPI()\n\n\n        @app.middleware(\"http\")\n        async def add_process_time_header(\n            request: Request, call_next: Callable[[Request], Awaitable[Response]]\n        ) -> Response:\n            start_time = time.time()\n            response = await call_next(request)\n            process_time = time.time() - start_time\n            response.headers[\"X-Process-Time\"] = str(process_time)\n            return response\n        ```\n        \"\"\"\n\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_middleware(BaseHTTPMiddleware, dispatch=func)\n            return func\n\n        return decorator\n\n    def exception_handler(\n        self,\n        exc_class_or_status_code: Annotated[\n            Union[int, Type[Exception]],\n            Doc(\n                \"\"\"\n                The Exception class this would handle, or a status code.\n                \"\"\"\n            ),\n        ],\n    ) -> Callable[[DecoratedCallable], DecoratedCallable]:\n        \"\"\"\n        Add an exception handler to the app.\n\n        Read more about it in the\n        [FastAPI docs for Handling Errors](https://fastapi.tiangolo.com/tutorial/handling-errors/).\n\n        ## Example\n\n        ```python\n        from fastapi import FastAPI, Request\n        from fastapi.responses import JSONResponse\n\n\n        class UnicornException(Exception):\n            def __init__(self, name: str):\n                self.name = name\n\n\n        app = FastAPI()\n\n\n        @app.exception_handler(UnicornException)\n        async def unicorn_exception_handler(request: Request, exc: UnicornException):\n            return JSONResponse(\n                status_code=418,\n                content={\"message\": f\"Oops! {exc.name} did something. There goes a rainbow...\"},\n            )\n        ```\n        \"\"\"\n\n        def decorator(func: DecoratedCallable) -> DecoratedCallable:\n            self.add_exception_handler(exc_class_or_status_code, func)\n            return func\n\n        return decorator\n", 4667], "/usr/local/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py": ["from __future__ import annotations\n\nimport ipaddress\n\nfrom uvicorn._types import ASGI3Application, ASGIReceiveCallable, ASGISendCallable, Scope\n\n\nclass ProxyHeadersMiddleware:\n    \"\"\"Middleware for handling known proxy headers\n\n    This middleware can be used when a known proxy is fronting the application,\n    and is trusted to be properly setting the `X-Forwarded-Proto` and\n    `X-Forwarded-For` headers with the connecting client information.\n\n    Modifies the `client` and `scheme` information so that they reference\n    the connecting client, rather that the connecting proxy.\n\n    References:\n    - <https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers#Proxies>\n    - <https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For>\n    \"\"\"\n\n    def __init__(self, app: ASGI3Application, trusted_hosts: list[str] | str = \"127.0.0.1\") -> None:\n        self.app = app\n        self.trusted_hosts = _TrustedHosts(trusted_hosts)\n\n    async def __call__(self, scope: Scope, receive: ASGIReceiveCallable, send: ASGISendCallable) -> None:\n        if scope[\"type\"] == \"lifespan\":\n            return await self.app(scope, receive, send)\n\n        client_addr = scope.get(\"client\")\n        client_host = client_addr[0] if client_addr else None\n\n        if client_host in self.trusted_hosts:\n            headers = dict(scope[\"headers\"])\n\n            if b\"x-forwarded-proto\" in headers:\n                x_forwarded_proto = headers[b\"x-forwarded-proto\"].decode(\"latin1\").strip()\n\n                if x_forwarded_proto in {\"http\", \"https\", \"ws\", \"wss\"}:\n                    if scope[\"type\"] == \"websocket\":\n                        scope[\"scheme\"] = x_forwarded_proto.replace(\"http\", \"ws\")\n                    else:\n                        scope[\"scheme\"] = x_forwarded_proto\n\n            if b\"x-forwarded-for\" in headers:\n                x_forwarded_for = headers[b\"x-forwarded-for\"].decode(\"latin1\")\n                host = self.trusted_hosts.get_trusted_client_host(x_forwarded_for)\n\n                if host:\n                    # If the x-forwarded-for header is empty then host is an empty string.\n                    # Only set the client if we actually got something usable.\n                    # See: https://github.com/Kludex/uvicorn/issues/1068\n\n                    # We've lost the connecting client's port information by now,\n                    # so only include the host.\n                    port = 0\n                    scope[\"client\"] = (host, port)\n\n        return await self.app(scope, receive, send)\n\n\ndef _parse_raw_hosts(value: str) -> list[str]:\n    return [item.strip() for item in value.split(\",\")]\n\n\nclass _TrustedHosts:\n    \"\"\"Container for trusted hosts and networks\"\"\"\n\n    def __init__(self, trusted_hosts: list[str] | str) -> None:\n        self.always_trust: bool = trusted_hosts in (\"*\", [\"*\"])\n\n        self.trusted_literals: set[str] = set()\n        self.trusted_hosts: set[ipaddress.IPv4Address | ipaddress.IPv6Address] = set()\n        self.trusted_networks: set[ipaddress.IPv4Network | ipaddress.IPv6Network] = set()\n\n        # Notes:\n        # - We separate hosts from literals as there are many ways to write\n        #   an IPv6 Address so we need to compare by object.\n        # - We don't convert IP Address to single host networks (e.g. /32 / 128) as\n        #   it more efficient to do an address lookup in a set than check for\n        #   membership in each network.\n        # - We still allow literals as it might be possible that we receive a\n        #   something that isn't an IP Address e.g. a unix socket.\n\n        if not self.always_trust:\n            if isinstance(trusted_hosts, str):\n                trusted_hosts = _parse_raw_hosts(trusted_hosts)\n\n            for host in trusted_hosts:\n                # Note: because we always convert invalid IP types to literals it\n                # is not possible for the user to know they provided a malformed IP\n                # type - this may lead to unexpected / difficult to debug behaviour.\n\n                if \"/\" in host:\n                    # Looks like a network\n                    try:\n                        self.trusted_networks.add(ipaddress.ip_network(host))\n                    except ValueError:\n                        # Was not a valid IP Network\n                        self.trusted_literals.add(host)\n                else:\n                    try:\n                        self.trusted_hosts.add(ipaddress.ip_address(host))\n                    except ValueError:\n                        # Was not a valid IP Address\n                        self.trusted_literals.add(host)\n\n    def __contains__(self, host: str | None) -> bool:\n        if self.always_trust:\n            return True\n\n        if not host:\n            return False\n\n        try:\n            ip = ipaddress.ip_address(host)\n            if ip in self.trusted_hosts:\n                return True\n            return any(ip in net for net in self.trusted_networks)\n\n        except ValueError:\n            return host in self.trusted_literals\n\n    def get_trusted_client_host(self, x_forwarded_for: str) -> str:\n        \"\"\"Extract the client host from x_forwarded_for header\n\n        In general this is the first \"untrusted\" host in the forwarded for list.\n        \"\"\"\n        x_forwarded_for_hosts = _parse_raw_hosts(x_forwarded_for)\n\n        if self.always_trust:\n            return x_forwarded_for_hosts[0]\n\n        # Note: each proxy appends to the header list so check it in reverse order\n        for host in reversed(x_forwarded_for_hosts):\n            if host not in self:\n                return host\n\n        # All hosts are trusted meaning that the client was also a trusted proxy\n        # See https://github.com/Kludex/uvicorn/issues/1068#issuecomment-855371576\n        return x_forwarded_for_hosts[0]\n", 142], "/usr/local/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py": ["from __future__ import annotations\n\nimport asyncio\nimport http\nimport logging\nimport re\nimport urllib\nfrom asyncio.events import TimerHandle\nfrom collections import deque\nfrom typing import Any, Callable, Literal, cast\n\nimport httptools\n\nfrom uvicorn._types import (\n    ASGI3Application,\n    ASGIReceiveEvent,\n    ASGISendEvent,\n    HTTPRequestEvent,\n    HTTPResponseStartEvent,\n    HTTPScope,\n)\nfrom uvicorn.config import Config\nfrom uvicorn.logging import TRACE_LOG_LEVEL\nfrom uvicorn.protocols.http.flow_control import CLOSE_HEADER, HIGH_WATER_LIMIT, FlowControl, service_unavailable\nfrom uvicorn.protocols.utils import get_client_addr, get_local_addr, get_path_with_query_string, get_remote_addr, is_ssl\nfrom uvicorn.server import ServerState\n\nHEADER_RE = re.compile(b'[\\x00-\\x1f\\x7f()<>@,;:[]={} \\t\\\\\"]')\nHEADER_VALUE_RE = re.compile(b\"[\\x00-\\x08\\x0a-\\x1f\\x7f]\")\n\n\ndef _get_status_line(status_code: int) -> bytes:\n    try:\n        phrase = http.HTTPStatus(status_code).phrase.encode()\n    except ValueError:\n        phrase = b\"\"\n    return b\"\".join([b\"HTTP/1.1 \", str(status_code).encode(), b\" \", phrase, b\"\\r\\n\"])\n\n\nSTATUS_LINE = {status_code: _get_status_line(status_code) for status_code in range(100, 600)}\n\n\nclass HttpToolsProtocol(asyncio.Protocol):\n    def __init__(\n        self,\n        config: Config,\n        server_state: ServerState,\n        app_state: dict[str, Any],\n        _loop: asyncio.AbstractEventLoop | None = None,\n    ) -> None:\n        if not config.loaded:\n            config.load()\n\n        self.config = config\n        self.app = config.loaded_app\n        self.loop = _loop or asyncio.get_event_loop()\n        self.logger = logging.getLogger(\"uvicorn.error\")\n        self.access_logger = logging.getLogger(\"uvicorn.access\")\n        self.access_log = self.access_logger.hasHandlers()\n        self.parser = httptools.HttpRequestParser(self)\n\n        try:\n            # Enable dangerous leniencies to allow server to a response on the first request from a pipelined request.\n            self.parser.set_dangerous_leniencies(lenient_data_after_close=True)\n        except AttributeError:  # pragma: no cover\n            # httptools < 0.6.3\n            pass\n\n        self.ws_protocol_class = config.ws_protocol_class\n        self.root_path = config.root_path\n        self.limit_concurrency = config.limit_concurrency\n        self.app_state = app_state\n\n        # Timeouts\n        self.timeout_keep_alive_task: TimerHandle | None = None\n        self.timeout_keep_alive = config.timeout_keep_alive\n\n        # Global state\n        self.server_state = server_state\n        self.connections = server_state.connections\n        self.tasks = server_state.tasks\n\n        # Per-connection state\n        self.transport: asyncio.Transport = None  # type: ignore[assignment]\n        self.flow: FlowControl = None  # type: ignore[assignment]\n        self.server: tuple[str, int] | None = None\n        self.client: tuple[str, int] | None = None\n        self.scheme: Literal[\"http\", \"https\"] | None = None\n        self.pipeline: deque[tuple[RequestResponseCycle, ASGI3Application]] = deque()\n\n        # Per-request state\n        self.scope: HTTPScope = None  # type: ignore[assignment]\n        self.headers: list[tuple[bytes, bytes]] = None  # type: ignore[assignment]\n        self.expect_100_continue = False\n        self.cycle: RequestResponseCycle = None  # type: ignore[assignment]\n\n    # Protocol interface\n    def connection_made(  # type: ignore[override]\n        self, transport: asyncio.Transport\n    ) -> None:\n        self.connections.add(self)\n\n        self.transport = transport\n        self.flow = FlowControl(transport)\n        self.server = get_local_addr(transport)\n        self.client = get_remote_addr(transport)\n        self.scheme = \"https\" if is_ssl(transport) else \"http\"\n\n        if self.logger.level <= TRACE_LOG_LEVEL:\n            prefix = \"%s:%d - \" % self.client if self.client else \"\"\n            self.logger.log(TRACE_LOG_LEVEL, \"%sHTTP connection made\", prefix)\n\n    def connection_lost(self, exc: Exception | None) -> None:\n        self.connections.discard(self)\n\n        if self.logger.level <= TRACE_LOG_LEVEL:\n            prefix = \"%s:%d - \" % self.client if self.client else \"\"\n            self.logger.log(TRACE_LOG_LEVEL, \"%sHTTP connection lost\", prefix)\n\n        if self.cycle and not self.cycle.response_complete:\n            self.cycle.disconnected = True\n        if self.cycle is not None:\n            self.cycle.message_event.set()\n        if self.flow is not None:\n            self.flow.resume_writing()\n        if exc is None:\n            self.transport.close()\n            self._unset_keepalive_if_required()\n\n        self.parser = None\n\n    def eof_received(self) -> None:\n        pass\n\n    def _unset_keepalive_if_required(self) -> None:\n        if self.timeout_keep_alive_task is not None:\n            self.timeout_keep_alive_task.cancel()\n            self.timeout_keep_alive_task = None\n\n    def _get_upgrade(self) -> bytes | None:\n        connection = []\n        upgrade = None\n        for name, value in self.headers:\n            if name == b\"connection\":\n                connection = [token.lower().strip() for token in value.split(b\",\")]\n            if name == b\"upgrade\":\n                upgrade = value.lower()\n        if b\"upgrade\" in connection:\n            return upgrade\n        return None  # pragma: full coverage\n\n    def _should_upgrade_to_ws(self) -> bool:\n        if self.ws_protocol_class is None:\n            return False\n        return True\n\n    def _unsupported_upgrade_warning(self) -> None:\n        self.logger.warning(\"Unsupported upgrade request.\")\n        if not self._should_upgrade_to_ws():\n            msg = \"No supported WebSocket library detected. Please use \\\"pip install 'uvicorn[standard]'\\\", or install 'websockets' or 'wsproto' manually.\"  # noqa: E501\n            self.logger.warning(msg)\n\n    def _should_upgrade(self) -> bool:\n        upgrade = self._get_upgrade()\n        return upgrade == b\"websocket\" and self._should_upgrade_to_ws()\n\n    def data_received(self, data: bytes) -> None:\n        self._unset_keepalive_if_required()\n\n        try:\n            self.parser.feed_data(data)\n        except httptools.HttpParserError:\n            msg = \"Invalid HTTP request received.\"\n            self.logger.warning(msg)\n            self.send_400_response(msg)\n            return\n        except httptools.HttpParserUpgrade:\n            if self._should_upgrade():\n                self.handle_websocket_upgrade()\n            else:\n                self._unsupported_upgrade_warning()\n\n    def handle_websocket_upgrade(self) -> None:\n        if self.logger.level <= TRACE_LOG_LEVEL:\n            prefix = \"%s:%d - \" % self.client if self.client else \"\"\n            self.logger.log(TRACE_LOG_LEVEL, \"%sUpgrading to WebSocket\", prefix)\n\n        self.connections.discard(self)\n        method = self.scope[\"method\"].encode()\n        output = [method, b\" \", self.url, b\" HTTP/1.1\\r\\n\"]\n        for name, value in self.scope[\"headers\"]:\n            output += [name, b\": \", value, b\"\\r\\n\"]\n        output.append(b\"\\r\\n\")\n        protocol = self.ws_protocol_class(  # type: ignore[call-arg, misc]\n            config=self.config,\n            server_state=self.server_state,\n            app_state=self.app_state,\n        )\n        protocol.connection_made(self.transport)\n        protocol.data_received(b\"\".join(output))\n        self.transport.set_protocol(protocol)\n\n    def send_400_response(self, msg: str) -> None:\n        content = [STATUS_LINE[400]]\n        for name, value in self.server_state.default_headers:\n            content.extend([name, b\": \", value, b\"\\r\\n\"])  # pragma: full coverage\n        content.extend(\n            [\n                b\"content-type: text/plain; charset=utf-8\\r\\n\",\n                b\"content-length: \" + str(len(msg)).encode(\"ascii\") + b\"\\r\\n\",\n                b\"connection: close\\r\\n\",\n                b\"\\r\\n\",\n                msg.encode(\"ascii\"),\n            ]\n        )\n        self.transport.write(b\"\".join(content))\n        self.transport.close()\n\n    def on_message_begin(self) -> None:\n        self.url = b\"\"\n        self.expect_100_continue = False\n        self.headers = []\n        self.scope = {  # type: ignore[typeddict-item]\n            \"type\": \"http\",\n            \"asgi\": {\"version\": self.config.asgi_version, \"spec_version\": \"2.3\"},\n            \"http_version\": \"1.1\",\n            \"server\": self.server,\n            \"client\": self.client,\n            \"scheme\": self.scheme,  # type: ignore[typeddict-item]\n            \"root_path\": self.root_path,\n            \"headers\": self.headers,\n            \"state\": self.app_state.copy(),\n        }\n\n    # Parser callbacks\n    def on_url(self, url: bytes) -> None:\n        self.url += url\n\n    def on_header(self, name: bytes, value: bytes) -> None:\n        name = name.lower()\n        if name == b\"expect\" and value.lower() == b\"100-continue\":\n            self.expect_100_continue = True\n        self.headers.append((name, value))\n\n    def on_headers_complete(self) -> None:\n        http_version = self.parser.get_http_version()\n        method = self.parser.get_method()\n        self.scope[\"method\"] = method.decode(\"ascii\")\n        if http_version != \"1.1\":\n            self.scope[\"http_version\"] = http_version\n        if self.parser.should_upgrade() and self._should_upgrade():\n            return\n        parsed_url = httptools.parse_url(self.url)\n        raw_path = parsed_url.path\n        path = raw_path.decode(\"ascii\")\n        if \"%\" in path:\n            path = urllib.parse.unquote(path)\n        full_path = self.root_path + path\n        full_raw_path = self.root_path.encode(\"ascii\") + raw_path\n        self.scope[\"path\"] = full_path\n        self.scope[\"raw_path\"] = full_raw_path\n        self.scope[\"query_string\"] = parsed_url.query or b\"\"\n\n        # Handle 503 responses when 'limit_concurrency' is exceeded.\n        if self.limit_concurrency is not None and (\n            len(self.connections) >= self.limit_concurrency or len(self.tasks) >= self.limit_concurrency\n        ):\n            app = service_unavailable\n            message = \"Exceeded concurrency limit.\"\n            self.logger.warning(message)\n        else:\n            app = self.app\n\n        existing_cycle = self.cycle\n        self.cycle = RequestResponseCycle(\n            scope=self.scope,\n            transport=self.transport,\n            flow=self.flow,\n            logger=self.logger,\n            access_logger=self.access_logger,\n            access_log=self.access_log,\n            default_headers=self.server_state.default_headers,\n            message_event=asyncio.Event(),\n            expect_100_continue=self.expect_100_continue,\n            keep_alive=http_version != \"1.0\",\n            on_response=self.on_response_complete,\n        )\n        if existing_cycle is None or existing_cycle.response_complete:\n            # Standard case - start processing the request.\n            task = self.loop.create_task(self.cycle.run_asgi(app))\n            task.add_done_callback(self.tasks.discard)\n            self.tasks.add(task)\n        else:\n            # Pipelined HTTP requests need to be queued up.\n            self.flow.pause_reading()\n            self.pipeline.appendleft((self.cycle, app))\n\n    def on_body(self, body: bytes) -> None:\n        if (self.parser.should_upgrade() and self._should_upgrade()) or self.cycle.response_complete:\n            return\n        self.cycle.body += body\n        if len(self.cycle.body) > HIGH_WATER_LIMIT:\n            self.flow.pause_reading()\n        self.cycle.message_event.set()\n\n    def on_message_complete(self) -> None:\n        if (self.parser.should_upgrade() and self._should_upgrade()) or self.cycle.response_complete:\n            return\n        self.cycle.more_body = False\n        self.cycle.message_event.set()\n\n    def on_response_complete(self) -> None:\n        # Callback for pipelined HTTP requests to be started.\n        self.server_state.total_requests += 1\n\n        if self.transport.is_closing():\n            return\n\n        self._unset_keepalive_if_required()\n\n        # Unpause data reads if needed.\n        self.flow.resume_reading()\n\n        # Unblock any pipelined events. If there are none, arm the\n        # Keep-Alive timeout instead.\n        if self.pipeline:\n            cycle, app = self.pipeline.pop()\n            task = self.loop.create_task(cycle.run_asgi(app))\n            task.add_done_callback(self.tasks.discard)\n            self.tasks.add(task)\n        else:\n            self.timeout_keep_alive_task = self.loop.call_later(\n                self.timeout_keep_alive, self.timeout_keep_alive_handler\n            )\n\n    def shutdown(self) -> None:\n        \"\"\"\n        Called by the server to commence a graceful shutdown.\n        \"\"\"\n        if self.cycle is None or self.cycle.response_complete:\n            self.transport.close()\n        else:\n            self.cycle.keep_alive = False\n\n    def pause_writing(self) -> None:\n        \"\"\"\n        Called by the transport when the write buffer exceeds the high water mark.\n        \"\"\"\n        self.flow.pause_writing()  # pragma: full coverage\n\n    def resume_writing(self) -> None:\n        \"\"\"\n        Called by the transport when the write buffer drops below the low water mark.\n        \"\"\"\n        self.flow.resume_writing()  # pragma: full coverage\n\n    def timeout_keep_alive_handler(self) -> None:\n        \"\"\"\n        Called on a keep-alive connection if no new data is received after a short\n        delay.\n        \"\"\"\n        if not self.transport.is_closing():\n            self.transport.close()\n\n\nclass RequestResponseCycle:\n    def __init__(\n        self,\n        scope: HTTPScope,\n        transport: asyncio.Transport,\n        flow: FlowControl,\n        logger: logging.Logger,\n        access_logger: logging.Logger,\n        access_log: bool,\n        default_headers: list[tuple[bytes, bytes]],\n        message_event: asyncio.Event,\n        expect_100_continue: bool,\n        keep_alive: bool,\n        on_response: Callable[..., None],\n    ):\n        self.scope = scope\n        self.transport = transport\n        self.flow = flow\n        self.logger = logger\n        self.access_logger = access_logger\n        self.access_log = access_log\n        self.default_headers = default_headers\n        self.message_event = message_event\n        self.on_response = on_response\n\n        # Connection state\n        self.disconnected = False\n        self.keep_alive = keep_alive\n        self.waiting_for_100_continue = expect_100_continue\n\n        # Request state\n        self.body = b\"\"\n        self.more_body = True\n\n        # Response state\n        self.response_started = False\n        self.response_complete = False\n        self.chunked_encoding: bool | None = None\n        self.expected_content_length = 0\n\n    # ASGI exception wrapper\n    async def run_asgi(self, app: ASGI3Application) -> None:\n        try:\n            result = await app(  # type: ignore[func-returns-value]\n                self.scope, self.receive, self.send\n            )\n        except BaseException as exc:\n            msg = \"Exception in ASGI application\\n\"\n            self.logger.error(msg, exc_info=exc)\n            if not self.response_started:\n                await self.send_500_response()\n            else:\n                self.transport.close()\n        else:\n            if result is not None:\n                msg = \"ASGI callable should return None, but returned '%s'.\"\n                self.logger.error(msg, result)\n                self.transport.close()\n            elif not self.response_started and not self.disconnected:\n                msg = \"ASGI callable returned without starting response.\"\n                self.logger.error(msg)\n                await self.send_500_response()\n            elif not self.response_complete and not self.disconnected:\n                msg = \"ASGI callable returned without completing response.\"\n                self.logger.error(msg)\n                self.transport.close()\n        finally:\n            self.on_response = lambda: None\n\n    async def send_500_response(self) -> None:\n        await self.send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": 500,\n                \"headers\": [\n                    (b\"content-type\", b\"text/plain; charset=utf-8\"),\n                    (b\"content-length\", b\"21\"),\n                    (b\"connection\", b\"close\"),\n                ],\n            }\n        )\n        await self.send({\"type\": \"http.response.body\", \"body\": b\"Internal Server Error\", \"more_body\": False})\n\n    # ASGI interface\n    async def send(self, message: ASGISendEvent) -> None:\n        message_type = message[\"type\"]\n\n        if self.flow.write_paused and not self.disconnected:\n            await self.flow.drain()  # pragma: full coverage\n\n        if self.disconnected:\n            return  # pragma: full coverage\n\n        if not self.response_started:\n            # Sending response status line and headers\n            if message_type != \"http.response.start\":\n                msg = \"Expected ASGI message 'http.response.start', but got '%s'.\"\n                raise RuntimeError(msg % message_type)\n            message = cast(\"HTTPResponseStartEvent\", message)\n\n            self.response_started = True\n            self.waiting_for_100_continue = False\n\n            status_code = message[\"status\"]\n            headers = self.default_headers + list(message.get(\"headers\", []))\n\n            if CLOSE_HEADER in self.scope[\"headers\"] and CLOSE_HEADER not in headers:\n                headers = headers + [CLOSE_HEADER]\n\n            if self.access_log:\n                self.access_logger.info(\n                    '%s - \"%s %s HTTP/%s\" %d',\n                    get_client_addr(self.scope),\n                    self.scope[\"method\"],\n                    get_path_with_query_string(self.scope),\n                    self.scope[\"http_version\"],\n                    status_code,\n                )\n\n            # Write response status line and headers\n            content = [STATUS_LINE[status_code]]\n\n            for name, value in headers:\n                if HEADER_RE.search(name):\n                    raise RuntimeError(\"Invalid HTTP header name.\")  # pragma: full coverage\n                if HEADER_VALUE_RE.search(value):\n                    raise RuntimeError(\"Invalid HTTP header value.\")\n\n                name = name.lower()\n                if name == b\"content-length\" and self.chunked_encoding is None:\n                    self.expected_content_length = int(value.decode())\n                    self.chunked_encoding = False\n                elif name == b\"transfer-encoding\" and value.lower() == b\"chunked\":\n                    self.expected_content_length = 0\n                    self.chunked_encoding = True\n                elif name == b\"connection\" and value.lower() == b\"close\":\n                    self.keep_alive = False\n                content.extend([name, b\": \", value, b\"\\r\\n\"])\n\n            if self.chunked_encoding is None and self.scope[\"method\"] != \"HEAD\" and status_code not in (204, 304):\n                # Neither content-length nor transfer-encoding specified\n                self.chunked_encoding = True\n                content.append(b\"transfer-encoding: chunked\\r\\n\")\n\n            content.append(b\"\\r\\n\")\n            self.transport.write(b\"\".join(content))\n\n        elif not self.response_complete:\n            # Sending response body\n            if message_type != \"http.response.body\":\n                msg = \"Expected ASGI message 'http.response.body', but got '%s'.\"\n                raise RuntimeError(msg % message_type)\n\n            body = cast(bytes, message.get(\"body\", b\"\"))\n            more_body = message.get(\"more_body\", False)\n\n            # Write response body\n            if self.scope[\"method\"] == \"HEAD\":\n                self.expected_content_length = 0\n            elif self.chunked_encoding:\n                if body:\n                    content = [b\"%x\\r\\n\" % len(body), body, b\"\\r\\n\"]\n                else:\n                    content = []\n                if not more_body:\n                    content.append(b\"0\\r\\n\\r\\n\")\n                self.transport.write(b\"\".join(content))\n            else:\n                num_bytes = len(body)\n                if num_bytes > self.expected_content_length:\n                    raise RuntimeError(\"Response content longer than Content-Length\")\n                else:\n                    self.expected_content_length -= num_bytes\n                self.transport.write(body)\n\n            # Handle response completion\n            if not more_body:\n                if self.expected_content_length != 0:\n                    raise RuntimeError(\"Response content shorter than Content-Length\")\n                self.response_complete = True\n                self.message_event.set()\n                if not self.keep_alive:\n                    self.transport.close()\n                self.on_response()\n\n        else:\n            # Response already sent\n            msg = \"Unexpected ASGI message '%s' sent, after response already completed.\"\n            raise RuntimeError(msg % message_type)\n\n    async def receive(self) -> ASGIReceiveEvent:\n        if self.waiting_for_100_continue and not self.transport.is_closing():\n            self.transport.write(b\"HTTP/1.1 100 Continue\\r\\n\\r\\n\")\n            self.waiting_for_100_continue = False\n\n        if not self.disconnected and not self.response_complete:\n            self.flow.resume_reading()\n            await self.message_event.wait()\n            self.message_event.clear()\n\n        if self.disconnected or self.response_complete:\n            return {\"type\": \"http.disconnect\"}\n        message: HTTPRequestEvent = {\"type\": \"http.request\", \"body\": self.body, \"more_body\": self.more_body}\n        self.body = b\"\"\n        return message\n", 570]}, "functions": {"sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)": ["/usr/local/lib/python3.11/asyncio/tasks.py", 637], "FinnhubProvider.get_company_profile (/app/app/providers/finnhub_provider.py:124)": ["/app/app/providers/finnhub_provider.py", 124], "MarketDataService.get_company_profile (/app/app/services/market_data.py:280)": ["/app/app/services/market_data.py", 280], "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)": ["/usr/local/lib/python3.11/asyncio/futures.py", 311], "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)": ["/usr/local/lib/python3.11/site-packages/uvicorn/server.py", 233], "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)": ["/usr/local/lib/python3.11/site-packages/uvicorn/server.py", 224], "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)": ["/usr/local/lib/python3.11/site-packages/uvicorn/server.py", 73], "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)": ["/usr/local/lib/python3.11/site-packages/uvicorn/server.py", 69], "MarketDataService._real_time_broadcast_task (/app/app/services/market_data.py:246)": ["/app/app/services/market_data.py", 246], "ListCommands.llen (/usr/local/lib/python3.11/site-packages/redis/commands/core.py:2665)": ["/usr/local/lib/python3.11/site-packages/redis/commands/core.py", 2665], "Redis.initialize (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:399)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py", 399], "deprecated_args.<locals>.decorator.<locals>.wrapper (/usr/local/lib/python3.11/site-packages/redis/utils.py:169)": ["/usr/local/lib/python3.11/site-packages/redis/utils.py", 169], "Lock.acquire.<locals>.<genexpr> (/usr/local/lib/python3.11/asyncio/locks.py:100)": ["/usr/local/lib/python3.11/asyncio/locks.py", 100], "Lock.acquire (/usr/local/lib/python3.11/asyncio/locks.py:93)": ["/usr/local/lib/python3.11/asyncio/locks.py", 93], "_ContextManagerMixin.__aenter__ (/usr/local/lib/python3.11/asyncio/locks.py:14)": ["/usr/local/lib/python3.11/asyncio/locks.py", 14], "ConnectionPool.get_available_connection (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1156)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 1156], "AbstractConnection.is_connected (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:259)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 259], "AbstractConnection.connect_check_health (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:298)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 298], "AbstractConnection.connect (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:294)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 294], "Timeout.__init__ (/usr/local/lib/python3.11/asyncio/timeouts.py:33)": ["/usr/local/lib/python3.11/asyncio/timeouts.py", 33], "timeout (/usr/local/lib/python3.11/asyncio/timeouts.py:129)": ["/usr/local/lib/python3.11/asyncio/timeouts.py", 129], "current_task (/usr/local/lib/python3.11/asyncio/tasks.py:35)": ["/usr/local/lib/python3.11/asyncio/tasks.py", 35], "Timeout.reschedule (/usr/local/lib/python3.11/asyncio/timeouts.py:50)": ["/usr/local/lib/python3.11/asyncio/timeouts.py", 50], "Timeout.__aenter__ (/usr/local/lib/python3.11/asyncio/timeouts.py:85)": ["/usr/local/lib/python3.11/asyncio/timeouts.py", 85], "StreamReader.at_eof (/usr/local/lib/python3.11/asyncio/streams.py:493)": ["/usr/local/lib/python3.11/asyncio/streams.py", 493], "Timeout.__aexit__ (/usr/local/lib/python3.11/asyncio/timeouts.py:97)": ["/usr/local/lib/python3.11/asyncio/timeouts.py", 97], "_AsyncRESPBase.can_read_destructive (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:242)": ["/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py", 242], "AbstractConnection.can_read_destructive (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:563)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 563], "ConnectionPool.ensure_connection (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1180)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 1180], "Lock._wake_up_first (/usr/local/lib/python3.11/asyncio/locks.py:142)": ["/usr/local/lib/python3.11/asyncio/locks.py", 142], "Lock.release (/usr/local/lib/python3.11/asyncio/locks.py:125)": ["/usr/local/lib/python3.11/asyncio/locks.py", 125], "_ContextManagerMixin.__aexit__ (/usr/local/lib/python3.11/asyncio/locks.py:20)": ["/usr/local/lib/python3.11/asyncio/locks.py", 20], "ConnectionPool.get_connection (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1139)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 1139], "AbstractBackoff.reset (/usr/local/lib/python3.11/site-packages/redis/backoff.py:13)": ["/usr/local/lib/python3.11/site-packages/redis/backoff.py", 13], "Redis.execute_command.<locals>.<lambda> (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:678)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py", 678], "Encoder.encode (/usr/local/lib/python3.11/site-packages/redis/_parsers/encoders.py:14)": ["/usr/local/lib/python3.11/site-packages/redis/_parsers/encoders.py", 14], "AbstractConnection.pack_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:630)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 630], "AbstractConnection.check_health (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:504)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 504], "StreamWriter.writelines (/usr/local/lib/python3.11/asyncio/streams.py:348)": ["/usr/local/lib/python3.11/asyncio/streams.py", 348], "StreamReader.exception (/usr/local/lib/python3.11/asyncio/streams.py:460)": ["/usr/local/lib/python3.11/asyncio/streams.py", 460], "FlowControlMixin._drain_helper (/usr/local/lib/python3.11/asyncio/streams.py:164)": ["/usr/local/lib/python3.11/asyncio/streams.py", 164], "StreamWriter.drain (/usr/local/lib/python3.11/asyncio/streams.py:369)": ["/usr/local/lib/python3.11/asyncio/streams.py", 369], "AbstractConnection.send_packed_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:516)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 516], "AbstractConnection.send_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:557)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 557], "Connection._host_error (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:781)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 781], "StreamReader._wait_for_data (/usr/local/lib/python3.11/asyncio/streams.py:519)": ["/usr/local/lib/python3.11/asyncio/streams.py", 519], "StreamReader.readuntil (/usr/local/lib/python3.11/asyncio/streams.py:578)": ["/usr/local/lib/python3.11/asyncio/streams.py", 578], "StreamReader.readline (/usr/local/lib/python3.11/asyncio/streams.py:547)": ["/usr/local/lib/python3.11/asyncio/streams.py", 547], "_AsyncRESPBase._readline (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:273)": ["/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py", 273], "_AsyncRESP2Parser._read_response (/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py:87)": ["/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py", 87], "_AsyncRESP2Parser.read_response (/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py:74)": ["/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py", 74], "AbstractConnection.read_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:572)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 572], "Redis.parse_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:689)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py", 689], "Redis._send_command_parse_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:647)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py", 647], "Retry.call_with_retry (/usr/local/lib/python3.11/site-packages/redis/asyncio/retry.py:37)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/retry.py", 37], "Redis.execute_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:667)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py", 667], "CollectorJobs.backfill_depth (/app/app/services/data_collector.py:97)": ["/app/app/services/data_collector.py", 97], "DataCollectorService.metrics_reporter (/app/app/services/data_collector.py:447)": ["/app/app/services/data_collector.py", 447], "StreamReaderProtocol._stream_reader (/usr/local/lib/python3.11/asyncio/streams.py:211)": ["/usr/local/lib/python3.11/asyncio/streams.py", 211], "StreamReader._wakeup_waiter (/usr/local/lib/python3.11/asyncio/streams.py:472)": ["/usr/local/lib/python3.11/asyncio/streams.py", 472], "StreamReader.feed_data (/usr/local/lib/python3.11/asyncio/streams.py:497)": ["/usr/local/lib/python3.11/asyncio/streams.py", 497], "StreamReaderProtocol.data_received (/usr/local/lib/python3.11/asyncio/streams.py:284)": ["/usr/local/lib/python3.11/asyncio/streams.py", 284], "StreamReader._maybe_resume_transport (/usr/local/lib/python3.11/asyncio/streams.py:484)": ["/usr/local/lib/python3.11/asyncio/streams.py", 484], "_AsyncRESPBase._clear (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:225)": ["/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py", 225], "CaseInsensitiveDict.__contains__ (/usr/local/lib/python3.11/site-packages/redis/client.py:88)": ["/usr/local/lib/python3.11/site-packages/redis/client.py", 88], "AfterConnectionReleasedEvent.__init__ (/usr/local/lib/python3.11/site-packages/redis/event.py:98)": ["/usr/local/lib/python3.11/site-packages/redis/event.py", 98], "AfterConnectionReleasedEvent.connection (/usr/local/lib/python3.11/site-packages/redis/event.py:101)": ["/usr/local/lib/python3.11/site-packages/redis/event.py", 101], "AbstractConnection.re_auth (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:717)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 717], "AsyncReAuthConnectionListener.listen (/usr/local/lib/python3.11/site-packages/redis/event.py:243)": ["/usr/local/lib/python3.11/site-packages/redis/event.py", 243], "EventDispatcher.dispatch_async (/usr/local/lib/python3.11/site-packages/redis/event.py:86)": ["/usr/local/lib/python3.11/site-packages/redis/event.py", 86], "ConnectionPool.release (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1196)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 1196], "MetricWrapperBase._is_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:67)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py", 67], "MetricWrapperBase._raise_if_not_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:73)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py", 73], "MutexValue.set (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:22)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/values.py", 22], "Gauge.set (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:409)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py", 409], "_format_timetuple_and_zone (/usr/local/lib/python3.11/email/utils.py:233)": ["/usr/local/lib/python3.11/email/utils.py", 233], "format_datetime (/usr/local/lib/python3.11/email/utils.py:271)": ["/usr/local/lib/python3.11/email/utils.py", 271], "formatdate (/usr/local/lib/python3.11/email/utils.py:242)": ["/usr/local/lib/python3.11/email/utils.py", 242], "ABCMeta.__instancecheck__ (<frozen abc>:117)": ["<frozen abc>", 117], "iscoroutine (/usr/local/lib/python3.11/asyncio/coroutines.py:34)": ["/usr/local/lib/python3.11/asyncio/coroutines.py", 34], "ismethod (/usr/local/lib/python3.11/inspect.py:300)": ["/usr/local/lib/python3.11/inspect.py", 300], "_unwrap_partial (/usr/local/lib/python3.11/functools.py:421)": ["/usr/local/lib/python3.11/functools.py", 421], "isfunction (/usr/local/lib/python3.11/inspect.py:378)": ["/usr/local/lib/python3.11/inspect.py", 378], "isclass (/usr/local/lib/python3.11/inspect.py:292)": ["/usr/local/lib/python3.11/inspect.py", 292], "_signature_is_functionlike (/usr/local/lib/python3.11/inspect.py:2075)": ["/usr/local/lib/python3.11/inspect.py", 2075], "_has_code_flag (/usr/local/lib/python3.11/inspect.py:391)": ["/usr/local/lib/python3.11/inspect.py", 391], "iscoroutinefunction (/usr/local/lib/python3.11/inspect.py:409)": ["/usr/local/lib/python3.11/inspect.py", 409], "iscoroutinefunction (/usr/local/lib/python3.11/asyncio/coroutines.py:21)": ["/usr/local/lib/python3.11/asyncio/coroutines.py", 21], "RLock (/usr/local/lib/python3.11/threading.py:90)": ["/usr/local/lib/python3.11/threading.py", 90], "Condition.__init__ (/usr/local/lib/python3.11/threading.py:243)": ["/usr/local/lib/python3.11/threading.py", 243], "Future.__init__ (/usr/local/lib/python3.11/concurrent/futures/_base.py:328)": ["/usr/local/lib/python3.11/concurrent/futures/_base.py", 328], "_WorkItem.__init__ (/usr/local/lib/python3.11/concurrent/futures/thread.py:47)": ["/usr/local/lib/python3.11/concurrent/futures/thread.py", 47], "Condition.__enter__ (/usr/local/lib/python3.11/threading.py:271)": ["/usr/local/lib/python3.11/threading.py", 271], "Condition.__exit__ (/usr/local/lib/python3.11/threading.py:274)": ["/usr/local/lib/python3.11/threading.py", 274], "Semaphore.acquire (/usr/local/lib/python3.11/threading.py:440)": ["/usr/local/lib/python3.11/threading.py", 440], "ThreadPoolExecutor._adjust_thread_count (/usr/local/lib/python3.11/concurrent/futures/thread.py:180)": ["/usr/local/lib/python3.11/concurrent/futures/thread.py", 180], "ThreadPoolExecutor.submit (/usr/local/lib/python3.11/concurrent/futures/thread.py:161)": ["/usr/local/lib/python3.11/concurrent/futures/thread.py", 161], "isfuture (/usr/local/lib/python3.11/asyncio/base_futures.py:14)": ["/usr/local/lib/python3.11/asyncio/base_futures.py", 14], "_get_loop (/usr/local/lib/python3.11/asyncio/futures.py:299)": ["/usr/local/lib/python3.11/asyncio/futures.py", 299], "Future.add_done_callback (/usr/local/lib/python3.11/concurrent/futures/_base.py:408)": ["/usr/local/lib/python3.11/concurrent/futures/_base.py", 408], "_chain_future (/usr/local/lib/python3.11/asyncio/futures.py:365)": ["/usr/local/lib/python3.11/asyncio/futures.py", 365], "wrap_future (/usr/local/lib/python3.11/asyncio/futures.py:409)": ["/usr/local/lib/python3.11/asyncio/futures.py", 409], "to_thread (/usr/local/lib/python3.11/asyncio/threads.py:12)": ["/usr/local/lib/python3.11/asyncio/threads.py", 12], "get_company_profile (/app/app/main.py:596)": ["/app/app/main.py", 596], "run_endpoint_function (/usr/local/lib/python3.11/site-packages/fastapi/routing.py:280)": ["/usr/local/lib/python3.11/site-packages/fastapi/routing.py", 280], "get_request_handler.<locals>.app (/usr/local/lib/python3.11/site-packages/fastapi/routing.py:316)": ["/usr/local/lib/python3.11/site-packages/fastapi/routing.py", 316], "request_response.<locals>.app.<locals>.app (/usr/local/lib/python3.11/site-packages/fastapi/routing.py:103)": ["/usr/local/lib/python3.11/site-packages/fastapi/routing.py", 103], "wrap_app_handling_exceptions.<locals>.wrapped_app (/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py:31)": ["/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py", 31], "request_response.<locals>.app (/usr/local/lib/python3.11/site-packages/fastapi/routing.py:100)": ["/usr/local/lib/python3.11/site-packages/fastapi/routing.py", 100], "Route.handle (/usr/local/lib/python3.11/site-packages/starlette/routing.py:281)": ["/usr/local/lib/python3.11/site-packages/starlette/routing.py", 281], "Router.app (/usr/local/lib/python3.11/site-packages/starlette/routing.py:718)": ["/usr/local/lib/python3.11/site-packages/starlette/routing.py", 718], "Router.__call__ (/usr/local/lib/python3.11/site-packages/starlette/routing.py:712)": ["/usr/local/lib/python3.11/site-packages/starlette/routing.py", 712], "AsyncExitStackMiddleware.__call__ (/usr/local/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:15)": ["/usr/local/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py", 15], "ExceptionMiddleware.__call__ (/usr/local/lib/python3.11/site-packages/starlette/middleware/exceptions.py:47)": ["/usr/local/lib/python3.11/site-packages/starlette/middleware/exceptions.py", 47], "CORSMiddleware.__call__ (/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py:75)": ["/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py", 75], "ServerErrorMiddleware.__call__ (/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py:149)": ["/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py", 149], "Starlette.__call__ (/usr/local/lib/python3.11/site-packages/starlette/applications.py:109)": ["/usr/local/lib/python3.11/site-packages/starlette/applications.py", 109], "FastAPI.__call__ (/usr/local/lib/python3.11/site-packages/fastapi/applications.py:1130)": ["/usr/local/lib/python3.11/site-packages/fastapi/applications.py", 1130], "ProxyHeadersMiddleware.__call__ (/usr/local/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py:27)": ["/usr/local/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py", 27], "RequestResponseCycle.run_asgi (/usr/local/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py:407)": ["/usr/local/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py", 407], "Future.done (/usr/local/lib/python3.11/concurrent/futures/_base.py:393)": ["/usr/local/lib/python3.11/concurrent/futures/_base.py", 393], "Future.cancelled (/usr/local/lib/python3.11/concurrent/futures/_base.py:383)": ["/usr/local/lib/python3.11/concurrent/futures/_base.py", 383], "Future.exception (/usr/local/lib/python3.11/concurrent/futures/_base.py:463)": ["/usr/local/lib/python3.11/concurrent/futures/_base.py", 463], "Future.__get_result (/usr/local/lib/python3.11/concurrent/futures/_base.py:398)": ["/usr/local/lib/python3.11/concurrent/futures/_base.py", 398], "Future.result (/usr/local/lib/python3.11/concurrent/futures/_base.py:428)": ["/usr/local/lib/python3.11/concurrent/futures/_base.py", 428], "_copy_future_state (/usr/local/lib/python3.11/asyncio/futures.py:345)": ["/usr/local/lib/python3.11/asyncio/futures.py", 345], "_chain_future.<locals>._set_state (/usr/local/lib/python3.11/asyncio/futures.py:381)": ["/usr/local/lib/python3.11/asyncio/futures.py", 381], "_chain_future.<locals>._call_check_cancel (/usr/local/lib/python3.11/asyncio/futures.py:387)": ["/usr/local/lib/python3.11/asyncio/futures.py", 387]}}}