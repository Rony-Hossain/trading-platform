{"traceEvents": [{"ph": "M", "pid": 1, "tid": 1, "name": "process_name", "args": {"name": "MainProcess"}}, {"ph": "M", "pid": 1, "tid": 1, "name": "thread_name", "args": {"name": "MainThread"}}, {"pid": 1, "tid": 1, "ts": 38293577658.32, "ph": "X", "dur": 15.584321729242317, "name": "DataCache.get (/app/app/services/cache.py:10)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577678.484, "ph": "X", "dur": 2.623076307931601, "name": "Logger.isEnabledFor (/usr/local/lib/python3.11/logging/__init__.py:1734)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577685.382, "ph": "X", "dur": 0.7132248343596664, "name": "<lambda> (/usr/local/lib/python3.11/logging/__init__.py:164)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577689.333, "ph": "X", "dur": 0.4038476792540017, "name": "normcase (<frozen posixpath>:52)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577687.998, "ph": "X", "dur": 2.0247629883254668, "name": "_is_internal_frame (/usr/local/lib/python3.11/logging/__init__.py:194)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577690.887, "ph": "X", "dur": 0.16850005769147813, "name": "normcase (<frozen posixpath>:52)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577690.598, "ph": "X", "dur": 0.5839493802619422, "name": "_is_internal_frame (/usr/local/lib/python3.11/logging/__init__.py:194)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577691.651, "ph": "X", "dur": 0.1381148013864575, "name": "normcase (<frozen posixpath>:52)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577691.42, "ph": "X", "dur": 0.8662560342958614, "name": "_is_internal_frame (/usr/local/lib/python3.11/logging/__init__.py:194)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577684.263, "ph": "X", "dur": 9.863606655815246, "name": "Logger.findCaller (/usr/local/lib/python3.11/logging/__init__.py:1561)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577704.268, "ph": "X", "dur": 2.8175419482837327, "name": "ABCMeta.__instancecheck__ (<frozen abc>:117)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577708.166, "ph": "X", "dur": 1.6993645162589728, "name": "getLevelName (/usr/local/lib/python3.11/logging/__init__.py:123)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577712.181, "ph": "X", "dur": 0.35191651393269363, "name": "_get_sep (<frozen posixpath>:41)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577711.669, "ph": "X", "dur": 2.0131613450090042, "name": "basename (<frozen posixpath>:140)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577716.456, "ph": "X", "dur": 2.654014023442167, "name": "_splitext (<frozen genericpath>:121)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577714.613, "ph": "X", "dur": 4.677119634150997, "name": "splitext (<frozen posixpath>:117)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577725.746, "ph": "X", "dur": 1.013762642176598, "name": "current_thread (/usr/local/lib/python3.11/threading.py:1453)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577727.078, "ph": "X", "dur": 1.061826593059085, "name": "Thread.name (/usr/local/lib/python3.11/threading.py:1152)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577733.177, "ph": "X", "dur": 1.4270021279248788, "name": "current_process (/usr/local/lib/python3.11/multiprocessing/process.py:37)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577735.865, "ph": "X", "dur": 1.4413660672690702, "name": "BaseProcess.name (/usr/local/lib/python3.11/multiprocessing/process.py:189)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577698.596, "ph": "X", "dur": 42.21451281416796, "name": "LogRecord.__init__ (/usr/local/lib/python3.11/logging/__init__.py:292)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577696.268, "ph": "X", "dur": 45.25966795513657, "name": "Logger.makeRecord (/usr/local/lib/python3.11/logging/__init__.py:1595)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577743.481, "ph": "X", "dur": 1.098841359830656, "name": "Filterer.filter (/usr/local/lib/python3.11/logging/__init__.py:815)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577752.032, "ph": "X", "dur": 0.9645937728830191, "name": "Filterer.filter (/usr/local/lib/python3.11/logging/__init__.py:815)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577753.885, "ph": "X", "dur": 2.4545762502401227, "name": "Handler.acquire (/usr/local/lib/python3.11/logging/__init__.py:922)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577761.082, "ph": "X", "dur": 1.771184212979931, "name": "LogRecord.getMessage (/usr/local/lib/python3.11/logging/__init__.py:368)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577767.508, "ph": "X", "dur": 1.8954875342277426, "name": "PercentStyle.usesTime (/usr/local/lib/python3.11/logging/__init__.py:432)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577764.451, "ph": "X", "dur": 5.037323036166877, "name": "Formatter.usesTime (/usr/local/lib/python3.11/logging/__init__.py:652)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577771.925, "ph": "X", "dur": 2.425848371551739, "name": "PercentStyle._format (/usr/local/lib/python3.11/logging/__init__.py:440)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577771.23, "ph": "X", "dur": 3.4639192187723538, "name": "PercentStyle.format (/usr/local/lib/python3.11/logging/__init__.py:447)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577770.238, "ph": "X", "dur": 4.612481907102134, "name": "Formatter.formatMessage (/usr/local/lib/python3.11/logging/__init__.py:658)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577760.078, "ph": "X", "dur": 15.4009052730011, "name": "Formatter.format (/usr/local/lib/python3.11/logging/__init__.py:674)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577758.105, "ph": "X", "dur": 17.7394650900766, "name": "Handler.format (/usr/local/lib/python3.11/logging/__init__.py:942)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577821.447, "ph": "X", "dur": 0.9463626191000066, "name": "Handler.acquire (/usr/local/lib/python3.11/logging/__init__.py:922)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577826.032, "ph": "X", "dur": 0.604390370867138, "name": "Handler.release (/usr/local/lib/python3.11/logging/__init__.py:929)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577821.078, "ph": "X", "dur": 5.762701973048552, "name": "StreamHandler.flush (/usr/local/lib/python3.11/logging/__init__.py:1087)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577757.168, "ph": "X", "dur": 69.82587144814299, "name": "StreamHandler.emit (/usr/local/lib/python3.11/logging/__init__.py:1098)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577827.242, "ph": "X", "dur": 0.29059354211710653, "name": "Handler.release (/usr/local/lib/python3.11/logging/__init__.py:929)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577751.615, "ph": "X", "dur": 76.07749981809961, "name": "Handler.handle (/usr/local/lib/python3.11/logging/__init__.py:965)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577745.416, "ph": "X", "dur": 84.01468122417654, "name": "Logger.callHandlers (/usr/local/lib/python3.11/logging/__init__.py:1690)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577742.114, "ph": "X", "dur": 87.62831688765182, "name": "Logger.handle (/usr/local/lib/python3.11/logging/__init__.py:1636)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577683.061, "ph": "X", "dur": 146.84531421169825, "name": "Logger._log (/usr/local/lib/python3.11/logging/__init__.py:1610)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577676.597, "ph": "X", "dur": 154.59742178391733, "name": "Logger.info (/usr/local/lib/python3.11/logging/__init__.py:1479)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577653.381, "ph": "X", "dur": 178.15372984918596, "name": "MarketDataService.get_stock_price (/app/app/services/market_data.py:134)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577834.878, "ph": "B", "name": "maybe_trace (/app/app/observability/trace.py:6)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38293577832.653, "ph": "B", "name": "_GeneratorContextManager.__exit__ (/usr/local/lib/python3.11/contextlib.py:141)", "cat": "FEE"}], "viztracer_metadata": {"version": "1.0.4", "overflow": false, "baseTimeNanoseconds": 1759980240576414830}, "file_info": {"files": {"/app/app/services/cache.py": ["from datetime import datetime, timedelta\nfrom typing import Dict, Optional\n\nclass DataCache:\n    \"\"\"Simple in-memory cache for stock data\"\"\"\n    def __init__(self, ttl_seconds: int = 5):\n        self.cache: Dict[str, Dict] = {}\n        self.ttl = ttl_seconds\n    \n    def get(self, key: str) -> Optional[Dict]:\n        if key in self.cache:\n            data, timestamp = self.cache[key]\n            if datetime.now() - timestamp < timedelta(seconds=self.ttl):\n                return data\n            else:\n                del self.cache[key]\n        return None\n    \n    def set(self, key: str, data: Dict):\n        self.cache[key] = (data, datetime.now())\n    \n    def clear_expired(self):\n        now = datetime.now()\n        expired_keys = [\n            key for key, (_, timestamp) in self.cache.items()\n            if now - timestamp >= timedelta(seconds=self.ttl)\n        ]\n        for key in expired_keys:\n            del self.cache[key]\n    \n    def stats(self) -> Dict:\n        return {\n            \"size\": len(self.cache),\n            \"ttl_seconds\": self.ttl\n        }", 34], "/usr/local/lib/python3.11/logging/__init__.py": ["# Copyright 2001-2019 by Vinay Sajip. All Rights Reserved.\n#\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose and without fee is hereby granted,\n# provided that the above copyright notice appear in all copies and that\n# both that copyright notice and this permission notice appear in\n# supporting documentation, and that the name of Vinay Sajip\n# not be used in advertising or publicity pertaining to distribution\n# of the software without specific, written prior permission.\n# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\n# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR\n# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER\n# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n\"\"\"\nLogging package for Python. Based on PEP 282 and comments thereto in\ncomp.lang.python.\n\nCopyright (C) 2001-2019 Vinay Sajip. All Rights Reserved.\n\nTo use, simply 'import logging' and log away!\n\"\"\"\n\nimport sys, os, time, io, re, traceback, warnings, weakref, collections.abc\n\nfrom types import GenericAlias\nfrom string import Template\nfrom string import Formatter as StrFormatter\n\n\n__all__ = ['BASIC_FORMAT', 'BufferingFormatter', 'CRITICAL', 'DEBUG', 'ERROR',\n           'FATAL', 'FileHandler', 'Filter', 'Formatter', 'Handler', 'INFO',\n           'LogRecord', 'Logger', 'LoggerAdapter', 'NOTSET', 'NullHandler',\n           'StreamHandler', 'WARN', 'WARNING', 'addLevelName', 'basicConfig',\n           'captureWarnings', 'critical', 'debug', 'disable', 'error',\n           'exception', 'fatal', 'getLevelName', 'getLogger', 'getLoggerClass',\n           'info', 'log', 'makeLogRecord', 'setLoggerClass', 'shutdown',\n           'warn', 'warning', 'getLogRecordFactory', 'setLogRecordFactory',\n           'lastResort', 'raiseExceptions', 'getLevelNamesMapping']\n\nimport threading\n\n__author__  = \"Vinay Sajip <vinay_sajip@red-dove.com>\"\n__status__  = \"production\"\n# The following module attributes are no longer updated.\n__version__ = \"0.5.1.2\"\n__date__    = \"07 February 2010\"\n\n#---------------------------------------------------------------------------\n#   Miscellaneous module data\n#---------------------------------------------------------------------------\n\n#\n#_startTime is used as the base when calculating the relative time of events\n#\n_startTime = time.time()\n\n#\n#raiseExceptions is used to see if exceptions during handling should be\n#propagated\n#\nraiseExceptions = True\n\n#\n# If you don't want threading information in the log, set this to zero\n#\nlogThreads = True\n\n#\n# If you don't want multiprocessing information in the log, set this to zero\n#\nlogMultiprocessing = True\n\n#\n# If you don't want process information in the log, set this to zero\n#\nlogProcesses = True\n\n#---------------------------------------------------------------------------\n#   Level related stuff\n#---------------------------------------------------------------------------\n#\n# Default levels and level names, these can be replaced with any positive set\n# of values having corresponding names. There is a pseudo-level, NOTSET, which\n# is only really there as a lower limit for user-defined levels. Handlers and\n# loggers are initialized with NOTSET so that they will log all messages, even\n# at user-defined levels.\n#\n\nCRITICAL = 50\nFATAL = CRITICAL\nERROR = 40\nWARNING = 30\nWARN = WARNING\nINFO = 20\nDEBUG = 10\nNOTSET = 0\n\n_levelToName = {\n    CRITICAL: 'CRITICAL',\n    ERROR: 'ERROR',\n    WARNING: 'WARNING',\n    INFO: 'INFO',\n    DEBUG: 'DEBUG',\n    NOTSET: 'NOTSET',\n}\n_nameToLevel = {\n    'CRITICAL': CRITICAL,\n    'FATAL': FATAL,\n    'ERROR': ERROR,\n    'WARN': WARNING,\n    'WARNING': WARNING,\n    'INFO': INFO,\n    'DEBUG': DEBUG,\n    'NOTSET': NOTSET,\n}\n\ndef getLevelNamesMapping():\n    return _nameToLevel.copy()\n\ndef getLevelName(level):\n    \"\"\"\n    Return the textual or numeric representation of logging level 'level'.\n\n    If the level is one of the predefined levels (CRITICAL, ERROR, WARNING,\n    INFO, DEBUG) then you get the corresponding string. If you have\n    associated levels with names using addLevelName then the name you have\n    associated with 'level' is returned.\n\n    If a numeric value corresponding to one of the defined levels is passed\n    in, the corresponding string representation is returned.\n\n    If a string representation of the level is passed in, the corresponding\n    numeric value is returned.\n\n    If no matching numeric or string value is passed in, the string\n    'Level %s' % level is returned.\n    \"\"\"\n    # See Issues #22386, #27937 and #29220 for why it's this way\n    result = _levelToName.get(level)\n    if result is not None:\n        return result\n    result = _nameToLevel.get(level)\n    if result is not None:\n        return result\n    return \"Level %s\" % level\n\ndef addLevelName(level, levelName):\n    \"\"\"\n    Associate 'levelName' with 'level'.\n\n    This is used when converting levels to text during message formatting.\n    \"\"\"\n    _acquireLock()\n    try:    #unlikely to cause an exception, but you never know...\n        _levelToName[level] = levelName\n        _nameToLevel[levelName] = level\n    finally:\n        _releaseLock()\n\nif hasattr(sys, \"_getframe\"):\n    currentframe = lambda: sys._getframe(1)\nelse: #pragma: no cover\n    def currentframe():\n        \"\"\"Return the frame object for the caller's stack frame.\"\"\"\n        try:\n            raise Exception\n        except Exception:\n            return sys.exc_info()[2].tb_frame.f_back\n\n#\n# _srcfile is used when walking the stack to check when we've got the first\n# caller stack frame, by skipping frames whose filename is that of this\n# module's source. It therefore should contain the filename of this module's\n# source file.\n#\n# Ordinarily we would use __file__ for this, but frozen modules don't always\n# have __file__ set, for some reason (see Issue #21736). Thus, we get the\n# filename from a handy code object from a function defined in this module.\n# (There's no particular reason for picking addLevelName.)\n#\n\n_srcfile = os.path.normcase(addLevelName.__code__.co_filename)\n\n# _srcfile is only used in conjunction with sys._getframe().\n# Setting _srcfile to None will prevent findCaller() from being called. This\n# way, you can avoid the overhead of fetching caller information.\n\n# The following is based on warnings._is_internal_frame. It makes sure that\n# frames of the import mechanism are skipped when logging at module level and\n# using a stacklevel value greater than one.\ndef _is_internal_frame(frame):\n    \"\"\"Signal whether the frame is a CPython or logging module internal.\"\"\"\n    filename = os.path.normcase(frame.f_code.co_filename)\n    return filename == _srcfile or (\n        \"importlib\" in filename and \"_bootstrap\" in filename\n    )\n\n\ndef _checkLevel(level):\n    if isinstance(level, int):\n        rv = level\n    elif str(level) == level:\n        if level not in _nameToLevel:\n            raise ValueError(\"Unknown level: %r\" % level)\n        rv = _nameToLevel[level]\n    else:\n        raise TypeError(\"Level not an integer or a valid string: %r\"\n                        % (level,))\n    return rv\n\n#---------------------------------------------------------------------------\n#   Thread-related stuff\n#---------------------------------------------------------------------------\n\n#\n#_lock is used to serialize access to shared data structures in this module.\n#This needs to be an RLock because fileConfig() creates and configures\n#Handlers, and so might arbitrary user threads. Since Handler code updates the\n#shared dictionary _handlers, it needs to acquire the lock. But if configuring,\n#the lock would already have been acquired - so we need an RLock.\n#The same argument applies to Loggers and Manager.loggerDict.\n#\n_lock = threading.RLock()\n\ndef _acquireLock():\n    \"\"\"\n    Acquire the module-level lock for serializing access to shared data.\n\n    This should be released with _releaseLock().\n    \"\"\"\n    if _lock:\n        _lock.acquire()\n\ndef _releaseLock():\n    \"\"\"\n    Release the module-level lock acquired by calling _acquireLock().\n    \"\"\"\n    if _lock:\n        _lock.release()\n\n\n# Prevent a held logging lock from blocking a child from logging.\n\nif not hasattr(os, 'register_at_fork'):  # Windows and friends.\n    def _register_at_fork_reinit_lock(instance):\n        pass  # no-op when os.register_at_fork does not exist.\nelse:\n    # A collection of instances with a _at_fork_reinit method (logging.Handler)\n    # to be called in the child after forking.  The weakref avoids us keeping\n    # discarded Handler instances alive.\n    _at_fork_reinit_lock_weakset = weakref.WeakSet()\n\n    def _register_at_fork_reinit_lock(instance):\n        _acquireLock()\n        try:\n            _at_fork_reinit_lock_weakset.add(instance)\n        finally:\n            _releaseLock()\n\n    def _after_at_fork_child_reinit_locks():\n        for handler in _at_fork_reinit_lock_weakset:\n            handler._at_fork_reinit()\n\n        # _acquireLock() was called in the parent before forking.\n        # The lock is reinitialized to unlocked state.\n        _lock._at_fork_reinit()\n\n    os.register_at_fork(before=_acquireLock,\n                        after_in_child=_after_at_fork_child_reinit_locks,\n                        after_in_parent=_releaseLock)\n\n\n#---------------------------------------------------------------------------\n#   The logging record\n#---------------------------------------------------------------------------\n\nclass LogRecord(object):\n    \"\"\"\n    A LogRecord instance represents an event being logged.\n\n    LogRecord instances are created every time something is logged. They\n    contain all the information pertinent to the event being logged. The\n    main information passed in is in msg and args, which are combined\n    using str(msg) % args to create the message field of the record. The\n    record also includes information such as when the record was created,\n    the source line where the logging call was made, and any exception\n    information to be logged.\n    \"\"\"\n    def __init__(self, name, level, pathname, lineno,\n                 msg, args, exc_info, func=None, sinfo=None, **kwargs):\n        \"\"\"\n        Initialize a logging record with interesting information.\n        \"\"\"\n        ct = time.time()\n        self.name = name\n        self.msg = msg\n        #\n        # The following statement allows passing of a dictionary as a sole\n        # argument, so that you can do something like\n        #  logging.debug(\"a %(a)d b %(b)s\", {'a':1, 'b':2})\n        # Suggested by Stefan Behnel.\n        # Note that without the test for args[0], we get a problem because\n        # during formatting, we test to see if the arg is present using\n        # 'if self.args:'. If the event being logged is e.g. 'Value is %d'\n        # and if the passed arg fails 'if self.args:' then no formatting\n        # is done. For example, logger.warning('Value is %d', 0) would log\n        # 'Value is %d' instead of 'Value is 0'.\n        # For the use case of passing a dictionary, this should not be a\n        # problem.\n        # Issue #21172: a request was made to relax the isinstance check\n        # to hasattr(args[0], '__getitem__'). However, the docs on string\n        # formatting still seem to suggest a mapping object is required.\n        # Thus, while not removing the isinstance check, it does now look\n        # for collections.abc.Mapping rather than, as before, dict.\n        if (args and len(args) == 1 and isinstance(args[0], collections.abc.Mapping)\n            and args[0]):\n            args = args[0]\n        self.args = args\n        self.levelname = getLevelName(level)\n        self.levelno = level\n        self.pathname = pathname\n        try:\n            self.filename = os.path.basename(pathname)\n            self.module = os.path.splitext(self.filename)[0]\n        except (TypeError, ValueError, AttributeError):\n            self.filename = pathname\n            self.module = \"Unknown module\"\n        self.exc_info = exc_info\n        self.exc_text = None      # used to cache the traceback text\n        self.stack_info = sinfo\n        self.lineno = lineno\n        self.funcName = func\n        self.created = ct\n        self.msecs = int((ct - int(ct)) * 1000) + 0.0  # see gh-89047\n        self.relativeCreated = (self.created - _startTime) * 1000\n        if logThreads:\n            self.thread = threading.get_ident()\n            self.threadName = threading.current_thread().name\n        else: # pragma: no cover\n            self.thread = None\n            self.threadName = None\n        if not logMultiprocessing: # pragma: no cover\n            self.processName = None\n        else:\n            self.processName = 'MainProcess'\n            mp = sys.modules.get('multiprocessing')\n            if mp is not None:\n                # Errors may occur if multiprocessing has not finished loading\n                # yet - e.g. if a custom import hook causes third-party code\n                # to run when multiprocessing calls import. See issue 8200\n                # for an example\n                try:\n                    self.processName = mp.current_process().name\n                except Exception: #pragma: no cover\n                    pass\n        if logProcesses and hasattr(os, 'getpid'):\n            self.process = os.getpid()\n        else:\n            self.process = None\n\n    def __repr__(self):\n        return '<LogRecord: %s, %s, %s, %s, \"%s\">'%(self.name, self.levelno,\n            self.pathname, self.lineno, self.msg)\n\n    def getMessage(self):\n        \"\"\"\n        Return the message for this LogRecord.\n\n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.\n        \"\"\"\n        msg = str(self.msg)\n        if self.args:\n            msg = msg % self.args\n        return msg\n\n#\n#   Determine which class to use when instantiating log records.\n#\n_logRecordFactory = LogRecord\n\ndef setLogRecordFactory(factory):\n    \"\"\"\n    Set the factory to be used when instantiating a log record.\n\n    :param factory: A callable which will be called to instantiate\n    a log record.\n    \"\"\"\n    global _logRecordFactory\n    _logRecordFactory = factory\n\ndef getLogRecordFactory():\n    \"\"\"\n    Return the factory to be used when instantiating a log record.\n    \"\"\"\n\n    return _logRecordFactory\n\ndef makeLogRecord(dict):\n    \"\"\"\n    Make a LogRecord whose attributes are defined by the specified dictionary,\n    This function is useful for converting a logging event received over\n    a socket connection (which is sent as a dictionary) into a LogRecord\n    instance.\n    \"\"\"\n    rv = _logRecordFactory(None, None, \"\", 0, \"\", (), None, None)\n    rv.__dict__.update(dict)\n    return rv\n\n\n#---------------------------------------------------------------------------\n#   Formatter classes and functions\n#---------------------------------------------------------------------------\n_str_formatter = StrFormatter()\ndel StrFormatter\n\n\nclass PercentStyle(object):\n\n    default_format = '%(message)s'\n    asctime_format = '%(asctime)s'\n    asctime_search = '%(asctime)'\n    validation_pattern = re.compile(r'%\\(\\w+\\)[#0+ -]*(\\*|\\d+)?(\\.(\\*|\\d+))?[diouxefgcrsa%]', re.I)\n\n    def __init__(self, fmt, *, defaults=None):\n        self._fmt = fmt or self.default_format\n        self._defaults = defaults\n\n    def usesTime(self):\n        return self._fmt.find(self.asctime_search) >= 0\n\n    def validate(self):\n        \"\"\"Validate the input format, ensure it matches the correct style\"\"\"\n        if not self.validation_pattern.search(self._fmt):\n            raise ValueError(\"Invalid format '%s' for '%s' style\" % (self._fmt, self.default_format[0]))\n\n    def _format(self, record):\n        if defaults := self._defaults:\n            values = defaults | record.__dict__\n        else:\n            values = record.__dict__\n        return self._fmt % values\n\n    def format(self, record):\n        try:\n            return self._format(record)\n        except KeyError as e:\n            raise ValueError('Formatting field not found in record: %s' % e)\n\n\nclass StrFormatStyle(PercentStyle):\n    default_format = '{message}'\n    asctime_format = '{asctime}'\n    asctime_search = '{asctime'\n\n    fmt_spec = re.compile(r'^(.?[<>=^])?[+ -]?#?0?(\\d+|{\\w+})?[,_]?(\\.(\\d+|{\\w+}))?[bcdefgnosx%]?$', re.I)\n    field_spec = re.compile(r'^(\\d+|\\w+)(\\.\\w+|\\[[^]]+\\])*$')\n\n    def _format(self, record):\n        if defaults := self._defaults:\n            values = defaults | record.__dict__\n        else:\n            values = record.__dict__\n        return self._fmt.format(**values)\n\n    def validate(self):\n        \"\"\"Validate the input format, ensure it is the correct string formatting style\"\"\"\n        fields = set()\n        try:\n            for _, fieldname, spec, conversion in _str_formatter.parse(self._fmt):\n                if fieldname:\n                    if not self.field_spec.match(fieldname):\n                        raise ValueError('invalid field name/expression: %r' % fieldname)\n                    fields.add(fieldname)\n                if conversion and conversion not in 'rsa':\n                    raise ValueError('invalid conversion: %r' % conversion)\n                if spec and not self.fmt_spec.match(spec):\n                    raise ValueError('bad specifier: %r' % spec)\n        except ValueError as e:\n            raise ValueError('invalid format: %s' % e)\n        if not fields:\n            raise ValueError('invalid format: no fields')\n\n\nclass StringTemplateStyle(PercentStyle):\n    default_format = '${message}'\n    asctime_format = '${asctime}'\n    asctime_search = '${asctime}'\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._tpl = Template(self._fmt)\n\n    def usesTime(self):\n        fmt = self._fmt\n        return fmt.find('$asctime') >= 0 or fmt.find(self.asctime_search) >= 0\n\n    def validate(self):\n        pattern = Template.pattern\n        fields = set()\n        for m in pattern.finditer(self._fmt):\n            d = m.groupdict()\n            if d['named']:\n                fields.add(d['named'])\n            elif d['braced']:\n                fields.add(d['braced'])\n            elif m.group(0) == '$':\n                raise ValueError('invalid format: bare \\'$\\' not allowed')\n        if not fields:\n            raise ValueError('invalid format: no fields')\n\n    def _format(self, record):\n        if defaults := self._defaults:\n            values = defaults | record.__dict__\n        else:\n            values = record.__dict__\n        return self._tpl.substitute(**values)\n\n\nBASIC_FORMAT = \"%(levelname)s:%(name)s:%(message)s\"\n\n_STYLES = {\n    '%': (PercentStyle, BASIC_FORMAT),\n    '{': (StrFormatStyle, '{levelname}:{name}:{message}'),\n    '$': (StringTemplateStyle, '${levelname}:${name}:${message}'),\n}\n\nclass Formatter(object):\n    \"\"\"\n    Formatter instances are used to convert a LogRecord to text.\n\n    Formatters need to know how a LogRecord is constructed. They are\n    responsible for converting a LogRecord to (usually) a string which can\n    be interpreted by either a human or an external system. The base Formatter\n    allows a formatting string to be specified. If none is supplied, the\n    style-dependent default value, \"%(message)s\", \"{message}\", or\n    \"${message}\", is used.\n\n    The Formatter can be initialized with a format string which makes use of\n    knowledge of the LogRecord attributes - e.g. the default value mentioned\n    above makes use of the fact that the user's message and arguments are pre-\n    formatted into a LogRecord's message attribute. Currently, the useful\n    attributes in a LogRecord are described by:\n\n    %(name)s            Name of the logger (logging channel)\n    %(levelno)s         Numeric logging level for the message (DEBUG, INFO,\n                        WARNING, ERROR, CRITICAL)\n    %(levelname)s       Text logging level for the message (\"DEBUG\", \"INFO\",\n                        \"WARNING\", \"ERROR\", \"CRITICAL\")\n    %(pathname)s        Full pathname of the source file where the logging\n                        call was issued (if available)\n    %(filename)s        Filename portion of pathname\n    %(module)s          Module (name portion of filename)\n    %(lineno)d          Source line number where the logging call was issued\n                        (if available)\n    %(funcName)s        Function name\n    %(created)f         Time when the LogRecord was created (time.time()\n                        return value)\n    %(asctime)s         Textual time when the LogRecord was created\n    %(msecs)d           Millisecond portion of the creation time\n    %(relativeCreated)d Time in milliseconds when the LogRecord was created,\n                        relative to the time the logging module was loaded\n                        (typically at application startup time)\n    %(thread)d          Thread ID (if available)\n    %(threadName)s      Thread name (if available)\n    %(process)d         Process ID (if available)\n    %(message)s         The result of record.getMessage(), computed just as\n                        the record is emitted\n    \"\"\"\n\n    converter = time.localtime\n\n    def __init__(self, fmt=None, datefmt=None, style='%', validate=True, *,\n                 defaults=None):\n        \"\"\"\n        Initialize the formatter with specified format strings.\n\n        Initialize the formatter either with the specified format string, or a\n        default as described above. Allow for specialized date formatting with\n        the optional datefmt argument. If datefmt is omitted, you get an\n        ISO8601-like (or RFC 3339-like) format.\n\n        Use a style parameter of '%', '{' or '$' to specify that you want to\n        use one of %-formatting, :meth:`str.format` (``{}``) formatting or\n        :class:`string.Template` formatting in your format string.\n\n        .. versionchanged:: 3.2\n           Added the ``style`` parameter.\n        \"\"\"\n        if style not in _STYLES:\n            raise ValueError('Style must be one of: %s' % ','.join(\n                             _STYLES.keys()))\n        self._style = _STYLES[style][0](fmt, defaults=defaults)\n        if validate:\n            self._style.validate()\n\n        self._fmt = self._style._fmt\n        self.datefmt = datefmt\n\n    default_time_format = '%Y-%m-%d %H:%M:%S'\n    default_msec_format = '%s,%03d'\n\n    def formatTime(self, record, datefmt=None):\n        \"\"\"\n        Return the creation time of the specified LogRecord as formatted text.\n\n        This method should be called from format() by a formatter which\n        wants to make use of a formatted time. This method can be overridden\n        in formatters to provide for any specific requirement, but the\n        basic behaviour is as follows: if datefmt (a string) is specified,\n        it is used with time.strftime() to format the creation time of the\n        record. Otherwise, an ISO8601-like (or RFC 3339-like) format is used.\n        The resulting string is returned. This function uses a user-configurable\n        function to convert the creation time to a tuple. By default,\n        time.localtime() is used; to change this for a particular formatter\n        instance, set the 'converter' attribute to a function with the same\n        signature as time.localtime() or time.gmtime(). To change it for all\n        formatters, for example if you want all logging times to be shown in GMT,\n        set the 'converter' attribute in the Formatter class.\n        \"\"\"\n        ct = self.converter(record.created)\n        if datefmt:\n            s = time.strftime(datefmt, ct)\n        else:\n            s = time.strftime(self.default_time_format, ct)\n            if self.default_msec_format:\n                s = self.default_msec_format % (s, record.msecs)\n        return s\n\n    def formatException(self, ei):\n        \"\"\"\n        Format and return the specified exception information as a string.\n\n        This default implementation just uses\n        traceback.print_exception()\n        \"\"\"\n        sio = io.StringIO()\n        tb = ei[2]\n        # See issues #9427, #1553375. Commented out for now.\n        #if getattr(self, 'fullstack', False):\n        #    traceback.print_stack(tb.tb_frame.f_back, file=sio)\n        traceback.print_exception(ei[0], ei[1], tb, None, sio)\n        s = sio.getvalue()\n        sio.close()\n        if s[-1:] == \"\\n\":\n            s = s[:-1]\n        return s\n\n    def usesTime(self):\n        \"\"\"\n        Check if the format uses the creation time of the record.\n        \"\"\"\n        return self._style.usesTime()\n\n    def formatMessage(self, record):\n        return self._style.format(record)\n\n    def formatStack(self, stack_info):\n        \"\"\"\n        This method is provided as an extension point for specialized\n        formatting of stack information.\n\n        The input data is a string as returned from a call to\n        :func:`traceback.print_stack`, but with the last trailing newline\n        removed.\n\n        The base implementation just returns the value passed in.\n        \"\"\"\n        return stack_info\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record as text.\n\n        The record's attribute dictionary is used as the operand to a\n        string formatting operation which yields the returned string.\n        Before formatting the dictionary, a couple of preparatory steps\n        are carried out. The message attribute of the record is computed\n        using LogRecord.getMessage(). If the formatting string uses the\n        time (as determined by a call to usesTime(), formatTime() is\n        called to format the event time. If there is exception information,\n        it is formatted using formatException() and appended to the message.\n        \"\"\"\n        record.message = record.getMessage()\n        if self.usesTime():\n            record.asctime = self.formatTime(record, self.datefmt)\n        s = self.formatMessage(record)\n        if record.exc_info:\n            # Cache the traceback text to avoid converting it multiple times\n            # (it's constant anyway)\n            if not record.exc_text:\n                record.exc_text = self.formatException(record.exc_info)\n        if record.exc_text:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            s = s + record.exc_text\n        if record.stack_info:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            s = s + self.formatStack(record.stack_info)\n        return s\n\n#\n#   The default formatter to use when no other is specified\n#\n_defaultFormatter = Formatter()\n\nclass BufferingFormatter(object):\n    \"\"\"\n    A formatter suitable for formatting a number of records.\n    \"\"\"\n    def __init__(self, linefmt=None):\n        \"\"\"\n        Optionally specify a formatter which will be used to format each\n        individual record.\n        \"\"\"\n        if linefmt:\n            self.linefmt = linefmt\n        else:\n            self.linefmt = _defaultFormatter\n\n    def formatHeader(self, records):\n        \"\"\"\n        Return the header string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def formatFooter(self, records):\n        \"\"\"\n        Return the footer string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def format(self, records):\n        \"\"\"\n        Format the specified records and return the result as a string.\n        \"\"\"\n        rv = \"\"\n        if len(records) > 0:\n            rv = rv + self.formatHeader(records)\n            for record in records:\n                rv = rv + self.linefmt.format(record)\n            rv = rv + self.formatFooter(records)\n        return rv\n\n#---------------------------------------------------------------------------\n#   Filter classes and functions\n#---------------------------------------------------------------------------\n\nclass Filter(object):\n    \"\"\"\n    Filter instances are used to perform arbitrary filtering of LogRecords.\n\n    Loggers and Handlers can optionally use Filter instances to filter\n    records as desired. The base filter class only allows events which are\n    below a certain point in the logger hierarchy. For example, a filter\n    initialized with \"A.B\" will allow events logged by loggers \"A.B\",\n    \"A.B.C\", \"A.B.C.D\", \"A.B.D\" etc. but not \"A.BB\", \"B.A.B\" etc. If\n    initialized with the empty string, all events are passed.\n    \"\"\"\n    def __init__(self, name=''):\n        \"\"\"\n        Initialize a filter.\n\n        Initialize with the name of the logger which, together with its\n        children, will have its events allowed through the filter. If no\n        name is specified, allow every event.\n        \"\"\"\n        self.name = name\n        self.nlen = len(name)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if the specified record is to be logged.\n\n        Returns True if the record should be logged, or False otherwise.\n        If deemed appropriate, the record may be modified in-place.\n        \"\"\"\n        if self.nlen == 0:\n            return True\n        elif self.name == record.name:\n            return True\n        elif record.name.find(self.name, 0, self.nlen) != 0:\n            return False\n        return (record.name[self.nlen] == \".\")\n\nclass Filterer(object):\n    \"\"\"\n    A base class for loggers and handlers which allows them to share\n    common code.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the list of filters to be an empty list.\n        \"\"\"\n        self.filters = []\n\n    def addFilter(self, filter):\n        \"\"\"\n        Add the specified filter to this handler.\n        \"\"\"\n        if not (filter in self.filters):\n            self.filters.append(filter)\n\n    def removeFilter(self, filter):\n        \"\"\"\n        Remove the specified filter from this handler.\n        \"\"\"\n        if filter in self.filters:\n            self.filters.remove(filter)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if a record is loggable by consulting all the filters.\n\n        The default is to allow the record to be logged; any filter can veto\n        this and the record is then dropped. Returns a zero value if a record\n        is to be dropped, else non-zero.\n\n        .. versionchanged:: 3.2\n\n           Allow filters to be just callables.\n        \"\"\"\n        rv = True\n        for f in self.filters:\n            if hasattr(f, 'filter'):\n                result = f.filter(record)\n            else:\n                result = f(record) # assume callable - will raise if not\n            if not result:\n                rv = False\n                break\n        return rv\n\n#---------------------------------------------------------------------------\n#   Handler classes and functions\n#---------------------------------------------------------------------------\n\n_handlers = weakref.WeakValueDictionary()  #map of handler names to handlers\n_handlerList = [] # added to allow handlers to be removed in reverse of order initialized\n\ndef _removeHandlerRef(wr):\n    \"\"\"\n    Remove a handler reference from the internal cleanup list.\n    \"\"\"\n    # This function can be called during module teardown, when globals are\n    # set to None. It can also be called from another thread. So we need to\n    # pre-emptively grab the necessary globals and check if they're None,\n    # to prevent race conditions and failures during interpreter shutdown.\n    acquire, release, handlers = _acquireLock, _releaseLock, _handlerList\n    if acquire and release and handlers:\n        acquire()\n        try:\n            handlers.remove(wr)\n        except ValueError:\n            pass\n        finally:\n            release()\n\ndef _addHandlerRef(handler):\n    \"\"\"\n    Add a handler to the internal cleanup list using a weak reference.\n    \"\"\"\n    _acquireLock()\n    try:\n        _handlerList.append(weakref.ref(handler, _removeHandlerRef))\n    finally:\n        _releaseLock()\n\nclass Handler(Filterer):\n    \"\"\"\n    Handler instances dispatch logging events to specific destinations.\n\n    The base handler class. Acts as a placeholder which defines the Handler\n    interface. Handlers can optionally use Formatter instances to format\n    records as desired. By default, no formatter is specified; in this case,\n    the 'raw' message as determined by record.message is logged.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initializes the instance - basically setting the formatter to None\n        and the filter list to empty.\n        \"\"\"\n        Filterer.__init__(self)\n        self._name = None\n        self.level = _checkLevel(level)\n        self.formatter = None\n        self._closed = False\n        # Add the handler to the global _handlerList (for cleanup on shutdown)\n        _addHandlerRef(self)\n        self.createLock()\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, name):\n        _acquireLock()\n        try:\n            if self._name in _handlers:\n                del _handlers[self._name]\n            self._name = name\n            if name:\n                _handlers[name] = self\n        finally:\n            _releaseLock()\n\n    name = property(get_name, set_name)\n\n    def createLock(self):\n        \"\"\"\n        Acquire a thread lock for serializing access to the underlying I/O.\n        \"\"\"\n        self.lock = threading.RLock()\n        _register_at_fork_reinit_lock(self)\n\n    def _at_fork_reinit(self):\n        self.lock._at_fork_reinit()\n\n    def acquire(self):\n        \"\"\"\n        Acquire the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.acquire()\n\n    def release(self):\n        \"\"\"\n        Release the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.release()\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this handler.  level must be an int or a str.\n        \"\"\"\n        self.level = _checkLevel(level)\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record.\n\n        If a formatter is set, use it. Otherwise, use the default formatter\n        for the module.\n        \"\"\"\n        if self.formatter:\n            fmt = self.formatter\n        else:\n            fmt = _defaultFormatter\n        return fmt.format(record)\n\n    def emit(self, record):\n        \"\"\"\n        Do whatever it takes to actually log the specified logging record.\n\n        This version is intended to be implemented by subclasses and so\n        raises a NotImplementedError.\n        \"\"\"\n        raise NotImplementedError('emit must be implemented '\n                                  'by Handler subclasses')\n\n    def handle(self, record):\n        \"\"\"\n        Conditionally emit the specified logging record.\n\n        Emission depends on filters which may have been added to the handler.\n        Wrap the actual emission of the record with acquisition/release of\n        the I/O thread lock. Returns whether the filter passed the record for\n        emission.\n        \"\"\"\n        rv = self.filter(record)\n        if rv:\n            self.acquire()\n            try:\n                self.emit(record)\n            finally:\n                self.release()\n        return rv\n\n    def setFormatter(self, fmt):\n        \"\"\"\n        Set the formatter for this handler.\n        \"\"\"\n        self.formatter = fmt\n\n    def flush(self):\n        \"\"\"\n        Ensure all logging output has been flushed.\n\n        This version does nothing and is intended to be implemented by\n        subclasses.\n        \"\"\"\n        pass\n\n    def close(self):\n        \"\"\"\n        Tidy up any resources used by the handler.\n\n        This version removes the handler from an internal map of handlers,\n        _handlers, which is used for handler lookup by name. Subclasses\n        should ensure that this gets called from overridden close()\n        methods.\n        \"\"\"\n        #get the module data lock, as we're updating a shared structure.\n        _acquireLock()\n        try:    #unlikely to raise an exception, but you never know...\n            self._closed = True\n            if self._name and self._name in _handlers:\n                del _handlers[self._name]\n        finally:\n            _releaseLock()\n\n    def handleError(self, record):\n        \"\"\"\n        Handle errors which occur during an emit() call.\n\n        This method should be called from handlers when an exception is\n        encountered during an emit() call. If raiseExceptions is false,\n        exceptions get silently ignored. This is what is mostly wanted\n        for a logging system - most users will not care about errors in\n        the logging system, they are more interested in application errors.\n        You could, however, replace this with a custom handler if you wish.\n        The record which was being processed is passed in to this method.\n        \"\"\"\n        if raiseExceptions and sys.stderr:  # see issue 13807\n            t, v, tb = sys.exc_info()\n            try:\n                sys.stderr.write('--- Logging error ---\\n')\n                traceback.print_exception(t, v, tb, None, sys.stderr)\n                sys.stderr.write('Call stack:\\n')\n                # Walk the stack frame up until we're out of logging,\n                # so as to print the calling context.\n                frame = tb.tb_frame\n                while (frame and os.path.dirname(frame.f_code.co_filename) ==\n                       __path__[0]):\n                    frame = frame.f_back\n                if frame:\n                    traceback.print_stack(frame, file=sys.stderr)\n                else:\n                    # couldn't find the right stack frame, for some reason\n                    sys.stderr.write('Logged from file %s, line %s\\n' % (\n                                     record.filename, record.lineno))\n                # Issue 18671: output logging message and arguments\n                try:\n                    sys.stderr.write('Message: %r\\n'\n                                     'Arguments: %s\\n' % (record.msg,\n                                                          record.args))\n                except RecursionError:  # See issue 36272\n                    raise\n                except Exception:\n                    sys.stderr.write('Unable to print the message and arguments'\n                                     ' - possible formatting error.\\nUse the'\n                                     ' traceback above to help find the error.\\n'\n                                    )\n            except OSError: #pragma: no cover\n                pass    # see issue 5971\n            finally:\n                del t, v, tb\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        return '<%s (%s)>' % (self.__class__.__name__, level)\n\nclass StreamHandler(Handler):\n    \"\"\"\n    A handler class which writes logging records, appropriately formatted,\n    to a stream. Note that this class does not close the stream, as\n    sys.stdout or sys.stderr may be used.\n    \"\"\"\n\n    terminator = '\\n'\n\n    def __init__(self, stream=None):\n        \"\"\"\n        Initialize the handler.\n\n        If stream is not specified, sys.stderr is used.\n        \"\"\"\n        Handler.__init__(self)\n        if stream is None:\n            stream = sys.stderr\n        self.stream = stream\n\n    def flush(self):\n        \"\"\"\n        Flushes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            if self.stream and hasattr(self.stream, \"flush\"):\n                self.stream.flush()\n        finally:\n            self.release()\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If a formatter is specified, it is used to format the record.\n        The record is then written to the stream with a trailing newline.  If\n        exception information is present, it is formatted using\n        traceback.print_exception and appended to the stream.  If the stream\n        has an 'encoding' attribute, it is used to determine how to do the\n        output to the stream.\n        \"\"\"\n        try:\n            msg = self.format(record)\n            stream = self.stream\n            # issue 35046: merged two stream.writes into one.\n            stream.write(msg + self.terminator)\n            self.flush()\n        except RecursionError:  # See issue 36272\n            raise\n        except Exception:\n            self.handleError(record)\n\n    def setStream(self, stream):\n        \"\"\"\n        Sets the StreamHandler's stream to the specified value,\n        if it is different.\n\n        Returns the old stream, if the stream was changed, or None\n        if it wasn't.\n        \"\"\"\n        if stream is self.stream:\n            result = None\n        else:\n            result = self.stream\n            self.acquire()\n            try:\n                self.flush()\n                self.stream = stream\n            finally:\n                self.release()\n        return result\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        name = getattr(self.stream, 'name', '')\n        #  bpo-36015: name can be an int\n        name = str(name)\n        if name:\n            name += ' '\n        return '<%s %s(%s)>' % (self.__class__.__name__, name, level)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass FileHandler(StreamHandler):\n    \"\"\"\n    A handler class which writes formatted logging records to disk files.\n    \"\"\"\n    def __init__(self, filename, mode='a', encoding=None, delay=False, errors=None):\n        \"\"\"\n        Open the specified file and use it as the stream for logging.\n        \"\"\"\n        # Issue #27493: add support for Path objects to be passed in\n        filename = os.fspath(filename)\n        #keep the absolute path, otherwise derived classes which use this\n        #may come a cropper when the current directory changes\n        self.baseFilename = os.path.abspath(filename)\n        self.mode = mode\n        self.encoding = encoding\n        if \"b\" not in mode:\n            self.encoding = io.text_encoding(encoding)\n        self.errors = errors\n        self.delay = delay\n        # bpo-26789: FileHandler keeps a reference to the builtin open()\n        # function to be able to open or reopen the file during Python\n        # finalization.\n        self._builtin_open = open\n        if delay:\n            #We don't open the stream, but we still need to call the\n            #Handler constructor to set level, formatter, lock etc.\n            Handler.__init__(self)\n            self.stream = None\n        else:\n            StreamHandler.__init__(self, self._open())\n\n    def close(self):\n        \"\"\"\n        Closes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            try:\n                if self.stream:\n                    try:\n                        self.flush()\n                    finally:\n                        stream = self.stream\n                        self.stream = None\n                        if hasattr(stream, \"close\"):\n                            stream.close()\n            finally:\n                # Issue #19523: call unconditionally to\n                # prevent a handler leak when delay is set\n                # Also see Issue #42378: we also rely on\n                # self._closed being set to True there\n                StreamHandler.close(self)\n        finally:\n            self.release()\n\n    def _open(self):\n        \"\"\"\n        Open the current base file with the (original) mode and encoding.\n        Return the resulting stream.\n        \"\"\"\n        open_func = self._builtin_open\n        return open_func(self.baseFilename, self.mode,\n                         encoding=self.encoding, errors=self.errors)\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If the stream was not opened because 'delay' was specified in the\n        constructor, open it before calling the superclass's emit.\n\n        If stream is not open, current mode is 'w' and `_closed=True`, record\n        will not be emitted (see Issue #42378).\n        \"\"\"\n        if self.stream is None:\n            if self.mode != 'w' or not self._closed:\n                self.stream = self._open()\n        if self.stream:\n            StreamHandler.emit(self, record)\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        return '<%s %s (%s)>' % (self.__class__.__name__, self.baseFilename, level)\n\n\nclass _StderrHandler(StreamHandler):\n    \"\"\"\n    This class is like a StreamHandler using sys.stderr, but always uses\n    whatever sys.stderr is currently set to rather than the value of\n    sys.stderr at handler construction time.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initialize the handler.\n        \"\"\"\n        Handler.__init__(self, level)\n\n    @property\n    def stream(self):\n        return sys.stderr\n\n\n_defaultLastResort = _StderrHandler(WARNING)\nlastResort = _defaultLastResort\n\n#---------------------------------------------------------------------------\n#   Manager classes and functions\n#---------------------------------------------------------------------------\n\nclass PlaceHolder(object):\n    \"\"\"\n    PlaceHolder instances are used in the Manager logger hierarchy to take\n    the place of nodes for which no loggers have been defined. This class is\n    intended for internal use only and not as part of the public API.\n    \"\"\"\n    def __init__(self, alogger):\n        \"\"\"\n        Initialize with the specified logger being a child of this placeholder.\n        \"\"\"\n        self.loggerMap = { alogger : None }\n\n    def append(self, alogger):\n        \"\"\"\n        Add the specified logger as a child of this placeholder.\n        \"\"\"\n        if alogger not in self.loggerMap:\n            self.loggerMap[alogger] = None\n\n#\n#   Determine which class to use when instantiating loggers.\n#\n\ndef setLoggerClass(klass):\n    \"\"\"\n    Set the class to be used when instantiating a logger. The class should\n    define __init__() such that only a name argument is required, and the\n    __init__() should call Logger.__init__()\n    \"\"\"\n    if klass != Logger:\n        if not issubclass(klass, Logger):\n            raise TypeError(\"logger not derived from logging.Logger: \"\n                            + klass.__name__)\n    global _loggerClass\n    _loggerClass = klass\n\ndef getLoggerClass():\n    \"\"\"\n    Return the class to be used when instantiating a logger.\n    \"\"\"\n    return _loggerClass\n\nclass Manager(object):\n    \"\"\"\n    There is [under normal circumstances] just one Manager instance, which\n    holds the hierarchy of loggers.\n    \"\"\"\n    def __init__(self, rootnode):\n        \"\"\"\n        Initialize the manager with the root node of the logger hierarchy.\n        \"\"\"\n        self.root = rootnode\n        self.disable = 0\n        self.emittedNoHandlerWarning = False\n        self.loggerDict = {}\n        self.loggerClass = None\n        self.logRecordFactory = None\n\n    @property\n    def disable(self):\n        return self._disable\n\n    @disable.setter\n    def disable(self, value):\n        self._disable = _checkLevel(value)\n\n    def getLogger(self, name):\n        \"\"\"\n        Get a logger with the specified name (channel name), creating it\n        if it doesn't yet exist. This name is a dot-separated hierarchical\n        name, such as \"a\", \"a.b\", \"a.b.c\" or similar.\n\n        If a PlaceHolder existed for the specified name [i.e. the logger\n        didn't exist but a child of it did], replace it with the created\n        logger and fix up the parent/child references which pointed to the\n        placeholder to now point to the logger.\n        \"\"\"\n        rv = None\n        if not isinstance(name, str):\n            raise TypeError('A logger name must be a string')\n        _acquireLock()\n        try:\n            if name in self.loggerDict:\n                rv = self.loggerDict[name]\n                if isinstance(rv, PlaceHolder):\n                    ph = rv\n                    rv = (self.loggerClass or _loggerClass)(name)\n                    rv.manager = self\n                    self.loggerDict[name] = rv\n                    self._fixupChildren(ph, rv)\n                    self._fixupParents(rv)\n            else:\n                rv = (self.loggerClass or _loggerClass)(name)\n                rv.manager = self\n                self.loggerDict[name] = rv\n                self._fixupParents(rv)\n        finally:\n            _releaseLock()\n        return rv\n\n    def setLoggerClass(self, klass):\n        \"\"\"\n        Set the class to be used when instantiating a logger with this Manager.\n        \"\"\"\n        if klass != Logger:\n            if not issubclass(klass, Logger):\n                raise TypeError(\"logger not derived from logging.Logger: \"\n                                + klass.__name__)\n        self.loggerClass = klass\n\n    def setLogRecordFactory(self, factory):\n        \"\"\"\n        Set the factory to be used when instantiating a log record with this\n        Manager.\n        \"\"\"\n        self.logRecordFactory = factory\n\n    def _fixupParents(self, alogger):\n        \"\"\"\n        Ensure that there are either loggers or placeholders all the way\n        from the specified logger to the root of the logger hierarchy.\n        \"\"\"\n        name = alogger.name\n        i = name.rfind(\".\")\n        rv = None\n        while (i > 0) and not rv:\n            substr = name[:i]\n            if substr not in self.loggerDict:\n                self.loggerDict[substr] = PlaceHolder(alogger)\n            else:\n                obj = self.loggerDict[substr]\n                if isinstance(obj, Logger):\n                    rv = obj\n                else:\n                    assert isinstance(obj, PlaceHolder)\n                    obj.append(alogger)\n            i = name.rfind(\".\", 0, i - 1)\n        if not rv:\n            rv = self.root\n        alogger.parent = rv\n\n    def _fixupChildren(self, ph, alogger):\n        \"\"\"\n        Ensure that children of the placeholder ph are connected to the\n        specified logger.\n        \"\"\"\n        name = alogger.name\n        namelen = len(name)\n        for c in ph.loggerMap.keys():\n            #The if means ... if not c.parent.name.startswith(nm)\n            if c.parent.name[:namelen] != name:\n                alogger.parent = c.parent\n                c.parent = alogger\n\n    def _clear_cache(self):\n        \"\"\"\n        Clear the cache for all loggers in loggerDict\n        Called when level changes are made\n        \"\"\"\n\n        _acquireLock()\n        for logger in self.loggerDict.values():\n            if isinstance(logger, Logger):\n                logger._cache.clear()\n        self.root._cache.clear()\n        _releaseLock()\n\n#---------------------------------------------------------------------------\n#   Logger classes and functions\n#---------------------------------------------------------------------------\n\nclass Logger(Filterer):\n    \"\"\"\n    Instances of the Logger class represent a single logging channel. A\n    \"logging channel\" indicates an area of an application. Exactly how an\n    \"area\" is defined is up to the application developer. Since an\n    application can have any number of areas, logging channels are identified\n    by a unique string. Application areas can be nested (e.g. an area\n    of \"input processing\" might include sub-areas \"read CSV files\", \"read\n    XLS files\" and \"read Gnumeric files\"). To cater for this natural nesting,\n    channel names are organized into a namespace hierarchy where levels are\n    separated by periods, much like the Java or Python package namespace. So\n    in the instance given above, channel names might be \"input\" for the upper\n    level, and \"input.csv\", \"input.xls\" and \"input.gnu\" for the sub-levels.\n    There is no arbitrary limit to the depth of nesting.\n    \"\"\"\n    def __init__(self, name, level=NOTSET):\n        \"\"\"\n        Initialize the logger with a name and an optional level.\n        \"\"\"\n        Filterer.__init__(self)\n        self.name = name\n        self.level = _checkLevel(level)\n        self.parent = None\n        self.propagate = True\n        self.handlers = []\n        self.disabled = False\n        self._cache = {}\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this logger.  level must be an int or a str.\n        \"\"\"\n        self.level = _checkLevel(level)\n        self.manager._clear_cache()\n\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'DEBUG'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.debug(\"Houston, we have a %s\", \"thorny problem\", exc_info=True)\n        \"\"\"\n        if self.isEnabledFor(DEBUG):\n            self._log(DEBUG, msg, args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'INFO'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.info(\"Houston, we have a %s\", \"interesting problem\", exc_info=True)\n        \"\"\"\n        if self.isEnabledFor(INFO):\n            self._log(INFO, msg, args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'WARNING'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.warning(\"Houston, we have a %s\", \"bit of a problem\", exc_info=True)\n        \"\"\"\n        if self.isEnabledFor(WARNING):\n            self._log(WARNING, msg, args, **kwargs)\n\n    def warn(self, msg, *args, **kwargs):\n        warnings.warn(\"The 'warn' method is deprecated, \"\n            \"use 'warning' instead\", DeprecationWarning, 2)\n        self.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'ERROR'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.error(\"Houston, we have a %s\", \"major problem\", exc_info=True)\n        \"\"\"\n        if self.isEnabledFor(ERROR):\n            self._log(ERROR, msg, args, **kwargs)\n\n    def exception(self, msg, *args, exc_info=True, **kwargs):\n        \"\"\"\n        Convenience method for logging an ERROR with exception information.\n        \"\"\"\n        self.error(msg, *args, exc_info=exc_info, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'CRITICAL'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.critical(\"Houston, we have a %s\", \"major disaster\", exc_info=True)\n        \"\"\"\n        if self.isEnabledFor(CRITICAL):\n            self._log(CRITICAL, msg, args, **kwargs)\n\n    def fatal(self, msg, *args, **kwargs):\n        \"\"\"\n        Don't use this method, use critical() instead.\n        \"\"\"\n        self.critical(msg, *args, **kwargs)\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with the integer severity 'level'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.log(level, \"We have a %s\", \"mysterious problem\", exc_info=True)\n        \"\"\"\n        if not isinstance(level, int):\n            if raiseExceptions:\n                raise TypeError(\"level must be an integer\")\n            else:\n                return\n        if self.isEnabledFor(level):\n            self._log(level, msg, args, **kwargs)\n\n    def findCaller(self, stack_info=False, stacklevel=1):\n        \"\"\"\n        Find the stack frame of the caller so that we can note the source\n        file name, line number and function name.\n        \"\"\"\n        f = currentframe()\n        #On some versions of IronPython, currentframe() returns None if\n        #IronPython isn't run with -X:Frames.\n        if f is None:\n            return \"(unknown file)\", 0, \"(unknown function)\", None\n        while stacklevel > 0:\n            next_f = f.f_back\n            if next_f is None:\n                ## We've got options here.\n                ## If we want to use the last (deepest) frame:\n                break\n                ## If we want to mimic the warnings module:\n                #return (\"sys\", 1, \"(unknown function)\", None)\n                ## If we want to be pedantic:\n                #raise ValueError(\"call stack is not deep enough\")\n            f = next_f\n            if not _is_internal_frame(f):\n                stacklevel -= 1\n        co = f.f_code\n        sinfo = None\n        if stack_info:\n            with io.StringIO() as sio:\n                sio.write(\"Stack (most recent call last):\\n\")\n                traceback.print_stack(f, file=sio)\n                sinfo = sio.getvalue()\n                if sinfo[-1] == '\\n':\n                    sinfo = sinfo[:-1]\n        return co.co_filename, f.f_lineno, co.co_name, sinfo\n\n    def makeRecord(self, name, level, fn, lno, msg, args, exc_info,\n                   func=None, extra=None, sinfo=None):\n        \"\"\"\n        A factory method which can be overridden in subclasses to create\n        specialized LogRecords.\n        \"\"\"\n        rv = _logRecordFactory(name, level, fn, lno, msg, args, exc_info, func,\n                             sinfo)\n        if extra is not None:\n            for key in extra:\n                if (key in [\"message\", \"asctime\"]) or (key in rv.__dict__):\n                    raise KeyError(\"Attempt to overwrite %r in LogRecord\" % key)\n                rv.__dict__[key] = extra[key]\n        return rv\n\n    def _log(self, level, msg, args, exc_info=None, extra=None, stack_info=False,\n             stacklevel=1):\n        \"\"\"\n        Low-level logging routine which creates a LogRecord and then calls\n        all the handlers of this logger to handle the record.\n        \"\"\"\n        sinfo = None\n        if _srcfile:\n            #IronPython doesn't track Python frames, so findCaller raises an\n            #exception on some versions of IronPython. We trap it here so that\n            #IronPython can use logging.\n            try:\n                fn, lno, func, sinfo = self.findCaller(stack_info, stacklevel)\n            except ValueError: # pragma: no cover\n                fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        else: # pragma: no cover\n            fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        if exc_info:\n            if isinstance(exc_info, BaseException):\n                exc_info = (type(exc_info), exc_info, exc_info.__traceback__)\n            elif not isinstance(exc_info, tuple):\n                exc_info = sys.exc_info()\n        record = self.makeRecord(self.name, level, fn, lno, msg, args,\n                                 exc_info, func, extra, sinfo)\n        self.handle(record)\n\n    def handle(self, record):\n        \"\"\"\n        Call the handlers for the specified record.\n\n        This method is used for unpickled records received from a socket, as\n        well as those created locally. Logger-level filtering is applied.\n        \"\"\"\n        if (not self.disabled) and self.filter(record):\n            self.callHandlers(record)\n\n    def addHandler(self, hdlr):\n        \"\"\"\n        Add the specified handler to this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if not (hdlr in self.handlers):\n                self.handlers.append(hdlr)\n        finally:\n            _releaseLock()\n\n    def removeHandler(self, hdlr):\n        \"\"\"\n        Remove the specified handler from this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if hdlr in self.handlers:\n                self.handlers.remove(hdlr)\n        finally:\n            _releaseLock()\n\n    def hasHandlers(self):\n        \"\"\"\n        See if this logger has any handlers configured.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. Return True if a handler was found, else False.\n        Stop searching up the hierarchy whenever a logger with the \"propagate\"\n        attribute set to zero is found - that will be the last logger which\n        is checked for the existence of handlers.\n        \"\"\"\n        c = self\n        rv = False\n        while c:\n            if c.handlers:\n                rv = True\n                break\n            if not c.propagate:\n                break\n            else:\n                c = c.parent\n        return rv\n\n    def callHandlers(self, record):\n        \"\"\"\n        Pass a record to all relevant handlers.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. If no handler was found, output a one-off error\n        message to sys.stderr. Stop searching up the hierarchy whenever a\n        logger with the \"propagate\" attribute set to zero is found - that\n        will be the last logger whose handlers are called.\n        \"\"\"\n        c = self\n        found = 0\n        while c:\n            for hdlr in c.handlers:\n                found = found + 1\n                if record.levelno >= hdlr.level:\n                    hdlr.handle(record)\n            if not c.propagate:\n                c = None    #break out\n            else:\n                c = c.parent\n        if (found == 0):\n            if lastResort:\n                if record.levelno >= lastResort.level:\n                    lastResort.handle(record)\n            elif raiseExceptions and not self.manager.emittedNoHandlerWarning:\n                sys.stderr.write(\"No handlers could be found for logger\"\n                                 \" \\\"%s\\\"\\n\" % self.name)\n                self.manager.emittedNoHandlerWarning = True\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for this logger.\n\n        Loop through this logger and its parents in the logger hierarchy,\n        looking for a non-zero logging level. Return the first one found.\n        \"\"\"\n        logger = self\n        while logger:\n            if logger.level:\n                return logger.level\n            logger = logger.parent\n        return NOTSET\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        if self.disabled:\n            return False\n\n        try:\n            return self._cache[level]\n        except KeyError:\n            _acquireLock()\n            try:\n                if self.manager.disable >= level:\n                    is_enabled = self._cache[level] = False\n                else:\n                    is_enabled = self._cache[level] = (\n                        level >= self.getEffectiveLevel()\n                    )\n            finally:\n                _releaseLock()\n            return is_enabled\n\n    def getChild(self, suffix):\n        \"\"\"\n        Get a logger which is a descendant to this one.\n\n        This is a convenience method, such that\n\n        logging.getLogger('abc').getChild('def.ghi')\n\n        is the same as\n\n        logging.getLogger('abc.def.ghi')\n\n        It's useful, for example, when the parent logger is named using\n        __name__ rather than a literal string.\n        \"\"\"\n        if self.root is not self:\n            suffix = '.'.join((self.name, suffix))\n        return self.manager.getLogger(suffix)\n\n    def __repr__(self):\n        level = getLevelName(self.getEffectiveLevel())\n        return '<%s %s (%s)>' % (self.__class__.__name__, self.name, level)\n\n    def __reduce__(self):\n        if getLogger(self.name) is not self:\n            import pickle\n            raise pickle.PicklingError('logger cannot be pickled')\n        return getLogger, (self.name,)\n\n\nclass RootLogger(Logger):\n    \"\"\"\n    A root logger is not that different to any other logger, except that\n    it must have a logging level and there is only one instance of it in\n    the hierarchy.\n    \"\"\"\n    def __init__(self, level):\n        \"\"\"\n        Initialize the logger with the name \"root\".\n        \"\"\"\n        Logger.__init__(self, \"root\", level)\n\n    def __reduce__(self):\n        return getLogger, ()\n\n_loggerClass = Logger\n\nclass LoggerAdapter(object):\n    \"\"\"\n    An adapter for loggers which makes it easier to specify contextual\n    information in logging output.\n    \"\"\"\n\n    def __init__(self, logger, extra=None):\n        \"\"\"\n        Initialize the adapter with a logger and a dict-like object which\n        provides contextual information. This constructor signature allows\n        easy stacking of LoggerAdapters, if so desired.\n\n        You can effectively pass keyword arguments as shown in the\n        following example:\n\n        adapter = LoggerAdapter(someLogger, dict(p1=v1, p2=\"v2\"))\n        \"\"\"\n        self.logger = logger\n        self.extra = extra\n\n    def process(self, msg, kwargs):\n        \"\"\"\n        Process the logging message and keyword arguments passed in to\n        a logging call to insert contextual information. You can either\n        manipulate the message itself, the keyword args or both. Return\n        the message and kwargs modified (or not) to suit your needs.\n\n        Normally, you'll only need to override this one method in a\n        LoggerAdapter subclass for your specific needs.\n        \"\"\"\n        kwargs[\"extra\"] = self.extra\n        return msg, kwargs\n\n    #\n    # Boilerplate convenience methods\n    #\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a debug call to the underlying logger.\n        \"\"\"\n        self.log(DEBUG, msg, *args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an info call to the underlying logger.\n        \"\"\"\n        self.log(INFO, msg, *args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a warning call to the underlying logger.\n        \"\"\"\n        self.log(WARNING, msg, *args, **kwargs)\n\n    def warn(self, msg, *args, **kwargs):\n        warnings.warn(\"The 'warn' method is deprecated, \"\n            \"use 'warning' instead\", DeprecationWarning, 2)\n        self.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an error call to the underlying logger.\n        \"\"\"\n        self.log(ERROR, msg, *args, **kwargs)\n\n    def exception(self, msg, *args, exc_info=True, **kwargs):\n        \"\"\"\n        Delegate an exception call to the underlying logger.\n        \"\"\"\n        self.log(ERROR, msg, *args, exc_info=exc_info, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a critical call to the underlying logger.\n        \"\"\"\n        self.log(CRITICAL, msg, *args, **kwargs)\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a log call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        if self.isEnabledFor(level):\n            msg, kwargs = self.process(msg, kwargs)\n            self.logger.log(level, msg, *args, **kwargs)\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        return self.logger.isEnabledFor(level)\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the specified level on the underlying logger.\n        \"\"\"\n        self.logger.setLevel(level)\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for the underlying logger.\n        \"\"\"\n        return self.logger.getEffectiveLevel()\n\n    def hasHandlers(self):\n        \"\"\"\n        See if the underlying logger has any handlers.\n        \"\"\"\n        return self.logger.hasHandlers()\n\n    def _log(self, level, msg, args, **kwargs):\n        \"\"\"\n        Low-level log implementation, proxied to allow nested logger adapters.\n        \"\"\"\n        return self.logger._log(level, msg, args, **kwargs)\n\n    @property\n    def manager(self):\n        return self.logger.manager\n\n    @manager.setter\n    def manager(self, value):\n        self.logger.manager = value\n\n    @property\n    def name(self):\n        return self.logger.name\n\n    def __repr__(self):\n        logger = self.logger\n        level = getLevelName(logger.getEffectiveLevel())\n        return '<%s %s (%s)>' % (self.__class__.__name__, logger.name, level)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\nroot = RootLogger(WARNING)\nLogger.root = root\nLogger.manager = Manager(Logger.root)\n\n#---------------------------------------------------------------------------\n# Configuration classes and functions\n#---------------------------------------------------------------------------\n\ndef basicConfig(**kwargs):\n    \"\"\"\n    Do basic configuration for the logging system.\n\n    This function does nothing if the root logger already has handlers\n    configured, unless the keyword argument *force* is set to ``True``.\n    It is a convenience method intended for use by simple scripts\n    to do one-shot configuration of the logging package.\n\n    The default behaviour is to create a StreamHandler which writes to\n    sys.stderr, set a formatter using the BASIC_FORMAT format string, and\n    add the handler to the root logger.\n\n    A number of optional keyword arguments may be specified, which can alter\n    the default behaviour.\n\n    filename  Specifies that a FileHandler be created, using the specified\n              filename, rather than a StreamHandler.\n    filemode  Specifies the mode to open the file, if filename is specified\n              (if filemode is unspecified, it defaults to 'a').\n    format    Use the specified format string for the handler.\n    datefmt   Use the specified date/time format.\n    style     If a format string is specified, use this to specify the\n              type of format string (possible values '%', '{', '$', for\n              %-formatting, :meth:`str.format` and :class:`string.Template`\n              - defaults to '%').\n    level     Set the root logger level to the specified level.\n    stream    Use the specified stream to initialize the StreamHandler. Note\n              that this argument is incompatible with 'filename' - if both\n              are present, 'stream' is ignored.\n    handlers  If specified, this should be an iterable of already created\n              handlers, which will be added to the root logger. Any handler\n              in the list which does not have a formatter assigned will be\n              assigned the formatter created in this function.\n    force     If this keyword  is specified as true, any existing handlers\n              attached to the root logger are removed and closed, before\n              carrying out the configuration as specified by the other\n              arguments.\n    encoding  If specified together with a filename, this encoding is passed to\n              the created FileHandler, causing it to be used when the file is\n              opened.\n    errors    If specified together with a filename, this value is passed to the\n              created FileHandler, causing it to be used when the file is\n              opened in text mode. If not specified, the default value is\n              `backslashreplace`.\n\n    Note that you could specify a stream created using open(filename, mode)\n    rather than passing the filename and mode in. However, it should be\n    remembered that StreamHandler does not close its stream (since it may be\n    using sys.stdout or sys.stderr), whereas FileHandler closes its stream\n    when the handler is closed.\n\n    .. versionchanged:: 3.2\n       Added the ``style`` parameter.\n\n    .. versionchanged:: 3.3\n       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for\n       incompatible arguments (e.g. ``handlers`` specified together with\n       ``filename``/``filemode``, or ``filename``/``filemode`` specified\n       together with ``stream``, or ``handlers`` specified together with\n       ``stream``.\n\n    .. versionchanged:: 3.8\n       Added the ``force`` parameter.\n\n    .. versionchanged:: 3.9\n       Added the ``encoding`` and ``errors`` parameters.\n    \"\"\"\n    # Add thread safety in case someone mistakenly calls\n    # basicConfig() from multiple threads\n    _acquireLock()\n    try:\n        force = kwargs.pop('force', False)\n        encoding = kwargs.pop('encoding', None)\n        errors = kwargs.pop('errors', 'backslashreplace')\n        if force:\n            for h in root.handlers[:]:\n                root.removeHandler(h)\n                h.close()\n        if len(root.handlers) == 0:\n            handlers = kwargs.pop(\"handlers\", None)\n            if handlers is None:\n                if \"stream\" in kwargs and \"filename\" in kwargs:\n                    raise ValueError(\"'stream' and 'filename' should not be \"\n                                     \"specified together\")\n            else:\n                if \"stream\" in kwargs or \"filename\" in kwargs:\n                    raise ValueError(\"'stream' or 'filename' should not be \"\n                                     \"specified together with 'handlers'\")\n            if handlers is None:\n                filename = kwargs.pop(\"filename\", None)\n                mode = kwargs.pop(\"filemode\", 'a')\n                if filename:\n                    if 'b' in mode:\n                        errors = None\n                    else:\n                        encoding = io.text_encoding(encoding)\n                    h = FileHandler(filename, mode,\n                                    encoding=encoding, errors=errors)\n                else:\n                    stream = kwargs.pop(\"stream\", None)\n                    h = StreamHandler(stream)\n                handlers = [h]\n            dfs = kwargs.pop(\"datefmt\", None)\n            style = kwargs.pop(\"style\", '%')\n            if style not in _STYLES:\n                raise ValueError('Style must be one of: %s' % ','.join(\n                                 _STYLES.keys()))\n            fs = kwargs.pop(\"format\", _STYLES[style][1])\n            fmt = Formatter(fs, dfs, style)\n            for h in handlers:\n                if h.formatter is None:\n                    h.setFormatter(fmt)\n                root.addHandler(h)\n            level = kwargs.pop(\"level\", None)\n            if level is not None:\n                root.setLevel(level)\n            if kwargs:\n                keys = ', '.join(kwargs.keys())\n                raise ValueError('Unrecognised argument(s): %s' % keys)\n    finally:\n        _releaseLock()\n\n#---------------------------------------------------------------------------\n# Utility functions at module level.\n# Basically delegate everything to the root logger.\n#---------------------------------------------------------------------------\n\ndef getLogger(name=None):\n    \"\"\"\n    Return a logger with the specified name, creating it if necessary.\n\n    If no name is specified, return the root logger.\n    \"\"\"\n    if not name or isinstance(name, str) and name == root.name:\n        return root\n    return Logger.manager.getLogger(name)\n\ndef critical(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'CRITICAL' on the root logger. If the logger\n    has no handlers, call basicConfig() to add a console handler with a\n    pre-defined format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.critical(msg, *args, **kwargs)\n\ndef fatal(msg, *args, **kwargs):\n    \"\"\"\n    Don't use this function, use critical() instead.\n    \"\"\"\n    critical(msg, *args, **kwargs)\n\ndef error(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.error(msg, *args, **kwargs)\n\ndef exception(msg, *args, exc_info=True, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger, with exception\n    information. If the logger has no handlers, basicConfig() is called to add\n    a console handler with a pre-defined format.\n    \"\"\"\n    error(msg, *args, exc_info=exc_info, **kwargs)\n\ndef warning(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'WARNING' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.warning(msg, *args, **kwargs)\n\ndef warn(msg, *args, **kwargs):\n    warnings.warn(\"The 'warn' function is deprecated, \"\n        \"use 'warning' instead\", DeprecationWarning, 2)\n    warning(msg, *args, **kwargs)\n\ndef info(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'INFO' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.info(msg, *args, **kwargs)\n\ndef debug(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'DEBUG' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.debug(msg, *args, **kwargs)\n\ndef log(level, msg, *args, **kwargs):\n    \"\"\"\n    Log 'msg % args' with the integer severity 'level' on the root logger. If\n    the logger has no handlers, call basicConfig() to add a console handler\n    with a pre-defined format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.log(level, msg, *args, **kwargs)\n\ndef disable(level=CRITICAL):\n    \"\"\"\n    Disable all logging calls of severity 'level' and below.\n    \"\"\"\n    root.manager.disable = level\n    root.manager._clear_cache()\n\ndef shutdown(handlerList=_handlerList):\n    \"\"\"\n    Perform any cleanup actions in the logging system (e.g. flushing\n    buffers).\n\n    Should be called at application exit.\n    \"\"\"\n    for wr in reversed(handlerList[:]):\n        #errors might occur, for example, if files are locked\n        #we just ignore them if raiseExceptions is not set\n        try:\n            h = wr()\n            if h:\n                try:\n                    h.acquire()\n                    h.flush()\n                    h.close()\n                except (OSError, ValueError):\n                    # Ignore errors which might be caused\n                    # because handlers have been closed but\n                    # references to them are still around at\n                    # application exit.\n                    pass\n                finally:\n                    h.release()\n        except: # ignore everything, as we're shutting down\n            if raiseExceptions:\n                raise\n            #else, swallow\n\n#Let's try and shutdown automatically on application exit...\nimport atexit\natexit.register(shutdown)\n\n# Null handler\n\nclass NullHandler(Handler):\n    \"\"\"\n    This handler does nothing. It's intended to be used to avoid the\n    \"No handlers could be found for logger XXX\" one-off warning. This is\n    important for library code, which may contain code to log events. If a user\n    of the library does not configure logging, the one-off warning might be\n    produced; to avoid this, the library developer simply needs to instantiate\n    a NullHandler and add it to the top-level logger of the library module or\n    package.\n    \"\"\"\n    def handle(self, record):\n        \"\"\"Stub.\"\"\"\n\n    def emit(self, record):\n        \"\"\"Stub.\"\"\"\n\n    def createLock(self):\n        self.lock = None\n\n    def _at_fork_reinit(self):\n        pass\n\n# Warnings integration\n\n_warnings_showwarning = None\n\ndef _showwarning(message, category, filename, lineno, file=None, line=None):\n    \"\"\"\n    Implementation of showwarnings which redirects to logging, which will first\n    check to see if the file parameter is None. If a file is specified, it will\n    delegate to the original warnings implementation of showwarning. Otherwise,\n    it will call warnings.formatwarning and will log the resulting string to a\n    warnings logger named \"py.warnings\" with level logging.WARNING.\n    \"\"\"\n    if file is not None:\n        if _warnings_showwarning is not None:\n            _warnings_showwarning(message, category, filename, lineno, file, line)\n    else:\n        s = warnings.formatwarning(message, category, filename, lineno, line)\n        logger = getLogger(\"py.warnings\")\n        if not logger.handlers:\n            logger.addHandler(NullHandler())\n        # bpo-46557: Log str(s) as msg instead of logger.warning(\"%s\", s)\n        # since some log aggregation tools group logs by the msg arg\n        logger.warning(str(s))\n\ndef captureWarnings(capture):\n    \"\"\"\n    If capture is true, redirect all warnings to the logging package.\n    If capture is False, ensure that warnings are not redirected to logging\n    but to their original destinations.\n    \"\"\"\n    global _warnings_showwarning\n    if capture:\n        if _warnings_showwarning is None:\n            _warnings_showwarning = warnings.showwarning\n            warnings.showwarning = _showwarning\n    else:\n        if _warnings_showwarning is not None:\n            warnings.showwarning = _warnings_showwarning\n            _warnings_showwarning = None\n", 2266], "<frozen posixpath>": ["\"\"\"Common operations on Posix pathnames.\n\nInstead of importing this module directly, import os and refer to\nthis module as os.path.  The \"os.path\" name is an alias for this\nmodule on Posix systems; on other systems (e.g. Windows),\nos.path provides the same operations in a manner specific to that\nplatform, and is an alias to another module (e.g. ntpath).\n\nSome of this can actually be useful on non-Posix systems too, e.g.\nfor manipulation of the pathname component of URLs.\n\"\"\"\n\n# Strings representing various path-related bits and pieces.\n# These are primarily for export; internally, they are hardcoded.\n# Should be set before imports for resolving cyclic dependency.\ncurdir = '.'\npardir = '..'\nextsep = '.'\nsep = '/'\npathsep = ':'\ndefpath = '/bin:/usr/bin'\naltsep = None\ndevnull = '/dev/null'\n\nimport os\nimport sys\nimport stat\nimport genericpath\nfrom genericpath import *\n\n__all__ = [\"normcase\",\"isabs\",\"join\",\"splitdrive\",\"split\",\"splitext\",\n           \"basename\",\"dirname\",\"commonprefix\",\"getsize\",\"getmtime\",\n           \"getatime\",\"getctime\",\"islink\",\"exists\",\"lexists\",\"isdir\",\"isfile\",\n           \"ismount\", \"expanduser\",\"expandvars\",\"normpath\",\"abspath\",\n           \"samefile\",\"sameopenfile\",\"samestat\",\n           \"curdir\",\"pardir\",\"sep\",\"pathsep\",\"defpath\",\"altsep\",\"extsep\",\n           \"devnull\",\"realpath\",\"supports_unicode_filenames\",\"relpath\",\n           \"commonpath\", \"ALLOW_MISSING\"]\n\n\ndef _get_sep(path):\n    if isinstance(path, bytes):\n        return b'/'\n    else:\n        return '/'\n\n# Normalize the case of a pathname.  Trivial in Posix, string.lower on Mac.\n# On MS-DOS this may also turn slashes into backslashes; however, other\n# normalizations (such as optimizing '../' away) are not allowed\n# (another function should be defined to do that).\n\ndef normcase(s):\n    \"\"\"Normalize case of pathname.  Has no effect under Posix\"\"\"\n    return os.fspath(s)\n\n\n# Return whether a path is absolute.\n# Trivial in Posix, harder on the Mac or MS-DOS.\n\ndef isabs(s):\n    \"\"\"Test whether a path is absolute\"\"\"\n    s = os.fspath(s)\n    sep = _get_sep(s)\n    return s.startswith(sep)\n\n\n# Join pathnames.\n# Ignore the previous parts if a part is absolute.\n# Insert a '/' unless the first part is empty or already ends in '/'.\n\ndef join(a, *p):\n    \"\"\"Join two or more pathname components, inserting '/' as needed.\n    If any component is an absolute path, all previous path components\n    will be discarded.  An empty last part will result in a path that\n    ends with a separator.\"\"\"\n    a = os.fspath(a)\n    sep = _get_sep(a)\n    path = a\n    try:\n        if not p:\n            path[:0] + sep  #23780: Ensure compatible data type even if p is null.\n        for b in map(os.fspath, p):\n            if b.startswith(sep):\n                path = b\n            elif not path or path.endswith(sep):\n                path += b\n            else:\n                path += sep + b\n    except (TypeError, AttributeError, BytesWarning):\n        genericpath._check_arg_types('join', a, *p)\n        raise\n    return path\n\n\n# Split a path in head (everything up to the last '/') and tail (the\n# rest).  If the path ends in '/', tail will be empty.  If there is no\n# '/' in the path, head  will be empty.\n# Trailing '/'es are stripped from head unless it is the root.\n\ndef split(p):\n    \"\"\"Split a pathname.  Returns tuple \"(head, tail)\" where \"tail\" is\n    everything after the final slash.  Either part may be empty.\"\"\"\n    p = os.fspath(p)\n    sep = _get_sep(p)\n    i = p.rfind(sep) + 1\n    head, tail = p[:i], p[i:]\n    if head and head != sep*len(head):\n        head = head.rstrip(sep)\n    return head, tail\n\n\n# Split a path in root and extension.\n# The extension is everything starting at the last dot in the last\n# pathname component; the root is everything before that.\n# It is always true that root + ext == p.\n\ndef splitext(p):\n    p = os.fspath(p)\n    if isinstance(p, bytes):\n        sep = b'/'\n        extsep = b'.'\n    else:\n        sep = '/'\n        extsep = '.'\n    return genericpath._splitext(p, sep, None, extsep)\nsplitext.__doc__ = genericpath._splitext.__doc__\n\n# Split a pathname into a drive specification and the rest of the\n# path.  Useful on DOS/Windows/NT; on Unix, the drive is always empty.\n\ndef splitdrive(p):\n    \"\"\"Split a pathname into drive and path. On Posix, drive is always\n    empty.\"\"\"\n    p = os.fspath(p)\n    return p[:0], p\n\n\n# Return the tail (basename) part of a path, same as split(path)[1].\n\ndef basename(p):\n    \"\"\"Returns the final component of a pathname\"\"\"\n    p = os.fspath(p)\n    sep = _get_sep(p)\n    i = p.rfind(sep) + 1\n    return p[i:]\n\n\n# Return the head (dirname) part of a path, same as split(path)[0].\n\ndef dirname(p):\n    \"\"\"Returns the directory component of a pathname\"\"\"\n    p = os.fspath(p)\n    sep = _get_sep(p)\n    i = p.rfind(sep) + 1\n    head = p[:i]\n    if head and head != sep*len(head):\n        head = head.rstrip(sep)\n    return head\n\n\n# Is a path a symbolic link?\n# This will always return false on systems where os.lstat doesn't exist.\n\ndef islink(path):\n    \"\"\"Test whether a path is a symbolic link\"\"\"\n    try:\n        st = os.lstat(path)\n    except (OSError, ValueError, AttributeError):\n        return False\n    return stat.S_ISLNK(st.st_mode)\n\n# Being true for dangling symbolic links is also useful.\n\ndef lexists(path):\n    \"\"\"Test whether a path exists.  Returns True for broken symbolic links\"\"\"\n    try:\n        os.lstat(path)\n    except (OSError, ValueError):\n        return False\n    return True\n\n\n# Is a path a mount point?\n# (Does this work for all UNIXes?  Is it even guaranteed to work by Posix?)\n\ndef ismount(path):\n    \"\"\"Test whether a path is a mount point\"\"\"\n    try:\n        s1 = os.lstat(path)\n    except (OSError, ValueError):\n        # It doesn't exist -- so not a mount point. :-)\n        return False\n    else:\n        # A symlink can never be a mount point\n        if stat.S_ISLNK(s1.st_mode):\n            return False\n\n    path = os.fspath(path)\n    if isinstance(path, bytes):\n        parent = join(path, b'..')\n    else:\n        parent = join(path, '..')\n    parent = realpath(parent)\n    try:\n        s2 = os.lstat(parent)\n    except (OSError, ValueError):\n        return False\n\n    dev1 = s1.st_dev\n    dev2 = s2.st_dev\n    if dev1 != dev2:\n        return True     # path/.. on a different device as path\n    ino1 = s1.st_ino\n    ino2 = s2.st_ino\n    if ino1 == ino2:\n        return True     # path/.. is the same i-node as path\n    return False\n\n\n# Expand paths beginning with '~' or '~user'.\n# '~' means $HOME; '~user' means that user's home directory.\n# If the path doesn't begin with '~', or if the user or $HOME is unknown,\n# the path is returned unchanged (leaving error reporting to whatever\n# function is called with the expanded path as argument).\n# See also module 'glob' for expansion of *, ? and [...] in pathnames.\n# (A function should also be defined to do full *sh-style environment\n# variable expansion.)\n\ndef expanduser(path):\n    \"\"\"Expand ~ and ~user constructions.  If user or $HOME is unknown,\n    do nothing.\"\"\"\n    path = os.fspath(path)\n    if isinstance(path, bytes):\n        tilde = b'~'\n    else:\n        tilde = '~'\n    if not path.startswith(tilde):\n        return path\n    sep = _get_sep(path)\n    i = path.find(sep, 1)\n    if i < 0:\n        i = len(path)\n    if i == 1:\n        if 'HOME' not in os.environ:\n            try:\n                import pwd\n            except ImportError:\n                # pwd module unavailable, return path unchanged\n                return path\n            try:\n                userhome = pwd.getpwuid(os.getuid()).pw_dir\n            except KeyError:\n                # bpo-10496: if the current user identifier doesn't exist in the\n                # password database, return the path unchanged\n                return path\n        else:\n            userhome = os.environ['HOME']\n    else:\n        try:\n            import pwd\n        except ImportError:\n            # pwd module unavailable, return path unchanged\n            return path\n        name = path[1:i]\n        if isinstance(name, bytes):\n            name = str(name, 'ASCII')\n        try:\n            pwent = pwd.getpwnam(name)\n        except KeyError:\n            # bpo-10496: if the user name from the path doesn't exist in the\n            # password database, return the path unchanged\n            return path\n        userhome = pwent.pw_dir\n    # if no user home, return the path unchanged on VxWorks\n    if userhome is None and sys.platform == \"vxworks\":\n        return path\n    if isinstance(path, bytes):\n        userhome = os.fsencode(userhome)\n        root = b'/'\n    else:\n        root = '/'\n    userhome = userhome.rstrip(root)\n    return (userhome + path[i:]) or root\n\n\n# Expand paths containing shell variable substitutions.\n# This expands the forms $variable and ${variable} only.\n# Non-existent variables are left unchanged.\n\n_varprog = None\n_varprogb = None\n\ndef expandvars(path):\n    \"\"\"Expand shell variables of form $var and ${var}.  Unknown variables\n    are left unchanged.\"\"\"\n    path = os.fspath(path)\n    global _varprog, _varprogb\n    if isinstance(path, bytes):\n        if b'$' not in path:\n            return path\n        if not _varprogb:\n            import re\n            _varprogb = re.compile(br'\\$(\\w+|\\{[^}]*\\})', re.ASCII)\n        search = _varprogb.search\n        start = b'{'\n        end = b'}'\n        environ = getattr(os, 'environb', None)\n    else:\n        if '$' not in path:\n            return path\n        if not _varprog:\n            import re\n            _varprog = re.compile(r'\\$(\\w+|\\{[^}]*\\})', re.ASCII)\n        search = _varprog.search\n        start = '{'\n        end = '}'\n        environ = os.environ\n    i = 0\n    while True:\n        m = search(path, i)\n        if not m:\n            break\n        i, j = m.span(0)\n        name = m.group(1)\n        if name.startswith(start) and name.endswith(end):\n            name = name[1:-1]\n        try:\n            if environ is None:\n                value = os.fsencode(os.environ[os.fsdecode(name)])\n            else:\n                value = environ[name]\n        except KeyError:\n            i = j\n        else:\n            tail = path[j:]\n            path = path[:i] + value\n            i = len(path)\n            path += tail\n    return path\n\n\n# Normalize a path, e.g. A//B, A/./B and A/foo/../B all become A/B.\n# It should be understood that this may change the meaning of the path\n# if it contains symbolic links!\n\ntry:\n    from posix import _path_normpath\n\nexcept ImportError:\n    def normpath(path):\n        \"\"\"Normalize path, eliminating double slashes, etc.\"\"\"\n        path = os.fspath(path)\n        if isinstance(path, bytes):\n            sep = b'/'\n            empty = b''\n            dot = b'.'\n            dotdot = b'..'\n        else:\n            sep = '/'\n            empty = ''\n            dot = '.'\n            dotdot = '..'\n        if path == empty:\n            return dot\n        initial_slashes = path.startswith(sep)\n        # POSIX allows one or two initial slashes, but treats three or more\n        # as single slash.\n        # (see https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap04.html#tag_04_13)\n        if (initial_slashes and\n            path.startswith(sep*2) and not path.startswith(sep*3)):\n            initial_slashes = 2\n        comps = path.split(sep)\n        new_comps = []\n        for comp in comps:\n            if comp in (empty, dot):\n                continue\n            if (comp != dotdot or (not initial_slashes and not new_comps) or\n                 (new_comps and new_comps[-1] == dotdot)):\n                new_comps.append(comp)\n            elif new_comps:\n                new_comps.pop()\n        comps = new_comps\n        path = sep.join(comps)\n        if initial_slashes:\n            path = sep*initial_slashes + path\n        return path or dot\n\nelse:\n    def normpath(path):\n        \"\"\"Normalize path, eliminating double slashes, etc.\"\"\"\n        path = os.fspath(path)\n        if isinstance(path, bytes):\n            return os.fsencode(_path_normpath(os.fsdecode(path))) or b\".\"\n        return _path_normpath(path) or \".\"\n\n\ndef abspath(path):\n    \"\"\"Return an absolute path.\"\"\"\n    path = os.fspath(path)\n    if not isabs(path):\n        if isinstance(path, bytes):\n            cwd = os.getcwdb()\n        else:\n            cwd = os.getcwd()\n        path = join(cwd, path)\n    return normpath(path)\n\n\n# Return a canonical path (i.e. the absolute location of a file on the\n# filesystem).\n\ndef realpath(filename, *, strict=False):\n    \"\"\"Return the canonical path of the specified filename, eliminating any\nsymbolic links encountered in the path.\"\"\"\n    filename = os.fspath(filename)\n    path, ok = _joinrealpath(filename[:0], filename, strict, {})\n    return abspath(path)\n\n# Join two paths, normalizing and eliminating any symbolic links\n# encountered in the second path.\ndef _joinrealpath(path, rest, strict, seen):\n    if isinstance(path, bytes):\n        sep = b'/'\n        curdir = b'.'\n        pardir = b'..'\n    else:\n        sep = '/'\n        curdir = '.'\n        pardir = '..'\n        getcwd = os.getcwd\n    if strict is ALLOW_MISSING:\n        ignored_error = FileNotFoundError\n    elif strict:\n        ignored_error = ()\n    else:\n        ignored_error = OSError\n\n    maxlinks = None\n\n    if isabs(rest):\n        rest = rest[1:]\n        path = sep\n\n    while rest:\n        name, _, rest = rest.partition(sep)\n        if not name or name == curdir:\n            # current dir\n            continue\n        if name == pardir:\n            # parent dir\n            if path:\n                path, name = split(path)\n                if name == pardir:\n                    path = join(path, pardir, pardir)\n            else:\n                path = pardir\n            continue\n        newpath = join(path, name)\n        try:\n            st = os.lstat(newpath)\n        except ignored_error:\n            is_link = False\n        else:\n            is_link = stat.S_ISLNK(st.st_mode)\n        if not is_link:\n            path = newpath\n            continue\n        # Resolve the symbolic link\n        if newpath in seen:\n            # Already seen this path\n            path = seen[newpath]\n            if path is not None:\n                # use cached value\n                continue\n            # The symlink is not resolved, so we must have a symlink loop.\n            if strict:\n                # Raise OSError(errno.ELOOP)\n                os.stat(newpath)\n            else:\n                # Return already resolved part + rest of the path unchanged.\n                return join(newpath, rest), False\n        seen[newpath] = None # not resolved symlink\n        path, ok = _joinrealpath(path, os.readlink(newpath), strict, seen)\n        if not ok:\n            return join(path, rest), False\n        seen[newpath] = path # resolved symlink\n\n    return path, True\n\n\nsupports_unicode_filenames = (sys.platform == 'darwin')\n\ndef relpath(path, start=None):\n    \"\"\"Return a relative version of a path\"\"\"\n\n    if not path:\n        raise ValueError(\"no path specified\")\n\n    path = os.fspath(path)\n    if isinstance(path, bytes):\n        curdir = b'.'\n        sep = b'/'\n        pardir = b'..'\n    else:\n        curdir = '.'\n        sep = '/'\n        pardir = '..'\n\n    if start is None:\n        start = curdir\n    else:\n        start = os.fspath(start)\n\n    try:\n        start_list = [x for x in abspath(start).split(sep) if x]\n        path_list = [x for x in abspath(path).split(sep) if x]\n        # Work out how much of the filepath is shared by start and path.\n        i = len(commonprefix([start_list, path_list]))\n\n        rel_list = [pardir] * (len(start_list)-i) + path_list[i:]\n        if not rel_list:\n            return curdir\n        return join(*rel_list)\n    except (TypeError, AttributeError, BytesWarning, DeprecationWarning):\n        genericpath._check_arg_types('relpath', path, start)\n        raise\n\n\n# Return the longest common sub-path of the sequence of paths given as input.\n# The paths are not normalized before comparing them (this is the\n# responsibility of the caller). Any trailing separator is stripped from the\n# returned path.\n\ndef commonpath(paths):\n    \"\"\"Given a sequence of path names, returns the longest common sub-path.\"\"\"\n\n    if not paths:\n        raise ValueError('commonpath() arg is an empty sequence')\n\n    paths = tuple(map(os.fspath, paths))\n    if isinstance(paths[0], bytes):\n        sep = b'/'\n        curdir = b'.'\n    else:\n        sep = '/'\n        curdir = '.'\n\n    try:\n        split_paths = [path.split(sep) for path in paths]\n\n        try:\n            isabs, = set(p[:1] == sep for p in paths)\n        except ValueError:\n            raise ValueError(\"Can't mix absolute and relative paths\") from None\n\n        split_paths = [[c for c in s if c and c != curdir] for s in split_paths]\n        s1 = min(split_paths)\n        s2 = max(split_paths)\n        common = s1\n        for i, c in enumerate(s1):\n            if c != s2[i]:\n                common = s1[:i]\n                break\n\n        prefix = sep if isabs else sep[:0]\n        return prefix + sep.join(common)\n    except (TypeError, AttributeError):\n        genericpath._check_arg_types('commonpath', *paths)\n        raise\n", 569], "<frozen abc>": ["# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Abstract Base Classes (ABCs) according to PEP 3119.\"\"\"\n\n\ndef abstractmethod(funcobj):\n    \"\"\"A decorator indicating abstract methods.\n\n    Requires that the metaclass is ABCMeta or derived from it.  A\n    class that has a metaclass derived from ABCMeta cannot be\n    instantiated unless all of its abstract methods are overridden.\n    The abstract methods can be called using any of the normal\n    'super' call mechanisms.  abstractmethod() may be used to declare\n    abstract methods for properties and descriptors.\n\n    Usage:\n\n        class C(metaclass=ABCMeta):\n            @abstractmethod\n            def my_abstract_method(self, arg1, arg2, argN):\n                ...\n    \"\"\"\n    funcobj.__isabstractmethod__ = True\n    return funcobj\n\n\nclass abstractclassmethod(classmethod):\n    \"\"\"A decorator indicating abstract classmethods.\n\n    Deprecated, use 'classmethod' with 'abstractmethod' instead:\n\n        class C(ABC):\n            @classmethod\n            @abstractmethod\n            def my_abstract_classmethod(cls, ...):\n                ...\n\n    \"\"\"\n\n    __isabstractmethod__ = True\n\n    def __init__(self, callable):\n        callable.__isabstractmethod__ = True\n        super().__init__(callable)\n\n\nclass abstractstaticmethod(staticmethod):\n    \"\"\"A decorator indicating abstract staticmethods.\n\n    Deprecated, use 'staticmethod' with 'abstractmethod' instead:\n\n        class C(ABC):\n            @staticmethod\n            @abstractmethod\n            def my_abstract_staticmethod(...):\n                ...\n\n    \"\"\"\n\n    __isabstractmethod__ = True\n\n    def __init__(self, callable):\n        callable.__isabstractmethod__ = True\n        super().__init__(callable)\n\n\nclass abstractproperty(property):\n    \"\"\"A decorator indicating abstract properties.\n\n    Deprecated, use 'property' with 'abstractmethod' instead:\n\n        class C(ABC):\n            @property\n            @abstractmethod\n            def my_abstract_property(self):\n                ...\n\n    \"\"\"\n\n    __isabstractmethod__ = True\n\n\ntry:\n    from _abc import (get_cache_token, _abc_init, _abc_register,\n                      _abc_instancecheck, _abc_subclasscheck, _get_dump,\n                      _reset_registry, _reset_caches)\nexcept ImportError:\n    from _py_abc import ABCMeta, get_cache_token\n    ABCMeta.__module__ = 'abc'\nelse:\n    class ABCMeta(type):\n        \"\"\"Metaclass for defining Abstract Base Classes (ABCs).\n\n        Use this metaclass to create an ABC.  An ABC can be subclassed\n        directly, and then acts as a mix-in class.  You can also register\n        unrelated concrete classes (even built-in classes) and unrelated\n        ABCs as 'virtual subclasses' -- these and their descendants will\n        be considered subclasses of the registering ABC by the built-in\n        issubclass() function, but the registering ABC won't show up in\n        their MRO (Method Resolution Order) nor will method\n        implementations defined by the registering ABC be callable (not\n        even via super()).\n        \"\"\"\n        def __new__(mcls, name, bases, namespace, /, **kwargs):\n            cls = super().__new__(mcls, name, bases, namespace, **kwargs)\n            _abc_init(cls)\n            return cls\n\n        def register(cls, subclass):\n            \"\"\"Register a virtual subclass of an ABC.\n\n            Returns the subclass, to allow usage as a class decorator.\n            \"\"\"\n            return _abc_register(cls, subclass)\n\n        def __instancecheck__(cls, instance):\n            \"\"\"Override for isinstance(instance, cls).\"\"\"\n            return _abc_instancecheck(cls, instance)\n\n        def __subclasscheck__(cls, subclass):\n            \"\"\"Override for issubclass(subclass, cls).\"\"\"\n            return _abc_subclasscheck(cls, subclass)\n\n        def _dump_registry(cls, file=None):\n            \"\"\"Debug helper to print the ABC registry.\"\"\"\n            print(f\"Class: {cls.__module__}.{cls.__qualname__}\", file=file)\n            print(f\"Inv. counter: {get_cache_token()}\", file=file)\n            (_abc_registry, _abc_cache, _abc_negative_cache,\n             _abc_negative_cache_version) = _get_dump(cls)\n            print(f\"_abc_registry: {_abc_registry!r}\", file=file)\n            print(f\"_abc_cache: {_abc_cache!r}\", file=file)\n            print(f\"_abc_negative_cache: {_abc_negative_cache!r}\", file=file)\n            print(f\"_abc_negative_cache_version: {_abc_negative_cache_version!r}\",\n                  file=file)\n\n        def _abc_registry_clear(cls):\n            \"\"\"Clear the registry (for debugging or testing).\"\"\"\n            _reset_registry(cls)\n\n        def _abc_caches_clear(cls):\n            \"\"\"Clear the caches (for debugging or testing).\"\"\"\n            _reset_caches(cls)\n\n\ndef update_abstractmethods(cls):\n    \"\"\"Recalculate the set of abstract methods of an abstract class.\n\n    If a class has had one of its abstract methods implemented after the\n    class was created, the method will not be considered implemented until\n    this function is called. Alternatively, if a new abstract method has been\n    added to the class, it will only be considered an abstract method of the\n    class after this function is called.\n\n    This function should be called before any use is made of the class,\n    usually in class decorators that add methods to the subject class.\n\n    Returns cls, to allow usage as a class decorator.\n\n    If cls is not an instance of ABCMeta, does nothing.\n    \"\"\"\n    if not hasattr(cls, '__abstractmethods__'):\n        # We check for __abstractmethods__ here because cls might by a C\n        # implementation or a python implementation (especially during\n        # testing), and we want to handle both cases.\n        return cls\n\n    abstracts = set()\n    # Check the existing abstract methods of the parents, keep only the ones\n    # that are not implemented.\n    for scls in cls.__bases__:\n        for name in getattr(scls, '__abstractmethods__', ()):\n            value = getattr(cls, name, None)\n            if getattr(value, \"__isabstractmethod__\", False):\n                abstracts.add(name)\n    # Also add any other newly added abstract methods.\n    for name, value in cls.__dict__.items():\n        if getattr(value, \"__isabstractmethod__\", False):\n            abstracts.add(name)\n    cls.__abstractmethods__ = frozenset(abstracts)\n    return cls\n\n\nclass ABC(metaclass=ABCMeta):\n    \"\"\"Helper class that provides a standard way to create an ABC using\n    inheritance.\n    \"\"\"\n    __slots__ = ()\n", 188], "<frozen genericpath>": ["\"\"\"\nPath operations common to more than one OS\nDo not use directly.  The OS specific modules import the appropriate\nfunctions from this module themselves.\n\"\"\"\nimport os\nimport stat\n\n__all__ = ['commonprefix', 'exists', 'getatime', 'getctime', 'getmtime',\n           'getsize', 'isdir', 'isfile', 'samefile', 'sameopenfile',\n           'samestat', 'ALLOW_MISSING']\n\n\n# Does a path exist?\n# This is false for dangling symbolic links on systems that support them.\ndef exists(path):\n    \"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\n    try:\n        os.stat(path)\n    except (OSError, ValueError):\n        return False\n    return True\n\n\n# This follows symbolic links, so both islink() and isdir() can be true\n# for the same path on systems that support symlinks\ndef isfile(path):\n    \"\"\"Test whether a path is a regular file\"\"\"\n    try:\n        st = os.stat(path)\n    except (OSError, ValueError):\n        return False\n    return stat.S_ISREG(st.st_mode)\n\n\n# Is a path a directory?\n# This follows symbolic links, so both islink() and isdir()\n# can be true for the same path on systems that support symlinks\ndef isdir(s):\n    \"\"\"Return true if the pathname refers to an existing directory.\"\"\"\n    try:\n        st = os.stat(s)\n    except (OSError, ValueError):\n        return False\n    return stat.S_ISDIR(st.st_mode)\n\n\ndef getsize(filename):\n    \"\"\"Return the size of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_size\n\n\ndef getmtime(filename):\n    \"\"\"Return the last modification time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_mtime\n\n\ndef getatime(filename):\n    \"\"\"Return the last access time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_atime\n\n\ndef getctime(filename):\n    \"\"\"Return the metadata change time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_ctime\n\n\n# Return the longest prefix of all list elements.\ndef commonprefix(m):\n    \"Given a list of pathnames, returns the longest common leading component\"\n    if not m: return ''\n    # Some people pass in a list of pathname parts to operate in an OS-agnostic\n    # fashion; don't try to translate in that case as that's an abuse of the\n    # API and they are already doing what they need to be OS-agnostic and so\n    # they most likely won't be using an os.PathLike object in the sublists.\n    if not isinstance(m[0], (list, tuple)):\n        m = tuple(map(os.fspath, m))\n    s1 = min(m)\n    s2 = max(m)\n    for i, c in enumerate(s1):\n        if c != s2[i]:\n            return s1[:i]\n    return s1\n\n# Are two stat buffers (obtained from stat, fstat or lstat)\n# describing the same file?\ndef samestat(s1, s2):\n    \"\"\"Test whether two stat buffers reference the same file\"\"\"\n    return (s1.st_ino == s2.st_ino and\n            s1.st_dev == s2.st_dev)\n\n\n# Are two filenames really pointing to the same file?\ndef samefile(f1, f2):\n    \"\"\"Test whether two pathnames reference the same actual file or directory\n\n    This is determined by the device number and i-node number and\n    raises an exception if an os.stat() call on either pathname fails.\n    \"\"\"\n    s1 = os.stat(f1)\n    s2 = os.stat(f2)\n    return samestat(s1, s2)\n\n\n# Are two open files really referencing the same file?\n# (Not necessarily the same file descriptor!)\ndef sameopenfile(fp1, fp2):\n    \"\"\"Test whether two open file objects reference the same file\"\"\"\n    s1 = os.fstat(fp1)\n    s2 = os.fstat(fp2)\n    return samestat(s1, s2)\n\n\n# Split a path in root and extension.\n# The extension is everything starting at the last dot in the last\n# pathname component; the root is everything before that.\n# It is always true that root + ext == p.\n\n# Generic implementation of splitext, to be parametrized with\n# the separators\ndef _splitext(p, sep, altsep, extsep):\n    \"\"\"Split the extension from a pathname.\n\n    Extension is everything from the last dot to the end, ignoring\n    leading dots.  Returns \"(root, ext)\"; ext may be empty.\"\"\"\n    # NOTE: This code must work for text and bytes strings.\n\n    sepIndex = p.rfind(sep)\n    if altsep:\n        altsepIndex = p.rfind(altsep)\n        sepIndex = max(sepIndex, altsepIndex)\n\n    dotIndex = p.rfind(extsep)\n    if dotIndex > sepIndex:\n        # skip all leading dots\n        filenameIndex = sepIndex + 1\n        while filenameIndex < dotIndex:\n            if p[filenameIndex:filenameIndex+1] != extsep:\n                return p[:dotIndex], p[dotIndex:]\n            filenameIndex += 1\n\n    return p, p[:0]\n\ndef _check_arg_types(funcname, *args):\n    hasstr = hasbytes = False\n    for s in args:\n        if isinstance(s, str):\n            hasstr = True\n        elif isinstance(s, bytes):\n            hasbytes = True\n        else:\n            raise TypeError(f'{funcname}() argument must be str, bytes, or '\n                            f'os.PathLike object, not {s.__class__.__name__!r}') from None\n    if hasstr and hasbytes:\n        raise TypeError(\"Can't mix strings and bytes in path components\") from None\n\n# A singleton with a true boolean value.\n@object.__new__\nclass ALLOW_MISSING:\n    \"\"\"Special value for use in realpath().\"\"\"\n    def __repr__(self):\n        return 'os.path.ALLOW_MISSING'\n    def __reduce__(self):\n        return self.__class__.__name__\n", 164], "/usr/local/lib/python3.11/threading.py": ["\"\"\"Thread module emulating a subset of Java's threading model.\"\"\"\n\nimport os as _os\nimport sys as _sys\nimport _thread\nimport functools\n\nfrom time import monotonic as _time\nfrom _weakrefset import WeakSet\nfrom itertools import islice as _islice, count as _count\ntry:\n    from _collections import deque as _deque\nexcept ImportError:\n    from collections import deque as _deque\n\n# Note regarding PEP 8 compliant names\n#  This threading model was originally inspired by Java, and inherited\n# the convention of camelCase function and method names from that\n# language. Those original names are not in any imminent danger of\n# being deprecated (even for Py3k),so this module provides them as an\n# alias for the PEP 8 compliant names\n# Note that using the new PEP 8 compliant names facilitates substitution\n# with the multiprocessing module, which doesn't provide the old\n# Java inspired names.\n\n__all__ = ['get_ident', 'active_count', 'Condition', 'current_thread',\n           'enumerate', 'main_thread', 'TIMEOUT_MAX',\n           'Event', 'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore', 'Thread',\n           'Barrier', 'BrokenBarrierError', 'Timer', 'ThreadError',\n           'setprofile', 'settrace', 'local', 'stack_size',\n           'excepthook', 'ExceptHookArgs', 'gettrace', 'getprofile']\n\n# Rename some stuff so \"from threading import *\" is safe\n_start_new_thread = _thread.start_new_thread\n_allocate_lock = _thread.allocate_lock\n_set_sentinel = _thread._set_sentinel\nget_ident = _thread.get_ident\ntry:\n    get_native_id = _thread.get_native_id\n    _HAVE_THREAD_NATIVE_ID = True\n    __all__.append('get_native_id')\nexcept AttributeError:\n    _HAVE_THREAD_NATIVE_ID = False\nThreadError = _thread.error\ntry:\n    _CRLock = _thread.RLock\nexcept AttributeError:\n    _CRLock = None\nTIMEOUT_MAX = _thread.TIMEOUT_MAX\ndel _thread\n\n\n# Support for profile and trace hooks\n\n_profile_hook = None\n_trace_hook = None\n\ndef setprofile(func):\n    \"\"\"Set a profile function for all threads started from the threading module.\n\n    The func will be passed to sys.setprofile() for each thread, before its\n    run() method is called.\n\n    \"\"\"\n    global _profile_hook\n    _profile_hook = func\n\ndef getprofile():\n    \"\"\"Get the profiler function as set by threading.setprofile().\"\"\"\n    return _profile_hook\n\ndef settrace(func):\n    \"\"\"Set a trace function for all threads started from the threading module.\n\n    The func will be passed to sys.settrace() for each thread, before its run()\n    method is called.\n\n    \"\"\"\n    global _trace_hook\n    _trace_hook = func\n\ndef gettrace():\n    \"\"\"Get the trace function as set by threading.settrace().\"\"\"\n    return _trace_hook\n\n# Synchronization classes\n\nLock = _allocate_lock\n\ndef RLock(*args, **kwargs):\n    \"\"\"Factory function that returns a new reentrant lock.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it again\n    without blocking; the thread must release it once for each time it has\n    acquired it.\n\n    \"\"\"\n    if _CRLock is None:\n        return _PyRLock(*args, **kwargs)\n    return _CRLock(*args, **kwargs)\n\nclass _RLock:\n    \"\"\"This class implements reentrant lock objects.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it\n    again without blocking; the thread must release it once for each time it\n    has acquired it.\n\n    \"\"\"\n\n    def __init__(self):\n        self._block = _allocate_lock()\n        self._owner = None\n        self._count = 0\n\n    def __repr__(self):\n        owner = self._owner\n        try:\n            owner = _active[owner].name\n        except KeyError:\n            pass\n        return \"<%s %s.%s object owner=%r count=%d at %s>\" % (\n            \"locked\" if self._block.locked() else \"unlocked\",\n            self.__class__.__module__,\n            self.__class__.__qualname__,\n            owner,\n            self._count,\n            hex(id(self))\n        )\n\n    def _at_fork_reinit(self):\n        self._block._at_fork_reinit()\n        self._owner = None\n        self._count = 0\n\n    def acquire(self, blocking=True, timeout=-1):\n        \"\"\"Acquire a lock, blocking or non-blocking.\n\n        When invoked without arguments: if this thread already owns the lock,\n        increment the recursion level by one, and return immediately. Otherwise,\n        if another thread owns the lock, block until the lock is unlocked. Once\n        the lock is unlocked (not owned by any thread), then grab ownership, set\n        the recursion level to one, and return. If more than one thread is\n        blocked waiting until the lock is unlocked, only one at a time will be\n        able to grab ownership of the lock. There is no return value in this\n        case.\n\n        When invoked with the blocking argument set to true, do the same thing\n        as when called without arguments, and return true.\n\n        When invoked with the blocking argument set to false, do not block. If a\n        call without an argument would block, return false immediately;\n        otherwise, do the same thing as when called without arguments, and\n        return true.\n\n        When invoked with the floating-point timeout argument set to a positive\n        value, block for at most the number of seconds specified by timeout\n        and as long as the lock cannot be acquired.  Return true if the lock has\n        been acquired, false if the timeout has elapsed.\n\n        \"\"\"\n        me = get_ident()\n        if self._owner == me:\n            self._count += 1\n            return 1\n        rc = self._block.acquire(blocking, timeout)\n        if rc:\n            self._owner = me\n            self._count = 1\n        return rc\n\n    __enter__ = acquire\n\n    def release(self):\n        \"\"\"Release a lock, decrementing the recursion level.\n\n        If after the decrement it is zero, reset the lock to unlocked (not owned\n        by any thread), and if any other threads are blocked waiting for the\n        lock to become unlocked, allow exactly one of them to proceed. If after\n        the decrement the recursion level is still nonzero, the lock remains\n        locked and owned by the calling thread.\n\n        Only call this method when the calling thread owns the lock. A\n        RuntimeError is raised if this method is called when the lock is\n        unlocked.\n\n        There is no return value.\n\n        \"\"\"\n        if self._owner != get_ident():\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        self._count = count = self._count - 1\n        if not count:\n            self._owner = None\n            self._block.release()\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n    # Internal methods used by condition variables\n\n    def _acquire_restore(self, state):\n        self._block.acquire()\n        self._count, self._owner = state\n\n    def _release_save(self):\n        if self._count == 0:\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        count = self._count\n        self._count = 0\n        owner = self._owner\n        self._owner = None\n        self._block.release()\n        return (count, owner)\n\n    def _is_owned(self):\n        return self._owner == get_ident()\n\n    # Internal method used for reentrancy checks\n\n    def _recursion_count(self):\n        if self._owner != get_ident():\n            return 0\n        return self._count\n\n_PyRLock = _RLock\n\n\nclass Condition:\n    \"\"\"Class that implements a condition variable.\n\n    A condition variable allows one or more threads to wait until they are\n    notified by another thread.\n\n    If the lock argument is given and not None, it must be a Lock or RLock\n    object, and it is used as the underlying lock. Otherwise, a new RLock object\n    is created and used as the underlying lock.\n\n    \"\"\"\n\n    def __init__(self, lock=None):\n        if lock is None:\n            lock = RLock()\n        self._lock = lock\n        # Export the lock's acquire() and release() methods\n        self.acquire = lock.acquire\n        self.release = lock.release\n        # If the lock defines _release_save() and/or _acquire_restore(),\n        # these override the default implementations (which just call\n        # release() and acquire() on the lock).  Ditto for _is_owned().\n        try:\n            self._release_save = lock._release_save\n        except AttributeError:\n            pass\n        try:\n            self._acquire_restore = lock._acquire_restore\n        except AttributeError:\n            pass\n        try:\n            self._is_owned = lock._is_owned\n        except AttributeError:\n            pass\n        self._waiters = _deque()\n\n    def _at_fork_reinit(self):\n        self._lock._at_fork_reinit()\n        self._waiters.clear()\n\n    def __enter__(self):\n        return self._lock.__enter__()\n\n    def __exit__(self, *args):\n        return self._lock.__exit__(*args)\n\n    def __repr__(self):\n        return \"<Condition(%s, %d)>\" % (self._lock, len(self._waiters))\n\n    def _release_save(self):\n        self._lock.release()           # No state to save\n\n    def _acquire_restore(self, x):\n        self._lock.acquire()           # Ignore saved state\n\n    def _is_owned(self):\n        # Return True if lock is owned by current_thread.\n        # This method is called only if _lock doesn't have _is_owned().\n        if self._lock.acquire(False):\n            self._lock.release()\n            return False\n        else:\n            return True\n\n    def wait(self, timeout=None):\n        \"\"\"Wait until notified or until a timeout occurs.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method releases the underlying lock, and then blocks until it is\n        awakened by a notify() or notify_all() call for the same condition\n        variable in another thread, or until the optional timeout occurs. Once\n        awakened or timed out, it re-acquires the lock and returns.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        When the underlying lock is an RLock, it is not released using its\n        release() method, since this may not actually unlock the lock when it\n        was acquired multiple times recursively. Instead, an internal interface\n        of the RLock class is used, which really unlocks it even when it has\n        been recursively acquired several times. Another internal interface is\n        then used to restore the recursion level when the lock is reacquired.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot wait on un-acquired lock\")\n        waiter = _allocate_lock()\n        waiter.acquire()\n        self._waiters.append(waiter)\n        saved_state = self._release_save()\n        gotit = False\n        try:    # restore state no matter what (e.g., KeyboardInterrupt)\n            if timeout is None:\n                waiter.acquire()\n                gotit = True\n            else:\n                if timeout > 0:\n                    gotit = waiter.acquire(True, timeout)\n                else:\n                    gotit = waiter.acquire(False)\n            return gotit\n        finally:\n            self._acquire_restore(saved_state)\n            if not gotit:\n                try:\n                    self._waiters.remove(waiter)\n                except ValueError:\n                    pass\n\n    def wait_for(self, predicate, timeout=None):\n        \"\"\"Wait until a condition evaluates to True.\n\n        predicate should be a callable which result will be interpreted as a\n        boolean value.  A timeout may be provided giving the maximum time to\n        wait.\n\n        \"\"\"\n        endtime = None\n        waittime = timeout\n        result = predicate()\n        while not result:\n            if waittime is not None:\n                if endtime is None:\n                    endtime = _time() + waittime\n                else:\n                    waittime = endtime - _time()\n                    if waittime <= 0:\n                        break\n            self.wait(waittime)\n            result = predicate()\n        return result\n\n    def notify(self, n=1):\n        \"\"\"Wake up one or more threads waiting on this condition, if any.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method wakes up at most n of the threads waiting for the condition\n        variable; it is a no-op if no threads are waiting.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot notify on un-acquired lock\")\n        waiters = self._waiters\n        while waiters and n > 0:\n            waiter = waiters[0]\n            try:\n                waiter.release()\n            except RuntimeError:\n                # gh-92530: The previous call of notify() released the lock,\n                # but was interrupted before removing it from the queue.\n                # It can happen if a signal handler raises an exception,\n                # like CTRL+C which raises KeyboardInterrupt.\n                pass\n            else:\n                n -= 1\n            try:\n                waiters.remove(waiter)\n            except ValueError:\n                pass\n\n    def notify_all(self):\n        \"\"\"Wake up all threads waiting on this condition.\n\n        If the calling thread has not acquired the lock when this method\n        is called, a RuntimeError is raised.\n\n        \"\"\"\n        self.notify(len(self._waiters))\n\n    def notifyAll(self):\n        \"\"\"Wake up all threads waiting on this condition.\n\n        This method is deprecated, use notify_all() instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('notifyAll() is deprecated, use notify_all() instead',\n                      DeprecationWarning, stacklevel=2)\n        self.notify_all()\n\n\nclass Semaphore:\n    \"\"\"This class implements semaphore objects.\n\n    Semaphores manage a counter representing the number of release() calls minus\n    the number of acquire() calls, plus an initial value. The acquire() method\n    blocks if necessary until it can return without making the counter\n    negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    # After Tim Peters' semaphore class, but not quite the same (no maximum)\n\n    def __init__(self, value=1):\n        if value < 0:\n            raise ValueError(\"semaphore initial value must be >= 0\")\n        self._cond = Condition(Lock())\n        self._value = value\n\n    def __repr__(self):\n        cls = self.__class__\n        return (f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}:\"\n                f\" value={self._value}>\")\n\n    def acquire(self, blocking=True, timeout=None):\n        \"\"\"Acquire a semaphore, decrementing the internal counter by one.\n\n        When invoked without arguments: if the internal counter is larger than\n        zero on entry, decrement it by one and return immediately. If it is zero\n        on entry, block, waiting until some other thread has called release() to\n        make it larger than zero. This is done with proper interlocking so that\n        if multiple acquire() calls are blocked, release() will wake exactly one\n        of them up. The implementation may pick one at random, so the order in\n        which blocked threads are awakened should not be relied on. There is no\n        return value in this case.\n\n        When invoked with blocking set to true, do the same thing as when called\n        without arguments, and return true.\n\n        When invoked with blocking set to false, do not block. If a call without\n        an argument would block, return false immediately; otherwise, do the\n        same thing as when called without arguments, and return true.\n\n        When invoked with a timeout other than None, it will block for at\n        most timeout seconds.  If acquire does not complete successfully in\n        that interval, return false.  Return true otherwise.\n\n        \"\"\"\n        if not blocking and timeout is not None:\n            raise ValueError(\"can't specify timeout for non-blocking acquire\")\n        rc = False\n        endtime = None\n        with self._cond:\n            while self._value == 0:\n                if not blocking:\n                    break\n                if timeout is not None:\n                    if endtime is None:\n                        endtime = _time() + timeout\n                    else:\n                        timeout = endtime - _time()\n                        if timeout <= 0:\n                            break\n                self._cond.wait(timeout)\n            else:\n                self._value -= 1\n                rc = True\n        return rc\n\n    __enter__ = acquire\n\n    def release(self, n=1):\n        \"\"\"Release a semaphore, incrementing the internal counter by one or more.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        \"\"\"\n        if n < 1:\n            raise ValueError('n must be one or more')\n        with self._cond:\n            self._value += n\n            for i in range(n):\n                self._cond.notify()\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n\nclass BoundedSemaphore(Semaphore):\n    \"\"\"Implements a bounded semaphore.\n\n    A bounded semaphore checks to make sure its current value doesn't exceed its\n    initial value. If it does, ValueError is raised. In most situations\n    semaphores are used to guard resources with limited capacity.\n\n    If the semaphore is released too many times it's a sign of a bug. If not\n    given, value defaults to 1.\n\n    Like regular semaphores, bounded semaphores manage a counter representing\n    the number of release() calls minus the number of acquire() calls, plus an\n    initial value. The acquire() method blocks if necessary until it can return\n    without making the counter negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    def __init__(self, value=1):\n        Semaphore.__init__(self, value)\n        self._initial_value = value\n\n    def __repr__(self):\n        cls = self.__class__\n        return (f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}:\"\n                f\" value={self._value}/{self._initial_value}>\")\n\n    def release(self, n=1):\n        \"\"\"Release a semaphore, incrementing the internal counter by one or more.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        If the number of releases exceeds the number of acquires,\n        raise a ValueError.\n\n        \"\"\"\n        if n < 1:\n            raise ValueError('n must be one or more')\n        with self._cond:\n            if self._value + n > self._initial_value:\n                raise ValueError(\"Semaphore released too many times\")\n            self._value += n\n            for i in range(n):\n                self._cond.notify()\n\n\nclass Event:\n    \"\"\"Class implementing event objects.\n\n    Events manage a flag that can be set to true with the set() method and reset\n    to false with the clear() method. The wait() method blocks until the flag is\n    true.  The flag is initially false.\n\n    \"\"\"\n\n    # After Tim Peters' event class (without is_posted())\n\n    def __init__(self):\n        self._cond = Condition(Lock())\n        self._flag = False\n\n    def __repr__(self):\n        cls = self.__class__\n        status = 'set' if self._flag else 'unset'\n        return f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}: {status}>\"\n\n    def _at_fork_reinit(self):\n        # Private method called by Thread._reset_internal_locks()\n        self._cond._at_fork_reinit()\n\n    def is_set(self):\n        \"\"\"Return true if and only if the internal flag is true.\"\"\"\n        return self._flag\n\n    def isSet(self):\n        \"\"\"Return true if and only if the internal flag is true.\n\n        This method is deprecated, use is_set() instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('isSet() is deprecated, use is_set() instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.is_set()\n\n    def set(self):\n        \"\"\"Set the internal flag to true.\n\n        All threads waiting for it to become true are awakened. Threads\n        that call wait() once the flag is true will not block at all.\n\n        \"\"\"\n        with self._cond:\n            self._flag = True\n            self._cond.notify_all()\n\n    def clear(self):\n        \"\"\"Reset the internal flag to false.\n\n        Subsequently, threads calling wait() will block until set() is called to\n        set the internal flag to true again.\n\n        \"\"\"\n        with self._cond:\n            self._flag = False\n\n    def wait(self, timeout=None):\n        \"\"\"Block until the internal flag is true.\n\n        If the internal flag is true on entry, return immediately. Otherwise,\n        block until another thread calls set() to set the flag to true, or until\n        the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        This method returns the internal flag on exit, so it will always return\n        True except if a timeout is given and the operation times out.\n\n        \"\"\"\n        with self._cond:\n            signaled = self._flag\n            if not signaled:\n                signaled = self._cond.wait(timeout)\n            return signaled\n\n\n# A barrier class.  Inspired in part by the pthread_barrier_* api and\n# the CyclicBarrier class from Java.  See\n# http://sourceware.org/pthreads-win32/manual/pthread_barrier_init.html and\n# http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/\n#        CyclicBarrier.html\n# for information.\n# We maintain two main states, 'filling' and 'draining' enabling the barrier\n# to be cyclic.  Threads are not allowed into it until it has fully drained\n# since the previous cycle.  In addition, a 'resetting' state exists which is\n# similar to 'draining' except that threads leave with a BrokenBarrierError,\n# and a 'broken' state in which all threads get the exception.\nclass Barrier:\n    \"\"\"Implements a Barrier.\n\n    Useful for synchronizing a fixed number of threads at known synchronization\n    points.  Threads block on 'wait()' and are simultaneously awoken once they\n    have all made that call.\n\n    \"\"\"\n\n    def __init__(self, parties, action=None, timeout=None):\n        \"\"\"Create a barrier, initialised to 'parties' threads.\n\n        'action' is a callable which, when supplied, will be called by one of\n        the threads after they have all entered the barrier and just prior to\n        releasing them all. If a 'timeout' is provided, it is used as the\n        default for all subsequent 'wait()' calls.\n\n        \"\"\"\n        self._cond = Condition(Lock())\n        self._action = action\n        self._timeout = timeout\n        self._parties = parties\n        self._state = 0  # 0 filling, 1 draining, -1 resetting, -2 broken\n        self._count = 0\n\n    def __repr__(self):\n        cls = self.__class__\n        if self.broken:\n            return f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}: broken>\"\n        return (f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}:\"\n                f\" waiters={self.n_waiting}/{self.parties}>\")\n\n    def wait(self, timeout=None):\n        \"\"\"Wait for the barrier.\n\n        When the specified number of threads have started waiting, they are all\n        simultaneously awoken. If an 'action' was provided for the barrier, one\n        of the threads will have executed that callback prior to returning.\n        Returns an individual index number from 0 to 'parties-1'.\n\n        \"\"\"\n        if timeout is None:\n            timeout = self._timeout\n        with self._cond:\n            self._enter() # Block while the barrier drains.\n            index = self._count\n            self._count += 1\n            try:\n                if index + 1 == self._parties:\n                    # We release the barrier\n                    self._release()\n                else:\n                    # We wait until someone releases us\n                    self._wait(timeout)\n                return index\n            finally:\n                self._count -= 1\n                # Wake up any threads waiting for barrier to drain.\n                self._exit()\n\n    # Block until the barrier is ready for us, or raise an exception\n    # if it is broken.\n    def _enter(self):\n        while self._state in (-1, 1):\n            # It is draining or resetting, wait until done\n            self._cond.wait()\n        #see if the barrier is in a broken state\n        if self._state < 0:\n            raise BrokenBarrierError\n        assert self._state == 0\n\n    # Optionally run the 'action' and release the threads waiting\n    # in the barrier.\n    def _release(self):\n        try:\n            if self._action:\n                self._action()\n            # enter draining state\n            self._state = 1\n            self._cond.notify_all()\n        except:\n            #an exception during the _action handler.  Break and reraise\n            self._break()\n            raise\n\n    # Wait in the barrier until we are released.  Raise an exception\n    # if the barrier is reset or broken.\n    def _wait(self, timeout):\n        if not self._cond.wait_for(lambda : self._state != 0, timeout):\n            #timed out.  Break the barrier\n            self._break()\n            raise BrokenBarrierError\n        if self._state < 0:\n            raise BrokenBarrierError\n        assert self._state == 1\n\n    # If we are the last thread to exit the barrier, signal any threads\n    # waiting for the barrier to drain.\n    def _exit(self):\n        if self._count == 0:\n            if self._state in (-1, 1):\n                #resetting or draining\n                self._state = 0\n                self._cond.notify_all()\n\n    def reset(self):\n        \"\"\"Reset the barrier to the initial state.\n\n        Any threads currently waiting will get the BrokenBarrier exception\n        raised.\n\n        \"\"\"\n        with self._cond:\n            if self._count > 0:\n                if self._state == 0:\n                    #reset the barrier, waking up threads\n                    self._state = -1\n                elif self._state == -2:\n                    #was broken, set it to reset state\n                    #which clears when the last thread exits\n                    self._state = -1\n            else:\n                self._state = 0\n            self._cond.notify_all()\n\n    def abort(self):\n        \"\"\"Place the barrier into a 'broken' state.\n\n        Useful in case of error.  Any currently waiting threads and threads\n        attempting to 'wait()' will have BrokenBarrierError raised.\n\n        \"\"\"\n        with self._cond:\n            self._break()\n\n    def _break(self):\n        # An internal error was detected.  The barrier is set to\n        # a broken state all parties awakened.\n        self._state = -2\n        self._cond.notify_all()\n\n    @property\n    def parties(self):\n        \"\"\"Return the number of threads required to trip the barrier.\"\"\"\n        return self._parties\n\n    @property\n    def n_waiting(self):\n        \"\"\"Return the number of threads currently waiting at the barrier.\"\"\"\n        # We don't need synchronization here since this is an ephemeral result\n        # anyway.  It returns the correct value in the steady state.\n        if self._state == 0:\n            return self._count\n        return 0\n\n    @property\n    def broken(self):\n        \"\"\"Return True if the barrier is in a broken state.\"\"\"\n        return self._state == -2\n\n# exception raised by the Barrier class\nclass BrokenBarrierError(RuntimeError):\n    pass\n\n\n# Helper to generate new thread names\n_counter = _count(1).__next__\ndef _newname(name_template):\n    return name_template % _counter()\n\n# Active thread administration.\n#\n# bpo-44422: Use a reentrant lock to allow reentrant calls to functions like\n# threading.enumerate().\n_active_limbo_lock = RLock()\n_active = {}    # maps thread id to Thread object\n_limbo = {}\n_dangling = WeakSet()\n\n# Set of Thread._tstate_lock locks of non-daemon threads used by _shutdown()\n# to wait until all Python thread states get deleted:\n# see Thread._set_tstate_lock().\n_shutdown_locks_lock = _allocate_lock()\n_shutdown_locks = set()\n\ndef _maintain_shutdown_locks():\n    \"\"\"\n    Drop any shutdown locks that don't correspond to running threads anymore.\n\n    Calling this from time to time avoids an ever-growing _shutdown_locks\n    set when Thread objects are not joined explicitly. See bpo-37788.\n\n    This must be called with _shutdown_locks_lock acquired.\n    \"\"\"\n    # If a lock was released, the corresponding thread has exited\n    to_remove = [lock for lock in _shutdown_locks if not lock.locked()]\n    _shutdown_locks.difference_update(to_remove)\n\n\n# Main class for threads\n\nclass Thread:\n    \"\"\"A class that represents a thread of control.\n\n    This class can be safely subclassed in a limited fashion. There are two ways\n    to specify the activity: by passing a callable object to the constructor, or\n    by overriding the run() method in a subclass.\n\n    \"\"\"\n\n    _initialized = False\n\n    def __init__(self, group=None, target=None, name=None,\n                 args=(), kwargs=None, *, daemon=None):\n        \"\"\"This constructor should always be called with keyword arguments. Arguments are:\n\n        *group* should be None; reserved for future extension when a ThreadGroup\n        class is implemented.\n\n        *target* is the callable object to be invoked by the run()\n        method. Defaults to None, meaning nothing is called.\n\n        *name* is the thread name. By default, a unique name is constructed of\n        the form \"Thread-N\" where N is a small decimal number.\n\n        *args* is a list or tuple of arguments for the target invocation. Defaults to ().\n\n        *kwargs* is a dictionary of keyword arguments for the target\n        invocation. Defaults to {}.\n\n        If a subclass overrides the constructor, it must make sure to invoke\n        the base class constructor (Thread.__init__()) before doing anything\n        else to the thread.\n\n        \"\"\"\n        assert group is None, \"group argument must be None for now\"\n        if kwargs is None:\n            kwargs = {}\n        if name:\n            name = str(name)\n        else:\n            name = _newname(\"Thread-%d\")\n            if target is not None:\n                try:\n                    target_name = target.__name__\n                    name += f\" ({target_name})\"\n                except AttributeError:\n                    pass\n\n        self._target = target\n        self._name = name\n        self._args = args\n        self._kwargs = kwargs\n        if daemon is not None:\n            self._daemonic = daemon\n        else:\n            self._daemonic = current_thread().daemon\n        self._ident = None\n        if _HAVE_THREAD_NATIVE_ID:\n            self._native_id = None\n        self._tstate_lock = None\n        self._started = Event()\n        self._is_stopped = False\n        self._initialized = True\n        # Copy of sys.stderr used by self._invoke_excepthook()\n        self._stderr = _sys.stderr\n        self._invoke_excepthook = _make_invoke_excepthook()\n        # For debugging and _after_fork()\n        _dangling.add(self)\n\n    def _reset_internal_locks(self, is_alive):\n        # private!  Called by _after_fork() to reset our internal locks as\n        # they may be in an invalid state leading to a deadlock or crash.\n        self._started._at_fork_reinit()\n        if is_alive:\n            # bpo-42350: If the fork happens when the thread is already stopped\n            # (ex: after threading._shutdown() has been called), _tstate_lock\n            # is None. Do nothing in this case.\n            if self._tstate_lock is not None:\n                self._tstate_lock._at_fork_reinit()\n                self._tstate_lock.acquire()\n        else:\n            # The thread isn't alive after fork: it doesn't have a tstate\n            # anymore.\n            self._is_stopped = True\n            self._tstate_lock = None\n\n    def __repr__(self):\n        assert self._initialized, \"Thread.__init__() was not called\"\n        status = \"initial\"\n        if self._started.is_set():\n            status = \"started\"\n        self.is_alive() # easy way to get ._is_stopped set when appropriate\n        if self._is_stopped:\n            status = \"stopped\"\n        if self._daemonic:\n            status += \" daemon\"\n        if self._ident is not None:\n            status += \" %s\" % self._ident\n        return \"<%s(%s, %s)>\" % (self.__class__.__name__, self._name, status)\n\n    def start(self):\n        \"\"\"Start the thread's activity.\n\n        It must be called at most once per thread object. It arranges for the\n        object's run() method to be invoked in a separate thread of control.\n\n        This method will raise a RuntimeError if called more than once on the\n        same thread object.\n\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"thread.__init__() not called\")\n\n        if self._started.is_set():\n            raise RuntimeError(\"threads can only be started once\")\n\n        with _active_limbo_lock:\n            _limbo[self] = self\n        try:\n            _start_new_thread(self._bootstrap, ())\n        except Exception:\n            with _active_limbo_lock:\n                del _limbo[self]\n            raise\n        self._started.wait()\n\n    def run(self):\n        \"\"\"Method representing the thread's activity.\n\n        You may override this method in a subclass. The standard run() method\n        invokes the callable object passed to the object's constructor as the\n        target argument, if any, with sequential and keyword arguments taken\n        from the args and kwargs arguments, respectively.\n\n        \"\"\"\n        try:\n            if self._target is not None:\n                self._target(*self._args, **self._kwargs)\n        finally:\n            # Avoid a refcycle if the thread is running a function with\n            # an argument that has a member that points to the thread.\n            del self._target, self._args, self._kwargs\n\n    def _bootstrap(self):\n        # Wrapper around the real bootstrap code that ignores\n        # exceptions during interpreter cleanup.  Those typically\n        # happen when a daemon thread wakes up at an unfortunate\n        # moment, finds the world around it destroyed, and raises some\n        # random exception *** while trying to report the exception in\n        # _bootstrap_inner() below ***.  Those random exceptions\n        # don't help anybody, and they confuse users, so we suppress\n        # them.  We suppress them only when it appears that the world\n        # indeed has already been destroyed, so that exceptions in\n        # _bootstrap_inner() during normal business hours are properly\n        # reported.  Also, we only suppress them for daemonic threads;\n        # if a non-daemonic encounters this, something else is wrong.\n        try:\n            self._bootstrap_inner()\n        except:\n            if self._daemonic and _sys is None:\n                return\n            raise\n\n    def _set_ident(self):\n        self._ident = get_ident()\n\n    if _HAVE_THREAD_NATIVE_ID:\n        def _set_native_id(self):\n            self._native_id = get_native_id()\n\n    def _set_tstate_lock(self):\n        \"\"\"\n        Set a lock object which will be released by the interpreter when\n        the underlying thread state (see pystate.h) gets deleted.\n        \"\"\"\n        self._tstate_lock = _set_sentinel()\n        self._tstate_lock.acquire()\n\n        if not self.daemon:\n            with _shutdown_locks_lock:\n                _maintain_shutdown_locks()\n                _shutdown_locks.add(self._tstate_lock)\n\n    def _bootstrap_inner(self):\n        try:\n            self._set_ident()\n            self._set_tstate_lock()\n            if _HAVE_THREAD_NATIVE_ID:\n                self._set_native_id()\n            self._started.set()\n            with _active_limbo_lock:\n                _active[self._ident] = self\n                del _limbo[self]\n\n            if _trace_hook:\n                _sys.settrace(_trace_hook)\n            if _profile_hook:\n                _sys.setprofile(_profile_hook)\n\n            try:\n                self.run()\n            except:\n                self._invoke_excepthook(self)\n        finally:\n            self._delete()\n\n    def _stop(self):\n        # After calling ._stop(), .is_alive() returns False and .join() returns\n        # immediately.  ._tstate_lock must be released before calling ._stop().\n        #\n        # Normal case:  C code at the end of the thread's life\n        # (release_sentinel in _threadmodule.c) releases ._tstate_lock, and\n        # that's detected by our ._wait_for_tstate_lock(), called by .join()\n        # and .is_alive().  Any number of threads _may_ call ._stop()\n        # simultaneously (for example, if multiple threads are blocked in\n        # .join() calls), and they're not serialized.  That's harmless -\n        # they'll just make redundant rebindings of ._is_stopped and\n        # ._tstate_lock.  Obscure:  we rebind ._tstate_lock last so that the\n        # \"assert self._is_stopped\" in ._wait_for_tstate_lock() always works\n        # (the assert is executed only if ._tstate_lock is None).\n        #\n        # Special case:  _main_thread releases ._tstate_lock via this\n        # module's _shutdown() function.\n        lock = self._tstate_lock\n        if lock is not None:\n            assert not lock.locked()\n        self._is_stopped = True\n        self._tstate_lock = None\n        if not self.daemon:\n            with _shutdown_locks_lock:\n                # Remove our lock and other released locks from _shutdown_locks\n                _maintain_shutdown_locks()\n\n    def _delete(self):\n        \"Remove current thread from the dict of currently running threads.\"\n        with _active_limbo_lock:\n            del _active[get_ident()]\n            # There must not be any python code between the previous line\n            # and after the lock is released.  Otherwise a tracing function\n            # could try to acquire the lock again in the same thread, (in\n            # current_thread()), and would block.\n\n    def join(self, timeout=None):\n        \"\"\"Wait until the thread terminates.\n\n        This blocks the calling thread until the thread whose join() method is\n        called terminates -- either normally or through an unhandled exception\n        or until the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof). As join() always returns None, you must call\n        is_alive() after join() to decide whether a timeout happened -- if the\n        thread is still alive, the join() call timed out.\n\n        When the timeout argument is not present or None, the operation will\n        block until the thread terminates.\n\n        A thread can be join()ed many times.\n\n        join() raises a RuntimeError if an attempt is made to join the current\n        thread as that would cause a deadlock. It is also an error to join() a\n        thread before it has been started and attempts to do so raises the same\n        exception.\n\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if not self._started.is_set():\n            raise RuntimeError(\"cannot join thread before it is started\")\n        if self is current_thread():\n            raise RuntimeError(\"cannot join current thread\")\n\n        if timeout is None:\n            self._wait_for_tstate_lock()\n        else:\n            # the behavior of a negative timeout isn't documented, but\n            # historically .join(timeout=x) for x<0 has acted as if timeout=0\n            self._wait_for_tstate_lock(timeout=max(timeout, 0))\n\n    def _wait_for_tstate_lock(self, block=True, timeout=-1):\n        # Issue #18808: wait for the thread state to be gone.\n        # At the end of the thread's life, after all knowledge of the thread\n        # is removed from C data structures, C code releases our _tstate_lock.\n        # This method passes its arguments to _tstate_lock.acquire().\n        # If the lock is acquired, the C code is done, and self._stop() is\n        # called.  That sets ._is_stopped to True, and ._tstate_lock to None.\n        lock = self._tstate_lock\n        if lock is None:\n            # already determined that the C code is done\n            assert self._is_stopped\n            return\n\n        try:\n            if lock.acquire(block, timeout):\n                lock.release()\n                self._stop()\n        except:\n            if lock.locked():\n                # bpo-45274: lock.acquire() acquired the lock, but the function\n                # was interrupted with an exception before reaching the\n                # lock.release(). It can happen if a signal handler raises an\n                # exception, like CTRL+C which raises KeyboardInterrupt.\n                lock.release()\n                self._stop()\n            raise\n\n    @property\n    def name(self):\n        \"\"\"A string used for identification purposes only.\n\n        It has no semantics. Multiple threads may be given the same name. The\n        initial name is set by the constructor.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        assert self._initialized, \"Thread.__init__() not called\"\n        self._name = str(name)\n\n    @property\n    def ident(self):\n        \"\"\"Thread identifier of this thread or None if it has not been started.\n\n        This is a nonzero integer. See the get_ident() function. Thread\n        identifiers may be recycled when a thread exits and another thread is\n        created. The identifier is available even after the thread has exited.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._ident\n\n    if _HAVE_THREAD_NATIVE_ID:\n        @property\n        def native_id(self):\n            \"\"\"Native integral thread ID of this thread, or None if it has not been started.\n\n            This is a non-negative integer. See the get_native_id() function.\n            This represents the Thread ID as reported by the kernel.\n\n            \"\"\"\n            assert self._initialized, \"Thread.__init__() not called\"\n            return self._native_id\n\n    def is_alive(self):\n        \"\"\"Return whether the thread is alive.\n\n        This method returns True just before the run() method starts until just\n        after the run() method terminates. See also the module function\n        enumerate().\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        if self._is_stopped or not self._started.is_set():\n            return False\n        self._wait_for_tstate_lock(False)\n        return not self._is_stopped\n\n    @property\n    def daemon(self):\n        \"\"\"A boolean value indicating whether this thread is a daemon thread.\n\n        This must be set before start() is called, otherwise RuntimeError is\n        raised. Its initial value is inherited from the creating thread; the\n        main thread is not a daemon thread and therefore all threads created in\n        the main thread default to daemon = False.\n\n        The entire Python program exits when only daemon threads are left.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._daemonic\n\n    @daemon.setter\n    def daemon(self, daemonic):\n        if not self._initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if self._started.is_set():\n            raise RuntimeError(\"cannot set daemon status of active thread\")\n        self._daemonic = daemonic\n\n    def isDaemon(self):\n        \"\"\"Return whether this thread is a daemon.\n\n        This method is deprecated, use the daemon attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('isDaemon() is deprecated, get the daemon attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.daemon\n\n    def setDaemon(self, daemonic):\n        \"\"\"Set whether this thread is a daemon.\n\n        This method is deprecated, use the .daemon property instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('setDaemon() is deprecated, set the daemon attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        self.daemon = daemonic\n\n    def getName(self):\n        \"\"\"Return a string used for identification purposes only.\n\n        This method is deprecated, use the name attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('getName() is deprecated, get the name attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.name\n\n    def setName(self, name):\n        \"\"\"Set the name string for this thread.\n\n        This method is deprecated, use the name attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('setName() is deprecated, set the name attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        self.name = name\n\n\ntry:\n    from _thread import (_excepthook as excepthook,\n                         _ExceptHookArgs as ExceptHookArgs)\nexcept ImportError:\n    # Simple Python implementation if _thread._excepthook() is not available\n    from traceback import print_exception as _print_exception\n    from collections import namedtuple\n\n    _ExceptHookArgs = namedtuple(\n        'ExceptHookArgs',\n        'exc_type exc_value exc_traceback thread')\n\n    def ExceptHookArgs(args):\n        return _ExceptHookArgs(*args)\n\n    def excepthook(args, /):\n        \"\"\"\n        Handle uncaught Thread.run() exception.\n        \"\"\"\n        if args.exc_type == SystemExit:\n            # silently ignore SystemExit\n            return\n\n        if _sys is not None and _sys.stderr is not None:\n            stderr = _sys.stderr\n        elif args.thread is not None:\n            stderr = args.thread._stderr\n            if stderr is None:\n                # do nothing if sys.stderr is None and sys.stderr was None\n                # when the thread was created\n                return\n        else:\n            # do nothing if sys.stderr is None and args.thread is None\n            return\n\n        if args.thread is not None:\n            name = args.thread.name\n        else:\n            name = get_ident()\n        print(f\"Exception in thread {name}:\",\n              file=stderr, flush=True)\n        _print_exception(args.exc_type, args.exc_value, args.exc_traceback,\n                         file=stderr)\n        stderr.flush()\n\n\n# Original value of threading.excepthook\n__excepthook__ = excepthook\n\n\ndef _make_invoke_excepthook():\n    # Create a local namespace to ensure that variables remain alive\n    # when _invoke_excepthook() is called, even if it is called late during\n    # Python shutdown. It is mostly needed for daemon threads.\n\n    old_excepthook = excepthook\n    old_sys_excepthook = _sys.excepthook\n    if old_excepthook is None:\n        raise RuntimeError(\"threading.excepthook is None\")\n    if old_sys_excepthook is None:\n        raise RuntimeError(\"sys.excepthook is None\")\n\n    sys_exc_info = _sys.exc_info\n    local_print = print\n    local_sys = _sys\n\n    def invoke_excepthook(thread):\n        global excepthook\n        try:\n            hook = excepthook\n            if hook is None:\n                hook = old_excepthook\n\n            args = ExceptHookArgs([*sys_exc_info(), thread])\n\n            hook(args)\n        except Exception as exc:\n            exc.__suppress_context__ = True\n            del exc\n\n            if local_sys is not None and local_sys.stderr is not None:\n                stderr = local_sys.stderr\n            else:\n                stderr = thread._stderr\n\n            local_print(\"Exception in threading.excepthook:\",\n                        file=stderr, flush=True)\n\n            if local_sys is not None and local_sys.excepthook is not None:\n                sys_excepthook = local_sys.excepthook\n            else:\n                sys_excepthook = old_sys_excepthook\n\n            sys_excepthook(*sys_exc_info())\n        finally:\n            # Break reference cycle (exception stored in a variable)\n            args = None\n\n    return invoke_excepthook\n\n\n# The timer class was contributed by Itamar Shtull-Trauring\n\nclass Timer(Thread):\n    \"\"\"Call a function after a specified number of seconds:\n\n            t = Timer(30.0, f, args=None, kwargs=None)\n            t.start()\n            t.cancel()     # stop the timer's action if it's still waiting\n\n    \"\"\"\n\n    def __init__(self, interval, function, args=None, kwargs=None):\n        Thread.__init__(self)\n        self.interval = interval\n        self.function = function\n        self.args = args if args is not None else []\n        self.kwargs = kwargs if kwargs is not None else {}\n        self.finished = Event()\n\n    def cancel(self):\n        \"\"\"Stop the timer if it hasn't finished yet.\"\"\"\n        self.finished.set()\n\n    def run(self):\n        self.finished.wait(self.interval)\n        if not self.finished.is_set():\n            self.function(*self.args, **self.kwargs)\n        self.finished.set()\n\n\n# Special thread class to represent the main thread\n\nclass _MainThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=\"MainThread\", daemon=False)\n        self._set_tstate_lock()\n        self._started.set()\n        self._set_ident()\n        if _HAVE_THREAD_NATIVE_ID:\n            self._set_native_id()\n        with _active_limbo_lock:\n            _active[self._ident] = self\n\n\n# Dummy thread class to represent threads not started here.\n# These aren't garbage collected when they die, nor can they be waited for.\n# If they invoke anything in threading.py that calls current_thread(), they\n# leave an entry in the _active dict forever after.\n# Their purpose is to return *something* from current_thread().\n# They are marked as daemon threads so we won't wait for them\n# when we exit (conform previous semantics).\n\nclass _DummyThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=_newname(\"Dummy-%d\"), daemon=True)\n\n        self._started.set()\n        self._set_ident()\n        if _HAVE_THREAD_NATIVE_ID:\n            self._set_native_id()\n        with _active_limbo_lock:\n            _active[self._ident] = self\n\n    def _stop(self):\n        pass\n\n    def is_alive(self):\n        assert not self._is_stopped and self._started.is_set()\n        return True\n\n    def join(self, timeout=None):\n        assert False, \"cannot join a dummy thread\"\n\n\n# Global API functions\n\ndef current_thread():\n    \"\"\"Return the current Thread object, corresponding to the caller's thread of control.\n\n    If the caller's thread of control was not created through the threading\n    module, a dummy thread object with limited functionality is returned.\n\n    \"\"\"\n    try:\n        return _active[get_ident()]\n    except KeyError:\n        return _DummyThread()\n\ndef currentThread():\n    \"\"\"Return the current Thread object, corresponding to the caller's thread of control.\n\n    This function is deprecated, use current_thread() instead.\n\n    \"\"\"\n    import warnings\n    warnings.warn('currentThread() is deprecated, use current_thread() instead',\n                  DeprecationWarning, stacklevel=2)\n    return current_thread()\n\ndef active_count():\n    \"\"\"Return the number of Thread objects currently alive.\n\n    The returned count is equal to the length of the list returned by\n    enumerate().\n\n    \"\"\"\n    with _active_limbo_lock:\n        return len(_active) + len(_limbo)\n\ndef activeCount():\n    \"\"\"Return the number of Thread objects currently alive.\n\n    This function is deprecated, use active_count() instead.\n\n    \"\"\"\n    import warnings\n    warnings.warn('activeCount() is deprecated, use active_count() instead',\n                  DeprecationWarning, stacklevel=2)\n    return active_count()\n\ndef _enumerate():\n    # Same as enumerate(), but without the lock. Internal use only.\n    return list(_active.values()) + list(_limbo.values())\n\ndef enumerate():\n    \"\"\"Return a list of all Thread objects currently alive.\n\n    The list includes daemonic threads, dummy thread objects created by\n    current_thread(), and the main thread. It excludes terminated threads and\n    threads that have not yet been started.\n\n    \"\"\"\n    with _active_limbo_lock:\n        return list(_active.values()) + list(_limbo.values())\n\n\n_threading_atexits = []\n_SHUTTING_DOWN = False\n\ndef _register_atexit(func, *arg, **kwargs):\n    \"\"\"CPython internal: register *func* to be called before joining threads.\n\n    The registered *func* is called with its arguments just before all\n    non-daemon threads are joined in `_shutdown()`. It provides a similar\n    purpose to `atexit.register()`, but its functions are called prior to\n    threading shutdown instead of interpreter shutdown.\n\n    For similarity to atexit, the registered functions are called in reverse.\n    \"\"\"\n    if _SHUTTING_DOWN:\n        raise RuntimeError(\"can't register atexit after shutdown\")\n\n    call = functools.partial(func, *arg, **kwargs)\n    _threading_atexits.append(call)\n\n\nfrom _thread import stack_size\n\n# Create the main thread object,\n# and make it available for the interpreter\n# (Py_Main) as threading._shutdown.\n\n_main_thread = _MainThread()\n\ndef _shutdown():\n    \"\"\"\n    Wait until the Python thread state of all non-daemon threads get deleted.\n    \"\"\"\n    # Obscure:  other threads may be waiting to join _main_thread.  That's\n    # dubious, but some code does it.  We can't wait for C code to release\n    # the main thread's tstate_lock - that won't happen until the interpreter\n    # is nearly dead.  So we release it here.  Note that just calling _stop()\n    # isn't enough:  other threads may already be waiting on _tstate_lock.\n    if _main_thread._is_stopped:\n        # _shutdown() was already called\n        return\n\n    global _SHUTTING_DOWN\n    _SHUTTING_DOWN = True\n\n    # Call registered threading atexit functions before threads are joined.\n    # Order is reversed, similar to atexit.\n    for atexit_call in reversed(_threading_atexits):\n        atexit_call()\n\n    # Main thread\n    if _main_thread.ident == get_ident():\n        tlock = _main_thread._tstate_lock\n        # The main thread isn't finished yet, so its thread state lock can't\n        # have been released.\n        assert tlock is not None\n        assert tlock.locked()\n        tlock.release()\n        _main_thread._stop()\n    else:\n        # bpo-1596321: _shutdown() must be called in the main thread.\n        # If the threading module was not imported by the main thread,\n        # _main_thread is the thread which imported the threading module.\n        # In this case, ignore _main_thread, similar behavior than for threads\n        # spawned by C libraries or using _thread.start_new_thread().\n        pass\n\n    # Join all non-deamon threads\n    while True:\n        with _shutdown_locks_lock:\n            locks = list(_shutdown_locks)\n            _shutdown_locks.clear()\n\n        if not locks:\n            break\n\n        for lock in locks:\n            # mimic Thread.join()\n            lock.acquire()\n            lock.release()\n\n        # new threads can be spawned while we were waiting for the other\n        # threads to complete\n\n\ndef main_thread():\n    \"\"\"Return the main thread object.\n\n    In normal conditions, the main thread is the thread from which the\n    Python interpreter was started.\n    \"\"\"\n    return _main_thread\n\n# get thread-local implementation, either from the thread\n# module, or from the python fallback\n\ntry:\n    from _thread import _local as local\nexcept ImportError:\n    from _threading_local import local\n\n\ndef _after_fork():\n    \"\"\"\n    Cleanup threading module state that should not exist after a fork.\n    \"\"\"\n    # Reset _active_limbo_lock, in case we forked while the lock was held\n    # by another (non-forked) thread.  http://bugs.python.org/issue874900\n    global _active_limbo_lock, _main_thread\n    global _shutdown_locks_lock, _shutdown_locks\n    _active_limbo_lock = RLock()\n\n    # fork() only copied the current thread; clear references to others.\n    new_active = {}\n\n    try:\n        current = _active[get_ident()]\n    except KeyError:\n        # fork() was called in a thread which was not spawned\n        # by threading.Thread. For example, a thread spawned\n        # by thread.start_new_thread().\n        current = _MainThread()\n\n    _main_thread = current\n\n    # reset _shutdown() locks: threads re-register their _tstate_lock below\n    _shutdown_locks_lock = _allocate_lock()\n    _shutdown_locks = set()\n\n    with _active_limbo_lock:\n        # Dangling thread instances must still have their locks reset,\n        # because someone may join() them.\n        threads = set(_enumerate())\n        threads.update(_dangling)\n        for thread in threads:\n            # Any lock/condition variable may be currently locked or in an\n            # invalid state, so we reinitialize them.\n            if thread is current:\n                # There is only one active thread. We reset the ident to\n                # its new value since it can have changed.\n                thread._reset_internal_locks(True)\n                ident = get_ident()\n                if isinstance(thread, _DummyThread):\n                    thread.__class__ = _MainThread\n                    thread._name = 'MainThread'\n                    thread._daemonic = False\n                    thread._set_tstate_lock()\n                thread._ident = ident\n                new_active[ident] = thread\n            else:\n                # All the others are already stopped.\n                thread._reset_internal_locks(False)\n                thread._stop()\n\n        _limbo.clear()\n        _active.clear()\n        _active.update(new_active)\n        assert len(_active) == 1\n\n\nif hasattr(_os, \"register_at_fork\"):\n    _os.register_at_fork(after_in_child=_after_fork)\n", 1673], "/usr/local/lib/python3.11/multiprocessing/process.py": ["#\n# Module providing the `Process` class which emulates `threading.Thread`\n#\n# multiprocessing/process.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# Licensed to PSF under a Contributor Agreement.\n#\n\n__all__ = ['BaseProcess', 'current_process', 'active_children',\n           'parent_process']\n\n#\n# Imports\n#\n\nimport os\nimport sys\nimport signal\nimport itertools\nimport threading\nfrom _weakrefset import WeakSet\n\n#\n#\n#\n\ntry:\n    ORIGINAL_DIR = os.path.abspath(os.getcwd())\nexcept OSError:\n    ORIGINAL_DIR = None\n\n#\n# Public functions\n#\n\ndef current_process():\n    '''\n    Return process object representing the current process\n    '''\n    return _current_process\n\ndef active_children():\n    '''\n    Return list of process objects corresponding to live child processes\n    '''\n    _cleanup()\n    return list(_children)\n\n\ndef parent_process():\n    '''\n    Return process object representing the parent process\n    '''\n    return _parent_process\n\n#\n#\n#\n\ndef _cleanup():\n    # check for processes which have finished\n    for p in list(_children):\n        if (child_popen := p._popen) and child_popen.poll() is not None:\n            _children.discard(p)\n\n#\n# The `Process` class\n#\n\nclass BaseProcess(object):\n    '''\n    Process objects represent activity that is run in a separate process\n\n    The class is analogous to `threading.Thread`\n    '''\n    def _Popen(self):\n        raise NotImplementedError\n\n    def __init__(self, group=None, target=None, name=None, args=(), kwargs={},\n                 *, daemon=None):\n        assert group is None, 'group argument must be None for now'\n        count = next(_process_counter)\n        self._identity = _current_process._identity + (count,)\n        self._config = _current_process._config.copy()\n        self._parent_pid = os.getpid()\n        self._parent_name = _current_process.name\n        self._popen = None\n        self._closed = False\n        self._target = target\n        self._args = tuple(args)\n        self._kwargs = dict(kwargs)\n        self._name = name or type(self).__name__ + '-' + \\\n                     ':'.join(str(i) for i in self._identity)\n        if daemon is not None:\n            self.daemon = daemon\n        _dangling.add(self)\n\n    def _check_closed(self):\n        if self._closed:\n            raise ValueError(\"process object is closed\")\n\n    def run(self):\n        '''\n        Method to be run in sub-process; can be overridden in sub-class\n        '''\n        if self._target:\n            self._target(*self._args, **self._kwargs)\n\n    def start(self):\n        '''\n        Start child process\n        '''\n        self._check_closed()\n        assert self._popen is None, 'cannot start a process twice'\n        assert self._parent_pid == os.getpid(), \\\n               'can only start a process object created by current process'\n        assert not _current_process._config.get('daemon'), \\\n               'daemonic processes are not allowed to have children'\n        _cleanup()\n        self._popen = self._Popen(self)\n        self._sentinel = self._popen.sentinel\n        # Avoid a refcycle if the target function holds an indirect\n        # reference to the process object (see bpo-30775)\n        del self._target, self._args, self._kwargs\n        _children.add(self)\n\n    def terminate(self):\n        '''\n        Terminate process; sends SIGTERM signal or uses TerminateProcess()\n        '''\n        self._check_closed()\n        self._popen.terminate()\n\n    def kill(self):\n        '''\n        Terminate process; sends SIGKILL signal or uses TerminateProcess()\n        '''\n        self._check_closed()\n        self._popen.kill()\n\n    def join(self, timeout=None):\n        '''\n        Wait until child process terminates\n        '''\n        self._check_closed()\n        assert self._parent_pid == os.getpid(), 'can only join a child process'\n        assert self._popen is not None, 'can only join a started process'\n        res = self._popen.wait(timeout)\n        if res is not None:\n            _children.discard(self)\n\n    def is_alive(self):\n        '''\n        Return whether process is alive\n        '''\n        self._check_closed()\n        if self is _current_process:\n            return True\n        assert self._parent_pid == os.getpid(), 'can only test a child process'\n\n        if self._popen is None:\n            return False\n\n        returncode = self._popen.poll()\n        if returncode is None:\n            return True\n        else:\n            _children.discard(self)\n            return False\n\n    def close(self):\n        '''\n        Close the Process object.\n\n        This method releases resources held by the Process object.  It is\n        an error to call this method if the child process is still running.\n        '''\n        if self._popen is not None:\n            if self._popen.poll() is None:\n                raise ValueError(\"Cannot close a process while it is still running. \"\n                                 \"You should first call join() or terminate().\")\n            self._popen.close()\n            self._popen = None\n            del self._sentinel\n            _children.discard(self)\n        self._closed = True\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        assert isinstance(name, str), 'name must be a string'\n        self._name = name\n\n    @property\n    def daemon(self):\n        '''\n        Return whether process is a daemon\n        '''\n        return self._config.get('daemon', False)\n\n    @daemon.setter\n    def daemon(self, daemonic):\n        '''\n        Set whether process is a daemon\n        '''\n        assert self._popen is None, 'process has already started'\n        self._config['daemon'] = daemonic\n\n    @property\n    def authkey(self):\n        return self._config['authkey']\n\n    @authkey.setter\n    def authkey(self, authkey):\n        '''\n        Set authorization key of process\n        '''\n        self._config['authkey'] = AuthenticationString(authkey)\n\n    @property\n    def exitcode(self):\n        '''\n        Return exit code of process or `None` if it has yet to stop\n        '''\n        self._check_closed()\n        if self._popen is None:\n            return self._popen\n        return self._popen.poll()\n\n    @property\n    def ident(self):\n        '''\n        Return identifier (PID) of process or `None` if it has yet to start\n        '''\n        self._check_closed()\n        if self is _current_process:\n            return os.getpid()\n        else:\n            return self._popen and self._popen.pid\n\n    pid = ident\n\n    @property\n    def sentinel(self):\n        '''\n        Return a file descriptor (Unix) or handle (Windows) suitable for\n        waiting for process termination.\n        '''\n        self._check_closed()\n        try:\n            return self._sentinel\n        except AttributeError:\n            raise ValueError(\"process not started\") from None\n\n    def __repr__(self):\n        exitcode = None\n        if self is _current_process:\n            status = 'started'\n        elif self._closed:\n            status = 'closed'\n        elif self._parent_pid != os.getpid():\n            status = 'unknown'\n        elif self._popen is None:\n            status = 'initial'\n        else:\n            exitcode = self._popen.poll()\n            if exitcode is not None:\n                status = 'stopped'\n            else:\n                status = 'started'\n\n        info = [type(self).__name__, 'name=%r' % self._name]\n        if self._popen is not None:\n            info.append('pid=%s' % self._popen.pid)\n        info.append('parent=%s' % self._parent_pid)\n        info.append(status)\n        if exitcode is not None:\n            exitcode = _exitcode_to_name.get(exitcode, exitcode)\n            info.append('exitcode=%s' % exitcode)\n        if self.daemon:\n            info.append('daemon')\n        return '<%s>' % ' '.join(info)\n\n    ##\n\n    def _bootstrap(self, parent_sentinel=None):\n        from . import util, context\n        global _current_process, _parent_process, _process_counter, _children\n\n        try:\n            if self._start_method is not None:\n                context._force_start_method(self._start_method)\n            _process_counter = itertools.count(1)\n            _children = set()\n            util._close_stdin()\n            old_process = _current_process\n            _current_process = self\n            _parent_process = _ParentProcess(\n                self._parent_name, self._parent_pid, parent_sentinel)\n            if threading._HAVE_THREAD_NATIVE_ID:\n                threading.main_thread()._set_native_id()\n            try:\n                self._after_fork()\n            finally:\n                # delay finalization of the old process object until after\n                # _run_after_forkers() is executed\n                del old_process\n            util.info('child process calling self.run()')\n            try:\n                self.run()\n                exitcode = 0\n            finally:\n                util._exit_function()\n        except SystemExit as e:\n            if e.code is None:\n                exitcode = 0\n            elif isinstance(e.code, int):\n                exitcode = e.code\n            else:\n                sys.stderr.write(str(e.code) + '\\n')\n                exitcode = 1\n        except:\n            exitcode = 1\n            import traceback\n            sys.stderr.write('Process %s:\\n' % self.name)\n            traceback.print_exc()\n        finally:\n            threading._shutdown()\n            util.info('process exiting with exitcode %d' % exitcode)\n            util._flush_std_streams()\n\n        return exitcode\n\n    @staticmethod\n    def _after_fork():\n        from . import util\n        util._finalizer_registry.clear()\n        util._run_after_forkers()\n\n\n#\n# We subclass bytes to avoid accidental transmission of auth keys over network\n#\n\nclass AuthenticationString(bytes):\n    def __reduce__(self):\n        from .context import get_spawning_popen\n        if get_spawning_popen() is None:\n            raise TypeError(\n                'Pickling an AuthenticationString object is '\n                'disallowed for security reasons'\n                )\n        return AuthenticationString, (bytes(self),)\n\n\n#\n# Create object representing the parent process\n#\n\nclass _ParentProcess(BaseProcess):\n\n    def __init__(self, name, pid, sentinel):\n        self._identity = ()\n        self._name = name\n        self._pid = pid\n        self._parent_pid = None\n        self._popen = None\n        self._closed = False\n        self._sentinel = sentinel\n        self._config = {}\n\n    def is_alive(self):\n        from multiprocessing.connection import wait\n        return not wait([self._sentinel], timeout=0)\n\n    @property\n    def ident(self):\n        return self._pid\n\n    def join(self, timeout=None):\n        '''\n        Wait until parent process terminates\n        '''\n        from multiprocessing.connection import wait\n        wait([self._sentinel], timeout=timeout)\n\n    pid = ident\n\n#\n# Create object representing the main process\n#\n\nclass _MainProcess(BaseProcess):\n\n    def __init__(self):\n        self._identity = ()\n        self._name = 'MainProcess'\n        self._parent_pid = None\n        self._popen = None\n        self._closed = False\n        self._config = {'authkey': AuthenticationString(os.urandom(32)),\n                        'semprefix': '/mp'}\n        # Note that some versions of FreeBSD only allow named\n        # semaphores to have names of up to 14 characters.  Therefore\n        # we choose a short prefix.\n        #\n        # On MacOSX in a sandbox it may be necessary to use a\n        # different prefix -- see #19478.\n        #\n        # Everything in self._config will be inherited by descendant\n        # processes.\n\n    def close(self):\n        pass\n\n\n_parent_process = None\n_current_process = _MainProcess()\n_process_counter = itertools.count(1)\n_children = set()\ndel _MainProcess\n\n#\n# Give names to some return codes\n#\n\n_exitcode_to_name = {}\n\nfor name, signum in list(signal.__dict__.items()):\n    if name[:3]=='SIG' and '_' not in name:\n        _exitcode_to_name[-signum] = f'-{name}'\ndel name, signum\n\n# For debug and leak testing\n_dangling = WeakSet()\n", 439], "/app/app/services/market_data.py": ["import asyncio\nimport logging\nimport os\nimport time\nfrom datetime import datetime, timezone\nfrom typing import Dict, List, Optional\n\nfrom fastapi import HTTPException\n\nfrom ..circuit_breaker import CircuitBreakerManager\nfrom ..core.config import get_settings\nfrom ..providers import FinnhubProvider, YahooFinanceProvider\nfrom ..providers.registry import ProviderRegistry\nfrom .cache import DataCache\nfrom .database import db_service\nfrom .data_collector import DataCollectorService\nfrom .macro_data_service import MacroFactorService\nfrom .options_service import options_service\nfrom .websocket import ConnectionManager\n\nlogger = logging.getLogger(__name__)\n\nclass MarketDataService:\n    SUPPORTED_INTRADAY_INTERVALS = {\n        \"1m\",\n        \"2m\",\n        \"5m\",\n        \"15m\",\n        \"30m\",\n        \"60m\",\n        \"90m\",\n    }\n\n    def __init__(self):\n        self.settings = get_settings()\n        self.breakers = CircuitBreakerManager()\n        self.registry = ProviderRegistry(self.breakers)\n\n        self._init_providers()\n        self.providers = [entry.adapter for entry in self.registry.providers.values()]\n\n        # Cache TTL configurable via env/settings\n        cache_ttl = int(os.getenv(\"CACHE_TTL_SECONDS\", self.settings.cache_ttl_seconds))\n        self.cache = DataCache(ttl_seconds=cache_ttl)\n        self.metrics_cache = DataCache(ttl_seconds=int(os.getenv(\"OPTIONS_METRICS_CACHE_TTL\", \"300\")))\n        self.options_metrics_ttl = int(os.getenv(\"OPTIONS_METRICS_TTL\", \"900\"))\n        self.update_interval = int(os.getenv(\"WEBSOCKET_UPDATE_INTERVAL\", \"5\"))\n        self.connection_manager = ConnectionManager()\n        self.macro_service = MacroFactorService(\n            providers=self.providers,\n            cache_ttl_seconds=getattr(self.settings, \"macro_cache_ttl_seconds\", 300),\n            refresh_interval_seconds=getattr(self.settings, \"macro_refresh_interval_seconds\", 900),\n        )\n        self.data_collector = DataCollectorService(db=db_service, registry=self.registry)\n        self.background_tasks_running = False\n\n    def _init_providers(self) -> None:\n        \"\"\"Register provider adapters with the routing registry.\"\"\"\n        finnhub_key = (\n            os.getenv(\"FINNHUB_API_KEY\")\n            or self.settings.finnhub.api_key\n            or self.settings.finnhub_api_key\n        )\n\n        if finnhub_key and finnhub_key not in {\"your_finnhub_api_key_here\", \"\"}:\n            try:\n                finnhub_provider = FinnhubProvider(api_key=finnhub_key)\n                self.registry.register(\"finnhub\", finnhub_provider, {\"bars_1m\", \"eod\", \"quotes_l1\"})\n                logger.info(\"Finnhub provider registered\")\n            except Exception as exc:  # pragma: no cover - defensive\n                logger.warning(\"Failed to initialize Finnhub provider: %s\", exc)\n        else:\n            logger.warning(\"No valid Finnhub API key provided; Finnhub disabled\")\n\n        if self.settings.yfinance_enabled:\n            try:\n                yahoo_provider = YahooFinanceProvider()\n                self.registry.register(\"yfinance\", yahoo_provider, {\"bars_1m\", \"eod\", \"options_chain\"})\n                logger.info(\"Yahoo Finance provider registered\")\n            except Exception as exc:  # pragma: no cover\n                logger.warning(\"Failed to initialize Yahoo Finance provider: %s\", exc)\n\n    def reload_configuration(self) -> None:\n        \"\"\"Reload providers and settings after a hot reload.\"\"\"\n        self.settings = get_settings()\n        self.registry.providers.clear()\n        self._init_providers()\n        self.providers = [entry.adapter for entry in self.registry.providers.values()]\n        if hasattr(self.macro_service, \"providers\"):\n            self.macro_service.providers = self.providers\n\n    async def _try_providers(\n        self,\n        *,\n        capability: str,\n        endpoint: str,\n        executor,\n        provider_hint: Optional[str] = None,\n    ):\n        ranked = self.registry.rank(capability, provider_hint=provider_hint)\n        if not ranked:\n            raise HTTPException(status_code=503, detail=f\"No providers available for {capability}\")\n\n        last_error: Optional[str] = None\n        for provider_name in ranked:\n            entry = self.registry.providers[provider_name]\n            adapter = entry.adapter\n            if not adapter.enabled:\n                continue\n\n            self.registry.record_selection(capability, provider_name)\n            started = time.perf_counter()\n\n            try:\n                result = await executor(adapter)\n                elapsed_ms = (time.perf_counter() - started) * 1000.0\n                self.registry.record_outcome(provider_name, elapsed_ms, error=False, endpoint=endpoint)\n                if result:\n                    return result, provider_name\n                last_error = f\"{provider_name}: empty result\"\n            except HTTPException:\n                raise\n            except Exception as exc:  # pragma: no cover - provider failure paths\n                elapsed_ms = (time.perf_counter() - started) * 1000.0\n                self.registry.record_outcome(provider_name, elapsed_ms, error=True, endpoint=endpoint)\n                self.registry.record_error(provider_name, endpoint, exc.__class__.__name__)\n                last_error = f\"{provider_name}: {exc}\"\n\n        raise HTTPException(\n            status_code=502,\n            detail=f\"All providers failed for {endpoint}. last_error={last_error}\",\n        )\n    \n    async def get_stock_price(self, symbol: str) -> Dict:\n        \"\"\"Get stock price with caching and health-aware routing.\"\"\"\n        symbol = symbol.upper()\n        cache_key = f\"price:{symbol}\"\n\n        cached_data = self.cache.get(cache_key)\n        if cached_data:\n            logger.info(\"Cache hit for %s\", symbol)\n            return cached_data\n\n        data, provider_used = await self._try_providers(\n            capability=\"quotes_l1\",\n            endpoint=\"quotes\",\n            executor=lambda adapter: adapter.get_price_safe(symbol),\n        )\n\n        if isinstance(data, dict):\n            data[\"provider_used\"] = provider_used\n        self.cache.set(cache_key, data)\n        logger.info(\"Fetched %s from %s\", symbol, provider_used)\n        return data\n    \n    async def get_historical_data(self, symbol: str, period: str = \"1mo\") -> Dict:\n        \"\"\"Get historical data.\"\"\"\n        symbol = symbol.upper()\n        data, provider_used = await self._try_providers(\n            capability=\"eod\",\n            endpoint=\"historical\",\n            executor=lambda adapter: adapter.get_historical_safe(symbol, period),\n        )\n\n        if isinstance(data, dict):\n            data[\"provider_used\"] = provider_used\n        return data\n\n    def _normalize_intraday_interval(self, interval: str) -> str:\n        interval_normalized = (interval or \"1m\").strip().lower()\n\n        # Map common aliases to supported values\n        alias_map = {\n            \"1\": \"1m\",\n            \"5\": \"5m\",\n            \"15\": \"15m\",\n            \"30\": \"30m\",\n            \"60\": \"60m\",\n            \"1min\": \"1m\",\n            \"5min\": \"5m\",\n            \"15min\": \"15m\",\n            \"30min\": \"30m\",\n            \"60min\": \"60m\",\n            \"1hour\": \"60m\",\n            \"1hr\": \"60m\",\n            \"1h\": \"60m\",\n        }\n\n        resolved = alias_map.get(interval_normalized, interval_normalized)\n        if resolved not in self.SUPPORTED_INTRADAY_INTERVALS:\n            raise HTTPException(\n                status_code=400,\n                detail=f\"Unsupported intraday interval '{interval}'\"\n            )\n        return resolved\n\n    async def get_intraday_data(self, symbol: str, interval: str = \"1m\") -> Dict:\n        \"\"\"Get intraday data with provider fallback and validation.\"\"\"\n        normalized_interval = self._normalize_intraday_interval(interval)\n\n        capability = \"bars_1m\" if normalized_interval.endswith(\"m\") else \"eod\"\n        data, provider_used = await self._try_providers(\n            capability=capability,\n            endpoint=\"intraday\",\n            executor=lambda adapter: adapter.get_intraday_safe(symbol, normalized_interval),\n        )\n\n        if isinstance(data, dict):\n            data[\"provider_used\"] = provider_used\n        return data\n    \n    async def start_background_tasks(self):\n        \"\"\"Start background tasks for cache cleanup and real-time updates\"\"\"\n        if self.background_tasks_running:\n            return\n\n        self.background_tasks_running = True\n\n        # Initialize database connections for macro storage\n        try:\n            await db_service.initialize()\n            logger.info(\"Database service initialized\")\n        except Exception as exc:\n            logger.error(f\"Failed to initialize database service: {exc}\")\n\n        # Start cache cleanup task\n        asyncio.create_task(self._cache_cleanup_task())\n\n        # Start macro refresh task\n        asyncio.create_task(self._macro_refresh_task())\n\n        # Start real-time data broadcasting\n        asyncio.create_task(self._real_time_broadcast_task())\n\n        # Start data collector (M2/M3: RLC consumer + gap detection + backfill)\n        asyncio.create_task(self.data_collector.run())\n\n        logger.info(\"Background tasks started (including data collector)\")\n    \n    async def _cache_cleanup_task(self):\n        \"\"\"Clean up expired cache entries\"\"\"\n        while self.background_tasks_running:\n            self.cache.clear_expired()\n            await asyncio.sleep(60)  # Clean up every minute\n    \n    async def _real_time_broadcast_task(self):\n        \"\"\"Broadcast real-time data to WebSocket clients\"\"\"\n        while self.background_tasks_running:\n            if self.connection_manager.symbol_subscribers:\n                for symbol in list(self.connection_manager.symbol_subscribers.keys()):\n                    try:\n                        data = await self.get_stock_price(symbol)\n                        await self.connection_manager.broadcast_to_symbol(symbol, data)\n                    except Exception as e:\n                        logger.error(f\"Error broadcasting {symbol}: {e}\")\n\n            await asyncio.sleep(self.update_interval)  # Update interval configurable\n\n    async def _macro_refresh_task(self):\n        \"\"\"Periodically refresh macro factors from configured providers.\"\"\"\n        interval = getattr(\n            self.macro_service,\n            \"refresh_interval_seconds\",\n            getattr(self.settings, \"macro_refresh_interval_seconds\", 900),\n        )\n        interval = max(60, interval or 900)\n\n        try:\n            await self.macro_service.refresh_all()\n        except Exception as exc:\n            logger.error(f\"Initial macro refresh failed: {exc}\")\n\n        while self.background_tasks_running:\n            await asyncio.sleep(interval)\n            try:\n                await self.macro_service.refresh_all()\n            except Exception as exc:\n                logger.error(f\"Macro refresh task failed: {exc}\")\n\n    async def get_company_profile(self, symbol: str) -> Dict:\n        \"\"\"Get company profile data (Finnhub only)\"\"\"\n        for provider in self.providers:\n            if hasattr(provider, 'get_company_profile'):\n                data = await provider.get_company_profile(symbol)\n                if data:\n                    return data\n        \n        raise HTTPException(\n            status_code=503,\n            detail=f\"Unable to fetch company profile for {symbol}\"\n        )\n    \n    async def get_news_sentiment(self, symbol: str) -> Dict:\n        \"\"\"Get news sentiment data (Finnhub only)\"\"\"\n        for provider in self.providers:\n            if hasattr(provider, 'get_news_sentiment'):\n                data = await provider.get_news_sentiment(symbol)\n                if data:\n                    return data\n        \n        raise HTTPException(\n            status_code=503,\n            detail=f\"Unable to fetch news sentiment for {symbol}\"\n        )\n    \n\n    async def get_options_metrics(self, symbol: str) -> Dict:\n        \"\"\"Return cached or freshly computed options metrics for a symbol.\"\"\"\n        symbol = symbol.upper()\n        cache_key = f\"options_metrics:{symbol}\"\n        cached = self.metrics_cache.get(cache_key)\n        if cached:\n            return cached\n\n        latest = await db_service.get_latest_options_metrics(symbol)\n        if latest:\n            try:\n                timestamp = datetime.fromisoformat(latest['as_of'])\n                if timestamp.tzinfo is None:\n                    timestamp = timestamp.replace(tzinfo=timezone.utc)\n                if (datetime.now(timezone.utc) - timestamp).total_seconds() < self.options_metrics_ttl:\n                    self.metrics_cache.set(cache_key, latest)\n                    return latest\n            except Exception:\n                pass\n\n        chain = await options_service.fetch_options_chain(symbol)\n        metrics = options_service.calculate_chain_metrics(chain)\n        record = metrics.to_db_record()\n        await db_service.store_options_metrics(record)\n        result = metrics.to_dict()\n        result['metadata'].setdefault('source', 'provider')\n        self.metrics_cache.set(cache_key, result)\n        return result\n\n    async def get_options_metrics_history(self, symbol: str, limit: int = 50) -> List[Dict]:\n        \"\"\"Return recent history of stored options metrics for a symbol.\"\"\"\n        history = await db_service.get_options_metrics_history(symbol.upper(), limit)\n        return history\n\n    async def get_stats(self) -> Dict:\n        macro_stats = await self.macro_service.stats()\n        return {\n            \"providers\": [\n                {\n                    \"name\": provider.name,\n                    \"available\": provider.is_available,\n                    \"last_error\": provider.last_error,\n                }\n                for provider in self.providers\n            ],\n            \"cache\": self.cache.stats(),\n            \"websocket\": self.connection_manager.stats(),\n            \"database\": {\n                \"connected\": db_service.pool is not None,\n                \"storage_enabled\": self.settings.store_historical_data,\n            },\n            \"macro\": macro_stats,\n            \"options_metrics\": {\n                \"cache\": self.metrics_cache.stats(),\n                \"ttl_seconds\": self.options_metrics_ttl,\n            },\n        }\n\n    async def get_macro_snapshot(self, factors: Optional[List[str]] = None) -> Dict:\n        \"\"\"Return the latest macro factor snapshot.\"\"\"\n        return await self.macro_service.get_snapshot(factors)\n\n    async def get_macro_history(self, factor_key: str, lookback_days: int = 30) -> Dict:\n        \"\"\"Return macro factor history for the requested lookback window.\"\"\"\n        return await self.macro_service.get_history(factor_key, lookback_days)\n\n    def list_macro_factors(self) -> List[str]:\n        \"\"\"List available macro factor keys.\"\"\"\n        return self.macro_service.available_factors()\n\n    async def refresh_macro_factors(self, factor_key: Optional[str] = None) -> Dict:\n        \"\"\"Refresh macro factors from providers.\"\"\"\n        if factor_key:\n            return await self.macro_service.refresh_factor(factor_key)\n        return await self.macro_service.refresh_all()\n    \n    async def get_unusual_options_activity(self, symbol: str, lookback_days: int = 20) -> Dict:\n        \"\"\"Get unusual options activity for a symbol\"\"\"\n        try:\n            unusual_activities = await options_service.detect_unusual_activity(symbol.upper(), lookback_days)\n            \n            return {\n                \"symbol\": symbol.upper(),\n                \"lookback_days\": lookback_days,\n                \"unusual_activities_count\": len(unusual_activities),\n                \"unusual_activities\": [\n                    {\n                        \"contract_symbol\": activity.contract_symbol,\n                        \"strike\": activity.strike,\n                        \"expiry\": activity.expiry.isoformat(),\n                        \"option_type\": activity.option_type,\n                        \"volume\": activity.volume,\n                        \"avg_volume_20d\": activity.avg_volume_20d,\n                        \"volume_ratio\": activity.volume_ratio,\n                        \"open_interest\": activity.open_interest,\n                        \"volume_spike\": activity.volume_spike,\n                        \"large_single_trades\": activity.large_single_trades,\n                        \"sweep_activity\": activity.sweep_activity,\n                        \"unusual_volume_vs_oi\": activity.unusual_volume_vs_oi,\n                        \"underlying_price\": activity.underlying_price,\n                        \"strike_distance_pct\": activity.strike_distance_pct,\n                        \"days_to_expiration\": activity.days_to_expiration,\n                        \"unusual_score\": activity.unusual_score,\n                        \"confidence_level\": activity.confidence_level,\n                        \"large_trades\": activity.large_trades,\n                        \"timestamp\": activity.timestamp.isoformat()\n                    }\n                    for activity in unusual_activities\n                ],\n                \"analysis_timestamp\": datetime.now().isoformat()\n            }\n        except Exception as e:\n            logger.error(f\"Error getting unusual options activity for {symbol}: {e}\")\n            return {\n                \"symbol\": symbol.upper(),\n                \"error\": str(e),\n                \"unusual_activities_count\": 0,\n                \"unusual_activities\": []\n            }\n    \n    async def get_options_flow_analysis(self, symbol: str) -> Dict:\n        \"\"\"Get comprehensive options flow analysis\"\"\"\n        try:\n            flow_analysis = await options_service.analyze_options_flow(symbol.upper())\n            \n            return {\n                \"symbol\": symbol.upper(),\n                \"timestamp\": flow_analysis.timestamp.isoformat(),\n                \"flow_metrics\": {\n                    \"total_call_volume\": flow_analysis.total_call_volume,\n                    \"total_put_volume\": flow_analysis.total_put_volume,\n                    \"call_put_ratio\": flow_analysis.call_put_ratio,\n                    \"large_trades_count\": flow_analysis.large_trades_count,\n                    \"block_trades_value\": flow_analysis.block_trades_value,\n                    \"sweep_trades_count\": flow_analysis.sweep_trades_count\n                },\n                \"sentiment_analysis\": {\n                    \"flow_sentiment\": flow_analysis.flow_sentiment,\n                    \"smart_money_score\": flow_analysis.smart_money_score,\n                    \"call_premium_bought\": flow_analysis.call_premium_bought,\n                    \"put_premium_bought\": flow_analysis.put_premium_bought,\n                    \"net_premium_flow\": flow_analysis.net_premium_flow\n                },\n                \"unusual_activities_summary\": {\n                    \"count\": len(flow_analysis.unusual_activities),\n                    \"top_activities\": [\n                        {\n                            \"contract_symbol\": activity.contract_symbol,\n                            \"unusual_score\": activity.unusual_score,\n                            \"volume_ratio\": activity.volume_ratio,\n                            \"volume\": activity.volume,\n                            \"strike\": activity.strike,\n                            \"option_type\": activity.option_type\n                        }\n                        for activity in flow_analysis.unusual_activities[:5]  # Top 5\n                    ]\n                }\n            }\n        except Exception as e:\n            logger.error(f\"Error getting options flow analysis for {symbol}: {e}\")\n            return {\n                \"symbol\": symbol.upper(),\n                \"error\": str(e),\n                \"flow_metrics\": {},\n                \"sentiment_analysis\": {},\n                \"unusual_activities_summary\": {\"count\": 0, \"top_activities\": []}\n            }\n\n", 474]}, "functions": {"DataCache.get (/app/app/services/cache.py:10)": ["/app/app/services/cache.py", 10], "Logger.isEnabledFor (/usr/local/lib/python3.11/logging/__init__.py:1734)": ["/usr/local/lib/python3.11/logging/__init__.py", 1734], "<lambda> (/usr/local/lib/python3.11/logging/__init__.py:164)": ["/usr/local/lib/python3.11/logging/__init__.py", 164], "normcase (<frozen posixpath>:52)": ["<frozen posixpath>", 52], "_is_internal_frame (/usr/local/lib/python3.11/logging/__init__.py:194)": ["/usr/local/lib/python3.11/logging/__init__.py", 194], "Logger.findCaller (/usr/local/lib/python3.11/logging/__init__.py:1561)": ["/usr/local/lib/python3.11/logging/__init__.py", 1561], "ABCMeta.__instancecheck__ (<frozen abc>:117)": ["<frozen abc>", 117], "getLevelName (/usr/local/lib/python3.11/logging/__init__.py:123)": ["/usr/local/lib/python3.11/logging/__init__.py", 123], "_get_sep (<frozen posixpath>:41)": ["<frozen posixpath>", 41], "basename (<frozen posixpath>:140)": ["<frozen posixpath>", 140], "_splitext (<frozen genericpath>:121)": ["<frozen genericpath>", 121], "splitext (<frozen posixpath>:117)": ["<frozen posixpath>", 117], "current_thread (/usr/local/lib/python3.11/threading.py:1453)": ["/usr/local/lib/python3.11/threading.py", 1453], "Thread.name (/usr/local/lib/python3.11/threading.py:1152)": ["/usr/local/lib/python3.11/threading.py", 1152], "current_process (/usr/local/lib/python3.11/multiprocessing/process.py:37)": ["/usr/local/lib/python3.11/multiprocessing/process.py", 37], "BaseProcess.name (/usr/local/lib/python3.11/multiprocessing/process.py:189)": ["/usr/local/lib/python3.11/multiprocessing/process.py", 189], "LogRecord.__init__ (/usr/local/lib/python3.11/logging/__init__.py:292)": ["/usr/local/lib/python3.11/logging/__init__.py", 292], "Logger.makeRecord (/usr/local/lib/python3.11/logging/__init__.py:1595)": ["/usr/local/lib/python3.11/logging/__init__.py", 1595], "Filterer.filter (/usr/local/lib/python3.11/logging/__init__.py:815)": ["/usr/local/lib/python3.11/logging/__init__.py", 815], "Handler.acquire (/usr/local/lib/python3.11/logging/__init__.py:922)": ["/usr/local/lib/python3.11/logging/__init__.py", 922], "LogRecord.getMessage (/usr/local/lib/python3.11/logging/__init__.py:368)": ["/usr/local/lib/python3.11/logging/__init__.py", 368], "PercentStyle.usesTime (/usr/local/lib/python3.11/logging/__init__.py:432)": ["/usr/local/lib/python3.11/logging/__init__.py", 432], "Formatter.usesTime (/usr/local/lib/python3.11/logging/__init__.py:652)": ["/usr/local/lib/python3.11/logging/__init__.py", 652], "PercentStyle._format (/usr/local/lib/python3.11/logging/__init__.py:440)": ["/usr/local/lib/python3.11/logging/__init__.py", 440], "PercentStyle.format (/usr/local/lib/python3.11/logging/__init__.py:447)": ["/usr/local/lib/python3.11/logging/__init__.py", 447], "Formatter.formatMessage (/usr/local/lib/python3.11/logging/__init__.py:658)": ["/usr/local/lib/python3.11/logging/__init__.py", 658], "Formatter.format (/usr/local/lib/python3.11/logging/__init__.py:674)": ["/usr/local/lib/python3.11/logging/__init__.py", 674], "Handler.format (/usr/local/lib/python3.11/logging/__init__.py:942)": ["/usr/local/lib/python3.11/logging/__init__.py", 942], "Handler.release (/usr/local/lib/python3.11/logging/__init__.py:929)": ["/usr/local/lib/python3.11/logging/__init__.py", 929], "StreamHandler.flush (/usr/local/lib/python3.11/logging/__init__.py:1087)": ["/usr/local/lib/python3.11/logging/__init__.py", 1087], "StreamHandler.emit (/usr/local/lib/python3.11/logging/__init__.py:1098)": ["/usr/local/lib/python3.11/logging/__init__.py", 1098], "Handler.handle (/usr/local/lib/python3.11/logging/__init__.py:965)": ["/usr/local/lib/python3.11/logging/__init__.py", 965], "Logger.callHandlers (/usr/local/lib/python3.11/logging/__init__.py:1690)": ["/usr/local/lib/python3.11/logging/__init__.py", 1690], "Logger.handle (/usr/local/lib/python3.11/logging/__init__.py:1636)": ["/usr/local/lib/python3.11/logging/__init__.py", 1636], "Logger._log (/usr/local/lib/python3.11/logging/__init__.py:1610)": ["/usr/local/lib/python3.11/logging/__init__.py", 1610], "Logger.info (/usr/local/lib/python3.11/logging/__init__.py:1479)": ["/usr/local/lib/python3.11/logging/__init__.py", 1479], "MarketDataService.get_stock_price (/app/app/services/market_data.py:134)": ["/app/app/services/market_data.py", 134]}}}