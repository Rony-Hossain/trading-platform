{"traceEvents": [{"ph": "M", "pid": 1, "tid": 1, "name": "process_name", "args": {"name": "MainProcess"}}, {"ph": "M", "pid": 1, "tid": 1, "name": "thread_name", "args": {"name": "MainThread"}}, {"pid": 1, "tid": 1, "ts": 38290841638.808, "ph": "X", "dur": 1.6065513697272735, "name": "DataCache.get (/app/app/services/cache.py:10)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841665.079, "ph": "X", "dur": 1.094974145391835, "name": "get_settings (/app/app/core/config.py:131)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841684.565, "ph": "X", "dur": 0.4242886698591974, "name": "get_settings (/app/app/core/config.py:131)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841720.841, "ph": "X", "dur": 1.7170432108364393, "name": "__create_fn__.<locals>.__init__ (<string>:2)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841748.13, "ph": "X", "dur": 2.6794271468972752, "name": "__create_fn__.<locals>.__init__ (<string>:2)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841754.371, "ph": "X", "dur": 0.7093576199208457, "name": "Lock.__init__ (/usr/local/lib/python3.11/asyncio/locks.py:78)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841744.085, "ph": "X", "dur": 11.632581031972995, "name": "CircuitBreaker.__init__ (/app/app/utils/circuit_breaker.py:42)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841696.733, "ph": "X", "dur": 59.68658764876037, "name": "ProviderCircuitBreakers.get_breaker (/app/app/utils/circuit_breaker.py:142)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841692.911, "ph": "X", "dur": 63.65711195901826, "name": "CircuitBreakerManager._get_breaker (/app/app/circuit_breaker.py:26)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841763.472, "ph": "X", "dur": 0.9071380155062527, "name": "Enum.name (/usr/local/lib/python3.11/enum.py:1252)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841761.613, "ph": "X", "dur": 2.8893616450046906, "name": "property.__get__ (/usr/local/lib/python3.11/enum.py:193)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841691.622, "ph": "X", "dur": 73.6726448963586, "name": "CircuitBreakerManager.get_state (/app/app/circuit_breaker.py:19)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841687.574, "ph": "X", "dur": 78.47351539255187, "name": "ProviderRegistry._breaker_score (/app/app/providers/registry.py:66)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841683.975, "ph": "X", "dur": 89.90168651847291, "name": "ProviderRegistry.health_score (/app/app/providers/registry.py:74)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841779.065, "ph": "X", "dur": 1.1397233410410472, "name": "ProviderRegistry.rank.<locals>.<dictcomp> (/app/app/providers/registry.py:121)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841783.896, "ph": "X", "dur": 1.465674272313087, "name": "ProviderRegistry.rank.<locals>.<lambda> (/app/app/providers/registry.py:122)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841662.902, "ph": "X", "dur": 125.93694311861076, "name": "ProviderRegistry.rank (/app/app/providers/registry.py:90)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841812.59, "ph": "X", "dur": 0.9507822927443733, "name": "MetricWrapperBase.labels.<locals>.<genexpr> (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:172)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841813.723, "ph": "X", "dur": 0.40992473051500583, "name": "MetricWrapperBase.labels.<locals>.<genexpr> (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:172)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841814.242, "ph": "X", "dur": 0.14971644470291992, "name": "MetricWrapperBase.labels.<locals>.<genexpr> (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:172)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841832.17, "ph": "X", "dur": 2.234145027227336, "name": "_build_full_name (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:25)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841842.864, "ph": "X", "dur": 0.2999853486113857, "name": "get_legacy_validation (/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py:17)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841841.388, "ph": "X", "dur": 6.501339930863327, "name": "_validate_labelname (/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py:75)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841849.293, "ph": "X", "dur": 0.10054757540934105, "name": "get_legacy_validation (/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py:17)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841849.026, "ph": "X", "dur": 1.23474632439493, "name": "_validate_labelname (/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py:75)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841836.821, "ph": "X", "dur": 13.790486688835008, "name": "_validate_labelnames (/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py:103)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841854.421, "ph": "X", "dur": 0.9076904747117986, "name": "_validate_metric_name (/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py:34)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841856.597, "ph": "X", "dur": 0.42539358827028906, "name": "MetricWrapperBase._is_parent (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:80)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841858.495, "ph": "X", "dur": 0.2800968172117358, "name": "MetricWrapperBase._is_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:67)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841865.23, "ph": "X", "dur": 2.5910336740099424, "name": "MutexValue.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:13)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841860.742, "ph": "X", "dur": 8.595160319882023, "name": "Counter._metric_init (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:281)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841827.33, "ph": "X", "dur": 42.29517185817765, "name": "MetricWrapperBase.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:102)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841804.885, "ph": "X", "dur": 66.14152100635786, "name": "MetricWrapperBase.labels (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:134)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841875.805, "ph": "X", "dur": 0.2701525515119108, "name": "MetricWrapperBase._is_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:67)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841875.43, "ph": "X", "dur": 0.9436003230722776, "name": "MetricWrapperBase._raise_if_not_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841878.905, "ph": "X", "dur": 1.5419136426784115, "name": "MutexValue.inc (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:18)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841873.951, "ph": "X", "dur": 6.743869522097946, "name": "Counter.inc (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:286)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841881.721, "ph": "X", "dur": 0.41600178177600994, "name": "ProviderCircuitBreakers.get_breaker (/app/app/utils/circuit_breaker.py:142)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841881.434, "ph": "X", "dur": 0.8386330740185699, "name": "CircuitBreakerManager._get_breaker (/app/app/circuit_breaker.py:26)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841883.276, "ph": "X", "dur": 0.15247874073064907, "name": "Enum.name (/usr/local/lib/python3.11/enum.py:1252)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841882.822, "ph": "X", "dur": 0.7292461513204955, "name": "property.__get__ (/usr/local/lib/python3.11/enum.py:193)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841881.235, "ph": "X", "dur": 2.755114058057054, "name": "CircuitBreakerManager.get_state (/app/app/circuit_breaker.py:19)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841889.185, "ph": "X", "dur": 0.3557837283715145, "name": "MetricWrapperBase.labels.<locals>.<genexpr> (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:172)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841889.673, "ph": "X", "dur": 0.6126772589503254, "name": "MetricWrapperBase.labels.<locals>.<genexpr> (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:172)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841922.923, "ph": "X", "dur": 0.8458150436906656, "name": "_build_full_name (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:25)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841925.136, "ph": "X", "dur": 0.08618363606514948, "name": "get_legacy_validation (/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py:17)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841924.964, "ph": "X", "dur": 2.4291631267850144, "name": "_validate_labelname (/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py:75)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841924.296, "ph": "X", "dur": 10.642574135634868, "name": "_validate_labelnames (/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py:103)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841936.158, "ph": "X", "dur": 0.6751051491770043, "name": "_validate_metric_name (/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py:34)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841937.156, "ph": "X", "dur": 0.5552215015735591, "name": "MetricWrapperBase._is_parent (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:80)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841938.038, "ph": "X", "dur": 0.3353427377663188, "name": "MetricWrapperBase._is_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:67)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841942.418, "ph": "X", "dur": 1.368441452137021, "name": "MutexValue.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:13)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841940.234, "ph": "X", "dur": 4.180458808365295, "name": "Gauge._metric_init (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:389)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841922.419, "ph": "X", "dur": 22.311617475173886, "name": "MetricWrapperBase.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:102)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841919.053, "ph": "X", "dur": 26.56665827628787, "name": "Gauge.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:362)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841886.003, "ph": "X", "dur": 60.98431432258753, "name": "MetricWrapperBase.labels (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:134)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841950.597, "ph": "X", "dur": 0.2508164793178068, "name": "MetricWrapperBase._is_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:67)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841950.012, "ph": "X", "dur": 1.0027134580656814, "name": "MetricWrapperBase._raise_if_not_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841952.966, "ph": "X", "dur": 1.1673463013183387, "name": "MutexValue.set (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:22)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841949.676, "ph": "X", "dur": 4.668280286862263, "name": "Gauge.set (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:409)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841793.061, "ph": "X", "dur": 161.41808313558616, "name": "ProviderRegistry.record_selection (/app/app/providers/registry.py:132)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841956.718, "ph": "X", "dur": 2.31977620408694, "name": "MarketDataService.get_stock_price.<locals>.<lambda> (/app/app/services/market_data.py:147)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841987.657, "ph": "X", "dur": 0.802170766452545, "name": "Lock.acquire (/usr/local/lib/python3.11/asyncio/locks.py:93)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841981.436, "ph": "X", "dur": 7.47201075500735, "name": "_ContextManagerMixin.__aenter__ (/usr/local/lib/python3.11/asyncio/locks.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841998.967, "ph": "X", "dur": 0.3171115839833064, "name": "Lock._wake_up_first (/usr/local/lib/python3.11/asyncio/locks.py:142)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841995.598, "ph": "X", "dur": 3.839591478543518, "name": "Lock.release (/usr/local/lib/python3.11/asyncio/locks.py:125)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841993.849, "ph": "X", "dur": 5.696959327588599, "name": "_ContextManagerMixin.__aexit__ (/usr/local/lib/python3.11/asyncio/locks.py:20)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290842050.332, "ph": "X", "dur": 3.634629113286015, "name": "isfuture (/usr/local/lib/python3.11/asyncio/base_futures.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290842057.451, "ph": "X", "dur": 1.4148480254028704, "name": "iscoroutine (/usr/local/lib/python3.11/asyncio/coroutines.py:34)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290842071.089, "ph": "X", "dur": 5.054449271538798, "name": "WeakSet.add (/usr/local/lib/python3.11/_weakrefset.py:85)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290842047.729, "ph": "X", "dur": 28.748319678988352, "name": "_ensure_future (/usr/local/lib/python3.11/asyncio/tasks.py:662)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290842043.093, "ph": "X", "dur": 33.4851049073383, "name": "ensure_future (/usr/local/lib/python3.11/asyncio/tasks.py:654)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290842011.999, "ph": "X", "dur": 66.67906381335395, "name": "wait_for (/usr/local/lib/python3.11/asyncio/tasks.py:436)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841978.755, "ph": "X", "dur": 100.04649490991098, "name": "CircuitBreaker.call (/app/app/utils/circuit_breaker.py:48)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841963.996, "ph": "X", "dur": 114.96565575567612, "name": "DataProvider.get_price_safe (/app/app/providers/base.py:80)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841652.168, "ph": "X", "dur": 426.8769412371796, "name": "MarketDataService._try_providers (/app/app/services/market_data.py:92)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290841613.169, "ph": "X", "dur": 465.9700595400136, "name": "MarketDataService.get_stock_price (/app/app/services/market_data.py:134)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290842190.071, "ph": "X", "dur": 7.506263225751192, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290842185.131, "ph": "X", "dur": 12.512096087201957, "name": "FinnhubProvider.get_price (/app/app/providers/finnhub_provider.py:17)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290842176.073, "ph": "X", "dur": 21.646456591696708, "name": "DataProvider._get_price_impl (/app/app/providers/base.py:125)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290927482.864, "ph": "X", "dur": 29.78362823018124, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290927588.694, "ph": "X", "dur": 4.629608142474055, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290927612.94, "ph": "X", "dur": 12.751863382408846, "name": "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290927634.228, "ph": "X", "dur": 1317.6588495040426, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290927558.466, "ph": "X", "dur": 1394.8302185491227, "name": "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290927551.996, "ph": "X", "dur": 1401.9834603425304, "name": "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38290927537.833, "ph": "X", "dur": 1416.9606294048779, "name": "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291029230.461, "ph": "X", "dur": 57.007160501863105, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291029311.616, "ph": "X", "dur": 3.9611325037636007, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291029324.854, "ph": "X", "dur": 7.955412559859951, "name": "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291029342.269, "ph": "X", "dur": 47.90484263129, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291029310.198, "ph": "X", "dur": 80.31651930225276, "name": "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291029308.843, "ph": "X", "dur": 81.86782475142545, "name": "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291029306.724, "ph": "X", "dur": 84.17931406742919, "name": "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291129623.29, "ph": "X", "dur": 34.761285672149164, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291129689.63, "ph": "X", "dur": 4.980419737995657, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291129704.999, "ph": "X", "dur": 8.65648329169761, "name": "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291129723.735, "ph": "X", "dur": 352.60708793962596, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291129687.992, "ph": "X", "dur": 389.00089056416306, "name": "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291129686.317, "ph": "X", "dur": 391.0709552073433, "name": "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291129683.78, "ph": "X", "dur": 393.84430041918336, "name": "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291230320.831, "ph": "X", "dur": 36.60705187787779, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291230385.911, "ph": "X", "dur": 5.090911579104823, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291230400.731, "ph": "X", "dur": 7.651559996809745, "name": "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291230417.584, "ph": "X", "dur": 52.40683269728297, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291230383.654, "ph": "X", "dur": 86.58637882599237, "name": "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291230382.759, "ph": "X", "dur": 87.69516445152287, "name": "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291230380.149, "ph": "X", "dur": 90.4983424604624, "name": "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291330831.282, "ph": "X", "dur": 36.80704211028537, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291330896.392, "ph": "X", "dur": 5.2174247371748175, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291330910.915, "ph": "X", "dur": 8.619468524926038, "name": "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291330929.391, "ph": "X", "dur": 53.96200536089449, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291330894.535, "ph": "X", "dur": 89.11056493613128, "name": "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291330892.536, "ph": "X", "dur": 91.32979356480888, "name": "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291330890.764, "ph": "X", "dur": 93.3214090008016, "name": "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291430217.76, "ph": "X", "dur": 33.780118123099776, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291430278.449, "ph": "X", "dur": 4.251726045880707, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291430291.475, "ph": "X", "dur": 7.506263225751192, "name": "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291430307.815, "ph": "X", "dur": 46.7065586144611, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291430276.513, "ph": "X", "dur": 78.3580514185928, "name": "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291430275.459, "ph": "X", "dur": 79.61102889677073, "name": "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291430272.837, "ph": "X", "dur": 82.41807412014909, "name": "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291530572.973, "ph": "X", "dur": 31.2260992158614, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291530629.342, "ph": "X", "dur": 4.129632561455079, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291530642.979, "ph": "X", "dur": 7.503500929723462, "name": "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291530659.065, "ph": "X", "dur": 46.00990755626781, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291530627.947, "ph": "X", "dur": 77.43544454533125, "name": "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291530626.558, "ph": "X", "dur": 79.03702378220862, "name": "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291530624.301, "ph": "X", "dur": 81.66893943742896, "name": "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291630978.806, "ph": "X", "dur": 37.832958854983985, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291631047.109, "ph": "X", "dur": 4.991468922106574, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291631061.727, "ph": "X", "dur": 9.147067066222306, "name": "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291631080.629, "ph": "X", "dur": 58.01097887833988, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291631045.019, "ph": "X", "dur": 93.91198789153007, "name": "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291631043.951, "ph": "X", "dur": 95.19535062601305, "name": "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291631041.299, "ph": "X", "dur": 98.14714016124442, "name": "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650315.243, "ph": "X", "dur": 6.110198813336879, "name": "StreamReaderProtocol._stream_reader (/usr/local/lib/python3.11/asyncio/streams.py:211)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650355.622, "ph": "X", "dur": 26.138502391989853, "name": "StreamReader._wakeup_waiter (/usr/local/lib/python3.11/asyncio/streams.py:472)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650339.979, "ph": "X", "dur": 52.926696809701596, "name": "StreamReader.feed_data (/usr/local/lib/python3.11/asyncio/streams.py:497)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650307.055, "ph": "X", "dur": 86.27644921168117, "name": "StreamReaderProtocol.data_received (/usr/local/lib/python3.11/asyncio/streams.py:284)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650604.978, "ph": "X", "dur": 2.6070549909707714, "name": "StreamReader._wait_for_data (/usr/local/lib/python3.11/asyncio/streams.py:519)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650629.233, "ph": "X", "dur": 1.0408331432483438, "name": "StreamReader._maybe_resume_transport (/usr/local/lib/python3.11/asyncio/streams.py:484)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650596.677, "ph": "X", "dur": 37.43518822699098, "name": "StreamReader.readuntil (/usr/local/lib/python3.11/asyncio/streams.py:578)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650581.254, "ph": "X", "dur": 54.43988257369163, "name": "StreamReader.readline (/usr/local/lib/python3.11/asyncio/streams.py:547)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650571.775, "ph": "X", "dur": 82.54072006378027, "name": "_AsyncRESPBase._readline (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:273)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650562.13, "ph": "X", "dur": 97.32287102657004, "name": "_AsyncRESP2Parser._read_response (/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py:87)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650664.464, "ph": "X", "dur": 4.4119392154889985, "name": "_AsyncRESPBase._clear (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:225)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650546.231, "ph": "X", "dur": 123.01443392127332, "name": "_AsyncRESP2Parser.read_response (/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py:74)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650537.538, "ph": "X", "dur": 141.84500594230292, "name": "AbstractConnection.read_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:572)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650695.128, "ph": "X", "dur": 10.331539602912565, "name": "CaseInsensitiveDict.__contains__ (/usr/local/lib/python3.11/site-packages/redis/client.py:88)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650710.349, "ph": "X", "dur": 0.19612301796876963, "name": "cast (/usr/local/lib/python3.11/typing.py:2287)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650713.995, "ph": "X", "dur": 4.1334997758939, "name": "CaseInsensitiveDict.__getitem__ (/usr/local/lib/python3.11/site-packages/redis/client.py:94)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650722.178, "ph": "X", "dur": 0.9380757310168192, "name": "<lambda> (/usr/local/lib/python3.11/site-packages/redis/_parsers/helpers.py:806)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650804.516, "ph": "X", "dur": 8.575824247687919, "name": "ABCMeta.__instancecheck__ (<frozen abc>:117)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650734.256, "ph": "X", "dur": 80.5352931476489, "name": "isawaitable (/usr/local/lib/python3.11/inspect.py:449)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650509.306, "ph": "X", "dur": 306.00660149262967, "name": "Redis.parse_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:689)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650495.174, "ph": "X", "dur": 321.531257627673, "name": "Redis._send_command_parse_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:647)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650491.569, "ph": "X", "dur": 327.1784956267625, "name": "Retry.call_with_retry (/usr/local/lib/python3.11/site-packages/redis/asyncio/retry.py:37)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650877.32, "ph": "X", "dur": 2.867815735988403, "name": "AfterConnectionReleasedEvent.__init__ (/usr/local/lib/python3.11/site-packages/redis/event.py:98)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650905.33, "ph": "X", "dur": 0.8452625844851197, "name": "AfterConnectionReleasedEvent.connection (/usr/local/lib/python3.11/site-packages/redis/event.py:101)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650914.993, "ph": "X", "dur": 1.8689694923615428, "name": "AbstractConnection.re_auth (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:717)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650901.702, "ph": "X", "dur": 16.434003987371806, "name": "AsyncReAuthConnectionListener.listen (/usr/local/lib/python3.11/site-packages/redis/event.py:243)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650888.486, "ph": "X", "dur": 31.25648447216642, "name": "EventDispatcher.dispatch_async (/usr/local/lib/python3.11/site-packages/redis/event.py:86)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650858.435, "ph": "X", "dur": 63.76428904489415, "name": "ConnectionPool.release (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1196)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650480.403, "ph": "X", "dur": 442.88334179945895, "name": "Redis.execute_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:667)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650465.366, "ph": "X", "dur": 463.78121616764105, "name": "CollectorJobs.pop_job (/app/app/services/data_collector.py:61)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650940.574, "ph": "X", "dur": 66.30173417596615, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291650456.487, "ph": "X", "dur": 550.904585341424, "name": "DataCollectorService.consume_jobs (/app/app/services/data_collector.py:184)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291731247.47, "ph": "X", "dur": 38.336249191236234, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291731316.506, "ph": "X", "dur": 5.135660774754035, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291731330.875, "ph": "X", "dur": 9.458654058150154, "name": "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291731350.98, "ph": "X", "dur": 58.591061044163, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291731314.31, "ph": "X", "dur": 95.55279173200121, "name": "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291731312.851, "ph": "X", "dur": 97.22895296162724, "name": "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291731310.256, "ph": "X", "dur": 100.1288113315373, "name": "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291751621.985, "ph": "X", "dur": 38.36497706992461, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291751688.519, "ph": "X", "dur": 5.41465267355468, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291751734.645, "ph": "X", "dur": 16.454997437182545, "name": "list_or_args (/usr/local/lib/python3.11/site-packages/redis/commands/helpers.py:10)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291751723.164, "ph": "X", "dur": 41.95872420200024, "name": "ListCommands.brpop (/usr/local/lib/python3.11/site-packages/redis/commands/core.py:2560)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291751781.521, "ph": "X", "dur": 1.3817004730701208, "name": "Redis.initialize (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:399)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291751805.18, "ph": "X", "dur": 41.506812571863755, "name": "deprecated_args.<locals>.decorator.<locals>.wrapper (/usr/local/lib/python3.11/site-packages/redis/utils.py:169)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291751879.228, "ph": "X", "dur": 0.5999706972227714, "name": "Lock.acquire.<locals>.<genexpr> (/usr/local/lib/python3.11/asyncio/locks.py:100)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291751870.132, "ph": "X", "dur": 12.043058221693547, "name": "Lock.acquire (/usr/local/lib/python3.11/asyncio/locks.py:93)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291751866.669, "ph": "X", "dur": 16.113025188949674, "name": "_ContextManagerMixin.__aenter__ (/usr/local/lib/python3.11/asyncio/locks.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291751890.916, "ph": "X", "dur": 7.508473062573374, "name": "ConnectionPool.get_available_connection (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1156)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291751944.29, "ph": "X", "dur": 2.9252714933651696, "name": "AbstractConnection.is_connected (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:259)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291751940.618, "ph": "X", "dur": 7.028386012954049, "name": "AbstractConnection.connect_check_health (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:298)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291751916.36, "ph": "X", "dur": 67.66907070969206, "name": "AbstractConnection.connect (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:294)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752179.338, "ph": "X", "dur": 13.99544905409251, "name": "Timeout.__init__ (/usr/local/lib/python3.11/asyncio/timeouts.py:33)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752147.06, "ph": "X", "dur": 47.107643997687376, "name": "timeout (/usr/local/lib/python3.11/asyncio/timeouts.py:129)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752212.527, "ph": "X", "dur": 6.873697435401216, "name": "current_task (/usr/local/lib/python3.11/asyncio/tasks.py:35)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752234.013, "ph": "X", "dur": 13.146319255168569, "name": "Timeout.reschedule (/usr/local/lib/python3.11/asyncio/timeouts.py:50)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752204.058, "ph": "X", "dur": 43.57577229663288, "name": "Timeout.__aenter__ (/usr/local/lib/python3.11/asyncio/timeouts.py:85)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752252.693, "ph": "X", "dur": 0.6104674221281421, "name": "StreamReader.at_eof (/usr/local/lib/python3.11/asyncio/streams.py:493)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752262.369, "ph": "X", "dur": 8.423345506957268, "name": "Timeout.__aexit__ (/usr/local/lib/python3.11/asyncio/timeouts.py:97)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752137.556, "ph": "X", "dur": 134.68126742399016, "name": "_AsyncRESPBase.can_read_destructive (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:242)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752123.459, "ph": "X", "dur": 149.27171504245553, "name": "AbstractConnection.can_read_destructive (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:563)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291751910.183, "ph": "X", "dur": 363.2723129026882, "name": "ConnectionPool.ensure_connection (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1180)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752280.355, "ph": "X", "dur": 1.6358317076212023, "name": "Lock._wake_up_first (/usr/local/lib/python3.11/asyncio/locks.py:142)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752278.223, "ph": "X", "dur": 4.271062118074811, "name": "Lock.release (/usr/local/lib/python3.11/asyncio/locks.py:125)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752276.528, "ph": "X", "dur": 6.567082576323281, "name": "_ContextManagerMixin.__aexit__ (/usr/local/lib/python3.11/asyncio/locks.py:20)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291751857.376, "ph": "X", "dur": 426.21620002734676, "name": "ConnectionPool.get_connection (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1139)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752299.399, "ph": "X", "dur": 0.5916838091395838, "name": "AbstractBackoff.reset (/usr/local/lib/python3.11/site-packages/redis/backoff.py:13)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752303.347, "ph": "X", "dur": 5.20913784909163, "name": "Redis.execute_command.<locals>.<lambda> (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:678)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752380.366, "ph": "X", "dur": 4.778772127971429, "name": "Encoder.encode (/usr/local/lib/python3.11/site-packages/redis/_parsers/encoders.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752393.21, "ph": "X", "dur": 8.485220937978402, "name": "Encoder.encode (/usr/local/lib/python3.11/site-packages/redis/_parsers/encoders.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752406.466, "ph": "X", "dur": 5.524592055458299, "name": "Encoder.encode (/usr/local/lib/python3.11/site-packages/redis/_parsers/encoders.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752338.624, "ph": "X", "dur": 78.3558415817706, "name": "AbstractConnection.pack_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:630)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752439.956, "ph": "X", "dur": 1.0021609988601357, "name": "AbstractConnection.is_connected (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:259)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752447.346, "ph": "X", "dur": 1.5634595516946987, "name": "AbstractConnection.check_health (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:504)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752454.545, "ph": "X", "dur": 11.666281043511292, "name": "StreamWriter.writelines (/usr/local/lib/python3.11/asyncio/streams.py:348)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752476.063, "ph": "X", "dur": 0.7673658365031578, "name": "StreamReader.exception (/usr/local/lib/python3.11/asyncio/streams.py:460)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752488.321, "ph": "X", "dur": 1.6910776281757856, "name": "FlowControlMixin._drain_helper (/usr/local/lib/python3.11/asyncio/streams.py:164)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752473.159, "ph": "X", "dur": 17.35661086063334, "name": "StreamWriter.drain (/usr/local/lib/python3.11/asyncio/streams.py:369)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752438.842, "ph": "X", "dur": 52.093035868532944, "name": "AbstractConnection.send_packed_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:516)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752316.049, "ph": "X", "dur": 175.47485516149422, "name": "AbstractConnection.send_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:557)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752503.153, "ph": "X", "dur": 2.987699383591848, "name": "Connection._host_error (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:781)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752541.142, "ph": "X", "dur": 12.463479677113922, "name": "StreamReader._wait_for_data (/usr/local/lib/python3.11/asyncio/streams.py:519)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752535.016, "ph": "X", "dur": 18.981393384143626, "name": "StreamReader.readuntil (/usr/local/lib/python3.11/asyncio/streams.py:578)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752531.678, "ph": "X", "dur": 22.522656891692392, "name": "StreamReader.readline (/usr/local/lib/python3.11/asyncio/streams.py:547)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752522.228, "ph": "X", "dur": 32.196770040005426, "name": "_AsyncRESPBase._readline (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:273)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752517.816, "ph": "X", "dur": 36.83521752976821, "name": "_AsyncRESP2Parser._read_response (/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py:87)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752512.113, "ph": "X", "dur": 42.78630809190789, "name": "_AsyncRESP2Parser.read_response (/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py:74)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752500.299, "ph": "X", "dur": 54.845940089767815, "name": "AbstractConnection.read_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:572)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752494.673, "ph": "X", "dur": 60.77106506924684, "name": "Redis.parse_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:689)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752309.279, "ph": "X", "dur": 246.40288272470116, "name": "Redis._send_command_parse_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:647)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291752293.359, "ph": "X", "dur": 262.5672866197666, "name": "Retry.call_with_retry (/usr/local/lib/python3.11/site-packages/redis/asyncio/retry.py:37)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291751766.901, "ph": "X", "dur": 789.312278443466, "name": "Redis.execute_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:667)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291751708.962, "ph": "X", "dur": 847.6906426934937, "name": "CollectorJobs.pop_job (/app/app/services/data_collector.py:61)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291751685.905, "ph": "X", "dur": 871.4546754200476, "name": "DataCollectorService.consume_jobs (/app/app/services/data_collector.py:184)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291832075.658, "ph": "X", "dur": 36.600422367411234, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291832140.637, "ph": "X", "dur": 5.432331368132146, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291832250.924, "ph": "X", "dur": 15.690393896707118, "name": "_format_timetuple_and_zone (/usr/local/lib/python3.11/email/utils.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291832214.054, "ph": "X", "dur": 85.8427687353277, "name": "format_datetime (/usr/local/lib/python3.11/email/utils.py:271)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291832178.461, "ph": "X", "dur": 123.40502257959423, "name": "formatdate (/usr/local/lib/python3.11/email/utils.py:242)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291832154.942, "ph": "X", "dur": 161.60426188785507, "name": "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291832325.673, "ph": "X", "dur": 52.413462207749525, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291832138.611, "ph": "X", "dur": 239.75735094119037, "name": "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291832137.326, "ph": "X", "dur": 241.43958922207742, "name": "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291832135.172, "ph": "X", "dur": 243.93118023908912, "name": "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291842556.92, "ph": "X", "dur": 33.36687863735149, "name": "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291842623.292, "ph": "X", "dur": 5.405813326265946, "name": "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291842742.951, "ph": "X", "dur": 8.44710125279574, "name": "ABCMeta.__instancecheck__ (<frozen abc>:117)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291842729.036, "ph": "X", "dur": 24.212629601457092, "name": "iscoroutine (/usr/local/lib/python3.11/asyncio/coroutines.py:34)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291842773.969, "ph": "X", "dur": 4.089855498655779, "name": "ismethod (/usr/local/lib/python3.11/inspect.py:300)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291842783.167, "ph": "X", "dur": 4.213053901492499, "name": "_unwrap_partial (/usr/local/lib/python3.11/functools.py:421)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291842789.603, "ph": "X", "dur": 1.7628973248967434, "name": "isfunction (/usr/local/lib/python3.11/inspect.py:378)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291842838.331, "ph": "X", "dur": 1.3955119532087665, "name": "isclass (/usr/local/lib/python3.11/inspect.py:292)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291842834.733, "ph": "X", "dur": 17.679247036672102, "name": "_signature_is_functionlike (/usr/local/lib/python3.11/inspect.py:2075)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291842771.324, "ph": "X", "dur": 81.9866034806178, "name": "_has_code_flag (/usr/local/lib/python3.11/inspect.py:391)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291842763.56, "ph": "X", "dur": 90.01770295163753, "name": "iscoroutinefunction (/usr/local/lib/python3.11/inspect.py:409)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291842757.561, "ph": "X", "dur": 99.40122255783345, "name": "iscoroutinefunction (/usr/local/lib/python3.11/asyncio/coroutines.py:21)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291842927.995, "ph": "X", "dur": 4.119135836549708, "name": "RLock (/usr/local/lib/python3.11/threading.py:90)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291842923.965, "ph": "X", "dur": 24.95458231450514, "name": "Condition.__init__ (/usr/local/lib/python3.11/threading.py:243)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291842908.635, "ph": "X", "dur": 45.857981274742706, "name": "Future.__init__ (/usr/local/lib/python3.11/concurrent/futures/_base.py:328)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291842960.791, "ph": "X", "dur": 3.406463461395587, "name": "_WorkItem.__init__ (/usr/local/lib/python3.11/concurrent/futures/thread.py:47)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291843076.399, "ph": "X", "dur": 3.486017586994187, "name": "Condition.__enter__ (/usr/local/lib/python3.11/threading.py:271)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291843086.073, "ph": "X", "dur": 2.3081745607704773, "name": "Condition.__exit__ (/usr/local/lib/python3.11/threading.py:274)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291843072.374, "ph": "X", "dur": 16.853872983586633, "name": "Semaphore.acquire (/usr/local/lib/python3.11/threading.py:440)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291843056.967, "ph": "X", "dur": 32.85419649460496, "name": "ThreadPoolExecutor._adjust_thread_count (/usr/local/lib/python3.11/concurrent/futures/thread.py:180)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291842886.557, "ph": "X", "dur": 205.44079492950559, "name": "ThreadPoolExecutor.submit (/usr/local/lib/python3.11/concurrent/futures/thread.py:161)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291843103.443, "ph": "X", "dur": 8.771394806451143, "name": "isfuture (/usr/local/lib/python3.11/asyncio/base_futures.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291843146.993, "ph": "X", "dur": 2.2440892929271614, "name": "isfuture (/usr/local/lib/python3.11/asyncio/base_futures.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291843150.912, "ph": "X", "dur": 1.9976924872537212, "name": "isfuture (/usr/local/lib/python3.11/asyncio/base_futures.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291843153.639, "ph": "X", "dur": 1.312643072376892, "name": "isfuture (/usr/local/lib/python3.11/asyncio/base_futures.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291843155.651, "ph": "X", "dur": 0.8093527361246409, "name": "isfuture (/usr/local/lib/python3.11/asyncio/base_futures.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291843161.18, "ph": "X", "dur": 1.4717513235740909, "name": "_get_loop (/usr/local/lib/python3.11/asyncio/futures.py:299)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291843179.549, "ph": "X", "dur": 2.293810621426286, "name": "Condition.__enter__ (/usr/local/lib/python3.11/threading.py:271)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291843187.403, "ph": "X", "dur": 1.4109808109640496, "name": "Condition.__exit__ (/usr/local/lib/python3.11/threading.py:274)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291843178.397, "ph": "X", "dur": 11.035925089983499, "name": "Future.add_done_callback (/usr/local/lib/python3.11/concurrent/futures/_base.py:408)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291843145.888, "ph": "X", "dur": 43.96967571018706, "name": "_chain_future (/usr/local/lib/python3.11/asyncio/futures.py:365)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291843101.107, "ph": "X", "dur": 89.24426006387337, "name": "wrap_future (/usr/local/lib/python3.11/asyncio/futures.py:409)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291842660.891, "ph": "X", "dur": 533.0955103914487, "name": "to_thread (/usr/local/lib/python3.11/asyncio/threads.py:12)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291842614.935, "ph": "X", "dur": 579.6418558363015, "name": "FinnhubProvider.get_price (/app/app/providers/finnhub_provider.py:17)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291842612.226, "ph": "X", "dur": 582.530112562895, "name": "DataProvider._get_price_impl (/app/app/providers/base.py:125)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870554.913, "ph": "X", "dur": 10.858585685003288, "name": "isfuture (/usr/local/lib/python3.11/asyncio/base_futures.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870594.342, "ph": "X", "dur": 11.05968083582197, "name": "Condition.__enter__ (/usr/local/lib/python3.11/threading.py:271)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870611.869, "ph": "X", "dur": 3.8053390077996765, "name": "Condition.__exit__ (/usr/local/lib/python3.11/threading.py:274)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870587.892, "ph": "X", "dur": 28.598050775079887, "name": "Future.done (/usr/local/lib/python3.11/concurrent/futures/_base.py:393)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870626.871, "ph": "X", "dur": 1.1065757887082972, "name": "Condition.__enter__ (/usr/local/lib/python3.11/threading.py:271)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870653.951, "ph": "X", "dur": 0.7590789484199704, "name": "Condition.__exit__ (/usr/local/lib/python3.11/threading.py:274)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870625.906, "ph": "X", "dur": 29.32895430401702, "name": "Future.cancelled (/usr/local/lib/python3.11/concurrent/futures/_base.py:383)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870668.23, "ph": "X", "dur": 0.9474675375110984, "name": "Condition.__enter__ (/usr/local/lib/python3.11/threading.py:271)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870671.499, "ph": "X", "dur": 0.5789772474120298, "name": "Condition.__exit__ (/usr/local/lib/python3.11/threading.py:274)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870667.12, "ph": "X", "dur": 5.472660890136992, "name": "Future.exception (/usr/local/lib/python3.11/concurrent/futures/_base.py:463)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870685.317, "ph": "X", "dur": 0.7971986336026327, "name": "Condition.__enter__ (/usr/local/lib/python3.11/threading.py:271)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870690.228, "ph": "X", "dur": 0.9507822927443733, "name": "Future.__get_result (/usr/local/lib/python3.11/concurrent/futures/_base.py:398)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870692.284, "ph": "X", "dur": 0.6248313614723336, "name": "Condition.__exit__ (/usr/local/lib/python3.11/threading.py:274)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870684.405, "ph": "X", "dur": 9.074142451090257, "name": "Future.result (/usr/local/lib/python3.11/concurrent/futures/_base.py:428)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870579.961, "ph": "X", "dur": 135.46686441427633, "name": "_copy_future_state (/usr/local/lib/python3.11/asyncio/futures.py:345)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870549.624, "ph": "X", "dur": 166.39408319993743, "name": "_chain_future.<locals>._set_state (/usr/local/lib/python3.11/asyncio/futures.py:381)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870734.111, "ph": "X", "dur": 0.9082429339173445, "name": "_chain_future.<locals>._call_check_cancel (/usr/local/lib/python3.11/asyncio/futures.py:387)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870754.898, "ph": "X", "dur": 1.712071077986527, "name": "to_thread (/usr/local/lib/python3.11/asyncio/threads.py:12)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870783.478, "ph": "X", "dur": 20.363093857213745, "name": "DataProvider.mark_available (/app/app/providers/base.py:74)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870753.607, "ph": "X", "dur": 221.34388562034786, "name": "FinnhubProvider.get_price (/app/app/providers/finnhub_provider.py:17)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291870750.978, "ph": "X", "dur": 226.16851186237957, "name": "DataProvider._get_price_impl (/app/app/providers/base.py:125)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871006.45, "ph": "X", "dur": 3.894837399098101, "name": "_release_waiter (/usr/local/lib/python3.11/asyncio/tasks.py:431)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871063.822, "ph": "X", "dur": 12.370666530582225, "name": "wait_for (/usr/local/lib/python3.11/asyncio/tasks.py:436)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871084.029, "ph": "X", "dur": 7.092471280797365, "name": "WeakSet.__init__.<locals>._remove (/usr/local/lib/python3.11/_weakrefset.py:39)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871106.628, "ph": "X", "dur": 2.0468613565473, "name": "Lock.acquire (/usr/local/lib/python3.11/asyncio/locks.py:93)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871101.353, "ph": "X", "dur": 7.926132221966022, "name": "_ContextManagerMixin.__aenter__ (/usr/local/lib/python3.11/asyncio/locks.py:14)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871128.064, "ph": "X", "dur": 1.0093429685322313, "name": "Lock._wake_up_first (/usr/local/lib/python3.11/asyncio/locks.py:142)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871126.017, "ph": "X", "dur": 3.573306141470428, "name": "Lock.release (/usr/local/lib/python3.11/asyncio/locks.py:125)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871124.031, "ph": "X", "dur": 5.94059383723431, "name": "_ContextManagerMixin.__aexit__ (/usr/local/lib/python3.11/asyncio/locks.py:20)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871062.313, "ph": "X", "dur": 68.35467258377444, "name": "CircuitBreaker.call (/app/app/utils/circuit_breaker.py:48)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871133.76, "ph": "X", "dur": 4.2229981671923245, "name": "DataProvider.mark_available (/app/app/providers/base.py:74)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871061.139, "ph": "X", "dur": 77.18573298442453, "name": "DataProvider.get_price_safe (/app/app/providers/base.py:80)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871253.826, "ph": "X", "dur": 5.995839757788893, "name": "RollingStats.update_latency (/app/app/providers/registry.py:28)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871263.871, "ph": "X", "dur": 2.5418648047163637, "name": "RollingStats.update_error (/app/app/providers/registry.py:31)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871292.946, "ph": "X", "dur": 3.397624114106854, "name": "MetricWrapperBase.labels.<locals>.<genexpr> (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:172)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871296.863, "ph": "X", "dur": 0.9607265584441983, "name": "MetricWrapperBase.labels.<locals>.<genexpr> (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:172)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871298.081, "ph": "X", "dur": 0.3889312807042643, "name": "MetricWrapperBase.labels.<locals>.<genexpr> (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:172)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871333.731, "ph": "X", "dur": 5.4616117060260745, "name": "Histogram._prepare_buckets.<locals>.<listcomp> (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:590)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871330.806, "ph": "X", "dur": 18.023981580932702, "name": "Histogram._prepare_buckets (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:589)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871356.449, "ph": "X", "dur": 2.5175565996723472, "name": "_build_full_name (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:25)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871366.587, "ph": "X", "dur": 0.5867116762896715, "name": "get_legacy_validation (/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py:17)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871364.23, "ph": "X", "dur": 33.1464474143387, "name": "_validate_labelname (/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py:75)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871400.809, "ph": "X", "dur": 0.25192139772889843, "name": "get_legacy_validation (/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py:17)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871400.312, "ph": "X", "dur": 2.8308009692168326, "name": "_validate_labelname (/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py:75)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871361.018, "ph": "X", "dur": 43.00894915174286, "name": "_validate_labelnames (/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py:103)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871407.892, "ph": "X", "dur": 1.881123594883551, "name": "_validate_metric_name (/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py:34)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871411.464, "ph": "X", "dur": 1.3076709395269794, "name": "MetricWrapperBase._is_parent (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:80)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871414.84, "ph": "X", "dur": 0.7436100906646871, "name": "MetricWrapperBase._is_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:67)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871460.526, "ph": "X", "dur": 5.7842478820648395, "name": "MutexValue.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:13)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871480.81, "ph": "X", "dur": 67.15362627091781, "name": "floatToGoString (/usr/local/lib/python3.11/site-packages/prometheus_client/utils.py:9)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871551.58, "ph": "X", "dur": 2.4711500264064976, "name": "MutexValue.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:13)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871570.513, "ph": "X", "dur": 4.257803097141711, "name": "floatToGoString (/usr/local/lib/python3.11/site-packages/prometheus_client/utils.py:9)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871576.028, "ph": "X", "dur": 1.429764423952608, "name": "MutexValue.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:13)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871579.827, "ph": "X", "dur": 2.587166459571122, "name": "floatToGoString (/usr/local/lib/python3.11/site-packages/prometheus_client/utils.py:9)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871583.379, "ph": "X", "dur": 1.2789430608385963, "name": "MutexValue.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:13)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871586.456, "ph": "X", "dur": 2.2026548525112237, "name": "floatToGoString (/usr/local/lib/python3.11/site-packages/prometheus_client/utils.py:9)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871589.566, "ph": "X", "dur": 1.136961045013318, "name": "MutexValue.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:13)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871592.47, "ph": "X", "dur": 2.179451565878299, "name": "floatToGoString (/usr/local/lib/python3.11/site-packages/prometheus_client/utils.py:9)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871595.653, "ph": "X", "dur": 4.302552292790923, "name": "MutexValue.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:13)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871602.047, "ph": "X", "dur": 2.378336879874798, "name": "floatToGoString (/usr/local/lib/python3.11/site-packages/prometheus_client/utils.py:9)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871605.851, "ph": "X", "dur": 1.117624972819214, "name": "MutexValue.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:13)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871609.015, "ph": "X", "dur": 2.4993254458893346, "name": "floatToGoString (/usr/local/lib/python3.11/site-packages/prometheus_client/utils.py:9)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871612.463, "ph": "X", "dur": 1.039175765631706, "name": "MutexValue.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:13)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871615.442, "ph": "X", "dur": 2.3435319499254104, "name": "floatToGoString (/usr/local/lib/python3.11/site-packages/prometheus_client/utils.py:9)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871618.784, "ph": "X", "dur": 0.9176347404116235, "name": "MutexValue.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:13)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871621.448, "ph": "X", "dur": 0.6817346596435541, "name": "floatToGoString (/usr/local/lib/python3.11/site-packages/prometheus_client/utils.py:9)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871623.29, "ph": "X", "dur": 0.7971986336026327, "name": "MutexValue.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:13)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871449.933, "ph": "X", "dur": 175.7637913259947, "name": "Histogram._metric_init (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:601)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871353.613, "ph": "X", "dur": 273.07837546448155, "name": "MetricWrapperBase.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:102)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871323.98, "ph": "X", "dur": 304.275194342449, "name": "Histogram.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:565)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871275.828, "ph": "X", "dur": 356.62954341520515, "name": "MetricWrapperBase.labels (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:134)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871645.069, "ph": "X", "dur": 1.0546446233869895, "name": "MetricWrapperBase._is_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:67)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871644.022, "ph": "X", "dur": 2.6628533707309003, "name": "MetricWrapperBase._raise_if_not_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871648.744, "ph": "X", "dur": 4.172171920282108, "name": "MutexValue.inc (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:18)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871660.309, "ph": "X", "dur": 1.9606777204821506, "name": "MutexValue.inc (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:18)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871642.211, "ph": "X", "dur": 20.89400715374329, "name": "Histogram.observe (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:616)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871673.104, "ph": "X", "dur": 3.623027469969553, "name": "ProviderCircuitBreakers.get_breaker (/app/app/utils/circuit_breaker.py:142)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871670.513, "ph": "X", "dur": 6.488633369135773, "name": "CircuitBreakerManager._get_breaker (/app/app/circuit_breaker.py:26)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871685.359, "ph": "X", "dur": 1.9341596786159507, "name": "Enum.name (/usr/local/lib/python3.11/enum.py:1252)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871681.814, "ph": "X", "dur": 5.75110032973209, "name": "property.__get__ (/usr/local/lib/python3.11/enum.py:193)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871668.967, "ph": "X", "dur": 20.07747244794655, "name": "CircuitBreakerManager.get_state (/app/app/circuit_breaker.py:19)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871701.566, "ph": "X", "dur": 1.7794711010631183, "name": "MetricWrapperBase.labels.<locals>.<genexpr> (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:172)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871703.664, "ph": "X", "dur": 0.4231837514481057, "name": "MetricWrapperBase.labels.<locals>.<genexpr> (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:172)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871693.312, "ph": "X", "dur": 17.24004196826317, "name": "MetricWrapperBase.labels (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:134)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871715.861, "ph": "X", "dur": 0.802170766452545, "name": "MetricWrapperBase._is_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:67)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871715.265, "ph": "X", "dur": 1.7164907516308936, "name": "MetricWrapperBase._raise_if_not_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:73)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871719.176, "ph": "X", "dur": 2.528605783783264, "name": "MutexValue.set (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:22)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871712.845, "ph": "X", "dur": 9.36528845241291, "name": "Gauge.set (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:409)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871729.039, "ph": "X", "dur": 2.348504082775323, "name": "get_settings (/app/app/core/config.py:131)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871735.112, "ph": "X", "dur": 0.7425051722535955, "name": "ProviderCircuitBreakers.get_breaker (/app/app/utils/circuit_breaker.py:142)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871734.5, "ph": "X", "dur": 1.5822431646832569, "name": "CircuitBreakerManager._get_breaker (/app/app/circuit_breaker.py:26)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871737.781, "ph": "X", "dur": 0.32595093127203967, "name": "Enum.name (/usr/local/lib/python3.11/enum.py:1252)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871737.008, "ph": "X", "dur": 1.3535250535872834, "name": "property.__get__ (/usr/local/lib/python3.11/enum.py:193)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871734.107, "ph": "X", "dur": 4.919096766180069, "name": "CircuitBreakerManager.get_state (/app/app/circuit_breaker.py:19)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871733.131, "ph": "X", "dur": 6.670944906965897, "name": "ProviderRegistry._breaker_score (/app/app/providers/registry.py:66)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871726.874, "ph": "X", "dur": 27.99863253706266, "name": "ProviderRegistry.health_score (/app/app/providers/registry.py:74)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871241.587, "ph": "X", "dur": 514.6047007818297, "name": "ProviderRegistry.record_outcome (/app/app/providers/registry.py:139)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871060.017, "ph": "X", "dur": 697.5968683531861, "name": "MarketDataService._try_providers (/app/app/services/market_data.py:92)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871778.663, "ph": "X", "dur": 7.3979812214642084, "name": "DataCache.set (/app/app/services/cache.py:19)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871802.401, "ph": "X", "dur": 4.615796662335409, "name": "Logger.isEnabledFor (/usr/local/lib/python3.11/logging/__init__.py:1734)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871835.081, "ph": "X", "dur": 4.111953866877612, "name": "<lambda> (/usr/local/lib/python3.11/logging/__init__.py:164)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871855.553, "ph": "X", "dur": 2.9136698500487075, "name": "normcase (<frozen posixpath>:52)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871846.678, "ph": "X", "dur": 12.596069886444923, "name": "_is_internal_frame (/usr/local/lib/python3.11/logging/__init__.py:194)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871861.94, "ph": "X", "dur": 0.5800821658231213, "name": "normcase (<frozen posixpath>:52)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871860.681, "ph": "X", "dur": 2.3054122647427486, "name": "_is_internal_frame (/usr/local/lib/python3.11/logging/__init__.py:194)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871864.866, "ph": "X", "dur": 0.5104723059243469, "name": "normcase (<frozen posixpath>:52)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871863.89, "ph": "X", "dur": 2.5733549794324757, "name": "_is_internal_frame (/usr/local/lib/python3.11/logging/__init__.py:194)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871832.414, "ph": "X", "dur": 38.31138852698667, "name": "Logger.findCaller (/usr/local/lib/python3.11/logging/__init__.py:1561)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871913.56, "ph": "X", "dur": 2.5059549563558847, "name": "getLevelName (/usr/local/lib/python3.11/logging/__init__.py:123)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871923.777, "ph": "X", "dur": 2.322538500114669, "name": "_get_sep (<frozen posixpath>:41)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871922.15, "ph": "X", "dur": 8.24987331641588, "name": "basename (<frozen posixpath>:140)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871946.311, "ph": "X", "dur": 5.055001730744344, "name": "_splitext (<frozen genericpath>:121)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871934.848, "ph": "X", "dur": 16.867132004519735, "name": "splitext (<frozen posixpath>:117)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871972.662, "ph": "X", "dur": 2.3269581737590355, "name": "current_thread (/usr/local/lib/python3.11/threading.py:1453)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871978.719, "ph": "X", "dur": 2.9948813532639442, "name": "Thread.name (/usr/local/lib/python3.11/threading.py:1152)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871991.633, "ph": "X", "dur": 121.79460399542813, "name": "current_process (/usr/local/lib/python3.11/multiprocessing/process.py:37)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872119.57, "ph": "X", "dur": 1.6280972787435608, "name": "BaseProcess.name (/usr/local/lib/python3.11/multiprocessing/process.py:189)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871904.521, "ph": "X", "dur": 229.75839178001638, "name": "LogRecord.__init__ (/usr/local/lib/python3.11/logging/__init__.py:292)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871879.046, "ph": "X", "dur": 256.966455193943, "name": "Logger.makeRecord (/usr/local/lib/python3.11/logging/__init__.py:1595)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872146.963, "ph": "X", "dur": 3.062833835546081, "name": "Filterer.filter (/usr/local/lib/python3.11/logging/__init__.py:815)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872178.11, "ph": "X", "dur": 1.5104234679622992, "name": "Filterer.filter (/usr/local/lib/python3.11/logging/__init__.py:815)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872182.901, "ph": "X", "dur": 4.098694845944512, "name": "Handler.acquire (/usr/local/lib/python3.11/logging/__init__.py:922)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872214.241, "ph": "X", "dur": 4.559998282575281, "name": "LogRecord.getMessage (/usr/local/lib/python3.11/logging/__init__.py:368)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872228.455, "ph": "X", "dur": 3.566676631003878, "name": "PercentStyle.usesTime (/usr/local/lib/python3.11/logging/__init__.py:432)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872222.371, "ph": "X", "dur": 9.903936177820093, "name": "Formatter.usesTime (/usr/local/lib/python3.11/logging/__init__.py:652)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872242.342, "ph": "X", "dur": 7.651559996809745, "name": "PercentStyle._format (/usr/local/lib/python3.11/logging/__init__.py:440)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872239.383, "ph": "X", "dur": 10.907754554296865, "name": "PercentStyle.format (/usr/local/lib/python3.11/logging/__init__.py:447)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872234.529, "ph": "X", "dur": 16.015239909568066, "name": "Formatter.formatMessage (/usr/local/lib/python3.11/logging/__init__.py:658)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872211.115, "ph": "X", "dur": 40.891373016885694, "name": "Formatter.format (/usr/local/lib/python3.11/logging/__init__.py:674)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872196.838, "ph": "X", "dur": 55.43320422526303, "name": "Handler.format (/usr/local/lib/python3.11/logging/__init__.py:942)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872332.731, "ph": "X", "dur": 2.4827516697229597, "name": "Handler.acquire (/usr/local/lib/python3.11/logging/__init__.py:922)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872344.13, "ph": "X", "dur": 1.1806053222514385, "name": "Handler.release (/usr/local/lib/python3.11/logging/__init__.py:929)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872331.716, "ph": "X", "dur": 13.884957212983343, "name": "StreamHandler.flush (/usr/local/lib/python3.11/logging/__init__.py:1087)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872193.899, "ph": "X", "dur": 152.05389960158433, "name": "StreamHandler.emit (/usr/local/lib/python3.11/logging/__init__.py:1098)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872346.831, "ph": "X", "dur": 0.737533039403683, "name": "Handler.release (/usr/local/lib/python3.11/logging/__init__.py:929)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872176.749, "ph": "X", "dur": 171.18445697122533, "name": "Handler.handle (/usr/local/lib/python3.11/logging/__init__.py:965)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872161.282, "ph": "X", "dur": 190.5580963913065, "name": "Logger.callHandlers (/usr/local/lib/python3.11/logging/__init__.py:1690)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872140.43, "ph": "X", "dur": 211.89020369504763, "name": "Logger.handle (/usr/local/lib/python3.11/logging/__init__.py:1636)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871819.278, "ph": "X", "dur": 533.3253334209556, "name": "Logger._log (/usr/local/lib/python3.11/logging/__init__.py:1610)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871793.217, "ph": "X", "dur": 563.3012174546668, "name": "Logger.info (/usr/local/lib/python3.11/logging/__init__.py:1479)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871058.8, "ph": "X", "dur": 1298.6537003740605, "name": "MarketDataService.get_stock_price (/app/app/services/market_data.py:134)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872381.766, "ph": "B", "name": "maybe_trace (/app/app/observability/trace.py:6)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291872376.482, "ph": "B", "name": "_GeneratorContextManager.__exit__ (/usr/local/lib/python3.11/contextlib.py:141)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871057.438, "ph": "B", "name": "get_stock_price (/app/app/main.py:229)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871055.668, "ph": "B", "name": "run_endpoint_function (/usr/local/lib/python3.11/site-packages/fastapi/routing.py:280)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871054.659, "ph": "B", "name": "get_request_handler.<locals>.app (/usr/local/lib/python3.11/site-packages/fastapi/routing.py:316)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871053.436, "ph": "B", "name": "request_response.<locals>.app.<locals>.app (/usr/local/lib/python3.11/site-packages/fastapi/routing.py:103)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871052.41, "ph": "B", "name": "wrap_app_handling_exceptions.<locals>.wrapped_app (/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py:31)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871051.569, "ph": "B", "name": "request_response.<locals>.app (/usr/local/lib/python3.11/site-packages/fastapi/routing.py:100)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871049.9, "ph": "B", "name": "Route.handle (/usr/local/lib/python3.11/site-packages/starlette/routing.py:281)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871048.98, "ph": "B", "name": "Router.app (/usr/local/lib/python3.11/site-packages/starlette/routing.py:718)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871047.626, "ph": "B", "name": "Router.__call__ (/usr/local/lib/python3.11/site-packages/starlette/routing.py:712)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871044.554, "ph": "B", "name": "AsyncExitStackMiddleware.__call__ (/usr/local/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:15)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871043.428, "ph": "B", "name": "wrap_app_handling_exceptions.<locals>.wrapped_app (/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py:31)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871040.61, "ph": "B", "name": "ExceptionMiddleware.__call__ (/usr/local/lib/python3.11/site-packages/starlette/middleware/exceptions.py:47)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871037.468, "ph": "B", "name": "CORSMiddleware.__call__ (/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py:75)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871032.534, "ph": "B", "name": "ServerErrorMiddleware.__call__ (/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py:149)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871030.042, "ph": "B", "name": "Starlette.__call__ (/usr/local/lib/python3.11/site-packages/starlette/applications.py:109)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871027.734, "ph": "B", "name": "FastAPI.__call__ (/usr/local/lib/python3.11/site-packages/fastapi/applications.py:1130)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871021.092, "ph": "B", "name": "ProxyHeadersMiddleware.__call__ (/usr/local/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py:27)", "cat": "FEE"}, {"pid": 1, "tid": 1, "ts": 38291871018.887, "ph": "B", "name": "RequestResponseCycle.run_asgi (/usr/local/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py:407)", "cat": "FEE"}], "viztracer_metadata": {"version": "1.0.4", "overflow": false, "baseTimeNanoseconds": 1759980240576414830}, "file_info": {"files": {"/app/app/services/cache.py": ["from datetime import datetime, timedelta\nfrom typing import Dict, Optional\n\nclass DataCache:\n    \"\"\"Simple in-memory cache for stock data\"\"\"\n    def __init__(self, ttl_seconds: int = 5):\n        self.cache: Dict[str, Dict] = {}\n        self.ttl = ttl_seconds\n    \n    def get(self, key: str) -> Optional[Dict]:\n        if key in self.cache:\n            data, timestamp = self.cache[key]\n            if datetime.now() - timestamp < timedelta(seconds=self.ttl):\n                return data\n            else:\n                del self.cache[key]\n        return None\n    \n    def set(self, key: str, data: Dict):\n        self.cache[key] = (data, datetime.now())\n    \n    def clear_expired(self):\n        now = datetime.now()\n        expired_keys = [\n            key for key, (_, timestamp) in self.cache.items()\n            if now - timestamp >= timedelta(seconds=self.ttl)\n        ]\n        for key in expired_keys:\n            del self.cache[key]\n    \n    def stats(self) -> Dict:\n        return {\n            \"size\": len(self.cache),\n            \"ttl_seconds\": self.ttl\n        }", 34], "/app/app/core/config.py": ["from __future__ import annotations\n\nimport time\nfrom typing import Dict, List, Literal, Optional, Tuple\n\nfrom pydantic import BaseModel, Field, ValidationError, field_validator\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\n_ALLOWED_PROVIDERS = {\"finnhub\", \"alphavantage\", \"yfinance\"}\n\n\nclass ProviderCfg(BaseModel):\n    enabled: bool = True\n    api_key: Optional[str] = None\n    base_url: Optional[str] = None\n    timeout_s: int = 8\n    retries: int = 3\n\n\nclass Settings(BaseSettings):\n    \"\"\"Central configuration for the market data service.\"\"\"\n\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        case_sensitive=False,\n        extra=\"ignore\",\n        env_nested_delimiter=\"__\",\n    )\n\n    # === Legacy fields kept for backwards compatibility ===\n    service_name: str = \"market-data-service\"\n    service_port: int = 8001\n    debug: bool = True\n\n    database_url: str = \"postgresql://trading_user:trading_pass@localhost:5432/trading_db\"\n    database_pool_size: int = 10\n    database_max_overflow: int = 20\n\n    cache_ttl_seconds: int = 5\n    store_historical_data: bool = True\n    historical_data_retention_days: int = 365\n    alpha_vantage_api_key: Optional[str] = None\n    finnhub_api_key: Optional[str] = None\n    polygon_api_key: Optional[str] = None\n    max_requests_per_minute: int = 300\n    websocket_heartbeat_interval: int = 30\n    websocket_update_interval: int = 5\n    macro_refresh_interval_seconds: int = 900\n    macro_cache_ttl_seconds: int = 300\n\n    # === New policy/version controls ===\n    env: str = Field(default=\"dev\")\n    policy_version: str = Field(default_factory=lambda: f\"{int(time.time())}\")\n\n    # Feature flags\n    USE_RLC: bool = False\n    ALLOW_SOURCE_OVERRIDE: bool = True\n    VIZTRACER_ENABLED: bool = False\n    LOCAL_SWEEP_ENABLED: bool = True\n\n    # Providers\n    finnhub: ProviderCfg = Field(default_factory=ProviderCfg)\n    alphavantage: ProviderCfg = Field(default_factory=ProviderCfg)\n    yfinance_enabled: bool = True\n\n    # Routing policy\n    POLICY_BARS_1M: List[str] = Field(default_factory=lambda: [\"finnhub\", \"alphavantage\", \"yfinance\"])\n    POLICY_EOD: List[str] = Field(default_factory=lambda: [\"yfinance\", \"alphavantage\", \"finnhub\"])\n    POLICY_QUOTES_L1: List[str] = Field(default_factory=lambda: [\"finnhub\", \"yfinance\"])\n    POLICY_OPTIONS_CHAIN: List[str] = Field(default_factory=lambda: [\"yfinance\", \"finnhub\"])\n\n    # Circuit breaker / health score tuning\n    BREAKER_DEMOTE_THRESHOLD: float = 0.55\n    BREAKER_PROMOTE_THRESHOLD: float = 0.70\n    BREAKER_PROMOTE_MIN_SAMPLES: int = 3\n    BREAKER_COOLDOWN_SEC: int = 60\n    RECENT_LATENCY_CAP_MS: int = 1000\n\n    # Tier cadence and sizing\n    LOCAL_SWEEP_BATCH: int = 200\n    LOCAL_SWEEP_TICK_SEC: int = 30\n    CADENCE_T0: Dict[str, int] = Field(default_factory=lambda: {\"bars_1m\": 60, \"quotes_l1\": 5})\n    CADENCE_T1: Dict[str, int] = Field(default_factory=lambda: {\"bars_1m\": 300, \"quotes_l1\": 15})\n    CADENCE_T2: Dict[str, int] = Field(default_factory=lambda: {\"eod\": 1})\n    TIER_MAXS: Dict[str, int] = Field(default_factory=lambda: {\"T0\": 1200, \"T1\": 3000})\n\n    # RLC / job ingestion\n    RLC_BROKER: Literal[\"redis\", \"kafka\"] = \"redis\"\n    RLC_REDIS_URL: str = \"redis://redis:6379/0\"\n    RLC_REDIS_JOBS_KEY: str = \"market:jobs\"\n    RLC_REDIS_BACKFILL_KEYS: Dict[str, str] = Field(\n        default_factory=lambda: {\"T0\": \"market:backfills:T0\", \"T1\": \"market:backfills:T1\", \"T2\": \"market:backfills:T2\"}\n    )\n\n    # Backfill controls\n    BACKFILL_CHUNK_MINUTES: int = 1440\n    BACKFILL_MAX_CONCURRENCY_T0: int = 4\n    BACKFILL_MAX_CONCURRENCY_T1: int = 2\n    BACKFILL_MAX_CONCURRENCY_T2: int = 1\n    BACKFILL_MAX_QUEUE_T0: int = 20000\n    BACKFILL_MAX_QUEUE_T1: int = 20000\n    BACKFILL_MAX_QUEUE_T2: int = 40000\n    BACKFILL_DISPATCH_RATE_PER_SEC: float = 2.0\n\n    # Storage / PIT\n    LIVE_BATCH_SIZE: int = 500\n    BACKFILL_BATCH_SIZE: int = 5000\n\n    # Websocket replay behaviour\n    WS_REPLAY_MAX_BARS: int = 120\n    WS_REPLAY_TTL_SEC: int = 4 * 3600\n\n    @field_validator(\"POLICY_BARS_1M\", \"POLICY_EOD\", \"POLICY_QUOTES_L1\", \"POLICY_OPTIONS_CHAIN\", mode=\"before\")\n    @classmethod\n    def _validate_policy(cls, value: List[str]) -> List[str]:\n        if isinstance(value, str):\n            # Allow comma separated or JSON-style strings\n            split = [v.strip() for v in value.strip(\"[]\").split(\",\") if v.strip()]\n            value = split\n        if not isinstance(value, list):\n            raise ValueError(\"policy must be provided as a list\")\n        invalid = [p for p in value if p not in _ALLOWED_PROVIDERS]\n        if invalid:\n            raise ValueError(f\"unknown provider(s) in policy: {invalid}\")\n        return value\n\n\n_settings: Optional[Settings] = None\n\n\ndef get_settings() -> Settings:\n    global _settings\n    if _settings is None:\n        _settings = Settings()\n    return _settings\n\n\ndef validate_settings(settings: Settings) -> Tuple[bool, str]:\n    try:\n        if not (0.0 <= settings.BREAKER_DEMOTE_THRESHOLD < settings.BREAKER_PROMOTE_THRESHOLD <= 1.0):\n            return False, \"invalid breaker thresholds\"\n        if not all(k in settings.RLC_REDIS_BACKFILL_KEYS for k in (\"T0\", \"T1\", \"T2\")):\n            return False, \"missing backfill keys for tiers\"\n        for cadence in (settings.CADENCE_T0, settings.CADENCE_T1):\n            if any(v <= 0 for v in cadence.values()):\n                return False, \"cadence values must be positive\"\n        return True, \"ok\"\n    except Exception as exc:  # pragma: no cover - defensive\n        return False, str(exc)\n\n\ndef hot_reload() -> Dict[str, str | bool]:\n    \"\"\"Reload configuration from the environment in a safe manner.\"\"\"\n    global _settings\n    try:\n        candidate = Settings()\n    except ValidationError as exc:  # pragma: no cover - config errors\n        return {\"ok\": False, \"error\": f\"pydantic validation failed: {exc}\"}\n\n    ok, reason = validate_settings(candidate)\n    if not ok:\n        return {\"ok\": False, \"error\": reason}\n\n    _settings = candidate\n    _settings.policy_version = f\"{int(time.time())}\"\n    return {\"ok\": True, \"policy_version\": _settings.policy_version}\n\n\n# Backwards compatible global export\nsettings = get_settings()\n", 170], "/usr/local/lib/python3.11/asyncio/locks.py": ["\"\"\"Synchronization primitives.\"\"\"\n\n__all__ = ('Lock', 'Event', 'Condition', 'Semaphore',\n           'BoundedSemaphore', 'Barrier')\n\nimport collections\nimport enum\n\nfrom . import exceptions\nfrom . import mixins\nfrom . import tasks\n\nclass _ContextManagerMixin:\n    async def __aenter__(self):\n        await self.acquire()\n        # We have no use for the \"as ...\"  clause in the with\n        # statement for locks.\n        return None\n\n    async def __aexit__(self, exc_type, exc, tb):\n        self.release()\n\n\nclass Lock(_ContextManagerMixin, mixins._LoopBoundMixin):\n    \"\"\"Primitive lock objects.\n\n    A primitive lock is a synchronization primitive that is not owned\n    by a particular coroutine when locked.  A primitive lock is in one\n    of two states, 'locked' or 'unlocked'.\n\n    It is created in the unlocked state.  It has two basic methods,\n    acquire() and release().  When the state is unlocked, acquire()\n    changes the state to locked and returns immediately.  When the\n    state is locked, acquire() blocks until a call to release() in\n    another coroutine changes it to unlocked, then the acquire() call\n    resets it to locked and returns.  The release() method should only\n    be called in the locked state; it changes the state to unlocked\n    and returns immediately.  If an attempt is made to release an\n    unlocked lock, a RuntimeError will be raised.\n\n    When more than one coroutine is blocked in acquire() waiting for\n    the state to turn to unlocked, only one coroutine proceeds when a\n    release() call resets the state to unlocked; first coroutine which\n    is blocked in acquire() is being processed.\n\n    acquire() is a coroutine and should be called with 'await'.\n\n    Locks also support the asynchronous context management protocol.\n    'async with lock' statement should be used.\n\n    Usage:\n\n        lock = Lock()\n        ...\n        await lock.acquire()\n        try:\n            ...\n        finally:\n            lock.release()\n\n    Context manager usage:\n\n        lock = Lock()\n        ...\n        async with lock:\n             ...\n\n    Lock objects can be tested for locking state:\n\n        if not lock.locked():\n           await lock.acquire()\n        else:\n           # lock is acquired\n           ...\n\n    \"\"\"\n\n    def __init__(self):\n        self._waiters = None\n        self._locked = False\n\n    def __repr__(self):\n        res = super().__repr__()\n        extra = 'locked' if self._locked else 'unlocked'\n        if self._waiters:\n            extra = f'{extra}, waiters:{len(self._waiters)}'\n        return f'<{res[1:-1]} [{extra}]>'\n\n    def locked(self):\n        \"\"\"Return True if lock is acquired.\"\"\"\n        return self._locked\n\n    async def acquire(self):\n        \"\"\"Acquire a lock.\n\n        This method blocks until the lock is unlocked, then sets it to\n        locked and returns True.\n        \"\"\"\n        if (not self._locked and (self._waiters is None or\n                all(w.cancelled() for w in self._waiters))):\n            self._locked = True\n            return True\n\n        if self._waiters is None:\n            self._waiters = collections.deque()\n        fut = self._get_loop().create_future()\n        self._waiters.append(fut)\n\n        # Finally block should be called before the CancelledError\n        # handling as we don't want CancelledError to call\n        # _wake_up_first() and attempt to wake up itself.\n        try:\n            try:\n                await fut\n            finally:\n                self._waiters.remove(fut)\n        except exceptions.CancelledError:\n            if not self._locked:\n                self._wake_up_first()\n            raise\n\n        self._locked = True\n        return True\n\n    def release(self):\n        \"\"\"Release a lock.\n\n        When the lock is locked, reset it to unlocked, and return.\n        If any other coroutines are blocked waiting for the lock to become\n        unlocked, allow exactly one of them to proceed.\n\n        When invoked on an unlocked lock, a RuntimeError is raised.\n\n        There is no return value.\n        \"\"\"\n        if self._locked:\n            self._locked = False\n            self._wake_up_first()\n        else:\n            raise RuntimeError('Lock is not acquired.')\n\n    def _wake_up_first(self):\n        \"\"\"Wake up the first waiter if it isn't done.\"\"\"\n        if not self._waiters:\n            return\n        try:\n            fut = next(iter(self._waiters))\n        except StopIteration:\n            return\n\n        # .done() necessarily means that a waiter will wake up later on and\n        # either take the lock, or, if it was cancelled and lock wasn't\n        # taken already, will hit this again and wake up a new waiter.\n        if not fut.done():\n            fut.set_result(True)\n\n\nclass Event(mixins._LoopBoundMixin):\n    \"\"\"Asynchronous equivalent to threading.Event.\n\n    Class implementing event objects. An event manages a flag that can be set\n    to true with the set() method and reset to false with the clear() method.\n    The wait() method blocks until the flag is true. The flag is initially\n    false.\n    \"\"\"\n\n    def __init__(self):\n        self._waiters = collections.deque()\n        self._value = False\n\n    def __repr__(self):\n        res = super().__repr__()\n        extra = 'set' if self._value else 'unset'\n        if self._waiters:\n            extra = f'{extra}, waiters:{len(self._waiters)}'\n        return f'<{res[1:-1]} [{extra}]>'\n\n    def is_set(self):\n        \"\"\"Return True if and only if the internal flag is true.\"\"\"\n        return self._value\n\n    def set(self):\n        \"\"\"Set the internal flag to true. All coroutines waiting for it to\n        become true are awakened. Coroutine that call wait() once the flag is\n        true will not block at all.\n        \"\"\"\n        if not self._value:\n            self._value = True\n\n            for fut in self._waiters:\n                if not fut.done():\n                    fut.set_result(True)\n\n    def clear(self):\n        \"\"\"Reset the internal flag to false. Subsequently, coroutines calling\n        wait() will block until set() is called to set the internal flag\n        to true again.\"\"\"\n        self._value = False\n\n    async def wait(self):\n        \"\"\"Block until the internal flag is true.\n\n        If the internal flag is true on entry, return True\n        immediately.  Otherwise, block until another coroutine calls\n        set() to set the flag to true, then return True.\n        \"\"\"\n        if self._value:\n            return True\n\n        fut = self._get_loop().create_future()\n        self._waiters.append(fut)\n        try:\n            await fut\n            return True\n        finally:\n            self._waiters.remove(fut)\n\n\nclass Condition(_ContextManagerMixin, mixins._LoopBoundMixin):\n    \"\"\"Asynchronous equivalent to threading.Condition.\n\n    This class implements condition variable objects. A condition variable\n    allows one or more coroutines to wait until they are notified by another\n    coroutine.\n\n    A new Lock object is created and used as the underlying lock.\n    \"\"\"\n\n    def __init__(self, lock=None):\n        if lock is None:\n            lock = Lock()\n\n        self._lock = lock\n        # Export the lock's locked(), acquire() and release() methods.\n        self.locked = lock.locked\n        self.acquire = lock.acquire\n        self.release = lock.release\n\n        self._waiters = collections.deque()\n\n    def __repr__(self):\n        res = super().__repr__()\n        extra = 'locked' if self.locked() else 'unlocked'\n        if self._waiters:\n            extra = f'{extra}, waiters:{len(self._waiters)}'\n        return f'<{res[1:-1]} [{extra}]>'\n\n    async def wait(self):\n        \"\"\"Wait until notified.\n\n        If the calling coroutine has not acquired the lock when this\n        method is called, a RuntimeError is raised.\n\n        This method releases the underlying lock, and then blocks\n        until it is awakened by a notify() or notify_all() call for\n        the same condition variable in another coroutine.  Once\n        awakened, it re-acquires the lock and returns True.\n        \"\"\"\n        if not self.locked():\n            raise RuntimeError('cannot wait on un-acquired lock')\n\n        self.release()\n        try:\n            fut = self._get_loop().create_future()\n            self._waiters.append(fut)\n            try:\n                await fut\n                return True\n            finally:\n                self._waiters.remove(fut)\n\n        finally:\n            # Must reacquire lock even if wait is cancelled\n            cancelled = False\n            while True:\n                try:\n                    await self.acquire()\n                    break\n                except exceptions.CancelledError:\n                    cancelled = True\n\n            if cancelled:\n                raise exceptions.CancelledError\n\n    async def wait_for(self, predicate):\n        \"\"\"Wait until a predicate becomes true.\n\n        The predicate should be a callable which result will be\n        interpreted as a boolean value.  The final predicate value is\n        the return value.\n        \"\"\"\n        result = predicate()\n        while not result:\n            await self.wait()\n            result = predicate()\n        return result\n\n    def notify(self, n=1):\n        \"\"\"By default, wake up one coroutine waiting on this condition, if any.\n        If the calling coroutine has not acquired the lock when this method\n        is called, a RuntimeError is raised.\n\n        This method wakes up at most n of the coroutines waiting for the\n        condition variable; it is a no-op if no coroutines are waiting.\n\n        Note: an awakened coroutine does not actually return from its\n        wait() call until it can reacquire the lock. Since notify() does\n        not release the lock, its caller should.\n        \"\"\"\n        if not self.locked():\n            raise RuntimeError('cannot notify on un-acquired lock')\n\n        idx = 0\n        for fut in self._waiters:\n            if idx >= n:\n                break\n\n            if not fut.done():\n                idx += 1\n                fut.set_result(False)\n\n    def notify_all(self):\n        \"\"\"Wake up all threads waiting on this condition. This method acts\n        like notify(), but wakes up all waiting threads instead of one. If the\n        calling thread has not acquired the lock when this method is called,\n        a RuntimeError is raised.\n        \"\"\"\n        self.notify(len(self._waiters))\n\n\nclass Semaphore(_ContextManagerMixin, mixins._LoopBoundMixin):\n    \"\"\"A Semaphore implementation.\n\n    A semaphore manages an internal counter which is decremented by each\n    acquire() call and incremented by each release() call. The counter\n    can never go below zero; when acquire() finds that it is zero, it blocks,\n    waiting until some other thread calls release().\n\n    Semaphores also support the context management protocol.\n\n    The optional argument gives the initial value for the internal\n    counter; it defaults to 1. If the value given is less than 0,\n    ValueError is raised.\n    \"\"\"\n\n    def __init__(self, value=1):\n        if value < 0:\n            raise ValueError(\"Semaphore initial value must be >= 0\")\n        self._waiters = None\n        self._value = value\n\n    def __repr__(self):\n        res = super().__repr__()\n        extra = 'locked' if self.locked() else f'unlocked, value:{self._value}'\n        if self._waiters:\n            extra = f'{extra}, waiters:{len(self._waiters)}'\n        return f'<{res[1:-1]} [{extra}]>'\n\n    def locked(self):\n        \"\"\"Returns True if semaphore cannot be acquired immediately.\"\"\"\n        return self._value == 0 or (\n            any(not w.cancelled() for w in (self._waiters or ())))\n\n    async def acquire(self):\n        \"\"\"Acquire a semaphore.\n\n        If the internal counter is larger than zero on entry,\n        decrement it by one and return True immediately.  If it is\n        zero on entry, block, waiting until some other coroutine has\n        called release() to make it larger than 0, and then return\n        True.\n        \"\"\"\n        if not self.locked():\n            self._value -= 1\n            return True\n\n        if self._waiters is None:\n            self._waiters = collections.deque()\n        fut = self._get_loop().create_future()\n        self._waiters.append(fut)\n\n        # Finally block should be called before the CancelledError\n        # handling as we don't want CancelledError to call\n        # _wake_up_first() and attempt to wake up itself.\n        try:\n            try:\n                await fut\n            finally:\n                self._waiters.remove(fut)\n        except exceptions.CancelledError:\n            if not fut.cancelled():\n                self._value += 1\n                self._wake_up_next()\n            raise\n\n        if self._value > 0:\n            self._wake_up_next()\n        return True\n\n    def release(self):\n        \"\"\"Release a semaphore, incrementing the internal counter by one.\n\n        When it was zero on entry and another coroutine is waiting for it to\n        become larger than zero again, wake up that coroutine.\n        \"\"\"\n        self._value += 1\n        self._wake_up_next()\n\n    def _wake_up_next(self):\n        \"\"\"Wake up the first waiter that isn't done.\"\"\"\n        if not self._waiters:\n            return\n\n        for fut in self._waiters:\n            if not fut.done():\n                self._value -= 1\n                fut.set_result(True)\n                return\n\n\nclass BoundedSemaphore(Semaphore):\n    \"\"\"A bounded semaphore implementation.\n\n    This raises ValueError in release() if it would increase the value\n    above the initial value.\n    \"\"\"\n\n    def __init__(self, value=1):\n        self._bound_value = value\n        super().__init__(value)\n\n    def release(self):\n        if self._value >= self._bound_value:\n            raise ValueError('BoundedSemaphore released too many times')\n        super().release()\n\n\n\nclass _BarrierState(enum.Enum):\n    FILLING = 'filling'\n    DRAINING = 'draining'\n    RESETTING = 'resetting'\n    BROKEN = 'broken'\n\n\nclass Barrier(mixins._LoopBoundMixin):\n    \"\"\"Asyncio equivalent to threading.Barrier\n\n    Implements a Barrier primitive.\n    Useful for synchronizing a fixed number of tasks at known synchronization\n    points. Tasks block on 'wait()' and are simultaneously awoken once they\n    have all made their call.\n    \"\"\"\n\n    def __init__(self, parties):\n        \"\"\"Create a barrier, initialised to 'parties' tasks.\"\"\"\n        if parties < 1:\n            raise ValueError('parties must be > 0')\n\n        self._cond = Condition() # notify all tasks when state changes\n\n        self._parties = parties\n        self._state = _BarrierState.FILLING\n        self._count = 0       # count tasks in Barrier\n\n    def __repr__(self):\n        res = super().__repr__()\n        extra = f'{self._state.value}'\n        if not self.broken:\n            extra += f', waiters:{self.n_waiting}/{self.parties}'\n        return f'<{res[1:-1]} [{extra}]>'\n\n    async def __aenter__(self):\n        # wait for the barrier reaches the parties number\n        # when start draining release and return index of waited task\n        return await self.wait()\n\n    async def __aexit__(self, *args):\n        pass\n\n    async def wait(self):\n        \"\"\"Wait for the barrier.\n\n        When the specified number of tasks have started waiting, they are all\n        simultaneously awoken.\n        Returns an unique and individual index number from 0 to 'parties-1'.\n        \"\"\"\n        async with self._cond:\n            await self._block() # Block while the barrier drains or resets.\n            try:\n                index = self._count\n                self._count += 1\n                if index + 1 == self._parties:\n                    # We release the barrier\n                    await self._release()\n                else:\n                    await self._wait()\n                return index\n            finally:\n                self._count -= 1\n                # Wake up any tasks waiting for barrier to drain.\n                self._exit()\n\n    async def _block(self):\n        # Block until the barrier is ready for us,\n        # or raise an exception if it is broken.\n        #\n        # It is draining or resetting, wait until done\n        # unless a CancelledError occurs\n        await self._cond.wait_for(\n            lambda: self._state not in (\n                _BarrierState.DRAINING, _BarrierState.RESETTING\n            )\n        )\n\n        # see if the barrier is in a broken state\n        if self._state is _BarrierState.BROKEN:\n            raise exceptions.BrokenBarrierError(\"Barrier aborted\")\n\n    async def _release(self):\n        # Release the tasks waiting in the barrier.\n\n        # Enter draining state.\n        # Next waiting tasks will be blocked until the end of draining.\n        self._state = _BarrierState.DRAINING\n        self._cond.notify_all()\n\n    async def _wait(self):\n        # Wait in the barrier until we are released. Raise an exception\n        # if the barrier is reset or broken.\n\n        # wait for end of filling\n        # unless a CancelledError occurs\n        await self._cond.wait_for(lambda: self._state is not _BarrierState.FILLING)\n\n        if self._state in (_BarrierState.BROKEN, _BarrierState.RESETTING):\n            raise exceptions.BrokenBarrierError(\"Abort or reset of barrier\")\n\n    def _exit(self):\n        # If we are the last tasks to exit the barrier, signal any tasks\n        # waiting for the barrier to drain.\n        if self._count == 0:\n            if self._state in (_BarrierState.RESETTING, _BarrierState.DRAINING):\n                self._state = _BarrierState.FILLING\n            self._cond.notify_all()\n\n    async def reset(self):\n        \"\"\"Reset the barrier to the initial state.\n\n        Any tasks currently waiting will get the BrokenBarrier exception\n        raised.\n        \"\"\"\n        async with self._cond:\n            if self._count > 0:\n                if self._state is not _BarrierState.RESETTING:\n                    #reset the barrier, waking up tasks\n                    self._state = _BarrierState.RESETTING\n            else:\n                self._state = _BarrierState.FILLING\n            self._cond.notify_all()\n\n    async def abort(self):\n        \"\"\"Place the barrier into a 'broken' state.\n\n        Useful in case of error.  Any currently waiting tasks and tasks\n        attempting to 'wait()' will have BrokenBarrierError raised.\n        \"\"\"\n        async with self._cond:\n            self._state = _BarrierState.BROKEN\n            self._cond.notify_all()\n\n    @property\n    def parties(self):\n        \"\"\"Return the number of tasks required to trip the barrier.\"\"\"\n        return self._parties\n\n    @property\n    def n_waiting(self):\n        \"\"\"Return the number of tasks currently waiting at the barrier.\"\"\"\n        if self._state is _BarrierState.FILLING:\n            return self._count\n        return 0\n\n    @property\n    def broken(self):\n        \"\"\"Return True if the barrier is in a broken state.\"\"\"\n        return self._state is _BarrierState.BROKEN\n", 587], "/app/app/utils/circuit_breaker.py": ["\"\"\"\nCircuit breaker implementation for provider resilience\n\"\"\"\n\nimport time\nimport asyncio\nimport logging\nfrom typing import Callable, Any, Optional\nfrom enum import Enum\nfrom dataclasses import dataclass, field\n\nlogger = logging.getLogger(__name__)\n\nclass CircuitState(Enum):\n    CLOSED = \"closed\"     # Normal operation\n    OPEN = \"open\"         # Circuit is open, calls are blocked\n    HALF_OPEN = \"half_open\"  # Testing if service is back\n\n@dataclass\nclass CircuitBreakerConfig:\n    failure_threshold: int = 5  # Number of failures before opening circuit\n    recovery_timeout: float = 60.0  # Seconds to wait before trying again\n    expected_exception: type = Exception\n    timeout: float = 30.0  # Request timeout in seconds\n\n@dataclass\nclass CircuitBreakerStats:\n    state: CircuitState = CircuitState.CLOSED\n    failure_count: int = 0\n    last_failure_time: float = 0.0\n    total_requests: int = 0\n    successful_requests: int = 0\n    failed_requests: int = 0\n    timeouts: int = 0\n    circuit_opens: int = 0\n\nclass CircuitBreaker:\n    \"\"\"\n    Circuit breaker pattern implementation for provider resilience\n    \"\"\"\n    \n    def __init__(self, name: str, config: CircuitBreakerConfig):\n        self.name = name\n        self.config = config\n        self.stats = CircuitBreakerStats()\n        self._lock = asyncio.Lock()\n    \n    async def call(self, func: Callable, *args, **kwargs) -> Any:\n        \"\"\"Execute function with circuit breaker protection\"\"\"\n        async with self._lock:\n            self.stats.total_requests += 1\n            \n            # Check if circuit is open\n            if self.stats.state == CircuitState.OPEN:\n                if time.time() - self.stats.last_failure_time < self.config.recovery_timeout:\n                    raise CircuitOpenException(f\"Circuit breaker {self.name} is OPEN\")\n                else:\n                    # Try to recover\n                    self.stats.state = CircuitState.HALF_OPEN\n                    logger.info(f\"Circuit breaker {self.name} entering HALF_OPEN state\")\n        \n        try:\n            # Execute with timeout\n            result = await asyncio.wait_for(\n                func(*args, **kwargs),\n                timeout=self.config.timeout\n            )\n            \n            # Success - reset circuit if it was half-open\n            async with self._lock:\n                if self.stats.state == CircuitState.HALF_OPEN:\n                    self.stats.state = CircuitState.CLOSED\n                    self.stats.failure_count = 0\n                    logger.info(f\"Circuit breaker {self.name} recovered to CLOSED state\")\n                \n                self.stats.successful_requests += 1\n            \n            return result\n            \n        except asyncio.TimeoutError:\n            async with self._lock:\n                self.stats.timeouts += 1\n                await self._record_failure()\n            raise CircuitTimeoutException(f\"Request to {self.name} timed out after {self.config.timeout}s\")\n            \n        except self.config.expected_exception as e:\n            async with self._lock:\n                await self._record_failure()\n            raise e\n    \n    async def _record_failure(self):\n        \"\"\"Record a failure and potentially open the circuit\"\"\"\n        self.stats.failed_requests += 1\n        self.stats.failure_count += 1\n        self.stats.last_failure_time = time.time()\n        \n        if self.stats.failure_count >= self.config.failure_threshold:\n            if self.stats.state != CircuitState.OPEN:\n                self.stats.state = CircuitState.OPEN\n                self.stats.circuit_opens += 1\n                logger.warning(\n                    f\"Circuit breaker {self.name} opened after {self.stats.failure_count} failures\"\n                )\n    \n    def get_stats(self) -> dict:\n        \"\"\"Get current circuit breaker statistics\"\"\"\n        return {\n            \"name\": self.name,\n            \"state\": self.stats.state.value,\n            \"failure_count\": self.stats.failure_count,\n            \"total_requests\": self.stats.total_requests,\n            \"successful_requests\": self.stats.successful_requests,\n            \"failed_requests\": self.stats.failed_requests,\n            \"timeouts\": self.stats.timeouts,\n            \"circuit_opens\": self.stats.circuit_opens,\n            \"success_rate\": (\n                self.stats.successful_requests / max(self.stats.total_requests, 1) * 100\n            ),\n            \"last_failure_time\": self.stats.last_failure_time,\n            \"is_healthy\": self.stats.state == CircuitState.CLOSED\n        }\n    \n    def reset(self):\n        \"\"\"Reset circuit breaker to initial state\"\"\"\n        self.stats = CircuitBreakerStats()\n        logger.info(f\"Circuit breaker {self.name} reset\")\n\nclass CircuitOpenException(Exception):\n    \"\"\"Raised when circuit breaker is open\"\"\"\n    pass\n\nclass CircuitTimeoutException(Exception):\n    \"\"\"Raised when request times out\"\"\"\n    pass\n\nclass ProviderCircuitBreakers:\n    \"\"\"Manager for multiple provider circuit breakers\"\"\"\n    \n    def __init__(self):\n        self.breakers: dict[str, CircuitBreaker] = {}\n    \n    def get_breaker(self, provider_name: str) -> CircuitBreaker:\n        \"\"\"Get or create circuit breaker for provider\"\"\"\n        if provider_name not in self.breakers:\n            config = CircuitBreakerConfig(\n                failure_threshold=5,\n                recovery_timeout=60.0,\n                timeout=30.0\n            )\n            self.breakers[provider_name] = CircuitBreaker(provider_name, config)\n        \n        return self.breakers[provider_name]\n    \n    def get_all_stats(self) -> dict[str, dict]:\n        \"\"\"Get stats for all circuit breakers\"\"\"\n        return {\n            name: breaker.get_stats() \n            for name, breaker in self.breakers.items()\n        }\n    \n    def get_health_summary(self) -> dict:\n        \"\"\"Get overall health summary\"\"\"\n        total_breakers = len(self.breakers)\n        healthy_breakers = sum(\n            1 for breaker in self.breakers.values() \n            if breaker.get_stats()[\"is_healthy\"]\n        )\n        \n        return {\n            \"total_providers\": total_breakers,\n            \"healthy_providers\": healthy_breakers,\n            \"unhealthy_providers\": total_breakers - healthy_breakers,\n            \"overall_health\": \"healthy\" if healthy_breakers == total_breakers else \"degraded\"\n        }\n\n# Global instance\ncircuit_breakers = ProviderCircuitBreakers()", 176], "/app/app/circuit_breaker.py": ["from __future__ import annotations\n\nfrom typing import Dict, Optional\n\nfrom .utils.circuit_breaker import (\n    CircuitBreaker,\n    CircuitState,\n    ProviderCircuitBreakers,\n    circuit_breakers,\n)\n\n\nclass CircuitBreakerManager:\n    \"\"\"Wrapper used by the provider registry to query breaker state.\"\"\"\n\n    def __init__(self, manager: Optional[ProviderCircuitBreakers] = None):\n        self._manager = manager or circuit_breakers\n\n    def get_state(self, provider_name: str) -> str:\n        breaker = self._get_breaker(provider_name)\n        return breaker.stats.state.name.upper()\n\n    def get_stats(self) -> Dict[str, Dict]:\n        return self._manager.get_all_stats()\n\n    def _get_breaker(self, provider_name: str) -> CircuitBreaker:\n        return self._manager.get_breaker(provider_name)\n\n\n__all__ = [\"CircuitBreakerManager\", \"CircuitState\"]\n", 30], "/usr/local/lib/python3.11/enum.py": ["import sys\nimport builtins as bltns\nfrom types import MappingProxyType, DynamicClassAttribute\nfrom operator import or_ as _or_\nfrom functools import reduce\n\n\n__all__ = [\n        'EnumType', 'EnumMeta',\n        'Enum', 'IntEnum', 'StrEnum', 'Flag', 'IntFlag', 'ReprEnum',\n        'auto', 'unique', 'property', 'verify', 'member', 'nonmember',\n        'FlagBoundary', 'STRICT', 'CONFORM', 'EJECT', 'KEEP',\n        'global_flag_repr', 'global_enum_repr', 'global_str', 'global_enum',\n        'EnumCheck', 'CONTINUOUS', 'NAMED_FLAGS', 'UNIQUE',\n        'pickle_by_global_name', 'pickle_by_enum_name',\n        ]\n\n\n# Dummy value for Enum and Flag as there are explicit checks for them\n# before they have been created.\n# This is also why there are checks in EnumType like `if Enum is not None`\nEnum = Flag = EJECT = _stdlib_enums = ReprEnum = None\n\nclass nonmember(object):\n    \"\"\"\n    Protects item from becoming an Enum member during class creation.\n    \"\"\"\n    def __init__(self, value):\n        self.value = value\n\nclass member(object):\n    \"\"\"\n    Forces item to become an Enum member during class creation.\n    \"\"\"\n    def __init__(self, value):\n        self.value = value\n\ndef _is_descriptor(obj):\n    \"\"\"\n    Returns True if obj is a descriptor, False otherwise.\n    \"\"\"\n    return (\n            hasattr(obj, '__get__') or\n            hasattr(obj, '__set__') or\n            hasattr(obj, '__delete__')\n            )\n\ndef _is_dunder(name):\n    \"\"\"\n    Returns True if a __dunder__ name, False otherwise.\n    \"\"\"\n    return (\n            len(name) > 4 and\n            name[:2] == name[-2:] == '__' and\n            name[2] != '_' and\n            name[-3] != '_'\n            )\n\ndef _is_sunder(name):\n    \"\"\"\n    Returns True if a _sunder_ name, False otherwise.\n    \"\"\"\n    return (\n            len(name) > 2 and\n            name[0] == name[-1] == '_' and\n            name[1:2] != '_' and\n            name[-2:-1] != '_'\n            )\n\ndef _is_internal_class(cls_name, obj):\n    # do not use `re` as `re` imports `enum`\n    if not isinstance(obj, type):\n        return False\n    qualname = getattr(obj, '__qualname__', '')\n    s_pattern = cls_name + '.' + getattr(obj, '__name__', '')\n    e_pattern = '.' + s_pattern\n    return qualname == s_pattern or qualname.endswith(e_pattern)\n\ndef _is_private(cls_name, name):\n    # do not use `re` as `re` imports `enum`\n    pattern = '_%s__' % (cls_name, )\n    pat_len = len(pattern)\n    if (\n            len(name) > pat_len\n            and name.startswith(pattern)\n            and name[pat_len:pat_len+1] != ['_']\n            and (name[-1] != '_' or name[-2] != '_')\n        ):\n        return True\n    else:\n        return False\n\ndef _is_single_bit(num):\n    \"\"\"\n    True if only one bit set in num (should be an int)\n    \"\"\"\n    if num == 0:\n        return False\n    num &= num - 1\n    return num == 0\n\ndef _make_class_unpicklable(obj):\n    \"\"\"\n    Make the given obj un-picklable.\n\n    obj should be either a dictionary, or an Enum\n    \"\"\"\n    def _break_on_call_reduce(self, proto):\n        raise TypeError('%r cannot be pickled' % self)\n    if isinstance(obj, dict):\n        obj['__reduce_ex__'] = _break_on_call_reduce\n        obj['__module__'] = '<unknown>'\n    else:\n        setattr(obj, '__reduce_ex__', _break_on_call_reduce)\n        setattr(obj, '__module__', '<unknown>')\n\ndef _iter_bits_lsb(num):\n    # num must be a positive integer\n    original = num\n    if isinstance(num, Enum):\n        num = num.value\n    if num < 0:\n        raise ValueError('%r is not a positive integer' % original)\n    while num:\n        b = num & (~num + 1)\n        yield b\n        num ^= b\n\ndef show_flag_values(value):\n    return list(_iter_bits_lsb(value))\n\ndef bin(num, max_bits=None):\n    \"\"\"\n    Like built-in bin(), except negative values are represented in\n    twos-compliment, and the leading bit always indicates sign\n    (0=positive, 1=negative).\n\n    >>> bin(10)\n    '0b0 1010'\n    >>> bin(~10)   # ~10 is -11\n    '0b1 0101'\n    \"\"\"\n\n    ceiling = 2 ** (num).bit_length()\n    if num >= 0:\n        s = bltns.bin(num + ceiling).replace('1', '0', 1)\n    else:\n        s = bltns.bin(~num ^ (ceiling - 1) + ceiling)\n    sign = s[:3]\n    digits = s[3:]\n    if max_bits is not None:\n        if len(digits) < max_bits:\n            digits = (sign[-1] * max_bits + digits)[-max_bits:]\n    return \"%s %s\" % (sign, digits)\n\ndef _dedent(text):\n    \"\"\"\n    Like textwrap.dedent.  Rewritten because we cannot import textwrap.\n    \"\"\"\n    lines = text.split('\\n')\n    blanks = 0\n    for i, ch in enumerate(lines[0]):\n        if ch != ' ':\n            break\n    for j, l in enumerate(lines):\n        lines[j] = l[i:]\n    return '\\n'.join(lines)\n\nclass _auto_null:\n    def __repr__(self):\n        return '_auto_null'\n_auto_null = _auto_null()\n\nclass auto:\n    \"\"\"\n    Instances are replaced with an appropriate value in Enum class suites.\n    \"\"\"\n    def __init__(self, value=_auto_null):\n        self.value = value\n\n    def __repr__(self):\n        return \"auto(%r)\" % self.value\n\nclass property(DynamicClassAttribute):\n    \"\"\"\n    This is a descriptor, used to define attributes that act differently\n    when accessed through an enum member and through an enum class.\n    Instance access is the same as property(), but access to an attribute\n    through the enum class will instead look in the class' _member_map_ for\n    a corresponding enum member.\n    \"\"\"\n\n    def __get__(self, instance, ownerclass=None):\n        if instance is None:\n            try:\n                return ownerclass._member_map_[self.name]\n            except KeyError:\n                raise AttributeError(\n                        '%r has no attribute %r' % (ownerclass, self.name)\n                        )\n        else:\n            if self.fget is None:\n                # look for a member by this name.\n                try:\n                    return ownerclass._member_map_[self.name]\n                except KeyError:\n                    raise AttributeError(\n                            '%r has no attribute %r' % (ownerclass, self.name)\n                            ) from None\n            else:\n                return self.fget(instance)\n\n    def __set__(self, instance, value):\n        if self.fset is None:\n            raise AttributeError(\n                    \"<enum %r> cannot set attribute %r\" % (self.clsname, self.name)\n                    )\n        else:\n            return self.fset(instance, value)\n\n    def __delete__(self, instance):\n        if self.fdel is None:\n            raise AttributeError(\n                    \"<enum %r> cannot delete attribute %r\" % (self.clsname, self.name)\n                    )\n        else:\n            return self.fdel(instance)\n\n    def __set_name__(self, ownerclass, name):\n        self.name = name\n        self.clsname = ownerclass.__name__\n\n\nclass _proto_member:\n    \"\"\"\n    intermediate step for enum members between class execution and final creation\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n    def __set_name__(self, enum_class, member_name):\n        \"\"\"\n        convert each quasi-member into an instance of the new enum class\n        \"\"\"\n        # first step: remove ourself from enum_class\n        delattr(enum_class, member_name)\n        # second step: create member based on enum_class\n        value = self.value\n        if not isinstance(value, tuple):\n            args = (value, )\n        else:\n            args = value\n        if enum_class._member_type_ is tuple:   # special case for tuple enums\n            args = (args, )     # wrap it one more time\n        if not enum_class._use_args_:\n            enum_member = enum_class._new_member_(enum_class)\n        else:\n            enum_member = enum_class._new_member_(enum_class, *args)\n        if not hasattr(enum_member, '_value_'):\n            if enum_class._member_type_ is object:\n                enum_member._value_ = value\n            else:\n                try:\n                    enum_member._value_ = enum_class._member_type_(*args)\n                except Exception as exc:\n                    new_exc = TypeError(\n                            '_value_ not set in __new__, unable to create it'\n                            )\n                    new_exc.__cause__ = exc\n                    raise new_exc\n        value = enum_member._value_\n        enum_member._name_ = member_name\n        enum_member.__objclass__ = enum_class\n        enum_member.__init__(*args)\n        enum_member._sort_order_ = len(enum_class._member_names_)\n\n        if Flag is not None and issubclass(enum_class, Flag):\n            if isinstance(value, int):\n                enum_class._flag_mask_ |= value\n                if _is_single_bit(value):\n                    enum_class._singles_mask_ |= value\n            enum_class._all_bits_ = 2 ** ((enum_class._flag_mask_).bit_length()) - 1\n\n        # If another member with the same value was already defined, the\n        # new member becomes an alias to the existing one.\n        try:\n            try:\n                # try to do a fast lookup to avoid the quadratic loop\n                enum_member = enum_class._value2member_map_[value]\n            except TypeError:\n                for name, canonical_member in enum_class._member_map_.items():\n                    if canonical_member._value_ == value:\n                        enum_member = canonical_member\n                        break\n                else:\n                    raise KeyError\n        except KeyError:\n            # this could still be an alias if the value is multi-bit and the\n            # class is a flag class\n            if (\n                    Flag is None\n                    or not issubclass(enum_class, Flag)\n                ):\n                # no other instances found, record this member in _member_names_\n                enum_class._member_names_.append(member_name)\n            elif (\n                    Flag is not None\n                    and issubclass(enum_class, Flag)\n                    and isinstance(value, int)\n                    and _is_single_bit(value)\n                ):\n                # no other instances found, record this member in _member_names_\n                enum_class._member_names_.append(member_name)\n        # if necessary, get redirect in place and then add it to _member_map_\n        found_descriptor = None\n        for base in enum_class.__mro__[1:]:\n            descriptor = base.__dict__.get(member_name)\n            if descriptor is not None:\n                if isinstance(descriptor, (property, DynamicClassAttribute)):\n                    found_descriptor = descriptor\n                    break\n                elif (\n                        hasattr(descriptor, 'fget') and\n                        hasattr(descriptor, 'fset') and\n                        hasattr(descriptor, 'fdel')\n                    ):\n                    found_descriptor = descriptor\n                    continue\n        if found_descriptor:\n            redirect = property()\n            redirect.member = enum_member\n            redirect.__set_name__(enum_class, member_name)\n            # earlier descriptor found; copy fget, fset, fdel to this one.\n            redirect.fget = found_descriptor.fget\n            redirect.fset = found_descriptor.fset\n            redirect.fdel = found_descriptor.fdel\n            setattr(enum_class, member_name, redirect)\n        else:\n            setattr(enum_class, member_name, enum_member)\n        # now add to _member_map_ (even aliases)\n        enum_class._member_map_[member_name] = enum_member\n        try:\n            # This may fail if value is not hashable. We can't add the value\n            # to the map, and by-value lookups for this value will be\n            # linear.\n            enum_class._value2member_map_.setdefault(value, enum_member)\n        except TypeError:\n            # keep track of the value in a list so containment checks are quick\n            enum_class._unhashable_values_.append(value)\n\n\nclass _EnumDict(dict):\n    \"\"\"\n    Track enum member order and ensure member names are not reused.\n\n    EnumType will use the names found in self._member_names as the\n    enumeration member names.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._member_names = {} # use a dict to keep insertion order\n        self._last_values = []\n        self._ignore = []\n        self._auto_called = False\n\n    def __setitem__(self, key, value):\n        \"\"\"\n        Changes anything not dundered or not a descriptor.\n\n        If an enum member name is used twice, an error is raised; duplicate\n        values are not checked for.\n\n        Single underscore (sunder) names are reserved.\n        \"\"\"\n        if _is_internal_class(self._cls_name, value):\n            import warnings\n            warnings.warn(\n                    \"In 3.13 classes created inside an enum will not become a member.  \"\n                    \"Use the `member` decorator to keep the current behavior.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                    )\n        if _is_private(self._cls_name, key):\n            # also do nothing, name will be a normal attribute\n            pass\n        elif _is_sunder(key):\n            if key not in (\n                    '_order_',\n                    '_generate_next_value_', '_numeric_repr_', '_missing_', '_ignore_',\n                    '_iter_member_', '_iter_member_by_value_', '_iter_member_by_def_',\n                    ):\n                raise ValueError(\n                        '_sunder_ names, such as %r, are reserved for future Enum use'\n                        % (key, )\n                        )\n            if key == '_generate_next_value_':\n                # check if members already defined as auto()\n                if self._auto_called:\n                    raise TypeError(\"_generate_next_value_ must be defined before members\")\n                _gnv = value.__func__ if isinstance(value, staticmethod) else value\n                setattr(self, '_generate_next_value', _gnv)\n            elif key == '_ignore_':\n                if isinstance(value, str):\n                    value = value.replace(',',' ').split()\n                else:\n                    value = list(value)\n                self._ignore = value\n                already = set(value) & set(self._member_names)\n                if already:\n                    raise ValueError(\n                            '_ignore_ cannot specify already set names: %r'\n                            % (already, )\n                            )\n        elif _is_dunder(key):\n            if key == '__order__':\n                key = '_order_'\n        elif key in self._member_names:\n            # descriptor overwriting an enum?\n            raise TypeError('%r already defined as %r' % (key, self[key]))\n        elif key in self._ignore:\n            pass\n        elif isinstance(value, nonmember):\n            # unwrap value here; it won't be processed by the below `else`\n            value = value.value\n        elif _is_descriptor(value):\n            pass\n        # TODO: uncomment next three lines in 3.13\n        # elif _is_internal_class(self._cls_name, value):\n        #     # do nothing, name will be a normal attribute\n        #     pass\n        else:\n            if key in self:\n                # enum overwriting a descriptor?\n                raise TypeError('%r already defined as %r' % (key, self[key]))\n            elif isinstance(value, member):\n                # unwrap value here -- it will become a member\n                value = value.value\n            non_auto_store = True\n            single = False\n            if isinstance(value, auto):\n                single = True\n                value = (value, )\n            if type(value) is tuple and any(isinstance(v, auto) for v in value):\n                # insist on an actual tuple, no subclasses, in keeping with only supporting\n                # top-level auto() usage (not contained in any other data structure)\n                auto_valued = []\n                for v in value:\n                    if isinstance(v, auto):\n                        non_auto_store = False\n                        if v.value == _auto_null:\n                            v.value = self._generate_next_value(\n                                    key, 1, len(self._member_names), self._last_values[:],\n                                    )\n                            self._auto_called = True\n                        v = v.value\n                        self._last_values.append(v)\n                    auto_valued.append(v)\n                if single:\n                    value = auto_valued[0]\n                else:\n                    value = tuple(auto_valued)\n            self._member_names[key] = None\n            if non_auto_store:\n                self._last_values.append(value)\n        super().__setitem__(key, value)\n\n    def update(self, members, **more_members):\n        try:\n            for name in members.keys():\n                self[name] = members[name]\n        except AttributeError:\n            for name, value in members:\n                self[name] = value\n        for name, value in more_members.items():\n            self[name] = value\n\n\nclass EnumType(type):\n    \"\"\"\n    Metaclass for Enum\n    \"\"\"\n\n    @classmethod\n    def __prepare__(metacls, cls, bases, **kwds):\n        # check that previous enum members do not exist\n        metacls._check_for_existing_members_(cls, bases)\n        # create the namespace dict\n        enum_dict = _EnumDict()\n        enum_dict._cls_name = cls\n        # inherit previous flags and _generate_next_value_ function\n        member_type, first_enum = metacls._get_mixins_(cls, bases)\n        if first_enum is not None:\n            enum_dict['_generate_next_value_'] = getattr(\n                    first_enum, '_generate_next_value_', None,\n                    )\n        return enum_dict\n\n    def __new__(metacls, cls, bases, classdict, *, boundary=None, _simple=False, **kwds):\n        # an Enum class is final once enumeration items have been defined; it\n        # cannot be mixed with other types (int, float, etc.) if it has an\n        # inherited __new__ unless a new __new__ is defined (or the resulting\n        # class will fail).\n        #\n        if _simple:\n            return super().__new__(metacls, cls, bases, classdict, **kwds)\n        #\n        # remove any keys listed in _ignore_\n        classdict.setdefault('_ignore_', []).append('_ignore_')\n        ignore = classdict['_ignore_']\n        for key in ignore:\n            classdict.pop(key, None)\n        #\n        # grab member names\n        member_names = classdict._member_names\n        #\n        # check for illegal enum names (any others?)\n        invalid_names = set(member_names) & {'mro', ''}\n        if invalid_names:\n            raise ValueError('invalid enum member name(s) %s'  % (\n                    ','.join(repr(n) for n in invalid_names)\n                    ))\n        #\n        # adjust the sunders\n        _order_ = classdict.pop('_order_', None)\n        # convert to normal dict\n        classdict = dict(classdict.items())\n        #\n        # data type of member and the controlling Enum class\n        member_type, first_enum = metacls._get_mixins_(cls, bases)\n        __new__, save_new, use_args = metacls._find_new_(\n                classdict, member_type, first_enum,\n                )\n        classdict['_new_member_'] = __new__\n        classdict['_use_args_'] = use_args\n        #\n        # convert future enum members into temporary _proto_members\n        for name in member_names:\n            value = classdict[name]\n            classdict[name] = _proto_member(value)\n        #\n        # house-keeping structures\n        classdict['_member_names_'] = []\n        classdict['_member_map_'] = {}\n        classdict['_value2member_map_'] = {}\n        classdict['_unhashable_values_'] = []\n        classdict['_member_type_'] = member_type\n        # now set the __repr__ for the value\n        classdict['_value_repr_'] = metacls._find_data_repr_(cls, bases)\n        #\n        # Flag structures (will be removed if final class is not a Flag\n        classdict['_boundary_'] = (\n                boundary\n                or getattr(first_enum, '_boundary_', None)\n                )\n        classdict['_flag_mask_'] = 0\n        classdict['_singles_mask_'] = 0\n        classdict['_all_bits_'] = 0\n        classdict['_inverted_'] = None\n        try:\n            exc = None\n            enum_class = super().__new__(metacls, cls, bases, classdict, **kwds)\n        except RuntimeError as e:\n            # any exceptions raised by member.__new__ will get converted to a\n            # RuntimeError, so get that original exception back and raise it instead\n            exc = e.__cause__ or e\n        if exc is not None:\n            raise exc\n        #\n        # update classdict with any changes made by __init_subclass__\n        classdict.update(enum_class.__dict__)\n        #\n        # double check that repr and friends are not the mixin's or various\n        # things break (such as pickle)\n        # however, if the method is defined in the Enum itself, don't replace\n        # it\n        #\n        # Also, special handling for ReprEnum\n        if ReprEnum is not None and ReprEnum in bases:\n            if member_type is object:\n                raise TypeError(\n                        'ReprEnum subclasses must be mixed with a data type (i.e.'\n                        ' int, str, float, etc.)'\n                        )\n            if '__format__' not in classdict:\n                enum_class.__format__ = member_type.__format__\n                classdict['__format__'] = enum_class.__format__\n            if '__str__' not in classdict:\n                method = member_type.__str__\n                if method is object.__str__:\n                    # if member_type does not define __str__, object.__str__ will use\n                    # its __repr__ instead, so we'll also use its __repr__\n                    method = member_type.__repr__\n                enum_class.__str__ = method\n                classdict['__str__'] = enum_class.__str__\n        for name in ('__repr__', '__str__', '__format__', '__reduce_ex__'):\n            if name not in classdict:\n                # check for mixin overrides before replacing\n                enum_method = getattr(first_enum, name)\n                found_method = getattr(enum_class, name)\n                object_method = getattr(object, name)\n                data_type_method = getattr(member_type, name)\n                if found_method in (data_type_method, object_method):\n                    setattr(enum_class, name, enum_method)\n        #\n        # for Flag, add __or__, __and__, __xor__, and __invert__\n        if Flag is not None and issubclass(enum_class, Flag):\n            for name in (\n                    '__or__', '__and__', '__xor__',\n                    '__ror__', '__rand__', '__rxor__',\n                    '__invert__'\n                ):\n                if name not in classdict:\n                    enum_method = getattr(Flag, name)\n                    setattr(enum_class, name, enum_method)\n                    classdict[name] = enum_method\n        #\n        # replace any other __new__ with our own (as long as Enum is not None,\n        # anyway) -- again, this is to support pickle\n        if Enum is not None:\n            # if the user defined their own __new__, save it before it gets\n            # clobbered in case they subclass later\n            if save_new:\n                enum_class.__new_member__ = __new__\n            enum_class.__new__ = Enum.__new__\n        #\n        # py3 support for definition order (helps keep py2/py3 code in sync)\n        #\n        # _order_ checking is spread out into three/four steps\n        # - if enum_class is a Flag:\n        #   - remove any non-single-bit flags from _order_\n        # - remove any aliases from _order_\n        # - check that _order_ and _member_names_ match\n        #\n        # step 1: ensure we have a list\n        if _order_ is not None:\n            if isinstance(_order_, str):\n                _order_ = _order_.replace(',', ' ').split()\n        #\n        # remove Flag structures if final class is not a Flag\n        if (\n                Flag is None and cls != 'Flag'\n                or Flag is not None and not issubclass(enum_class, Flag)\n            ):\n            delattr(enum_class, '_boundary_')\n            delattr(enum_class, '_flag_mask_')\n            delattr(enum_class, '_singles_mask_')\n            delattr(enum_class, '_all_bits_')\n            delattr(enum_class, '_inverted_')\n        elif Flag is not None and issubclass(enum_class, Flag):\n            # set correct __iter__\n            member_list = [m._value_ for m in enum_class]\n            if member_list != sorted(member_list):\n                enum_class._iter_member_ = enum_class._iter_member_by_def_\n            if _order_:\n                # _order_ step 2: remove any items from _order_ that are not single-bit\n                _order_ = [\n                        o\n                        for o in _order_\n                        if o not in enum_class._member_map_ or _is_single_bit(enum_class[o]._value_)\n                        ]\n        #\n        if _order_:\n            # _order_ step 3: remove aliases from _order_\n            _order_ = [\n                    o\n                    for o in _order_\n                    if (\n                        o not in enum_class._member_map_\n                        or\n                        (o in enum_class._member_map_ and o in enum_class._member_names_)\n                        )]\n            # _order_ step 4: verify that _order_ and _member_names_ match\n            if _order_ != enum_class._member_names_:\n                raise TypeError(\n                        'member order does not match _order_:\\n  %r\\n  %r'\n                        % (enum_class._member_names_, _order_)\n                        )\n\n        return enum_class\n\n    def __bool__(cls):\n        \"\"\"\n        classes/types should always be True.\n        \"\"\"\n        return True\n\n    def __call__(cls, value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None):\n        \"\"\"\n        Either returns an existing member, or creates a new enum class.\n\n        This method is used both when an enum class is given a value to match\n        to an enumeration member (i.e. Color(3)) and for the functional API\n        (i.e. Color = Enum('Color', names='RED GREEN BLUE')).\n\n        When used for the functional API:\n\n        `value` will be the name of the new class.\n\n        `names` should be either a string of white-space/comma delimited names\n        (values will start at `start`), or an iterator/mapping of name, value pairs.\n\n        `module` should be set to the module this class is being created in;\n        if it is not set, an attempt to find that module will be made, but if\n        it fails the class will not be picklable.\n\n        `qualname` should be set to the actual location this class can be found\n        at in its module; by default it is set to the global scope.  If this is\n        not correct, unpickling will fail in some circumstances.\n\n        `type`, if set, will be mixed in as the first base class.\n        \"\"\"\n        if names is None:  # simple value lookup\n            return cls.__new__(cls, value)\n        # otherwise, functional API: we're creating a new Enum type\n        return cls._create_(\n                value,\n                names,\n                module=module,\n                qualname=qualname,\n                type=type,\n                start=start,\n                boundary=boundary,\n                )\n\n    def __contains__(cls, member):\n        \"\"\"\n        Return True if member is a member of this enum\n        raises TypeError if member is not an enum member\n\n        note: in 3.12 TypeError will no longer be raised, and True will also be\n        returned if member is the value of a member in this enum\n        \"\"\"\n        if not isinstance(member, Enum):\n            import warnings\n            warnings.warn(\n                    \"in 3.12 __contains__ will no longer raise TypeError, but will return True or\\n\"\n                    \"False depending on whether the value is a member or the value of a member\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                    )\n            raise TypeError(\n                \"unsupported operand type(s) for 'in': '%s' and '%s'\" % (\n                    type(member).__qualname__, cls.__class__.__qualname__))\n        return isinstance(member, cls) and member._name_ in cls._member_map_\n\n    def __delattr__(cls, attr):\n        # nicer error message when someone tries to delete an attribute\n        # (see issue19025).\n        if attr in cls._member_map_:\n            raise AttributeError(\"%r cannot delete member %r.\" % (cls.__name__, attr))\n        super().__delattr__(attr)\n\n    def __dir__(cls):\n        interesting = set([\n                '__class__', '__contains__', '__doc__', '__getitem__',\n                '__iter__', '__len__', '__members__', '__module__',\n                '__name__', '__qualname__',\n                ]\n                + cls._member_names_\n                )\n        if cls._new_member_ is not object.__new__:\n            interesting.add('__new__')\n        if cls.__init_subclass__ is not object.__init_subclass__:\n            interesting.add('__init_subclass__')\n        if cls._member_type_ is object:\n            return sorted(interesting)\n        else:\n            # return whatever mixed-in data type has\n            return sorted(set(dir(cls._member_type_)) | interesting)\n\n    def __getattr__(cls, name):\n        \"\"\"\n        Return the enum member matching `name`\n\n        We use __getattr__ instead of descriptors or inserting into the enum\n        class' __dict__ in order to support `name` and `value` being both\n        properties for enum members (which live in the class' __dict__) and\n        enum members themselves.\n        \"\"\"\n        if _is_dunder(name):\n            raise AttributeError(name)\n        try:\n            return cls._member_map_[name]\n        except KeyError:\n            raise AttributeError(name) from None\n\n    def __getitem__(cls, name):\n        \"\"\"\n        Return the member matching `name`.\n        \"\"\"\n        return cls._member_map_[name]\n\n    def __iter__(cls):\n        \"\"\"\n        Return members in definition order.\n        \"\"\"\n        return (cls._member_map_[name] for name in cls._member_names_)\n\n    def __len__(cls):\n        \"\"\"\n        Return the number of members (no aliases)\n        \"\"\"\n        return len(cls._member_names_)\n\n    @bltns.property\n    def __members__(cls):\n        \"\"\"\n        Returns a mapping of member name->value.\n\n        This mapping lists all enum members, including aliases. Note that this\n        is a read-only view of the internal mapping.\n        \"\"\"\n        return MappingProxyType(cls._member_map_)\n\n    def __repr__(cls):\n        if Flag is not None and issubclass(cls, Flag):\n            return \"<flag %r>\" % cls.__name__\n        else:\n            return \"<enum %r>\" % cls.__name__\n\n    def __reversed__(cls):\n        \"\"\"\n        Return members in reverse definition order.\n        \"\"\"\n        return (cls._member_map_[name] for name in reversed(cls._member_names_))\n\n    def __setattr__(cls, name, value):\n        \"\"\"\n        Block attempts to reassign Enum members.\n\n        A simple assignment to the class namespace only changes one of the\n        several possible ways to get an Enum member from the Enum class,\n        resulting in an inconsistent Enumeration.\n        \"\"\"\n        member_map = cls.__dict__.get('_member_map_', {})\n        if name in member_map:\n            raise AttributeError('cannot reassign member %r' % (name, ))\n        super().__setattr__(name, value)\n\n    def _create_(cls, class_name, names, *, module=None, qualname=None, type=None, start=1, boundary=None):\n        \"\"\"\n        Convenience method to create a new Enum class.\n\n        `names` can be:\n\n        * A string containing member names, separated either with spaces or\n          commas.  Values are incremented by 1 from `start`.\n        * An iterable of member names.  Values are incremented by 1 from `start`.\n        * An iterable of (member name, value) pairs.\n        * A mapping of member name -> value pairs.\n        \"\"\"\n        metacls = cls.__class__\n        bases = (cls, ) if type is None else (type, cls)\n        _, first_enum = cls._get_mixins_(class_name, bases)\n        classdict = metacls.__prepare__(class_name, bases)\n\n        # special processing needed for names?\n        if isinstance(names, str):\n            names = names.replace(',', ' ').split()\n        if isinstance(names, (tuple, list)) and names and isinstance(names[0], str):\n            original_names, names = names, []\n            last_values = []\n            for count, name in enumerate(original_names):\n                value = first_enum._generate_next_value_(name, start, count, last_values[:])\n                last_values.append(value)\n                names.append((name, value))\n        if names is None:\n            names = ()\n\n        # Here, names is either an iterable of (name, value) or a mapping.\n        for item in names:\n            if isinstance(item, str):\n                member_name, member_value = item, names[item]\n            else:\n                member_name, member_value = item\n            classdict[member_name] = member_value\n\n        # TODO: replace the frame hack if a blessed way to know the calling\n        # module is ever developed\n        if module is None:\n            try:\n                module = sys._getframe(2).f_globals['__name__']\n            except (AttributeError, ValueError, KeyError):\n                pass\n        if module is None:\n            _make_class_unpicklable(classdict)\n        else:\n            classdict['__module__'] = module\n        if qualname is not None:\n            classdict['__qualname__'] = qualname\n\n        return metacls.__new__(metacls, class_name, bases, classdict, boundary=boundary)\n\n    def _convert_(cls, name, module, filter, source=None, *, boundary=None, as_global=False):\n        \"\"\"\n        Create a new Enum subclass that replaces a collection of global constants\n        \"\"\"\n        # convert all constants from source (or module) that pass filter() to\n        # a new Enum called name, and export the enum and its members back to\n        # module;\n        # also, replace the __reduce_ex__ method so unpickling works in\n        # previous Python versions\n        module_globals = sys.modules[module].__dict__\n        if source:\n            source = source.__dict__\n        else:\n            source = module_globals\n        # _value2member_map_ is populated in the same order every time\n        # for a consistent reverse mapping of number to name when there\n        # are multiple names for the same number.\n        members = [\n                (name, value)\n                for name, value in source.items()\n                if filter(name)]\n        try:\n            # sort by value\n            members.sort(key=lambda t: (t[1], t[0]))\n        except TypeError:\n            # unless some values aren't comparable, in which case sort by name\n            members.sort(key=lambda t: t[0])\n        body = {t[0]: t[1] for t in members}\n        body['__module__'] = module\n        tmp_cls = type(name, (object, ), body)\n        cls = _simple_enum(etype=cls, boundary=boundary or KEEP)(tmp_cls)\n        if as_global:\n            global_enum(cls)\n        else:\n            sys.modules[cls.__module__].__dict__.update(cls.__members__)\n        module_globals[name] = cls\n        return cls\n\n    @classmethod\n    def _check_for_existing_members_(mcls, class_name, bases):\n        for chain in bases:\n            for base in chain.__mro__:\n                if isinstance(base, EnumType) and base._member_names_:\n                    raise TypeError(\n                            \"<enum %r> cannot extend %r\"\n                            % (class_name, base)\n                            )\n\n    @classmethod\n    def _get_mixins_(mcls, class_name, bases):\n        \"\"\"\n        Returns the type for creating enum members, and the first inherited\n        enum class.\n\n        bases: the tuple of bases that was given to __new__\n        \"\"\"\n        if not bases:\n            return object, Enum\n\n        mcls._check_for_existing_members_(class_name, bases)\n\n        # ensure final parent class is an Enum derivative, find any concrete\n        # data type, and check that Enum has no members\n        first_enum = bases[-1]\n        if not isinstance(first_enum, EnumType):\n            raise TypeError(\"new enumerations should be created as \"\n                    \"`EnumName([mixin_type, ...] [data_type,] enum_type)`\")\n        member_type = mcls._find_data_type_(class_name, bases) or object\n        return member_type, first_enum\n\n    @classmethod\n    def _find_data_repr_(mcls, class_name, bases):\n        for chain in bases:\n            for base in chain.__mro__:\n                if base is object:\n                    continue\n                elif isinstance(base, EnumType):\n                    # if we hit an Enum, use it's _value_repr_\n                    return base._value_repr_\n                elif '__repr__' in base.__dict__:\n                    # this is our data repr\n                    return base.__dict__['__repr__']\n        return None\n\n    @classmethod\n    def _find_data_type_(mcls, class_name, bases):\n        # a datatype has a __new__ method\n        data_types = set()\n        base_chain = set()\n        for chain in bases:\n            candidate = None\n            for base in chain.__mro__:\n                base_chain.add(base)\n                if base is object:\n                    continue\n                elif isinstance(base, EnumType):\n                    if base._member_type_ is not object:\n                        data_types.add(base._member_type_)\n                        break\n                elif '__new__' in base.__dict__ or '__dataclass_fields__' in base.__dict__:\n                    if isinstance(base, EnumType):\n                        continue\n                    data_types.add(candidate or base)\n                    break\n                else:\n                    candidate = candidate or base\n        if len(data_types) > 1:\n            raise TypeError('too many data types for %r: %r' % (class_name, data_types))\n        elif data_types:\n            return data_types.pop()\n        else:\n            return None\n\n    @classmethod\n    def _find_new_(mcls, classdict, member_type, first_enum):\n        \"\"\"\n        Returns the __new__ to be used for creating the enum members.\n\n        classdict: the class dictionary given to __new__\n        member_type: the data type whose __new__ will be used by default\n        first_enum: enumeration to check for an overriding __new__\n        \"\"\"\n        # now find the correct __new__, checking to see of one was defined\n        # by the user; also check earlier enum classes in case a __new__ was\n        # saved as __new_member__\n        __new__ = classdict.get('__new__', None)\n\n        # should __new__ be saved as __new_member__ later?\n        save_new = first_enum is not None and __new__ is not None\n\n        if __new__ is None:\n            # check all possibles for __new_member__ before falling back to\n            # __new__\n            for method in ('__new_member__', '__new__'):\n                for possible in (member_type, first_enum):\n                    target = getattr(possible, method, None)\n                    if target not in {\n                            None,\n                            None.__new__,\n                            object.__new__,\n                            Enum.__new__,\n                            }:\n                        __new__ = target\n                        break\n                if __new__ is not None:\n                    break\n            else:\n                __new__ = object.__new__\n\n        # if a non-object.__new__ is used then whatever value/tuple was\n        # assigned to the enum member name will be passed to __new__ and to the\n        # new enum member's __init__\n        if first_enum is None or __new__ in (Enum.__new__, object.__new__):\n            use_args = False\n        else:\n            use_args = True\n        return __new__, save_new, use_args\nEnumMeta = EnumType\n\n\nclass Enum(metaclass=EnumType):\n    \"\"\"\n    Create a collection of name/value pairs.\n\n    Example enumeration:\n\n    >>> class Color(Enum):\n    ...     RED = 1\n    ...     BLUE = 2\n    ...     GREEN = 3\n\n    Access them by:\n\n    - attribute access::\n\n    >>> Color.RED\n    <Color.RED: 1>\n\n    - value lookup:\n\n    >>> Color(1)\n    <Color.RED: 1>\n\n    - name lookup:\n\n    >>> Color['RED']\n    <Color.RED: 1>\n\n    Enumerations can be iterated over, and know how many members they have:\n\n    >>> len(Color)\n    3\n\n    >>> list(Color)\n    [<Color.RED: 1>, <Color.BLUE: 2>, <Color.GREEN: 3>]\n\n    Methods can be added to enumerations, and members can have their own\n    attributes -- see the documentation for details.\n    \"\"\"\n\n    def __new__(cls, value):\n        # all enum instances are actually created during class construction\n        # without calling this method; this method is called by the metaclass'\n        # __call__ (i.e. Color(3) ), and by pickle\n        if type(value) is cls:\n            # For lookups like Color(Color.RED)\n            return value\n        # by-value search for a matching enum member\n        # see if it's in the reverse mapping (for hashable values)\n        try:\n            return cls._value2member_map_[value]\n        except KeyError:\n            # Not found, no need to do long O(n) search\n            pass\n        except TypeError:\n            # not there, now do long search -- O(n) behavior\n            for member in cls._member_map_.values():\n                if member._value_ == value:\n                    return member\n        # still not found -- verify that members exist, in-case somebody got here mistakenly\n        # (such as via super when trying to override __new__)\n        if not cls._member_map_:\n            raise TypeError(\"%r has no members defined\" % cls)\n        #\n        # still not found -- try _missing_ hook\n        try:\n            exc = None\n            result = cls._missing_(value)\n        except Exception as e:\n            exc = e\n            result = None\n        try:\n            if isinstance(result, cls):\n                return result\n            elif (\n                    Flag is not None and issubclass(cls, Flag)\n                    and cls._boundary_ is EJECT and isinstance(result, int)\n                ):\n                return result\n            else:\n                ve_exc = ValueError(\"%r is not a valid %s\" % (value, cls.__qualname__))\n                if result is None and exc is None:\n                    raise ve_exc\n                elif exc is None:\n                    exc = TypeError(\n                            'error in %s._missing_: returned %r instead of None or a valid member'\n                            % (cls.__name__, result)\n                            )\n                if not isinstance(exc, ValueError):\n                    exc.__context__ = ve_exc\n                raise exc\n        finally:\n            # ensure all variables that could hold an exception are destroyed\n            exc = None\n            ve_exc = None\n\n    def __init__(self, *args, **kwds):\n        pass\n\n    def _generate_next_value_(name, start, count, last_values):\n        \"\"\"\n        Generate the next value when not given.\n\n        name: the name of the member\n        start: the initial start value or None\n        count: the number of existing members\n        last_values: the list of values assigned\n        \"\"\"\n        if not last_values:\n            return start\n        try:\n            last = last_values[-1]\n            last_values.sort()\n            if last == last_values[-1]:\n                # no difference between old and new methods\n                return last + 1\n            else:\n                # trigger old method (with warning)\n                raise TypeError\n        except TypeError:\n            import warnings\n            warnings.warn(\n                    \"In 3.13 the default `auto()`/`_generate_next_value_` will require all values to be sortable and support adding +1\\n\"\n                    \"and the value returned will be the largest value in the enum incremented by 1\",\n                    DeprecationWarning,\n                    stacklevel=3,\n                    )\n            for v in reversed(last_values):\n                try:\n                    return v + 1\n                except TypeError:\n                    pass\n            return start\n\n    @classmethod\n    def _missing_(cls, value):\n        return None\n\n    def __repr__(self):\n        v_repr = self.__class__._value_repr_ or repr\n        return \"<%s.%s: %s>\" % (self.__class__.__name__, self._name_, v_repr(self._value_))\n\n    def __str__(self):\n        return \"%s.%s\" % (self.__class__.__name__, self._name_, )\n\n    def __dir__(self):\n        \"\"\"\n        Returns public methods and other interesting attributes.\n        \"\"\"\n        interesting = set()\n        if self.__class__._member_type_ is not object:\n            interesting = set(object.__dir__(self))\n        for name in getattr(self, '__dict__', []):\n            if name[0] != '_' and name not in self._member_map_:\n                interesting.add(name)\n        for cls in self.__class__.mro():\n            for name, obj in cls.__dict__.items():\n                if name[0] == '_':\n                    continue\n                if isinstance(obj, property):\n                    # that's an enum.property\n                    if obj.fget is not None or name not in self._member_map_:\n                        interesting.add(name)\n                    else:\n                        # in case it was added by `dir(self)`\n                        interesting.discard(name)\n                elif name not in self._member_map_:\n                    interesting.add(name)\n        names = sorted(\n                set(['__class__', '__doc__', '__eq__', '__hash__', '__module__'])\n                | interesting\n                )\n        return names\n\n    def __format__(self, format_spec):\n        return str.__format__(str(self), format_spec)\n\n    def __hash__(self):\n        return hash(self._name_)\n\n    def __reduce_ex__(self, proto):\n        return self.__class__, (self._value_, )\n\n    def __deepcopy__(self,memo):\n        return self\n\n    def __copy__(self):\n        return self\n\n    # enum.property is used to provide access to the `name` and\n    # `value` attributes of enum members while keeping some measure of\n    # protection from modification, while still allowing for an enumeration\n    # to have members named `name` and `value`.  This works because enumeration\n    # members are not set directly on the enum class; they are kept in a\n    # separate structure, _member_map_, which is where enum.property looks for\n    # them\n\n    @property\n    def name(self):\n        \"\"\"The name of the Enum member.\"\"\"\n        return self._name_\n\n    @property\n    def value(self):\n        \"\"\"The value of the Enum member.\"\"\"\n        return self._value_\n\n\nclass ReprEnum(Enum):\n    \"\"\"\n    Only changes the repr(), leaving str() and format() to the mixed-in type.\n    \"\"\"\n\n\nclass IntEnum(int, ReprEnum):\n    \"\"\"\n    Enum where members are also (and must be) ints\n    \"\"\"\n\n\nclass StrEnum(str, ReprEnum):\n    \"\"\"\n    Enum where members are also (and must be) strings\n    \"\"\"\n\n    def __new__(cls, *values):\n        \"values must already be of type `str`\"\n        if len(values) > 3:\n            raise TypeError('too many arguments for str(): %r' % (values, ))\n        if len(values) == 1:\n            # it must be a string\n            if not isinstance(values[0], str):\n                raise TypeError('%r is not a string' % (values[0], ))\n        if len(values) >= 2:\n            # check that encoding argument is a string\n            if not isinstance(values[1], str):\n                raise TypeError('encoding must be a string, not %r' % (values[1], ))\n        if len(values) == 3:\n            # check that errors argument is a string\n            if not isinstance(values[2], str):\n                raise TypeError('errors must be a string, not %r' % (values[2]))\n        value = str(*values)\n        member = str.__new__(cls, value)\n        member._value_ = value\n        return member\n\n    def _generate_next_value_(name, start, count, last_values):\n        \"\"\"\n        Return the lower-cased version of the member name.\n        \"\"\"\n        return name.lower()\n\n\ndef pickle_by_global_name(self, proto):\n    # should not be used with Flag-type enums\n    return self.name\n_reduce_ex_by_global_name = pickle_by_global_name\n\ndef pickle_by_enum_name(self, proto):\n    # should not be used with Flag-type enums\n    return getattr, (self.__class__, self._name_)\n\nclass FlagBoundary(StrEnum):\n    \"\"\"\n    control how out of range values are handled\n    \"strict\" -> error is raised             [default for Flag]\n    \"conform\" -> extra bits are discarded\n    \"eject\" -> lose flag status\n    \"keep\" -> keep flag status and all bits [default for IntFlag]\n    \"\"\"\n    STRICT = auto()\n    CONFORM = auto()\n    EJECT = auto()\n    KEEP = auto()\nSTRICT, CONFORM, EJECT, KEEP = FlagBoundary\n\n\nclass Flag(Enum, boundary=STRICT):\n    \"\"\"\n    Support for flags\n    \"\"\"\n\n    _numeric_repr_ = repr\n\n    def _generate_next_value_(name, start, count, last_values):\n        \"\"\"\n        Generate the next value when not given.\n\n        name: the name of the member\n        start: the initial start value or None\n        count: the number of existing members\n        last_values: the last value assigned or None\n        \"\"\"\n        if not count:\n            return start if start is not None else 1\n        last_value = max(last_values)\n        try:\n            high_bit = _high_bit(last_value)\n        except Exception:\n            raise TypeError('invalid flag value %r' % last_value) from None\n        return 2 ** (high_bit+1)\n\n    @classmethod\n    def _iter_member_by_value_(cls, value):\n        \"\"\"\n        Extract all members from the value in definition (i.e. increasing value) order.\n        \"\"\"\n        for val in _iter_bits_lsb(value & cls._flag_mask_):\n            yield cls._value2member_map_.get(val)\n\n    _iter_member_ = _iter_member_by_value_\n\n    @classmethod\n    def _iter_member_by_def_(cls, value):\n        \"\"\"\n        Extract all members from the value in definition order.\n        \"\"\"\n        yield from sorted(\n                cls._iter_member_by_value_(value),\n                key=lambda m: m._sort_order_,\n                )\n\n    @classmethod\n    def _missing_(cls, value):\n        \"\"\"\n        Create a composite member containing all canonical members present in `value`.\n\n        If non-member values are present, result depends on `_boundary_` setting.\n        \"\"\"\n        if not isinstance(value, int):\n            raise ValueError(\n                    \"%r is not a valid %s\" % (value, cls.__qualname__)\n                    )\n        # check boundaries\n        # - value must be in range (e.g. -16 <-> +15, i.e. ~15 <-> 15)\n        # - value must not include any skipped flags (e.g. if bit 2 is not\n        #   defined, then 0d10 is invalid)\n        flag_mask = cls._flag_mask_\n        singles_mask = cls._singles_mask_\n        all_bits = cls._all_bits_\n        neg_value = None\n        if (\n                not ~all_bits <= value <= all_bits\n                or value & (all_bits ^ flag_mask)\n            ):\n            if cls._boundary_ is STRICT:\n                max_bits = max(value.bit_length(), flag_mask.bit_length())\n                raise ValueError(\n                        \"%r invalid value %r\\n    given %s\\n  allowed %s\" % (\n                            cls, value, bin(value, max_bits), bin(flag_mask, max_bits),\n                            ))\n            elif cls._boundary_ is CONFORM:\n                value = value & flag_mask\n            elif cls._boundary_ is EJECT:\n                return value\n            elif cls._boundary_ is KEEP:\n                if value < 0:\n                    value = (\n                            max(all_bits+1, 2**(value.bit_length()))\n                            + value\n                            )\n            else:\n                raise ValueError(\n                        '%r unknown flag boundary %r' % (cls, cls._boundary_, )\n                        )\n        if value < 0:\n            neg_value = value\n            value = all_bits + 1 + value\n        # get members and unknown\n        unknown = value & ~flag_mask\n        aliases = value & ~singles_mask\n        member_value = value & singles_mask\n        if unknown and cls._boundary_ is not KEEP:\n            raise ValueError(\n                    '%s(%r) -->  unknown values %r [%s]'\n                    % (cls.__name__, value, unknown, bin(unknown))\n                    )\n        # normal Flag?\n        if cls._member_type_ is object:\n            # construct a singleton enum pseudo-member\n            pseudo_member = object.__new__(cls)\n        else:\n            pseudo_member = cls._member_type_.__new__(cls, value)\n        if not hasattr(pseudo_member, '_value_'):\n            pseudo_member._value_ = value\n        if member_value or aliases:\n            members = []\n            combined_value = 0\n            for m in cls._iter_member_(member_value):\n                members.append(m)\n                combined_value |= m._value_\n            if aliases:\n                value = member_value | aliases\n                for n, pm in cls._member_map_.items():\n                    if pm not in members and pm._value_ and pm._value_ & value == pm._value_:\n                        members.append(pm)\n                        combined_value |= pm._value_\n            unknown = value ^ combined_value\n            pseudo_member._name_ = '|'.join([m._name_ for m in members])\n            if not combined_value:\n                pseudo_member._name_ = None\n            elif unknown and cls._boundary_ is STRICT:\n                raise ValueError('%r: no members with value %r' % (cls, unknown))\n            elif unknown:\n                pseudo_member._name_ += '|%s' % cls._numeric_repr_(unknown)\n        else:\n            pseudo_member._name_ = None\n        # use setdefault in case another thread already created a composite\n        # with this value\n        # note: zero is a special case -- always add it\n        pseudo_member = cls._value2member_map_.setdefault(value, pseudo_member)\n        if neg_value is not None:\n            cls._value2member_map_[neg_value] = pseudo_member\n        return pseudo_member\n\n    def __contains__(self, other):\n        \"\"\"\n        Returns True if self has at least the same flags set as other.\n        \"\"\"\n        if not isinstance(other, self.__class__):\n            raise TypeError(\n                \"unsupported operand type(s) for 'in': %r and %r\" % (\n                    type(other).__qualname__, self.__class__.__qualname__))\n        return other._value_ & self._value_ == other._value_\n\n    def __iter__(self):\n        \"\"\"\n        Returns flags in definition order.\n        \"\"\"\n        yield from self._iter_member_(self._value_)\n\n    def __len__(self):\n        return self._value_.bit_count()\n\n    def __repr__(self):\n        cls_name = self.__class__.__name__\n        v_repr = self.__class__._value_repr_ or repr\n        if self._name_ is None:\n            return \"<%s: %s>\" % (cls_name, v_repr(self._value_))\n        else:\n            return \"<%s.%s: %s>\" % (cls_name, self._name_, v_repr(self._value_))\n\n    def __str__(self):\n        cls_name = self.__class__.__name__\n        if self._name_ is None:\n            return '%s(%r)' % (cls_name, self._value_)\n        else:\n            return \"%s.%s\" % (cls_name, self._name_)\n\n    def __bool__(self):\n        return bool(self._value_)\n\n    def _get_value(self, flag):\n        if isinstance(flag, self.__class__):\n            return flag._value_\n        elif self._member_type_ is not object and isinstance(flag, self._member_type_):\n            return flag\n        return NotImplemented\n\n    def __or__(self, other):\n        other_value = self._get_value(other)\n        if other_value is NotImplemented:\n            return NotImplemented\n\n        for flag in self, other:\n            if self._get_value(flag) is None:\n                raise TypeError(f\"'{flag}' cannot be combined with other flags with |\")\n        value = self._value_\n        return self.__class__(value | other_value)\n\n    def __and__(self, other):\n        other_value = self._get_value(other)\n        if other_value is NotImplemented:\n            return NotImplemented\n\n        for flag in self, other:\n            if self._get_value(flag) is None:\n                raise TypeError(f\"'{flag}' cannot be combined with other flags with &\")\n        value = self._value_\n        return self.__class__(value & other_value)\n\n    def __xor__(self, other):\n        other_value = self._get_value(other)\n        if other_value is NotImplemented:\n            return NotImplemented\n\n        for flag in self, other:\n            if self._get_value(flag) is None:\n                raise TypeError(f\"'{flag}' cannot be combined with other flags with ^\")\n        value = self._value_\n        return self.__class__(value ^ other_value)\n\n    def __invert__(self):\n        if self._get_value(self) is None:\n            raise TypeError(f\"'{self}' cannot be inverted\")\n\n        if self._inverted_ is None:\n            if self._boundary_ in (EJECT, KEEP):\n                self._inverted_ = self.__class__(~self._value_)\n            else:\n                self._inverted_ = self.__class__(self._singles_mask_ & ~self._value_)\n        return self._inverted_\n\n    __rand__ = __and__\n    __ror__ = __or__\n    __rxor__ = __xor__\n\n\nclass IntFlag(int, ReprEnum, Flag, boundary=KEEP):\n    \"\"\"\n    Support for integer-based Flags\n    \"\"\"\n\n\ndef _high_bit(value):\n    \"\"\"\n    returns index of highest bit, or -1 if value is zero or negative\n    \"\"\"\n    return value.bit_length() - 1\n\ndef unique(enumeration):\n    \"\"\"\n    Class decorator for enumerations ensuring unique member values.\n    \"\"\"\n    duplicates = []\n    for name, member in enumeration.__members__.items():\n        if name != member.name:\n            duplicates.append((name, member.name))\n    if duplicates:\n        alias_details = ', '.join(\n                [\"%s -> %s\" % (alias, name) for (alias, name) in duplicates])\n        raise ValueError('duplicate values found in %r: %s' %\n                (enumeration, alias_details))\n    return enumeration\n\ndef _power_of_two(value):\n    if value < 1:\n        return False\n    return value == 2 ** _high_bit(value)\n\ndef global_enum_repr(self):\n    \"\"\"\n    use module.enum_name instead of class.enum_name\n\n    the module is the last module in case of a multi-module name\n    \"\"\"\n    module = self.__class__.__module__.split('.')[-1]\n    return '%s.%s' % (module, self._name_)\n\ndef global_flag_repr(self):\n    \"\"\"\n    use module.flag_name instead of class.flag_name\n\n    the module is the last module in case of a multi-module name\n    \"\"\"\n    module = self.__class__.__module__.split('.')[-1]\n    cls_name = self.__class__.__name__\n    if self._name_ is None:\n        return \"%s.%s(%r)\" % (module, cls_name, self._value_)\n    if _is_single_bit(self._value_):\n        return '%s.%s' % (module, self._name_)\n    if self._boundary_ is not FlagBoundary.KEEP:\n        return '|'.join(['%s.%s' % (module, name) for name in self.name.split('|')])\n    else:\n        name = []\n        for n in self._name_.split('|'):\n            if n[0].isdigit():\n                name.append(n)\n            else:\n                name.append('%s.%s' % (module, n))\n        return '|'.join(name)\n\ndef global_str(self):\n    \"\"\"\n    use enum_name instead of class.enum_name\n    \"\"\"\n    if self._name_ is None:\n        cls_name = self.__class__.__name__\n        return \"%s(%r)\" % (cls_name, self._value_)\n    else:\n        return self._name_\n\ndef global_enum(cls, update_str=False):\n    \"\"\"\n    decorator that makes the repr() of an enum member reference its module\n    instead of its class; also exports all members to the enum's module's\n    global namespace\n    \"\"\"\n    if issubclass(cls, Flag):\n        cls.__repr__ = global_flag_repr\n    else:\n        cls.__repr__ = global_enum_repr\n    if not issubclass(cls, ReprEnum) or update_str:\n        cls.__str__ = global_str\n    sys.modules[cls.__module__].__dict__.update(cls.__members__)\n    return cls\n\ndef _simple_enum(etype=Enum, *, boundary=None, use_args=None):\n    \"\"\"\n    Class decorator that converts a normal class into an :class:`Enum`.  No\n    safety checks are done, and some advanced behavior (such as\n    :func:`__init_subclass__`) is not available.  Enum creation can be faster\n    using :func:`simple_enum`.\n\n        >>> from enum import Enum, _simple_enum\n        >>> @_simple_enum(Enum)\n        ... class Color:\n        ...     RED = auto()\n        ...     GREEN = auto()\n        ...     BLUE = auto()\n        >>> Color\n        <enum 'Color'>\n    \"\"\"\n    def convert_class(cls):\n        nonlocal use_args\n        cls_name = cls.__name__\n        if use_args is None:\n            use_args = etype._use_args_\n        __new__ = cls.__dict__.get('__new__')\n        if __new__ is not None:\n            new_member = __new__.__func__\n        else:\n            new_member = etype._member_type_.__new__\n        attrs = {}\n        body = {}\n        if __new__ is not None:\n            body['__new_member__'] = new_member\n        body['_new_member_'] = new_member\n        body['_use_args_'] = use_args\n        body['_generate_next_value_'] = gnv = etype._generate_next_value_\n        body['_member_names_'] = member_names = []\n        body['_member_map_'] = member_map = {}\n        body['_value2member_map_'] = value2member_map = {}\n        body['_unhashable_values_'] = []\n        body['_member_type_'] = member_type = etype._member_type_\n        body['_value_repr_'] = etype._value_repr_\n        if issubclass(etype, Flag):\n            body['_boundary_'] = boundary or etype._boundary_\n            body['_flag_mask_'] = None\n            body['_all_bits_'] = None\n            body['_singles_mask_'] = None\n            body['_inverted_'] = None\n            body['__or__'] = Flag.__or__\n            body['__xor__'] = Flag.__xor__\n            body['__and__'] = Flag.__and__\n            body['__ror__'] = Flag.__ror__\n            body['__rxor__'] = Flag.__rxor__\n            body['__rand__'] = Flag.__rand__\n            body['__invert__'] = Flag.__invert__\n        for name, obj in cls.__dict__.items():\n            if name in ('__dict__', '__weakref__'):\n                continue\n            if _is_dunder(name) or _is_private(cls_name, name) or _is_sunder(name) or _is_descriptor(obj):\n                body[name] = obj\n            else:\n                attrs[name] = obj\n        if cls.__dict__.get('__doc__') is None:\n            body['__doc__'] = 'An enumeration.'\n        #\n        # double check that repr and friends are not the mixin's or various\n        # things break (such as pickle)\n        # however, if the method is defined in the Enum itself, don't replace\n        # it\n        enum_class = type(cls_name, (etype, ), body, boundary=boundary, _simple=True)\n        for name in ('__repr__', '__str__', '__format__', '__reduce_ex__'):\n            if name not in body:\n                # check for mixin overrides before replacing\n                enum_method = getattr(etype, name)\n                found_method = getattr(enum_class, name)\n                object_method = getattr(object, name)\n                data_type_method = getattr(member_type, name)\n                if found_method in (data_type_method, object_method):\n                    setattr(enum_class, name, enum_method)\n        gnv_last_values = []\n        if issubclass(enum_class, Flag):\n            # Flag / IntFlag\n            single_bits = multi_bits = 0\n            for name, value in attrs.items():\n                if isinstance(value, auto) and auto.value is _auto_null:\n                    value = gnv(name, 1, len(member_names), gnv_last_values)\n                if value in value2member_map:\n                    # an alias to an existing member\n                    redirect = property()\n                    redirect.__set_name__(enum_class, name)\n                    setattr(enum_class, name, redirect)\n                    member_map[name] = value2member_map[value]\n                else:\n                    # create the member\n                    if use_args:\n                        if not isinstance(value, tuple):\n                            value = (value, )\n                        member = new_member(enum_class, *value)\n                        value = value[0]\n                    else:\n                        member = new_member(enum_class)\n                    if __new__ is None:\n                        member._value_ = value\n                    member._name_ = name\n                    member.__objclass__ = enum_class\n                    member.__init__(value)\n                    redirect = property()\n                    redirect.__set_name__(enum_class, name)\n                    setattr(enum_class, name, redirect)\n                    member_map[name] = member\n                    member._sort_order_ = len(member_names)\n                    value2member_map[value] = member\n                    if _is_single_bit(value):\n                        # not a multi-bit alias, record in _member_names_ and _flag_mask_\n                        member_names.append(name)\n                        single_bits |= value\n                    else:\n                        multi_bits |= value\n                    gnv_last_values.append(value)\n            enum_class._flag_mask_ = single_bits | multi_bits\n            enum_class._singles_mask_ = single_bits\n            enum_class._all_bits_ = 2 ** ((single_bits|multi_bits).bit_length()) - 1\n            # set correct __iter__\n            member_list = [m._value_ for m in enum_class]\n            if member_list != sorted(member_list):\n                enum_class._iter_member_ = enum_class._iter_member_by_def_\n        else:\n            # Enum / IntEnum / StrEnum\n            for name, value in attrs.items():\n                if isinstance(value, auto):\n                    if value.value is _auto_null:\n                        value.value = gnv(name, 1, len(member_names), gnv_last_values)\n                    value = value.value\n                if value in value2member_map:\n                    # an alias to an existing member\n                    redirect = property()\n                    redirect.__set_name__(enum_class, name)\n                    setattr(enum_class, name, redirect)\n                    member_map[name] = value2member_map[value]\n                else:\n                    # create the member\n                    if use_args:\n                        if not isinstance(value, tuple):\n                            value = (value, )\n                        member = new_member(enum_class, *value)\n                        value = value[0]\n                    else:\n                        member = new_member(enum_class)\n                    if __new__ is None:\n                        member._value_ = value\n                    member._name_ = name\n                    member.__objclass__ = enum_class\n                    member.__init__(value)\n                    member._sort_order_ = len(member_names)\n                    redirect = property()\n                    redirect.__set_name__(enum_class, name)\n                    setattr(enum_class, name, redirect)\n                    member_map[name] = member\n                    value2member_map[value] = member\n                    member_names.append(name)\n                    gnv_last_values.append(value)\n        if '__new__' in body:\n            enum_class.__new_member__ = enum_class.__new__\n        enum_class.__new__ = Enum.__new__\n        return enum_class\n    return convert_class\n\n@_simple_enum(StrEnum)\nclass EnumCheck:\n    \"\"\"\n    various conditions to check an enumeration for\n    \"\"\"\n    CONTINUOUS = \"no skipped integer values\"\n    NAMED_FLAGS = \"multi-flag aliases may not contain unnamed flags\"\n    UNIQUE = \"one name per value\"\nCONTINUOUS, NAMED_FLAGS, UNIQUE = EnumCheck\n\n\nclass verify:\n    \"\"\"\n    Check an enumeration for various constraints. (see EnumCheck)\n    \"\"\"\n    def __init__(self, *checks):\n        self.checks = checks\n    def __call__(self, enumeration):\n        checks = self.checks\n        cls_name = enumeration.__name__\n        if Flag is not None and issubclass(enumeration, Flag):\n            enum_type = 'flag'\n        elif issubclass(enumeration, Enum):\n            enum_type = 'enum'\n        else:\n            raise TypeError(\"the 'verify' decorator only works with Enum and Flag\")\n        for check in checks:\n            if check is UNIQUE:\n                # check for duplicate names\n                duplicates = []\n                for name, member in enumeration.__members__.items():\n                    if name != member.name:\n                        duplicates.append((name, member.name))\n                if duplicates:\n                    alias_details = ', '.join(\n                            [\"%s -> %s\" % (alias, name) for (alias, name) in duplicates])\n                    raise ValueError('aliases found in %r: %s' %\n                            (enumeration, alias_details))\n            elif check is CONTINUOUS:\n                values = set(e.value for e in enumeration)\n                if len(values) < 2:\n                    continue\n                low, high = min(values), max(values)\n                missing = []\n                if enum_type == 'flag':\n                    # check for powers of two\n                    for i in range(_high_bit(low)+1, _high_bit(high)):\n                        if 2**i not in values:\n                            missing.append(2**i)\n                elif enum_type == 'enum':\n                    # check for powers of one\n                    for i in range(low+1, high):\n                        if i not in values:\n                            missing.append(i)\n                else:\n                    raise Exception('verify: unknown type %r' % enum_type)\n                if missing:\n                    raise ValueError(('invalid %s %r: missing values %s' % (\n                            enum_type, cls_name, ', '.join((str(m) for m in missing)))\n                            )[:256])\n                            # limit max length to protect against DOS attacks\n            elif check is NAMED_FLAGS:\n                # examine each alias and check for unnamed flags\n                member_names = enumeration._member_names_\n                member_values = [m.value for m in enumeration]\n                missing_names = []\n                missing_value = 0\n                for name, alias in enumeration._member_map_.items():\n                    if name in member_names:\n                        # not an alias\n                        continue\n                    if alias.value < 0:\n                        # negative numbers are not checked\n                        continue\n                    values = list(_iter_bits_lsb(alias.value))\n                    missed = [v for v in values if v not in member_values]\n                    if missed:\n                        missing_names.append(name)\n                        missing_value |= reduce(_or_, missed)\n                if missing_names:\n                    if len(missing_names) == 1:\n                        alias = 'alias %s is missing' % missing_names[0]\n                    else:\n                        alias = 'aliases %s and %s are missing' % (\n                                ', '.join(missing_names[:-1]), missing_names[-1]\n                                )\n                    if _is_single_bit(missing_value):\n                        value = 'value 0x%x' % missing_value\n                    else:\n                        value = 'combined values of 0x%x' % missing_value\n                    raise ValueError(\n                            'invalid Flag %r: %s %s [use enum.show_flag_values(value) for details]'\n                            % (cls_name, alias, value)\n                            )\n        return enumeration\n\ndef _test_simple_enum(checked_enum, simple_enum):\n    \"\"\"\n    A function that can be used to test an enum created with :func:`_simple_enum`\n    against the version created by subclassing :class:`Enum`::\n\n        >>> from enum import Enum, _simple_enum, _test_simple_enum\n        >>> @_simple_enum(Enum)\n        ... class Color:\n        ...     RED = auto()\n        ...     GREEN = auto()\n        ...     BLUE = auto()\n        >>> class CheckedColor(Enum):\n        ...     RED = auto()\n        ...     GREEN = auto()\n        ...     BLUE = auto()\n        >>> _test_simple_enum(CheckedColor, Color)\n\n    If differences are found, a :exc:`TypeError` is raised.\n    \"\"\"\n    failed = []\n    if checked_enum.__dict__ != simple_enum.__dict__:\n        checked_dict = checked_enum.__dict__\n        checked_keys = list(checked_dict.keys())\n        simple_dict = simple_enum.__dict__\n        simple_keys = list(simple_dict.keys())\n        member_names = set(\n                list(checked_enum._member_map_.keys())\n                + list(simple_enum._member_map_.keys())\n                )\n        for key in set(checked_keys + simple_keys):\n            if key in ('__module__', '_member_map_', '_value2member_map_', '__doc__'):\n                # keys known to be different, or very long\n                continue\n            elif key in member_names:\n                # members are checked below\n                continue\n            elif key not in simple_keys:\n                failed.append(\"missing key: %r\" % (key, ))\n            elif key not in checked_keys:\n                failed.append(\"extra key:   %r\" % (key, ))\n            else:\n                checked_value = checked_dict[key]\n                simple_value = simple_dict[key]\n                if callable(checked_value) or isinstance(checked_value, bltns.property):\n                    continue\n                if key == '__doc__':\n                    # remove all spaces/tabs\n                    compressed_checked_value = checked_value.replace(' ','').replace('\\t','')\n                    compressed_simple_value = simple_value.replace(' ','').replace('\\t','')\n                    if compressed_checked_value != compressed_simple_value:\n                        failed.append(\"%r:\\n         %s\\n         %s\" % (\n                                key,\n                                \"checked -> %r\" % (checked_value, ),\n                                \"simple  -> %r\" % (simple_value, ),\n                                ))\n                elif checked_value != simple_value:\n                    failed.append(\"%r:\\n         %s\\n         %s\" % (\n                            key,\n                            \"checked -> %r\" % (checked_value, ),\n                            \"simple  -> %r\" % (simple_value, ),\n                            ))\n        failed.sort()\n        for name in member_names:\n            failed_member = []\n            if name not in simple_keys:\n                failed.append('missing member from simple enum: %r' % name)\n            elif name not in checked_keys:\n                failed.append('extra member in simple enum: %r' % name)\n            else:\n                checked_member_dict = checked_enum[name].__dict__\n                checked_member_keys = list(checked_member_dict.keys())\n                simple_member_dict = simple_enum[name].__dict__\n                simple_member_keys = list(simple_member_dict.keys())\n                for key in set(checked_member_keys + simple_member_keys):\n                    if key in ('__module__', '__objclass__', '_inverted_'):\n                        # keys known to be different or absent\n                        continue\n                    elif key not in simple_member_keys:\n                        failed_member.append(\"missing key %r not in the simple enum member %r\" % (key, name))\n                    elif key not in checked_member_keys:\n                        failed_member.append(\"extra key %r in simple enum member %r\" % (key, name))\n                    else:\n                        checked_value = checked_member_dict[key]\n                        simple_value = simple_member_dict[key]\n                        if checked_value != simple_value:\n                            failed_member.append(\"%r:\\n         %s\\n         %s\" % (\n                                    key,\n                                    \"checked member -> %r\" % (checked_value, ),\n                                    \"simple member  -> %r\" % (simple_value, ),\n                                    ))\n            if failed_member:\n                failed.append('%r member mismatch:\\n      %s' % (\n                        name, '\\n      '.join(failed_member),\n                        ))\n        for method in (\n                '__str__', '__repr__', '__reduce_ex__', '__format__',\n                '__getnewargs_ex__', '__getnewargs__', '__reduce_ex__', '__reduce__'\n            ):\n            if method in simple_keys and method in checked_keys:\n                # cannot compare functions, and it exists in both, so we're good\n                continue\n            elif method not in simple_keys and method not in checked_keys:\n                # method is inherited -- check it out\n                checked_method = getattr(checked_enum, method, None)\n                simple_method = getattr(simple_enum, method, None)\n                if hasattr(checked_method, '__func__'):\n                    checked_method = checked_method.__func__\n                    simple_method = simple_method.__func__\n                if checked_method != simple_method:\n                    failed.append(\"%r:  %-30s %s\" % (\n                            method,\n                            \"checked -> %r\" % (checked_method, ),\n                            \"simple -> %r\" % (simple_method, ),\n                            ))\n            else:\n                # if the method existed in only one of the enums, it will have been caught\n                # in the first checks above\n                pass\n    if failed:\n        raise TypeError('enum mismatch:\\n   %s' % '\\n   '.join(failed))\n\ndef _old_convert_(etype, name, module, filter, source=None, *, boundary=None):\n    \"\"\"\n    Create a new Enum subclass that replaces a collection of global constants\n    \"\"\"\n    # convert all constants from source (or module) that pass filter() to\n    # a new Enum called name, and export the enum and its members back to\n    # module;\n    # also, replace the __reduce_ex__ method so unpickling works in\n    # previous Python versions\n    module_globals = sys.modules[module].__dict__\n    if source:\n        source = source.__dict__\n    else:\n        source = module_globals\n    # _value2member_map_ is populated in the same order every time\n    # for a consistent reverse mapping of number to name when there\n    # are multiple names for the same number.\n    members = [\n            (name, value)\n            for name, value in source.items()\n            if filter(name)]\n    try:\n        # sort by value\n        members.sort(key=lambda t: (t[1], t[0]))\n    except TypeError:\n        # unless some values aren't comparable, in which case sort by name\n        members.sort(key=lambda t: t[0])\n    cls = etype(name, members, module=module, boundary=boundary or KEEP)\n    return cls\n\n_stdlib_enums = IntEnum, StrEnum, IntFlag\n", 2063], "/app/app/providers/registry.py": ["from __future__ import annotations\n\nimport time\nfrom collections import deque\nfrom dataclasses import dataclass, field\nfrom typing import Deque, Dict, List, Optional, Set, Tuple\n\nfrom app.circuit_breaker import CircuitBreakerManager\nfrom app.core.config import get_settings\nfrom app.observability.metrics import (\n    CIRCUIT_STATE,\n    PROVIDER_ERRORS,\n    PROVIDER_LATENCY,\n    PROVIDER_SELECTED,\n)\nfrom app.providers.base import DataProvider\n\n\n@dataclass\nclass RollingStats:\n    \"\"\"EWMA approximations for routing inputs.\"\"\"\n\n    alpha: float = 0.3\n    latency_p95_ms: float = 0.0\n    error_ewma: float = 0.0\n    completeness_deficit: float = 0.0\n\n    def update_latency(self, sample_ms: float) -> None:\n        self.latency_p95_ms = (1 - self.alpha) * self.latency_p95_ms + self.alpha * sample_ms\n\n    def update_error(self, is_error: bool) -> None:\n        x = 1.0 if is_error else 0.0\n        self.error_ewma = (1 - self.alpha) * self.error_ewma + self.alpha * x\n\n    def set_completeness(self, deficit_0_1: float) -> None:\n        self.completeness_deficit = max(0.0, min(1.0, deficit_0_1))\n\n\n@dataclass\nclass ProviderEntry:\n    name: str\n    adapter: DataProvider\n    capabilities: Set[str]\n    stats: RollingStats = field(default_factory=RollingStats)\n    h_history: Deque[Tuple[float, float]] = field(default_factory=lambda: deque(maxlen=60))\n\n\nclass ProviderRegistry:\n    \"\"\"Runtime registry that ranks providers per capability using deterministic policies.\"\"\"\n\n    def __init__(self, breakers: CircuitBreakerManager):\n        self.breakers = breakers\n        self.providers: Dict[str, ProviderEntry] = {}\n\n    def register(self, name: str, adapter: DataProvider, capabilities: Set[str]) -> None:\n        adapter.enabled = True\n        self.providers[name] = ProviderEntry(name=name, adapter=adapter, capabilities=capabilities)\n\n    def capabilities_map(self) -> Dict[str, List[str]]:\n        out: Dict[str, List[str]] = {}\n        for name, entry in self.providers.items():\n            for cap in entry.capabilities:\n                out.setdefault(cap, []).append(name)\n        return out\n\n    def _breaker_score(self, name: str) -> float:\n        state = self.breakers.get_state(name)\n        if state == \"CLOSED\":\n            return 1.0\n        if state == \"HALF_OPEN\":\n            return 0.4\n        return 0.0\n\n    def health_score(self, name: str) -> float:\n        settings = get_settings()\n        entry = self.providers[name]\n        breaker_component = self._breaker_score(name)\n        latency_cap = float(settings.RECENT_LATENCY_CAP_MS)\n        latency_norm = min(entry.stats.latency_p95_ms / latency_cap, 1.0) if latency_cap > 0 else 1.0\n        error_component = entry.stats.error_ewma\n        completeness_component = entry.stats.completeness_deficit\n        score = (\n            0.45 * breaker_component\n            + 0.25 * (1.0 - latency_norm)\n            + 0.20 * (1.0 - error_component)\n            + 0.10 * (1.0 - completeness_component)\n        )\n        return max(0.0, min(1.0, score))\n\n    def rank(self, capability: str, provider_hint: Optional[str] = None) -> List[str]:\n        settings = get_settings()\n        policy_key = {\n            \"bars_1m\": \"POLICY_BARS_1M\",\n            \"eod\": \"POLICY_EOD\",\n            \"quotes_l1\": \"POLICY_QUOTES_L1\",\n            \"options_chain\": \"POLICY_OPTIONS_CHAIN\",\n        }.get(capability)\n        if not policy_key:\n            return []\n\n        base_policy = list(getattr(settings, policy_key, []))\n\n        candidates: List[Tuple[str, float]] = []\n        for provider_name in base_policy:\n            entry = self.providers.get(provider_name)\n            if not entry:\n                continue\n            if capability not in entry.capabilities:\n                continue\n            if not entry.adapter.enabled:\n                continue\n            candidates.append((provider_name, self.health_score(provider_name)))\n\n        if provider_hint and settings.ALLOW_SOURCE_OVERRIDE:\n            hint = next((c for c in candidates if c[0] == provider_hint), None)\n            if hint:\n                state = self.breakers.get_state(provider_hint)\n                if hint[1] >= settings.BREAKER_DEMOTE_THRESHOLD and state != \"OPEN\":\n                    candidates.sort(key=lambda kv: 0 if kv[0] == provider_hint else 1)\n\n        base_index = {name: idx for idx, name in enumerate(base_policy)}\n        candidates.sort(key=lambda kv: (-kv[1], base_index.get(kv[0], len(base_policy))))\n\n        now = time.time()\n        ranked = []\n        for name, score in candidates:\n            ranked.append(name)\n            entry = self.providers[name]\n            entry.h_history.append((now, score))\n        return ranked\n\n    def record_selection(self, capability: str, provider_name: str) -> None:\n        PROVIDER_SELECTED.labels(capability=capability, provider=provider_name).inc()\n        state = self.breakers.get_state(provider_name)\n        CIRCUIT_STATE.labels(provider=provider_name).set(\n            1.0 if state == \"CLOSED\" else 0.5 if state == \"HALF_OPEN\" else 0.0\n        )\n\n    def record_outcome(self, name: str, latency_ms: float, error: bool, endpoint: str = \"generic\") -> None:\n        entry = self.providers.get(name)\n        if not entry:\n            return\n        entry.stats.update_latency(latency_ms)\n        entry.stats.update_error(error)\n        PROVIDER_LATENCY.labels(provider=name, endpoint=endpoint).observe(latency_ms)\n        state = self.breakers.get_state(name)\n        CIRCUIT_STATE.labels(provider=name).set(\n            1.0 if state == \"CLOSED\" else 0.5 if state == \"HALF_OPEN\" else 0.0\n        )\n        entry.h_history.append((time.time(), self.health_score(name)))\n\n    def record_error(self, provider: str, endpoint: str, code: str) -> None:\n        PROVIDER_ERRORS.labels(provider=provider, endpoint=endpoint, code=code).inc()\n\n    def set_completeness(self, name: str, deficit_0_1: float) -> None:\n        entry = self.providers.get(name)\n        if not entry:\n            return\n        entry.stats.set_completeness(deficit_0_1)\n", 159], "/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py": ["import os\nfrom threading import Lock\nimport time\nimport types\nfrom typing import (\n    Any, Callable, Dict, Iterable, List, Literal, Optional, Sequence, Tuple,\n    Type, TypeVar, Union,\n)\nimport warnings\n\nfrom . import values  # retain this import style for testability\nfrom .context_managers import ExceptionCounter, InprogressTracker, Timer\nfrom .metrics_core import Metric\nfrom .registry import Collector, CollectorRegistry, REGISTRY\nfrom .samples import Exemplar, Sample\nfrom .utils import floatToGoString, INF\nfrom .validation import (\n    _validate_exemplar, _validate_labelnames, _validate_metric_name,\n)\n\nT = TypeVar('T', bound='MetricWrapperBase')\nF = TypeVar(\"F\", bound=Callable[..., Any])\n\n\ndef _build_full_name(metric_type, name, namespace, subsystem, unit):\n    if not name:\n        raise ValueError('Metric name should not be empty')\n    full_name = ''\n    if namespace:\n        full_name += namespace + '_'\n    if subsystem:\n        full_name += subsystem + '_'\n    full_name += name\n    if metric_type == 'counter' and full_name.endswith('_total'):\n        full_name = full_name[:-6]  # Munge to OpenMetrics.\n    if unit and not full_name.endswith(\"_\" + unit):\n        full_name += \"_\" + unit\n    if unit and metric_type in ('info', 'stateset'):\n        raise ValueError('Metric name is of a type that cannot have a unit: ' + full_name)\n    return full_name\n\n\n\ndef _get_use_created() -> bool:\n    return os.environ.get(\"PROMETHEUS_DISABLE_CREATED_SERIES\", 'False').lower() not in ('true', '1', 't')\n\n\n_use_created = _get_use_created()\n\n\ndef disable_created_metrics():\n    \"\"\"Disable exporting _created metrics on counters, histograms, and summaries.\"\"\"\n    global _use_created\n    _use_created = False\n\n\ndef enable_created_metrics():\n    \"\"\"Enable exporting _created metrics on counters, histograms, and summaries.\"\"\"\n    global _use_created\n    _use_created = True\n\n\nclass MetricWrapperBase(Collector):\n    _type: Optional[str] = None\n    _reserved_labelnames: Sequence[str] = ()\n\n    def _is_observable(self):\n        # Whether this metric is observable, i.e.\n        # * a metric without label names and values, or\n        # * the child of a labelled metric.\n        return not self._labelnames or (self._labelnames and self._labelvalues)\n\n    def _raise_if_not_observable(self):\n        # Functions that mutate the state of the metric, for example incrementing\n        # a counter, will fail if the metric is not observable, because only if a\n        # metric is observable will the value be initialized.\n        if not self._is_observable():\n            raise ValueError('%s metric is missing label values' % str(self._type))\n\n    def _is_parent(self):\n        return self._labelnames and not self._labelvalues\n\n    def _get_metric(self):\n        return Metric(self._name, self._documentation, self._type, self._unit)\n\n    def describe(self) -> Iterable[Metric]:\n        return [self._get_metric()]\n\n    def collect(self) -> Iterable[Metric]:\n        metric = self._get_metric()\n        for suffix, labels, value, timestamp, exemplar, native_histogram_value in self._samples():\n            metric.add_sample(self._name + suffix, labels, value, timestamp, exemplar, native_histogram_value)\n        return [metric]\n\n    def __str__(self) -> str:\n        return f\"{self._type}:{self._name}\"\n\n    def __repr__(self) -> str:\n        metric_type = type(self)\n        return f\"{metric_type.__module__}.{metric_type.__name__}({self._name})\"\n\n    def __init__(self: T,\n                 name: str,\n                 documentation: str,\n                 labelnames: Iterable[str] = (),\n                 namespace: str = '',\n                 subsystem: str = '',\n                 unit: str = '',\n                 registry: Optional[CollectorRegistry] = REGISTRY,\n                 _labelvalues: Optional[Sequence[str]] = None,\n                 ) -> None:\n        self._name = _build_full_name(self._type, name, namespace, subsystem, unit)\n        self._labelnames = _validate_labelnames(self, labelnames)\n        self._labelvalues = tuple(_labelvalues or ())\n        self._kwargs: Dict[str, Any] = {}\n        self._documentation = documentation\n        self._unit = unit\n\n        _validate_metric_name(self._name)\n\n        if self._is_parent():\n            # Prepare the fields needed for child metrics.\n            self._lock = Lock()\n            self._metrics: Dict[Sequence[str], T] = {}\n\n        if self._is_observable():\n            self._metric_init()\n\n        if not self._labelvalues:\n            # Register the multi-wrapper parent metric, or if a label-less metric, the whole shebang.\n            if registry:\n                registry.register(self)\n\n    def labels(self: T, *labelvalues: Any, **labelkwargs: Any) -> T:\n        \"\"\"Return the child for the given labelset.\n\n        All metrics can have labels, allowing grouping of related time series.\n        Taking a counter as an example:\n\n            from prometheus_client import Counter\n\n            c = Counter('my_requests_total', 'HTTP Failures', ['method', 'endpoint'])\n            c.labels('get', '/').inc()\n            c.labels('post', '/submit').inc()\n\n        Labels can also be provided as keyword arguments:\n\n            from prometheus_client import Counter\n\n            c = Counter('my_requests_total', 'HTTP Failures', ['method', 'endpoint'])\n            c.labels(method='get', endpoint='/').inc()\n            c.labels(method='post', endpoint='/submit').inc()\n\n        See the best practices on [naming](http://prometheus.io/docs/practices/naming/)\n        and [labels](http://prometheus.io/docs/practices/instrumentation/#use-labels).\n        \"\"\"\n        if not self._labelnames:\n            raise ValueError('No label names were set when constructing %s' % self)\n\n        if self._labelvalues:\n            raise ValueError('{} already has labels set ({}); can not chain calls to .labels()'.format(\n                self,\n                dict(zip(self._labelnames, self._labelvalues))\n            ))\n\n        if labelvalues and labelkwargs:\n            raise ValueError(\"Can't pass both *args and **kwargs\")\n\n        if labelkwargs:\n            if sorted(labelkwargs) != sorted(self._labelnames):\n                raise ValueError('Incorrect label names')\n            labelvalues = tuple(str(labelkwargs[l]) for l in self._labelnames)\n        else:\n            if len(labelvalues) != len(self._labelnames):\n                raise ValueError('Incorrect label count')\n            labelvalues = tuple(str(l) for l in labelvalues)\n        with self._lock:\n            if labelvalues not in self._metrics:\n                self._metrics[labelvalues] = self.__class__(\n                    self._name,\n                    documentation=self._documentation,\n                    labelnames=self._labelnames,\n                    unit=self._unit,\n                    _labelvalues=labelvalues,\n                    **self._kwargs\n                )\n            return self._metrics[labelvalues]\n\n    def remove(self, *labelvalues: Any) -> None:\n        if 'prometheus_multiproc_dir' in os.environ or 'PROMETHEUS_MULTIPROC_DIR' in os.environ:\n            warnings.warn(\n                \"Removal of labels has not been implemented in  multi-process mode yet.\",\n                UserWarning)\n\n        if not self._labelnames:\n            raise ValueError('No label names were set when constructing %s' % self)\n\n        \"\"\"Remove the given labelset from the metric.\"\"\"\n        if len(labelvalues) != len(self._labelnames):\n            raise ValueError('Incorrect label count (expected %d, got %s)' % (len(self._labelnames), labelvalues))\n        labelvalues = tuple(str(l) for l in labelvalues)\n        with self._lock:\n            if labelvalues in self._metrics:\n                del self._metrics[labelvalues]\n\n    def clear(self) -> None:\n        \"\"\"Remove all labelsets from the metric\"\"\"\n        if 'prometheus_multiproc_dir' in os.environ or 'PROMETHEUS_MULTIPROC_DIR' in os.environ:\n            warnings.warn(\n                \"Clearing labels has not been implemented in multi-process mode yet\",\n                UserWarning)\n        with self._lock:\n            self._metrics = {}\n\n    def _samples(self) -> Iterable[Sample]:\n        if self._is_parent():\n            return self._multi_samples()\n        else:\n            return self._child_samples()\n\n    def _multi_samples(self) -> Iterable[Sample]:\n        with self._lock:\n            metrics = self._metrics.copy()\n        for labels, metric in metrics.items():\n            series_labels = list(zip(self._labelnames, labels))\n            for suffix, sample_labels, value, timestamp, exemplar, native_histogram_value in metric._samples():\n                yield Sample(suffix, dict(series_labels + list(sample_labels.items())), value, timestamp, exemplar, native_histogram_value)\n\n    def _child_samples(self) -> Iterable[Sample]:  # pragma: no cover\n        raise NotImplementedError('_child_samples() must be implemented by %r' % self)\n\n    def _metric_init(self):  # pragma: no cover\n        \"\"\"\n        Initialize the metric object as a child, i.e. when it has labels (if any) set.\n\n        This is factored as a separate function to allow for deferred initialization.\n        \"\"\"\n        raise NotImplementedError('_metric_init() must be implemented by %r' % self)\n\n\nclass Counter(MetricWrapperBase):\n    \"\"\"A Counter tracks counts of events or running totals.\n\n    Example use cases for Counters:\n    - Number of requests processed\n    - Number of items that were inserted into a queue\n    - Total amount of data that a system has processed\n\n    Counters can only go up (and be reset when the process restarts). If your use case can go down,\n    you should use a Gauge instead.\n\n    An example for a Counter:\n\n        from prometheus_client import Counter\n\n        c = Counter('my_failures_total', 'Description of counter')\n        c.inc()     # Increment by 1\n        c.inc(1.6)  # Increment by given value\n\n    There are utilities to count exceptions raised:\n\n        @c.count_exceptions()\n        def f():\n            pass\n\n        with c.count_exceptions():\n            pass\n\n        # Count only one type of exception\n        with c.count_exceptions(ValueError):\n            pass\n            \n    You can also reset the counter to zero in case your logical \"process\" restarts\n    without restarting the actual python process.\n\n       c.reset()\n\n    \"\"\"\n    _type = 'counter'\n\n    def _metric_init(self) -> None:\n        self._value = values.ValueClass(self._type, self._name, self._name + '_total', self._labelnames,\n                                        self._labelvalues, self._documentation)\n        self._created = time.time()\n\n    def inc(self, amount: float = 1, exemplar: Optional[Dict[str, str]] = None) -> None:\n        \"\"\"Increment counter by the given amount.\"\"\"\n        self._raise_if_not_observable()\n        if amount < 0:\n            raise ValueError('Counters can only be incremented by non-negative amounts.')\n        self._value.inc(amount)\n        if exemplar:\n            _validate_exemplar(exemplar)\n            self._value.set_exemplar(Exemplar(exemplar, amount, time.time()))\n\n    def reset(self) -> None:\n        \"\"\"Reset the counter to zero. Use this when a logical process restarts without restarting the actual python process.\"\"\"\n        self._value.set(0)\n        self._created = time.time()\n\n    def count_exceptions(self, exception: Union[Type[BaseException], Tuple[Type[BaseException], ...]] = Exception) -> ExceptionCounter:\n        \"\"\"Count exceptions in a block of code or function.\n\n        Can be used as a function decorator or context manager.\n        Increments the counter when an exception of the given\n        type is raised up out of the code.\n        \"\"\"\n        self._raise_if_not_observable()\n        return ExceptionCounter(self, exception)\n\n    def _child_samples(self) -> Iterable[Sample]:\n        sample = Sample('_total', {}, self._value.get(), None, self._value.get_exemplar())\n        if _use_created:\n            return (\n                sample,\n                Sample('_created', {}, self._created, None, None)\n            )\n        return (sample,)\n\n\nclass Gauge(MetricWrapperBase):\n    \"\"\"Gauge metric, to report instantaneous values.\n\n     Examples of Gauges include:\n        - Inprogress requests\n        - Number of items in a queue\n        - Free memory\n        - Total memory\n        - Temperature\n\n     Gauges can go both up and down.\n\n        from prometheus_client import Gauge\n\n        g = Gauge('my_inprogress_requests', 'Description of gauge')\n        g.inc()      # Increment by 1\n        g.dec(10)    # Decrement by given value\n        g.set(4.2)   # Set to a given value\n\n     There are utilities for common use cases:\n\n        g.set_to_current_time()   # Set to current unixtime\n\n        # Increment when entered, decrement when exited.\n        @g.track_inprogress()\n        def f():\n            pass\n\n        with g.track_inprogress():\n            pass\n\n     A Gauge can also take its value from a callback:\n\n        d = Gauge('data_objects', 'Number of objects')\n        my_dict = {}\n        d.set_function(lambda: len(my_dict))\n    \"\"\"\n    _type = 'gauge'\n    _MULTIPROC_MODES = frozenset(('all', 'liveall', 'min', 'livemin', 'max', 'livemax', 'sum', 'livesum', 'mostrecent', 'livemostrecent'))\n    _MOST_RECENT_MODES = frozenset(('mostrecent', 'livemostrecent'))\n\n    def __init__(self,\n                 name: str,\n                 documentation: str,\n                 labelnames: Iterable[str] = (),\n                 namespace: str = '',\n                 subsystem: str = '',\n                 unit: str = '',\n                 registry: Optional[CollectorRegistry] = REGISTRY,\n                 _labelvalues: Optional[Sequence[str]] = None,\n                 multiprocess_mode: Literal['all', 'liveall', 'min', 'livemin', 'max', 'livemax', 'sum', 'livesum', 'mostrecent', 'livemostrecent'] = 'all',\n                 ):\n        self._multiprocess_mode = multiprocess_mode\n        if multiprocess_mode not in self._MULTIPROC_MODES:\n            raise ValueError('Invalid multiprocess mode: ' + multiprocess_mode)\n        super().__init__(\n            name=name,\n            documentation=documentation,\n            labelnames=labelnames,\n            namespace=namespace,\n            subsystem=subsystem,\n            unit=unit,\n            registry=registry,\n            _labelvalues=_labelvalues,\n        )\n        self._kwargs['multiprocess_mode'] = self._multiprocess_mode\n        self._is_most_recent = self._multiprocess_mode in self._MOST_RECENT_MODES\n\n    def _metric_init(self) -> None:\n        self._value = values.ValueClass(\n            self._type, self._name, self._name, self._labelnames, self._labelvalues,\n            self._documentation, multiprocess_mode=self._multiprocess_mode\n        )\n\n    def inc(self, amount: float = 1) -> None:\n        \"\"\"Increment gauge by the given amount.\"\"\"\n        if self._is_most_recent:\n            raise RuntimeError(\"inc must not be used with the mostrecent mode\")\n        self._raise_if_not_observable()\n        self._value.inc(amount)\n\n    def dec(self, amount: float = 1) -> None:\n        \"\"\"Decrement gauge by the given amount.\"\"\"\n        if self._is_most_recent:\n            raise RuntimeError(\"dec must not be used with the mostrecent mode\")\n        self._raise_if_not_observable()\n        self._value.inc(-amount)\n\n    def set(self, value: float) -> None:\n        \"\"\"Set gauge to the given value.\"\"\"\n        self._raise_if_not_observable()\n        if self._is_most_recent:\n            self._value.set(float(value), timestamp=time.time())\n        else:\n            self._value.set(float(value))\n\n    def set_to_current_time(self) -> None:\n        \"\"\"Set gauge to the current unixtime.\"\"\"\n        self.set(time.time())\n\n    def track_inprogress(self) -> InprogressTracker:\n        \"\"\"Track inprogress blocks of code or functions.\n\n        Can be used as a function decorator or context manager.\n        Increments the gauge when the code is entered,\n        and decrements when it is exited.\n        \"\"\"\n        self._raise_if_not_observable()\n        return InprogressTracker(self)\n\n    def time(self) -> Timer:\n        \"\"\"Time a block of code or function, and set the duration in seconds.\n\n        Can be used as a function decorator or context manager.\n        \"\"\"\n        return Timer(self, 'set')\n\n    def set_function(self, f: Callable[[], float]) -> None:\n        \"\"\"Call the provided function to return the Gauge value.\n\n        The function must return a float, and may be called from\n        multiple threads. All other methods of the Gauge become NOOPs.\n        \"\"\"\n\n        self._raise_if_not_observable()\n\n        def samples(_: Gauge) -> Iterable[Sample]:\n            return (Sample('', {}, float(f()), None, None),)\n\n        self._child_samples = types.MethodType(samples, self)  # type: ignore\n\n    def _child_samples(self) -> Iterable[Sample]:\n        return (Sample('', {}, self._value.get(), None, None),)\n\n\nclass Summary(MetricWrapperBase):\n    \"\"\"A Summary tracks the size and number of events.\n\n    Example use cases for Summaries:\n    - Response latency\n    - Request size\n\n    Example for a Summary:\n\n        from prometheus_client import Summary\n\n        s = Summary('request_size_bytes', 'Request size (bytes)')\n        s.observe(512)  # Observe 512 (bytes)\n\n    Example for a Summary using time:\n\n        from prometheus_client import Summary\n\n        REQUEST_TIME = Summary('response_latency_seconds', 'Response latency (seconds)')\n\n        @REQUEST_TIME.time()\n        def create_response(request):\n          '''A dummy function'''\n          time.sleep(1)\n\n    Example for using the same Summary object as a context manager:\n\n        with REQUEST_TIME.time():\n            pass  # Logic to be timed\n    \"\"\"\n    _type = 'summary'\n    _reserved_labelnames = ['quantile']\n\n    def _metric_init(self) -> None:\n        self._count = values.ValueClass(self._type, self._name, self._name + '_count', self._labelnames,\n                                        self._labelvalues, self._documentation)\n        self._sum = values.ValueClass(self._type, self._name, self._name + '_sum', self._labelnames, self._labelvalues, self._documentation)\n        self._created = time.time()\n\n    def observe(self, amount: float) -> None:\n        \"\"\"Observe the given amount.\n\n        The amount is usually positive or zero. Negative values are\n        accepted but prevent current versions of Prometheus from\n        properly detecting counter resets in the sum of\n        observations. See\n        https://prometheus.io/docs/practices/histograms/#count-and-sum-of-observations\n        for details.\n        \"\"\"\n        self._raise_if_not_observable()\n        self._count.inc(1)\n        self._sum.inc(amount)\n\n    def time(self) -> Timer:\n        \"\"\"Time a block of code or function, and observe the duration in seconds.\n\n        Can be used as a function decorator or context manager.\n        \"\"\"\n        return Timer(self, 'observe')\n\n    def _child_samples(self) -> Iterable[Sample]:\n        samples = [\n            Sample('_count', {}, self._count.get(), None, None),\n            Sample('_sum', {}, self._sum.get(), None, None),\n        ]\n        if _use_created:\n            samples.append(Sample('_created', {}, self._created, None, None))\n        return tuple(samples)\n\n\nclass Histogram(MetricWrapperBase):\n    \"\"\"A Histogram tracks the size and number of events in buckets.\n\n    You can use Histograms for aggregatable calculation of quantiles.\n\n    Example use cases:\n    - Response latency\n    - Request size\n\n    Example for a Histogram:\n\n        from prometheus_client import Histogram\n\n        h = Histogram('request_size_bytes', 'Request size (bytes)')\n        h.observe(512)  # Observe 512 (bytes)\n\n    Example for a Histogram using time:\n\n        from prometheus_client import Histogram\n\n        REQUEST_TIME = Histogram('response_latency_seconds', 'Response latency (seconds)')\n\n        @REQUEST_TIME.time()\n        def create_response(request):\n          '''A dummy function'''\n          time.sleep(1)\n\n    Example of using the same Histogram object as a context manager:\n\n        with REQUEST_TIME.time():\n            pass  # Logic to be timed\n\n    The default buckets are intended to cover a typical web/rpc request from milliseconds to seconds.\n    They can be overridden by passing `buckets` keyword argument to `Histogram`.\n    \"\"\"\n    _type = 'histogram'\n    _reserved_labelnames = ['le']\n    DEFAULT_BUCKETS = (.005, .01, .025, .05, .075, .1, .25, .5, .75, 1.0, 2.5, 5.0, 7.5, 10.0, INF)\n\n    def __init__(self,\n                 name: str,\n                 documentation: str,\n                 labelnames: Iterable[str] = (),\n                 namespace: str = '',\n                 subsystem: str = '',\n                 unit: str = '',\n                 registry: Optional[CollectorRegistry] = REGISTRY,\n                 _labelvalues: Optional[Sequence[str]] = None,\n                 buckets: Sequence[Union[float, str]] = DEFAULT_BUCKETS,\n                 ):\n        self._prepare_buckets(buckets)\n        super().__init__(\n            name=name,\n            documentation=documentation,\n            labelnames=labelnames,\n            namespace=namespace,\n            subsystem=subsystem,\n            unit=unit,\n            registry=registry,\n            _labelvalues=_labelvalues,\n        )\n        self._kwargs['buckets'] = buckets\n\n    def _prepare_buckets(self, source_buckets: Sequence[Union[float, str]]) -> None:\n        buckets = [float(b) for b in source_buckets]\n        if buckets != sorted(buckets):\n            # This is probably an error on the part of the user,\n            # so raise rather than sorting for them.\n            raise ValueError('Buckets not in sorted order')\n        if buckets and buckets[-1] != INF:\n            buckets.append(INF)\n        if len(buckets) < 2:\n            raise ValueError('Must have at least two buckets')\n        self._upper_bounds = buckets\n\n    def _metric_init(self) -> None:\n        self._buckets: List[values.ValueClass] = []\n        self._created = time.time()\n        bucket_labelnames = self._labelnames + ('le',)\n        self._sum = values.ValueClass(self._type, self._name, self._name + '_sum', self._labelnames, self._labelvalues, self._documentation)\n        for b in self._upper_bounds:\n            self._buckets.append(values.ValueClass(\n                self._type,\n                self._name,\n                self._name + '_bucket',\n                bucket_labelnames,\n                self._labelvalues + (floatToGoString(b),),\n                self._documentation)\n            )\n\n    def observe(self, amount: float, exemplar: Optional[Dict[str, str]] = None) -> None:\n        \"\"\"Observe the given amount.\n\n        The amount is usually positive or zero. Negative values are\n        accepted but prevent current versions of Prometheus from\n        properly detecting counter resets in the sum of\n        observations. See\n        https://prometheus.io/docs/practices/histograms/#count-and-sum-of-observations\n        for details.\n        \"\"\"\n        self._raise_if_not_observable()\n        self._sum.inc(amount)\n        for i, bound in enumerate(self._upper_bounds):\n            if amount <= bound:\n                self._buckets[i].inc(1)\n                if exemplar:\n                    _validate_exemplar(exemplar)\n                    self._buckets[i].set_exemplar(Exemplar(exemplar, amount, time.time()))\n                break\n\n    def time(self) -> Timer:\n        \"\"\"Time a block of code or function, and observe the duration in seconds.\n\n        Can be used as a function decorator or context manager.\n        \"\"\"\n        return Timer(self, 'observe')\n\n    def _child_samples(self) -> Iterable[Sample]:\n        samples = []\n        acc = 0.0\n        for i, bound in enumerate(self._upper_bounds):\n            acc += self._buckets[i].get()\n            samples.append(Sample('_bucket', {'le': floatToGoString(bound)}, acc, None, self._buckets[i].get_exemplar()))\n        samples.append(Sample('_count', {}, acc, None, None))\n        if self._upper_bounds[0] >= 0:\n            samples.append(Sample('_sum', {}, self._sum.get(), None, None))\n        if _use_created:\n            samples.append(Sample('_created', {}, self._created, None, None))\n        return tuple(samples)\n\n\nclass Info(MetricWrapperBase):\n    \"\"\"Info metric, key-value pairs.\n\n     Examples of Info include:\n        - Build information\n        - Version information\n        - Potential target metadata\n\n     Example usage:\n        from prometheus_client import Info\n\n        i = Info('my_build', 'Description of info')\n        i.info({'version': '1.2.3', 'buildhost': 'foo@bar'})\n\n     Info metrics do not work in multiprocess mode.\n    \"\"\"\n    _type = 'info'\n\n    def _metric_init(self):\n        self._labelname_set = set(self._labelnames)\n        self._lock = Lock()\n        self._value = {}\n\n    def info(self, val: Dict[str, str]) -> None:\n        \"\"\"Set info metric.\"\"\"\n        if self._labelname_set.intersection(val.keys()):\n            raise ValueError('Overlapping labels for Info metric, metric: {} child: {}'.format(\n                self._labelnames, val))\n        if any(i is None for i in val.values()):\n            raise ValueError('Label value cannot be None')\n        with self._lock:\n            self._value = dict(val)\n\n    def _child_samples(self) -> Iterable[Sample]:\n        with self._lock:\n            return (Sample('_info', self._value, 1.0, None, None),)\n\n\nclass Enum(MetricWrapperBase):\n    \"\"\"Enum metric, which of a set of states is true.\n\n     Example usage:\n        from prometheus_client import Enum\n\n        e = Enum('task_state', 'Description of enum',\n          states=['starting', 'running', 'stopped'])\n        e.state('running')\n\n     The first listed state will be the default.\n     Enum metrics do not work in multiprocess mode.\n    \"\"\"\n    _type = 'stateset'\n\n    def __init__(self,\n                 name: str,\n                 documentation: str,\n                 labelnames: Sequence[str] = (),\n                 namespace: str = '',\n                 subsystem: str = '',\n                 unit: str = '',\n                 registry: Optional[CollectorRegistry] = REGISTRY,\n                 _labelvalues: Optional[Sequence[str]] = None,\n                 states: Optional[Sequence[str]] = None,\n                 ):\n        super().__init__(\n            name=name,\n            documentation=documentation,\n            labelnames=labelnames,\n            namespace=namespace,\n            subsystem=subsystem,\n            unit=unit,\n            registry=registry,\n            _labelvalues=_labelvalues,\n        )\n        if name in labelnames:\n            raise ValueError(f'Overlapping labels for Enum metric: {name}')\n        if not states:\n            raise ValueError(f'No states provided for Enum metric: {name}')\n        self._kwargs['states'] = self._states = states\n\n    def _metric_init(self) -> None:\n        self._value = 0\n        self._lock = Lock()\n\n    def state(self, state: str) -> None:\n        \"\"\"Set enum metric state.\"\"\"\n        self._raise_if_not_observable()\n        with self._lock:\n            self._value = self._states.index(state)\n\n    def _child_samples(self) -> Iterable[Sample]:\n        with self._lock:\n            return [\n                Sample('', {self._name: s}, 1 if i == self._value else 0, None, None)\n                for i, s\n                in enumerate(self._states)\n            ]\n", 753], "/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py": ["import os\nimport re\n\nMETRIC_NAME_RE = re.compile(r'^[a-zA-Z_:][a-zA-Z0-9_:]*$')\nMETRIC_LABEL_NAME_RE = re.compile(r'^[a-zA-Z_][a-zA-Z0-9_]*$')\nRESERVED_METRIC_LABEL_NAME_RE = re.compile(r'^__.*$')\n\n\ndef _init_legacy_validation() -> bool:\n    \"\"\"Retrieve name validation setting from environment.\"\"\"\n    return os.environ.get(\"PROMETHEUS_LEGACY_NAME_VALIDATION\", 'False').lower() in ('true', '1', 't')\n\n\n_legacy_validation = _init_legacy_validation()\n\n\ndef get_legacy_validation() -> bool:\n    \"\"\"Return the current status of the legacy validation setting.\"\"\"\n    return _legacy_validation\n\n\ndef disable_legacy_validation():\n    \"\"\"Disable legacy name validation, instead allowing all UTF8 characters.\"\"\"\n    global _legacy_validation\n    _legacy_validation = False\n\n\ndef enable_legacy_validation():\n    \"\"\"Enable legacy name validation instead of allowing all UTF8 characters.\"\"\"\n    global _legacy_validation\n    _legacy_validation = True\n\n\ndef _validate_metric_name(name: str) -> None:\n    \"\"\"Raises ValueError if the provided name is not a valid metric name.\n    \n    This check uses the global legacy validation setting to determine the validation scheme.\n    \"\"\"\n    if not name:\n        raise ValueError(\"metric name cannot be empty\")\n    if _legacy_validation:\n        if not METRIC_NAME_RE.match(name):\n            raise ValueError(\"invalid metric name \" + name)\n    try:\n        name.encode('utf-8')\n    except UnicodeDecodeError:\n        raise ValueError(\"invalid metric name \" + name)\n\n\ndef _is_valid_legacy_metric_name(name: str) -> bool:\n    \"\"\"Returns true if the provided metric name conforms to the legacy validation scheme.\"\"\"\n    if len(name) == 0:\n        return False\n    return METRIC_NAME_RE.match(name) is not None\n\n\ndef _validate_metric_label_name_token(tok: str) -> None:\n    \"\"\"Raises ValueError if a parsed label name token is invalid. \n    \n    UTF-8 names must be quoted.\n    \"\"\"\n    if not tok:\n        raise ValueError(\"invalid label name token \" + tok)\n    quoted = tok[0] == '\"' and tok[-1] == '\"'\n    if not quoted or _legacy_validation:\n        if not METRIC_LABEL_NAME_RE.match(tok):\n            raise ValueError(\"invalid label name token \" + tok)\n        return\n    try:\n        tok.encode('utf-8')\n    except UnicodeDecodeError:\n        raise ValueError(\"invalid label name token \" + tok)\n\n\ndef _validate_labelname(l):\n    \"\"\"Raises ValueError if the provided name is not a valid label name.\n    \n    This check uses the global legacy validation setting to determine the validation scheme.\n    \"\"\"\n    if get_legacy_validation():\n        if not METRIC_LABEL_NAME_RE.match(l):\n            raise ValueError('Invalid label metric name: ' + l)\n        if RESERVED_METRIC_LABEL_NAME_RE.match(l):\n            raise ValueError('Reserved label metric name: ' + l)\n    else:\n        try:\n            l.encode('utf-8')\n        except UnicodeDecodeError:\n            raise ValueError('Invalid label metric name: ' + l)\n        if RESERVED_METRIC_LABEL_NAME_RE.match(l):\n            raise ValueError('Reserved label metric name: ' + l)\n        \n\ndef _is_valid_legacy_labelname(l: str) -> bool:\n    \"\"\"Returns true if the provided label name conforms to the legacy validation scheme.\"\"\"\n    if len(l) == 0:\n        return False\n    if METRIC_LABEL_NAME_RE.match(l) is None:\n        return False\n    return RESERVED_METRIC_LABEL_NAME_RE.match(l) is None\n\n\ndef _validate_labelnames(cls, labelnames):\n    \"\"\"Raises ValueError if any of the provided names is not a valid label name.\n    \n    This check uses the global legacy validation setting to determine the validation scheme.\n    \"\"\"\n    labelnames = tuple(labelnames)\n    for l in labelnames:\n        _validate_labelname(l)\n        if l in cls._reserved_labelnames:\n            raise ValueError('Reserved label methe fric name: ' + l)\n    return labelnames\n\n\ndef _validate_exemplar(exemplar):\n    \"\"\"Raises ValueError if the exemplar is invalid.\"\"\"\n    runes = 0\n    for k, v in exemplar.items():\n        _validate_labelname(k)\n        runes += len(k)\n        runes += len(v)\n    if runes > 128:\n        raise ValueError('Exemplar labels have %d UTF-8 characters, exceeding the limit of 128')\n", 124], "/usr/local/lib/python3.11/site-packages/prometheus_client/values.py": ["import os\nfrom threading import Lock\nimport warnings\n\nfrom .mmap_dict import mmap_key, MmapedDict\n\n\nclass MutexValue:\n    \"\"\"A float protected by a mutex.\"\"\"\n\n    _multiprocess = False\n\n    def __init__(self, typ, metric_name, name, labelnames, labelvalues, help_text, **kwargs):\n        self._value = 0.0\n        self._exemplar = None\n        self._lock = Lock()\n\n    def inc(self, amount):\n        with self._lock:\n            self._value += amount\n\n    def set(self, value, timestamp=None):\n        with self._lock:\n            self._value = value\n\n    def set_exemplar(self, exemplar):\n        with self._lock:\n            self._exemplar = exemplar\n\n    def get(self):\n        with self._lock:\n            return self._value\n\n    def get_exemplar(self):\n        with self._lock:\n            return self._exemplar\n\n\ndef MultiProcessValue(process_identifier=os.getpid):\n    \"\"\"Returns a MmapedValue class based on a process_identifier function.\n\n    The 'process_identifier' function MUST comply with this simple rule:\n    when called in simultaneously running processes it MUST return distinct values.\n\n    Using a different function than the default 'os.getpid' is at your own risk.\n    \"\"\"\n    files = {}\n    values = []\n    pid = {'value': process_identifier()}\n    # Use a single global lock when in multi-processing mode\n    # as we presume this means there is no threading going on.\n    # This avoids the need to also have mutexes in __MmapDict.\n    lock = Lock()\n\n    class MmapedValue:\n        \"\"\"A float protected by a mutex backed by a per-process mmaped file.\"\"\"\n\n        _multiprocess = True\n\n        def __init__(self, typ, metric_name, name, labelnames, labelvalues, help_text, multiprocess_mode='', **kwargs):\n            self._params = typ, metric_name, name, labelnames, labelvalues, help_text, multiprocess_mode\n            # This deprecation warning can go away in a few releases when removing the compatibility\n            if 'prometheus_multiproc_dir' in os.environ and 'PROMETHEUS_MULTIPROC_DIR' not in os.environ:\n                os.environ['PROMETHEUS_MULTIPROC_DIR'] = os.environ['prometheus_multiproc_dir']\n                warnings.warn(\"prometheus_multiproc_dir variable has been deprecated in favor of the upper case naming PROMETHEUS_MULTIPROC_DIR\", DeprecationWarning)\n            with lock:\n                self.__check_for_pid_change()\n                self.__reset()\n                values.append(self)\n\n        def __reset(self):\n            typ, metric_name, name, labelnames, labelvalues, help_text, multiprocess_mode = self._params\n            if typ == 'gauge':\n                file_prefix = typ + '_' + multiprocess_mode\n            else:\n                file_prefix = typ\n            if file_prefix not in files:\n                filename = os.path.join(\n                    os.environ.get('PROMETHEUS_MULTIPROC_DIR'),\n                    '{}_{}.db'.format(file_prefix, pid['value']))\n\n                files[file_prefix] = MmapedDict(filename)\n            self._file = files[file_prefix]\n            self._key = mmap_key(metric_name, name, labelnames, labelvalues, help_text)\n            self._value, self._timestamp = self._file.read_value(self._key)\n\n        def __check_for_pid_change(self):\n            actual_pid = process_identifier()\n            if pid['value'] != actual_pid:\n                pid['value'] = actual_pid\n                # There has been a fork(), reset all the values.\n                for f in files.values():\n                    f.close()\n                files.clear()\n                for value in values:\n                    value.__reset()\n\n        def inc(self, amount):\n            with lock:\n                self.__check_for_pid_change()\n                self._value += amount\n                self._timestamp = 0.0\n                self._file.write_value(self._key, self._value, self._timestamp)\n\n        def set(self, value, timestamp=None):\n            with lock:\n                self.__check_for_pid_change()\n                self._value = value\n                self._timestamp = timestamp or 0.0\n                self._file.write_value(self._key, self._value, self._timestamp)\n\n        def set_exemplar(self, exemplar):\n            # TODO: Implement exemplars for multiprocess mode.\n            return\n\n        def get(self):\n            with lock:\n                self.__check_for_pid_change()\n                return self._value\n\n        def get_exemplar(self):\n            # TODO: Implement exemplars for multiprocess mode.\n            return None\n\n    return MmapedValue\n\n\ndef get_value_class():\n    # Should we enable multi-process mode?\n    # This needs to be chosen before the first metric is constructed,\n    # and as that may be in some arbitrary library the user/admin has\n    # no control over we use an environment variable.\n    if 'prometheus_multiproc_dir' in os.environ or 'PROMETHEUS_MULTIPROC_DIR' in os.environ:\n        return MultiProcessValue()\n    else:\n        return MutexValue\n\n\nValueClass = get_value_class()\n", 139], "/app/app/services/market_data.py": ["import asyncio\nimport logging\nimport os\nimport time\nfrom datetime import datetime, timezone\nfrom typing import Dict, List, Optional\n\nfrom fastapi import HTTPException\n\nfrom ..circuit_breaker import CircuitBreakerManager\nfrom ..core.config import get_settings\nfrom ..providers import FinnhubProvider, YahooFinanceProvider\nfrom ..providers.registry import ProviderRegistry\nfrom .cache import DataCache\nfrom .database import db_service\nfrom .data_collector import DataCollectorService\nfrom .macro_data_service import MacroFactorService\nfrom .options_service import options_service\nfrom .websocket import ConnectionManager\n\nlogger = logging.getLogger(__name__)\n\nclass MarketDataService:\n    SUPPORTED_INTRADAY_INTERVALS = {\n        \"1m\",\n        \"2m\",\n        \"5m\",\n        \"15m\",\n        \"30m\",\n        \"60m\",\n        \"90m\",\n    }\n\n    def __init__(self):\n        self.settings = get_settings()\n        self.breakers = CircuitBreakerManager()\n        self.registry = ProviderRegistry(self.breakers)\n\n        self._init_providers()\n        self.providers = [entry.adapter for entry in self.registry.providers.values()]\n\n        # Cache TTL configurable via env/settings\n        cache_ttl = int(os.getenv(\"CACHE_TTL_SECONDS\", self.settings.cache_ttl_seconds))\n        self.cache = DataCache(ttl_seconds=cache_ttl)\n        self.metrics_cache = DataCache(ttl_seconds=int(os.getenv(\"OPTIONS_METRICS_CACHE_TTL\", \"300\")))\n        self.options_metrics_ttl = int(os.getenv(\"OPTIONS_METRICS_TTL\", \"900\"))\n        self.update_interval = int(os.getenv(\"WEBSOCKET_UPDATE_INTERVAL\", \"5\"))\n        self.connection_manager = ConnectionManager()\n        self.macro_service = MacroFactorService(\n            providers=self.providers,\n            cache_ttl_seconds=getattr(self.settings, \"macro_cache_ttl_seconds\", 300),\n            refresh_interval_seconds=getattr(self.settings, \"macro_refresh_interval_seconds\", 900),\n        )\n        self.data_collector = DataCollectorService(db=db_service, registry=self.registry)\n        self.background_tasks_running = False\n\n    def _init_providers(self) -> None:\n        \"\"\"Register provider adapters with the routing registry.\"\"\"\n        finnhub_key = (\n            os.getenv(\"FINNHUB_API_KEY\")\n            or self.settings.finnhub.api_key\n            or self.settings.finnhub_api_key\n        )\n\n        if finnhub_key and finnhub_key not in {\"your_finnhub_api_key_here\", \"\"}:\n            try:\n                finnhub_provider = FinnhubProvider(api_key=finnhub_key)\n                self.registry.register(\"finnhub\", finnhub_provider, {\"bars_1m\", \"eod\", \"quotes_l1\"})\n                logger.info(\"Finnhub provider registered\")\n            except Exception as exc:  # pragma: no cover - defensive\n                logger.warning(\"Failed to initialize Finnhub provider: %s\", exc)\n        else:\n            logger.warning(\"No valid Finnhub API key provided; Finnhub disabled\")\n\n        if self.settings.yfinance_enabled:\n            try:\n                yahoo_provider = YahooFinanceProvider()\n                self.registry.register(\"yfinance\", yahoo_provider, {\"bars_1m\", \"eod\", \"options_chain\"})\n                logger.info(\"Yahoo Finance provider registered\")\n            except Exception as exc:  # pragma: no cover\n                logger.warning(\"Failed to initialize Yahoo Finance provider: %s\", exc)\n\n    def reload_configuration(self) -> None:\n        \"\"\"Reload providers and settings after a hot reload.\"\"\"\n        self.settings = get_settings()\n        self.registry.providers.clear()\n        self._init_providers()\n        self.providers = [entry.adapter for entry in self.registry.providers.values()]\n        if hasattr(self.macro_service, \"providers\"):\n            self.macro_service.providers = self.providers\n\n    async def _try_providers(\n        self,\n        *,\n        capability: str,\n        endpoint: str,\n        executor,\n        provider_hint: Optional[str] = None,\n    ):\n        ranked = self.registry.rank(capability, provider_hint=provider_hint)\n        if not ranked:\n            raise HTTPException(status_code=503, detail=f\"No providers available for {capability}\")\n\n        last_error: Optional[str] = None\n        for provider_name in ranked:\n            entry = self.registry.providers[provider_name]\n            adapter = entry.adapter\n            if not adapter.enabled:\n                continue\n\n            self.registry.record_selection(capability, provider_name)\n            started = time.perf_counter()\n\n            try:\n                result = await executor(adapter)\n                elapsed_ms = (time.perf_counter() - started) * 1000.0\n                self.registry.record_outcome(provider_name, elapsed_ms, error=False, endpoint=endpoint)\n                if result:\n                    return result, provider_name\n                last_error = f\"{provider_name}: empty result\"\n            except HTTPException:\n                raise\n            except Exception as exc:  # pragma: no cover - provider failure paths\n                elapsed_ms = (time.perf_counter() - started) * 1000.0\n                self.registry.record_outcome(provider_name, elapsed_ms, error=True, endpoint=endpoint)\n                self.registry.record_error(provider_name, endpoint, exc.__class__.__name__)\n                last_error = f\"{provider_name}: {exc}\"\n\n        raise HTTPException(\n            status_code=502,\n            detail=f\"All providers failed for {endpoint}. last_error={last_error}\",\n        )\n    \n    async def get_stock_price(self, symbol: str) -> Dict:\n        \"\"\"Get stock price with caching and health-aware routing.\"\"\"\n        symbol = symbol.upper()\n        cache_key = f\"price:{symbol}\"\n\n        cached_data = self.cache.get(cache_key)\n        if cached_data:\n            logger.info(\"Cache hit for %s\", symbol)\n            return cached_data\n\n        data, provider_used = await self._try_providers(\n            capability=\"quotes_l1\",\n            endpoint=\"quotes\",\n            executor=lambda adapter: adapter.get_price_safe(symbol),\n        )\n\n        if isinstance(data, dict):\n            data[\"provider_used\"] = provider_used\n        self.cache.set(cache_key, data)\n        logger.info(\"Fetched %s from %s\", symbol, provider_used)\n        return data\n    \n    async def get_historical_data(self, symbol: str, period: str = \"1mo\") -> Dict:\n        \"\"\"Get historical data.\"\"\"\n        symbol = symbol.upper()\n        data, provider_used = await self._try_providers(\n            capability=\"eod\",\n            endpoint=\"historical\",\n            executor=lambda adapter: adapter.get_historical_safe(symbol, period),\n        )\n\n        if isinstance(data, dict):\n            data[\"provider_used\"] = provider_used\n        return data\n\n    def _normalize_intraday_interval(self, interval: str) -> str:\n        interval_normalized = (interval or \"1m\").strip().lower()\n\n        # Map common aliases to supported values\n        alias_map = {\n            \"1\": \"1m\",\n            \"5\": \"5m\",\n            \"15\": \"15m\",\n            \"30\": \"30m\",\n            \"60\": \"60m\",\n            \"1min\": \"1m\",\n            \"5min\": \"5m\",\n            \"15min\": \"15m\",\n            \"30min\": \"30m\",\n            \"60min\": \"60m\",\n            \"1hour\": \"60m\",\n            \"1hr\": \"60m\",\n            \"1h\": \"60m\",\n        }\n\n        resolved = alias_map.get(interval_normalized, interval_normalized)\n        if resolved not in self.SUPPORTED_INTRADAY_INTERVALS:\n            raise HTTPException(\n                status_code=400,\n                detail=f\"Unsupported intraday interval '{interval}'\"\n            )\n        return resolved\n\n    async def get_intraday_data(self, symbol: str, interval: str = \"1m\") -> Dict:\n        \"\"\"Get intraday data with provider fallback and validation.\"\"\"\n        normalized_interval = self._normalize_intraday_interval(interval)\n\n        capability = \"bars_1m\" if normalized_interval.endswith(\"m\") else \"eod\"\n        data, provider_used = await self._try_providers(\n            capability=capability,\n            endpoint=\"intraday\",\n            executor=lambda adapter: adapter.get_intraday_safe(symbol, normalized_interval),\n        )\n\n        if isinstance(data, dict):\n            data[\"provider_used\"] = provider_used\n        return data\n    \n    async def start_background_tasks(self):\n        \"\"\"Start background tasks for cache cleanup and real-time updates\"\"\"\n        if self.background_tasks_running:\n            return\n\n        self.background_tasks_running = True\n\n        # Initialize database connections for macro storage\n        try:\n            await db_service.initialize()\n            logger.info(\"Database service initialized\")\n        except Exception as exc:\n            logger.error(f\"Failed to initialize database service: {exc}\")\n\n        # Start cache cleanup task\n        asyncio.create_task(self._cache_cleanup_task())\n\n        # Start macro refresh task\n        asyncio.create_task(self._macro_refresh_task())\n\n        # Start real-time data broadcasting\n        asyncio.create_task(self._real_time_broadcast_task())\n\n        # Start data collector (M2/M3: RLC consumer + gap detection + backfill)\n        asyncio.create_task(self.data_collector.run())\n\n        logger.info(\"Background tasks started (including data collector)\")\n    \n    async def _cache_cleanup_task(self):\n        \"\"\"Clean up expired cache entries\"\"\"\n        while self.background_tasks_running:\n            self.cache.clear_expired()\n            await asyncio.sleep(60)  # Clean up every minute\n    \n    async def _real_time_broadcast_task(self):\n        \"\"\"Broadcast real-time data to WebSocket clients\"\"\"\n        while self.background_tasks_running:\n            if self.connection_manager.symbol_subscribers:\n                for symbol in list(self.connection_manager.symbol_subscribers.keys()):\n                    try:\n                        data = await self.get_stock_price(symbol)\n                        await self.connection_manager.broadcast_to_symbol(symbol, data)\n                    except Exception as e:\n                        logger.error(f\"Error broadcasting {symbol}: {e}\")\n\n            await asyncio.sleep(self.update_interval)  # Update interval configurable\n\n    async def _macro_refresh_task(self):\n        \"\"\"Periodically refresh macro factors from configured providers.\"\"\"\n        interval = getattr(\n            self.macro_service,\n            \"refresh_interval_seconds\",\n            getattr(self.settings, \"macro_refresh_interval_seconds\", 900),\n        )\n        interval = max(60, interval or 900)\n\n        try:\n            await self.macro_service.refresh_all()\n        except Exception as exc:\n            logger.error(f\"Initial macro refresh failed: {exc}\")\n\n        while self.background_tasks_running:\n            await asyncio.sleep(interval)\n            try:\n                await self.macro_service.refresh_all()\n            except Exception as exc:\n                logger.error(f\"Macro refresh task failed: {exc}\")\n\n    async def get_company_profile(self, symbol: str) -> Dict:\n        \"\"\"Get company profile data (Finnhub only)\"\"\"\n        for provider in self.providers:\n            if hasattr(provider, 'get_company_profile'):\n                data = await provider.get_company_profile(symbol)\n                if data:\n                    return data\n        \n        raise HTTPException(\n            status_code=503,\n            detail=f\"Unable to fetch company profile for {symbol}\"\n        )\n    \n    async def get_news_sentiment(self, symbol: str) -> Dict:\n        \"\"\"Get news sentiment data (Finnhub only)\"\"\"\n        for provider in self.providers:\n            if hasattr(provider, 'get_news_sentiment'):\n                data = await provider.get_news_sentiment(symbol)\n                if data:\n                    return data\n        \n        raise HTTPException(\n            status_code=503,\n            detail=f\"Unable to fetch news sentiment for {symbol}\"\n        )\n    \n\n    async def get_options_metrics(self, symbol: str) -> Dict:\n        \"\"\"Return cached or freshly computed options metrics for a symbol.\"\"\"\n        symbol = symbol.upper()\n        cache_key = f\"options_metrics:{symbol}\"\n        cached = self.metrics_cache.get(cache_key)\n        if cached:\n            return cached\n\n        latest = await db_service.get_latest_options_metrics(symbol)\n        if latest:\n            try:\n                timestamp = datetime.fromisoformat(latest['as_of'])\n                if timestamp.tzinfo is None:\n                    timestamp = timestamp.replace(tzinfo=timezone.utc)\n                if (datetime.now(timezone.utc) - timestamp).total_seconds() < self.options_metrics_ttl:\n                    self.metrics_cache.set(cache_key, latest)\n                    return latest\n            except Exception:\n                pass\n\n        chain = await options_service.fetch_options_chain(symbol)\n        metrics = options_service.calculate_chain_metrics(chain)\n        record = metrics.to_db_record()\n        await db_service.store_options_metrics(record)\n        result = metrics.to_dict()\n        result['metadata'].setdefault('source', 'provider')\n        self.metrics_cache.set(cache_key, result)\n        return result\n\n    async def get_options_metrics_history(self, symbol: str, limit: int = 50) -> List[Dict]:\n        \"\"\"Return recent history of stored options metrics for a symbol.\"\"\"\n        history = await db_service.get_options_metrics_history(symbol.upper(), limit)\n        return history\n\n    async def get_stats(self) -> Dict:\n        macro_stats = await self.macro_service.stats()\n        return {\n            \"providers\": [\n                {\n                    \"name\": provider.name,\n                    \"available\": provider.is_available,\n                    \"last_error\": provider.last_error,\n                }\n                for provider in self.providers\n            ],\n            \"cache\": self.cache.stats(),\n            \"websocket\": self.connection_manager.stats(),\n            \"database\": {\n                \"connected\": db_service.pool is not None,\n                \"storage_enabled\": self.settings.store_historical_data,\n            },\n            \"macro\": macro_stats,\n            \"options_metrics\": {\n                \"cache\": self.metrics_cache.stats(),\n                \"ttl_seconds\": self.options_metrics_ttl,\n            },\n        }\n\n    async def get_macro_snapshot(self, factors: Optional[List[str]] = None) -> Dict:\n        \"\"\"Return the latest macro factor snapshot.\"\"\"\n        return await self.macro_service.get_snapshot(factors)\n\n    async def get_macro_history(self, factor_key: str, lookback_days: int = 30) -> Dict:\n        \"\"\"Return macro factor history for the requested lookback window.\"\"\"\n        return await self.macro_service.get_history(factor_key, lookback_days)\n\n    def list_macro_factors(self) -> List[str]:\n        \"\"\"List available macro factor keys.\"\"\"\n        return self.macro_service.available_factors()\n\n    async def refresh_macro_factors(self, factor_key: Optional[str] = None) -> Dict:\n        \"\"\"Refresh macro factors from providers.\"\"\"\n        if factor_key:\n            return await self.macro_service.refresh_factor(factor_key)\n        return await self.macro_service.refresh_all()\n    \n    async def get_unusual_options_activity(self, symbol: str, lookback_days: int = 20) -> Dict:\n        \"\"\"Get unusual options activity for a symbol\"\"\"\n        try:\n            unusual_activities = await options_service.detect_unusual_activity(symbol.upper(), lookback_days)\n            \n            return {\n                \"symbol\": symbol.upper(),\n                \"lookback_days\": lookback_days,\n                \"unusual_activities_count\": len(unusual_activities),\n                \"unusual_activities\": [\n                    {\n                        \"contract_symbol\": activity.contract_symbol,\n                        \"strike\": activity.strike,\n                        \"expiry\": activity.expiry.isoformat(),\n                        \"option_type\": activity.option_type,\n                        \"volume\": activity.volume,\n                        \"avg_volume_20d\": activity.avg_volume_20d,\n                        \"volume_ratio\": activity.volume_ratio,\n                        \"open_interest\": activity.open_interest,\n                        \"volume_spike\": activity.volume_spike,\n                        \"large_single_trades\": activity.large_single_trades,\n                        \"sweep_activity\": activity.sweep_activity,\n                        \"unusual_volume_vs_oi\": activity.unusual_volume_vs_oi,\n                        \"underlying_price\": activity.underlying_price,\n                        \"strike_distance_pct\": activity.strike_distance_pct,\n                        \"days_to_expiration\": activity.days_to_expiration,\n                        \"unusual_score\": activity.unusual_score,\n                        \"confidence_level\": activity.confidence_level,\n                        \"large_trades\": activity.large_trades,\n                        \"timestamp\": activity.timestamp.isoformat()\n                    }\n                    for activity in unusual_activities\n                ],\n                \"analysis_timestamp\": datetime.now().isoformat()\n            }\n        except Exception as e:\n            logger.error(f\"Error getting unusual options activity for {symbol}: {e}\")\n            return {\n                \"symbol\": symbol.upper(),\n                \"error\": str(e),\n                \"unusual_activities_count\": 0,\n                \"unusual_activities\": []\n            }\n    \n    async def get_options_flow_analysis(self, symbol: str) -> Dict:\n        \"\"\"Get comprehensive options flow analysis\"\"\"\n        try:\n            flow_analysis = await options_service.analyze_options_flow(symbol.upper())\n            \n            return {\n                \"symbol\": symbol.upper(),\n                \"timestamp\": flow_analysis.timestamp.isoformat(),\n                \"flow_metrics\": {\n                    \"total_call_volume\": flow_analysis.total_call_volume,\n                    \"total_put_volume\": flow_analysis.total_put_volume,\n                    \"call_put_ratio\": flow_analysis.call_put_ratio,\n                    \"large_trades_count\": flow_analysis.large_trades_count,\n                    \"block_trades_value\": flow_analysis.block_trades_value,\n                    \"sweep_trades_count\": flow_analysis.sweep_trades_count\n                },\n                \"sentiment_analysis\": {\n                    \"flow_sentiment\": flow_analysis.flow_sentiment,\n                    \"smart_money_score\": flow_analysis.smart_money_score,\n                    \"call_premium_bought\": flow_analysis.call_premium_bought,\n                    \"put_premium_bought\": flow_analysis.put_premium_bought,\n                    \"net_premium_flow\": flow_analysis.net_premium_flow\n                },\n                \"unusual_activities_summary\": {\n                    \"count\": len(flow_analysis.unusual_activities),\n                    \"top_activities\": [\n                        {\n                            \"contract_symbol\": activity.contract_symbol,\n                            \"unusual_score\": activity.unusual_score,\n                            \"volume_ratio\": activity.volume_ratio,\n                            \"volume\": activity.volume,\n                            \"strike\": activity.strike,\n                            \"option_type\": activity.option_type\n                        }\n                        for activity in flow_analysis.unusual_activities[:5]  # Top 5\n                    ]\n                }\n            }\n        except Exception as e:\n            logger.error(f\"Error getting options flow analysis for {symbol}: {e}\")\n            return {\n                \"symbol\": symbol.upper(),\n                \"error\": str(e),\n                \"flow_metrics\": {},\n                \"sentiment_analysis\": {},\n                \"unusual_activities_summary\": {\"count\": 0, \"top_activities\": []}\n            }\n\n", 474], "/usr/local/lib/python3.11/asyncio/base_futures.py": ["__all__ = ()\n\nimport reprlib\nfrom _thread import get_ident\n\nfrom . import format_helpers\n\n# States for Future.\n_PENDING = 'PENDING'\n_CANCELLED = 'CANCELLED'\n_FINISHED = 'FINISHED'\n\n\ndef isfuture(obj):\n    \"\"\"Check for a Future.\n\n    This returns True when obj is a Future instance or is advertising\n    itself as duck-type compatible by setting _asyncio_future_blocking.\n    See comment in Future for more details.\n    \"\"\"\n    return (hasattr(obj.__class__, '_asyncio_future_blocking') and\n            obj._asyncio_future_blocking is not None)\n\n\ndef _format_callbacks(cb):\n    \"\"\"helper function for Future.__repr__\"\"\"\n    size = len(cb)\n    if not size:\n        cb = ''\n\n    def format_cb(callback):\n        return format_helpers._format_callback_source(callback, ())\n\n    if size == 1:\n        cb = format_cb(cb[0][0])\n    elif size == 2:\n        cb = '{}, {}'.format(format_cb(cb[0][0]), format_cb(cb[1][0]))\n    elif size > 2:\n        cb = '{}, <{} more>, {}'.format(format_cb(cb[0][0]),\n                                        size - 2,\n                                        format_cb(cb[-1][0]))\n    return f'cb=[{cb}]'\n\n\ndef _future_repr_info(future):\n    # (Future) -> str\n    \"\"\"helper function for Future.__repr__\"\"\"\n    info = [future._state.lower()]\n    if future._state == _FINISHED:\n        if future._exception is not None:\n            info.append(f'exception={future._exception!r}')\n        else:\n            # use reprlib to limit the length of the output, especially\n            # for very long strings\n            result = reprlib.repr(future._result)\n            info.append(f'result={result}')\n    if future._callbacks:\n        info.append(_format_callbacks(future._callbacks))\n    if future._source_traceback:\n        frame = future._source_traceback[-1]\n        info.append(f'created at {frame[0]}:{frame[1]}')\n    return info\n\n\n@reprlib.recursive_repr()\ndef _future_repr(future):\n    info = ' '.join(_future_repr_info(future))\n    return f'<{future.__class__.__name__} {info}>'\n", 68], "/usr/local/lib/python3.11/asyncio/coroutines.py": ["__all__ = 'iscoroutinefunction', 'iscoroutine'\n\nimport collections.abc\nimport inspect\nimport os\nimport sys\nimport traceback\nimport types\n\n\ndef _is_debug_mode():\n    # See: https://docs.python.org/3/library/asyncio-dev.html#asyncio-debug-mode.\n    return sys.flags.dev_mode or (not sys.flags.ignore_environment and\n                                  bool(os.environ.get('PYTHONASYNCIODEBUG')))\n\n\n# A marker for iscoroutinefunction.\n_is_coroutine = object()\n\n\ndef iscoroutinefunction(func):\n    \"\"\"Return True if func is a decorated coroutine function.\"\"\"\n    return (inspect.iscoroutinefunction(func) or\n            getattr(func, '_is_coroutine', None) is _is_coroutine)\n\n\n# Prioritize native coroutine check to speed-up\n# asyncio.iscoroutine.\n_COROUTINE_TYPES = (types.CoroutineType, types.GeneratorType,\n                    collections.abc.Coroutine)\n_iscoroutine_typecache = set()\n\n\ndef iscoroutine(obj):\n    \"\"\"Return True if obj is a coroutine object.\"\"\"\n    if type(obj) in _iscoroutine_typecache:\n        return True\n\n    if isinstance(obj, _COROUTINE_TYPES):\n        # Just in case we don't want to cache more than 100\n        # positive types.  That shouldn't ever happen, unless\n        # someone stressing the system on purpose.\n        if len(_iscoroutine_typecache) < 100:\n            _iscoroutine_typecache.add(type(obj))\n        return True\n    else:\n        return False\n\n\ndef _format_coroutine(coro):\n    assert iscoroutine(coro)\n\n    def get_name(coro):\n        # Coroutines compiled with Cython sometimes don't have\n        # proper __qualname__ or __name__.  While that is a bug\n        # in Cython, asyncio shouldn't crash with an AttributeError\n        # in its __repr__ functions.\n        if hasattr(coro, '__qualname__') and coro.__qualname__:\n            coro_name = coro.__qualname__\n        elif hasattr(coro, '__name__') and coro.__name__:\n            coro_name = coro.__name__\n        else:\n            # Stop masking Cython bugs, expose them in a friendly way.\n            coro_name = f'<{type(coro).__name__} without __name__>'\n        return f'{coro_name}()'\n\n    def is_running(coro):\n        try:\n            return coro.cr_running\n        except AttributeError:\n            try:\n                return coro.gi_running\n            except AttributeError:\n                return False\n\n    coro_code = None\n    if hasattr(coro, 'cr_code') and coro.cr_code:\n        coro_code = coro.cr_code\n    elif hasattr(coro, 'gi_code') and coro.gi_code:\n        coro_code = coro.gi_code\n\n    coro_name = get_name(coro)\n\n    if not coro_code:\n        # Built-in types might not have __qualname__ or __name__.\n        if is_running(coro):\n            return f'{coro_name} running'\n        else:\n            return coro_name\n\n    coro_frame = None\n    if hasattr(coro, 'gi_frame') and coro.gi_frame:\n        coro_frame = coro.gi_frame\n    elif hasattr(coro, 'cr_frame') and coro.cr_frame:\n        coro_frame = coro.cr_frame\n\n    # If Cython's coroutine has a fake code object without proper\n    # co_filename -- expose that.\n    filename = coro_code.co_filename or '<empty co_filename>'\n\n    lineno = 0\n\n    if coro_frame is not None:\n        lineno = coro_frame.f_lineno\n        coro_repr = f'{coro_name} running at {filename}:{lineno}'\n\n    else:\n        lineno = coro_code.co_firstlineno\n        coro_repr = f'{coro_name} done, defined at {filename}:{lineno}'\n\n    return coro_repr\n", 111], "/usr/local/lib/python3.11/_weakrefset.py": ["# Access WeakSet through the weakref module.\n# This code is separated-out because it is needed\n# by abc.py to load everything else at startup.\n\nfrom _weakref import ref\nfrom types import GenericAlias\n\n__all__ = ['WeakSet']\n\n\nclass _IterationGuard:\n    # This context manager registers itself in the current iterators of the\n    # weak container, such as to delay all removals until the context manager\n    # exits.\n    # This technique should be relatively thread-safe (since sets are).\n\n    def __init__(self, weakcontainer):\n        # Don't create cycles\n        self.weakcontainer = ref(weakcontainer)\n\n    def __enter__(self):\n        w = self.weakcontainer()\n        if w is not None:\n            w._iterating.add(self)\n        return self\n\n    def __exit__(self, e, t, b):\n        w = self.weakcontainer()\n        if w is not None:\n            s = w._iterating\n            s.remove(self)\n            if not s:\n                w._commit_removals()\n\n\nclass WeakSet:\n    def __init__(self, data=None):\n        self.data = set()\n        def _remove(item, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(item)\n                else:\n                    self.data.discard(item)\n        self._remove = _remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        if data is not None:\n            self.update(data)\n\n    def _commit_removals(self):\n        pop = self._pending_removals.pop\n        discard = self.data.discard\n        while True:\n            try:\n                item = pop()\n            except IndexError:\n                return\n            discard(item)\n\n    def __iter__(self):\n        with _IterationGuard(self):\n            for itemref in self.data:\n                item = itemref()\n                if item is not None:\n                    # Caveat: the iterator will keep a strong reference to\n                    # `item` until it is resumed or closed.\n                    yield item\n\n    def __len__(self):\n        return len(self.data) - len(self._pending_removals)\n\n    def __contains__(self, item):\n        try:\n            wr = ref(item)\n        except TypeError:\n            return False\n        return wr in self.data\n\n    def __reduce__(self):\n        return self.__class__, (list(self),), self.__getstate__()\n\n    def add(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.add(ref(item, self._remove))\n\n    def clear(self):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.clear()\n\n    def copy(self):\n        return self.__class__(self)\n\n    def pop(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while True:\n            try:\n                itemref = self.data.pop()\n            except KeyError:\n                raise KeyError('pop from empty WeakSet') from None\n            item = itemref()\n            if item is not None:\n                return item\n\n    def remove(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.remove(ref(item))\n\n    def discard(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.discard(ref(item))\n\n    def update(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        for element in other:\n            self.add(element)\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def difference(self, other):\n        newset = self.copy()\n        newset.difference_update(other)\n        return newset\n    __sub__ = difference\n\n    def difference_update(self, other):\n        self.__isub__(other)\n    def __isub__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.difference_update(ref(item) for item in other)\n        return self\n\n    def intersection(self, other):\n        return self.__class__(item for item in other if item in self)\n    __and__ = intersection\n\n    def intersection_update(self, other):\n        self.__iand__(other)\n    def __iand__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.intersection_update(ref(item) for item in other)\n        return self\n\n    def issubset(self, other):\n        return self.data.issubset(ref(item) for item in other)\n    __le__ = issubset\n\n    def __lt__(self, other):\n        return self.data < set(map(ref, other))\n\n    def issuperset(self, other):\n        return self.data.issuperset(ref(item) for item in other)\n    __ge__ = issuperset\n\n    def __gt__(self, other):\n        return self.data > set(map(ref, other))\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return self.data == set(map(ref, other))\n\n    def symmetric_difference(self, other):\n        newset = self.copy()\n        newset.symmetric_difference_update(other)\n        return newset\n    __xor__ = symmetric_difference\n\n    def symmetric_difference_update(self, other):\n        self.__ixor__(other)\n    def __ixor__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.symmetric_difference_update(ref(item, self._remove) for item in other)\n        return self\n\n    def union(self, other):\n        return self.__class__(e for s in (self, other) for e in s)\n    __or__ = union\n\n    def isdisjoint(self, other):\n        return len(self.intersection(other)) == 0\n\n    def __repr__(self):\n        return repr(self.data)\n\n    __class_getitem__ = classmethod(GenericAlias)\n", 205], "/usr/local/lib/python3.11/asyncio/tasks.py": ["\"\"\"Support for tasks, coroutines and the scheduler.\"\"\"\n\n__all__ = (\n    'Task', 'create_task',\n    'FIRST_COMPLETED', 'FIRST_EXCEPTION', 'ALL_COMPLETED',\n    'wait', 'wait_for', 'as_completed', 'sleep',\n    'gather', 'shield', 'ensure_future', 'run_coroutine_threadsafe',\n    'current_task', 'all_tasks',\n    '_register_task', '_unregister_task', '_enter_task', '_leave_task',\n)\n\nimport concurrent.futures\nimport contextvars\nimport functools\nimport inspect\nimport itertools\nimport types\nimport warnings\nimport weakref\nfrom types import GenericAlias\n\nfrom . import base_tasks\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import futures\nfrom .coroutines import _is_coroutine\n\n# Helper to generate new task names\n# This uses itertools.count() instead of a \"+= 1\" operation because the latter\n# is not thread safe. See bpo-11866 for a longer explanation.\n_task_name_counter = itertools.count(1).__next__\n\n\ndef current_task(loop=None):\n    \"\"\"Return a currently executed task.\"\"\"\n    if loop is None:\n        loop = events.get_running_loop()\n    return _current_tasks.get(loop)\n\n\ndef all_tasks(loop=None):\n    \"\"\"Return a set of all tasks for the loop.\"\"\"\n    if loop is None:\n        loop = events.get_running_loop()\n    # Looping over a WeakSet (_all_tasks) isn't safe as it can be updated from another\n    # thread while we do so. Therefore we cast it to list prior to filtering. The list\n    # cast itself requires iteration, so we repeat it several times ignoring\n    # RuntimeErrors (which are not very likely to occur). See issues 34970 and 36607 for\n    # details.\n    i = 0\n    while True:\n        try:\n            tasks = list(_all_tasks)\n        except RuntimeError:\n            i += 1\n            if i >= 1000:\n                raise\n        else:\n            break\n    return {t for t in tasks\n            if futures._get_loop(t) is loop and not t.done()}\n\n\ndef _set_task_name(task, name):\n    if name is not None:\n        try:\n            set_name = task.set_name\n        except AttributeError:\n            warnings.warn(\"Task.set_name() was added in Python 3.8, \"\n                      \"the method support will be mandatory for third-party \"\n                      \"task implementations since 3.13.\",\n                      DeprecationWarning, stacklevel=3)\n        else:\n            set_name(name)\n\n\nclass Task(futures._PyFuture):  # Inherit Python Task implementation\n                                # from a Python Future implementation.\n\n    \"\"\"A coroutine wrapped in a Future.\"\"\"\n\n    # An important invariant maintained while a Task not done:\n    # _fut_waiter is either None or a Future.  The Future\n    # can be either done() or not done().\n    # The task can be in any of 3 states:\n    #\n    # - 1: _fut_waiter is not None and not _fut_waiter.done():\n    #      __step() is *not* scheduled and the Task is waiting for _fut_waiter.\n    # - 2: (_fut_waiter is None or _fut_waiter.done()) and __step() is scheduled:\n    #       the Task is waiting for __step() to be executed.\n    # - 3:  _fut_waiter is None and __step() is *not* scheduled:\n    #       the Task is currently executing (in __step()).\n    #\n    # * In state 1, one of the callbacks of __fut_waiter must be __wakeup().\n    # * The transition from 1 to 2 happens when _fut_waiter becomes done(),\n    #   as it schedules __wakeup() to be called (which calls __step() so\n    #   we way that __step() is scheduled).\n    # * It transitions from 2 to 3 when __step() is executed, and it clears\n    #   _fut_waiter to None.\n\n    # If False, don't log a message if the task is destroyed while its\n    # status is still pending\n    _log_destroy_pending = True\n\n    def __init__(self, coro, *, loop=None, name=None, context=None):\n        super().__init__(loop=loop)\n        if self._source_traceback:\n            del self._source_traceback[-1]\n        if not coroutines.iscoroutine(coro):\n            # raise after Future.__init__(), attrs are required for __del__\n            # prevent logging for pending task in __del__\n            self._log_destroy_pending = False\n            raise TypeError(f\"a coroutine was expected, got {coro!r}\")\n\n        if name is None:\n            self._name = f'Task-{_task_name_counter()}'\n        else:\n            self._name = str(name)\n\n        self._num_cancels_requested = 0\n        self._must_cancel = False\n        self._fut_waiter = None\n        self._coro = coro\n        if context is None:\n            self._context = contextvars.copy_context()\n        else:\n            self._context = context\n\n        self._loop.call_soon(self.__step, context=self._context)\n        _register_task(self)\n\n    def __del__(self):\n        if self._state == futures._PENDING and self._log_destroy_pending:\n            context = {\n                'task': self,\n                'message': 'Task was destroyed but it is pending!',\n            }\n            if self._source_traceback:\n                context['source_traceback'] = self._source_traceback\n            self._loop.call_exception_handler(context)\n        super().__del__()\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    def __repr__(self):\n        return base_tasks._task_repr(self)\n\n    def get_coro(self):\n        return self._coro\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, value):\n        self._name = str(value)\n\n    def set_result(self, result):\n        raise RuntimeError('Task does not support set_result operation')\n\n    def set_exception(self, exception):\n        raise RuntimeError('Task does not support set_exception operation')\n\n    def get_stack(self, *, limit=None):\n        \"\"\"Return the list of stack frames for this task's coroutine.\n\n        If the coroutine is not done, this returns the stack where it is\n        suspended.  If the coroutine has completed successfully or was\n        cancelled, this returns an empty list.  If the coroutine was\n        terminated by an exception, this returns the list of traceback\n        frames.\n\n        The frames are always ordered from oldest to newest.\n\n        The optional limit gives the maximum number of frames to\n        return; by default all available frames are returned.  Its\n        meaning differs depending on whether a stack or a traceback is\n        returned: the newest frames of a stack are returned, but the\n        oldest frames of a traceback are returned.  (This matches the\n        behavior of the traceback module.)\n\n        For reasons beyond our control, only one stack frame is\n        returned for a suspended coroutine.\n        \"\"\"\n        return base_tasks._task_get_stack(self, limit)\n\n    def print_stack(self, *, limit=None, file=None):\n        \"\"\"Print the stack or traceback for this task's coroutine.\n\n        This produces output similar to that of the traceback module,\n        for the frames retrieved by get_stack().  The limit argument\n        is passed to get_stack().  The file argument is an I/O stream\n        to which the output is written; by default output is written\n        to sys.stderr.\n        \"\"\"\n        return base_tasks._task_print_stack(self, limit, file)\n\n    def cancel(self, msg=None):\n        \"\"\"Request that this task cancel itself.\n\n        This arranges for a CancelledError to be thrown into the\n        wrapped coroutine on the next cycle through the event loop.\n        The coroutine then has a chance to clean up or even deny\n        the request using try/except/finally.\n\n        Unlike Future.cancel, this does not guarantee that the\n        task will be cancelled: the exception might be caught and\n        acted upon, delaying cancellation of the task or preventing\n        cancellation completely.  The task may also return a value or\n        raise a different exception.\n\n        Immediately after this method is called, Task.cancelled() will\n        not return True (unless the task was already cancelled).  A\n        task will be marked as cancelled when the wrapped coroutine\n        terminates with a CancelledError exception (even if cancel()\n        was not called).\n\n        This also increases the task's count of cancellation requests.\n        \"\"\"\n        self._log_traceback = False\n        if self.done():\n            return False\n        self._num_cancels_requested += 1\n        # These two lines are controversial.  See discussion starting at\n        # https://github.com/python/cpython/pull/31394#issuecomment-1053545331\n        # Also remember that this is duplicated in _asynciomodule.c.\n        # if self._num_cancels_requested > 1:\n        #     return False\n        if self._fut_waiter is not None:\n            if self._fut_waiter.cancel(msg=msg):\n                # Leave self._fut_waiter; it may be a Task that\n                # catches and ignores the cancellation so we may have\n                # to cancel it again later.\n                return True\n        # It must be the case that self.__step is already scheduled.\n        self._must_cancel = True\n        self._cancel_message = msg\n        return True\n\n    def cancelling(self):\n        \"\"\"Return the count of the task's cancellation requests.\n\n        This count is incremented when .cancel() is called\n        and may be decremented using .uncancel().\n        \"\"\"\n        return self._num_cancels_requested\n\n    def uncancel(self):\n        \"\"\"Decrement the task's count of cancellation requests.\n\n        This should be called by the party that called `cancel()` on the task\n        beforehand.\n\n        Returns the remaining number of cancellation requests.\n        \"\"\"\n        if self._num_cancels_requested > 0:\n            self._num_cancels_requested -= 1\n        return self._num_cancels_requested\n\n    def __step(self, exc=None):\n        if self.done():\n            raise exceptions.InvalidStateError(\n                f'_step(): already done: {self!r}, {exc!r}')\n        if self._must_cancel:\n            if not isinstance(exc, exceptions.CancelledError):\n                exc = self._make_cancelled_error()\n            self._must_cancel = False\n        coro = self._coro\n        self._fut_waiter = None\n\n        _enter_task(self._loop, self)\n        # Call either coro.throw(exc) or coro.send(None).\n        try:\n            if exc is None:\n                # We use the `send` method directly, because coroutines\n                # don't have `__iter__` and `__next__` methods.\n                result = coro.send(None)\n            else:\n                result = coro.throw(exc)\n        except StopIteration as exc:\n            if self._must_cancel:\n                # Task is cancelled right before coro stops.\n                self._must_cancel = False\n                super().cancel(msg=self._cancel_message)\n            else:\n                super().set_result(exc.value)\n        except exceptions.CancelledError as exc:\n            # Save the original exception so we can chain it later.\n            self._cancelled_exc = exc\n            super().cancel()  # I.e., Future.cancel(self).\n        except (KeyboardInterrupt, SystemExit) as exc:\n            super().set_exception(exc)\n            raise\n        except BaseException as exc:\n            super().set_exception(exc)\n        else:\n            blocking = getattr(result, '_asyncio_future_blocking', None)\n            if blocking is not None:\n                # Yielded Future must come from Future.__iter__().\n                if futures._get_loop(result) is not self._loop:\n                    new_exc = RuntimeError(\n                        f'Task {self!r} got Future '\n                        f'{result!r} attached to a different loop')\n                    self._loop.call_soon(\n                        self.__step, new_exc, context=self._context)\n                elif blocking:\n                    if result is self:\n                        new_exc = RuntimeError(\n                            f'Task cannot await on itself: {self!r}')\n                        self._loop.call_soon(\n                            self.__step, new_exc, context=self._context)\n                    else:\n                        result._asyncio_future_blocking = False\n                        result.add_done_callback(\n                            self.__wakeup, context=self._context)\n                        self._fut_waiter = result\n                        if self._must_cancel:\n                            if self._fut_waiter.cancel(\n                                    msg=self._cancel_message):\n                                self._must_cancel = False\n                else:\n                    new_exc = RuntimeError(\n                        f'yield was used instead of yield from '\n                        f'in task {self!r} with {result!r}')\n                    self._loop.call_soon(\n                        self.__step, new_exc, context=self._context)\n\n            elif result is None:\n                # Bare yield relinquishes control for one event loop iteration.\n                self._loop.call_soon(self.__step, context=self._context)\n            elif inspect.isgenerator(result):\n                # Yielding a generator is just wrong.\n                new_exc = RuntimeError(\n                    f'yield was used instead of yield from for '\n                    f'generator in task {self!r} with {result!r}')\n                self._loop.call_soon(\n                    self.__step, new_exc, context=self._context)\n            else:\n                # Yielding something else is an error.\n                new_exc = RuntimeError(f'Task got bad yield: {result!r}')\n                self._loop.call_soon(\n                    self.__step, new_exc, context=self._context)\n        finally:\n            _leave_task(self._loop, self)\n            self = None  # Needed to break cycles when an exception occurs.\n\n    def __wakeup(self, future):\n        try:\n            future.result()\n        except BaseException as exc:\n            # This may also be a cancellation.\n            self.__step(exc)\n        else:\n            # Don't pass the value of `future.result()` explicitly,\n            # as `Future.__iter__` and `Future.__await__` don't need it.\n            # If we call `_step(value, None)` instead of `_step()`,\n            # Python eval loop would use `.send(value)` method call,\n            # instead of `__next__()`, which is slower for futures\n            # that return non-generator iterators from their `__iter__`.\n            self.__step()\n        self = None  # Needed to break cycles when an exception occurs.\n\n\n_PyTask = Task\n\n\ntry:\n    import _asyncio\nexcept ImportError:\n    pass\nelse:\n    # _CTask is needed for tests.\n    Task = _CTask = _asyncio.Task\n\n\ndef create_task(coro, *, name=None, context=None):\n    \"\"\"Schedule the execution of a coroutine object in a spawn task.\n\n    Return a Task object.\n    \"\"\"\n    loop = events.get_running_loop()\n    if context is None:\n        # Use legacy API if context is not needed\n        task = loop.create_task(coro)\n    else:\n        task = loop.create_task(coro, context=context)\n\n    _set_task_name(task, name)\n    return task\n\n\n# wait() and as_completed() similar to those in PEP 3148.\n\nFIRST_COMPLETED = concurrent.futures.FIRST_COMPLETED\nFIRST_EXCEPTION = concurrent.futures.FIRST_EXCEPTION\nALL_COMPLETED = concurrent.futures.ALL_COMPLETED\n\n\nasync def wait(fs, *, timeout=None, return_when=ALL_COMPLETED):\n    \"\"\"Wait for the Futures or Tasks given by fs to complete.\n\n    The fs iterable must not be empty.\n\n    Coroutines will be wrapped in Tasks.\n\n    Returns two sets of Future: (done, pending).\n\n    Usage:\n\n        done, pending = await asyncio.wait(fs)\n\n    Note: This does not raise TimeoutError! Futures that aren't done\n    when the timeout occurs are returned in the second set.\n    \"\"\"\n    if futures.isfuture(fs) or coroutines.iscoroutine(fs):\n        raise TypeError(f\"expect a list of futures, not {type(fs).__name__}\")\n    if not fs:\n        raise ValueError('Set of Tasks/Futures is empty.')\n    if return_when not in (FIRST_COMPLETED, FIRST_EXCEPTION, ALL_COMPLETED):\n        raise ValueError(f'Invalid return_when value: {return_when}')\n\n    fs = set(fs)\n\n    if any(coroutines.iscoroutine(f) for f in fs):\n        raise TypeError(\"Passing coroutines is forbidden, use tasks explicitly.\")\n\n    loop = events.get_running_loop()\n    return await _wait(fs, timeout, return_when, loop)\n\n\ndef _release_waiter(waiter, *args):\n    if not waiter.done():\n        waiter.set_result(None)\n\n\nasync def wait_for(fut, timeout):\n    \"\"\"Wait for the single Future or coroutine to complete, with timeout.\n\n    Coroutine will be wrapped in Task.\n\n    Returns result of the Future or coroutine.  When a timeout occurs,\n    it cancels the task and raises TimeoutError.  To avoid the task\n    cancellation, wrap it in shield().\n\n    If the wait is cancelled, the task is also cancelled.\n\n    This function is a coroutine.\n    \"\"\"\n    loop = events.get_running_loop()\n\n    if timeout is None:\n        return await fut\n\n    if timeout <= 0:\n        fut = ensure_future(fut, loop=loop)\n\n        if fut.done():\n            return fut.result()\n\n        await _cancel_and_wait(fut, loop=loop)\n        try:\n            return fut.result()\n        except exceptions.CancelledError as exc:\n            raise exceptions.TimeoutError() from exc\n\n    waiter = loop.create_future()\n    timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\n    cb = functools.partial(_release_waiter, waiter)\n\n    fut = ensure_future(fut, loop=loop)\n    fut.add_done_callback(cb)\n\n    try:\n        # wait until the future completes or the timeout\n        try:\n            await waiter\n        except exceptions.CancelledError:\n            if fut.done():\n                return fut.result()\n            else:\n                fut.remove_done_callback(cb)\n                # We must ensure that the task is not running\n                # after wait_for() returns.\n                # See https://bugs.python.org/issue32751\n                await _cancel_and_wait(fut, loop=loop)\n                raise\n\n        if fut.done():\n            return fut.result()\n        else:\n            fut.remove_done_callback(cb)\n            # We must ensure that the task is not running\n            # after wait_for() returns.\n            # See https://bugs.python.org/issue32751\n            await _cancel_and_wait(fut, loop=loop)\n            # In case task cancellation failed with some\n            # exception, we should re-raise it\n            # See https://bugs.python.org/issue40607\n            try:\n                return fut.result()\n            except exceptions.CancelledError as exc:\n                raise exceptions.TimeoutError() from exc\n    finally:\n        timeout_handle.cancel()\n\n\nasync def _wait(fs, timeout, return_when, loop):\n    \"\"\"Internal helper for wait().\n\n    The fs argument must be a collection of Futures.\n    \"\"\"\n    assert fs, 'Set of Futures is empty.'\n    waiter = loop.create_future()\n    timeout_handle = None\n    if timeout is not None:\n        timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\n    counter = len(fs)\n\n    def _on_completion(f):\n        nonlocal counter\n        counter -= 1\n        if (counter <= 0 or\n            return_when == FIRST_COMPLETED or\n            return_when == FIRST_EXCEPTION and (not f.cancelled() and\n                                                f.exception() is not None)):\n            if timeout_handle is not None:\n                timeout_handle.cancel()\n            if not waiter.done():\n                waiter.set_result(None)\n\n    for f in fs:\n        f.add_done_callback(_on_completion)\n\n    try:\n        await waiter\n    finally:\n        if timeout_handle is not None:\n            timeout_handle.cancel()\n        for f in fs:\n            f.remove_done_callback(_on_completion)\n\n    done, pending = set(), set()\n    for f in fs:\n        if f.done():\n            done.add(f)\n        else:\n            pending.add(f)\n    return done, pending\n\n\nasync def _cancel_and_wait(fut, loop):\n    \"\"\"Cancel the *fut* future or task and wait until it completes.\"\"\"\n\n    waiter = loop.create_future()\n    cb = functools.partial(_release_waiter, waiter)\n    fut.add_done_callback(cb)\n\n    try:\n        fut.cancel()\n        # We cannot wait on *fut* directly to make\n        # sure _cancel_and_wait itself is reliably cancellable.\n        await waiter\n    finally:\n        fut.remove_done_callback(cb)\n\n\n# This is *not* a @coroutine!  It is just an iterator (yielding Futures).\ndef as_completed(fs, *, timeout=None):\n    \"\"\"Return an iterator whose values are coroutines.\n\n    When waiting for the yielded coroutines you'll get the results (or\n    exceptions!) of the original Futures (or coroutines), in the order\n    in which and as soon as they complete.\n\n    This differs from PEP 3148; the proper way to use this is:\n\n        for f in as_completed(fs):\n            result = await f  # The 'await' may raise.\n            # Use result.\n\n    If a timeout is specified, the 'await' will raise\n    TimeoutError when the timeout occurs before all Futures are done.\n\n    Note: The futures 'f' are not necessarily members of fs.\n    \"\"\"\n    if futures.isfuture(fs) or coroutines.iscoroutine(fs):\n        raise TypeError(f\"expect an iterable of futures, not {type(fs).__name__}\")\n\n    from .queues import Queue  # Import here to avoid circular import problem.\n    done = Queue()\n\n    loop = events._get_event_loop()\n    todo = {ensure_future(f, loop=loop) for f in set(fs)}\n    timeout_handle = None\n\n    def _on_timeout():\n        for f in todo:\n            f.remove_done_callback(_on_completion)\n            done.put_nowait(None)  # Queue a dummy value for _wait_for_one().\n        todo.clear()  # Can't do todo.remove(f) in the loop.\n\n    def _on_completion(f):\n        if not todo:\n            return  # _on_timeout() was here first.\n        todo.remove(f)\n        done.put_nowait(f)\n        if not todo and timeout_handle is not None:\n            timeout_handle.cancel()\n\n    async def _wait_for_one():\n        f = await done.get()\n        if f is None:\n            # Dummy value from _on_timeout().\n            raise exceptions.TimeoutError\n        return f.result()  # May raise f.exception().\n\n    for f in todo:\n        f.add_done_callback(_on_completion)\n    if todo and timeout is not None:\n        timeout_handle = loop.call_later(timeout, _on_timeout)\n    for _ in range(len(todo)):\n        yield _wait_for_one()\n\n\n@types.coroutine\ndef __sleep0():\n    \"\"\"Skip one event loop run cycle.\n\n    This is a private helper for 'asyncio.sleep()', used\n    when the 'delay' is set to 0.  It uses a bare 'yield'\n    expression (which Task.__step knows how to handle)\n    instead of creating a Future object.\n    \"\"\"\n    yield\n\n\nasync def sleep(delay, result=None):\n    \"\"\"Coroutine that completes after a given time (in seconds).\"\"\"\n    if delay <= 0:\n        await __sleep0()\n        return result\n\n    loop = events.get_running_loop()\n    future = loop.create_future()\n    h = loop.call_later(delay,\n                        futures._set_result_unless_cancelled,\n                        future, result)\n    try:\n        return await future\n    finally:\n        h.cancel()\n\n\ndef ensure_future(coro_or_future, *, loop=None):\n    \"\"\"Wrap a coroutine or an awaitable in a future.\n\n    If the argument is a Future, it is returned directly.\n    \"\"\"\n    return _ensure_future(coro_or_future, loop=loop)\n\n\ndef _ensure_future(coro_or_future, *, loop=None):\n    if futures.isfuture(coro_or_future):\n        if loop is not None and loop is not futures._get_loop(coro_or_future):\n            raise ValueError('The future belongs to a different loop than '\n                            'the one specified as the loop argument')\n        return coro_or_future\n    called_wrap_awaitable = False\n    if not coroutines.iscoroutine(coro_or_future):\n        if inspect.isawaitable(coro_or_future):\n            coro_or_future = _wrap_awaitable(coro_or_future)\n            called_wrap_awaitable = True\n        else:\n            raise TypeError('An asyncio.Future, a coroutine or an awaitable '\n                            'is required')\n\n    if loop is None:\n        loop = events._get_event_loop(stacklevel=4)\n    try:\n        return loop.create_task(coro_or_future)\n    except RuntimeError:\n        if not called_wrap_awaitable:\n            coro_or_future.close()\n        raise\n\n\n@types.coroutine\ndef _wrap_awaitable(awaitable):\n    \"\"\"Helper for asyncio.ensure_future().\n\n    Wraps awaitable (an object with __await__) into a coroutine\n    that will later be wrapped in a Task by ensure_future().\n    \"\"\"\n    return (yield from awaitable.__await__())\n\n_wrap_awaitable._is_coroutine = _is_coroutine\n\n\nclass _GatheringFuture(futures.Future):\n    \"\"\"Helper for gather().\n\n    This overrides cancel() to cancel all the children and act more\n    like Task.cancel(), which doesn't immediately mark itself as\n    cancelled.\n    \"\"\"\n\n    def __init__(self, children, *, loop):\n        assert loop is not None\n        super().__init__(loop=loop)\n        self._children = children\n        self._cancel_requested = False\n\n    def cancel(self, msg=None):\n        if self.done():\n            return False\n        ret = False\n        for child in self._children:\n            if child.cancel(msg=msg):\n                ret = True\n        if ret:\n            # If any child tasks were actually cancelled, we should\n            # propagate the cancellation request regardless of\n            # *return_exceptions* argument.  See issue 32684.\n            self._cancel_requested = True\n        return ret\n\n\ndef gather(*coros_or_futures, return_exceptions=False):\n    \"\"\"Return a future aggregating results from the given coroutines/futures.\n\n    Coroutines will be wrapped in a future and scheduled in the event\n    loop. They will not necessarily be scheduled in the same order as\n    passed in.\n\n    All futures must share the same event loop.  If all the tasks are\n    done successfully, the returned future's result is the list of\n    results (in the order of the original sequence, not necessarily\n    the order of results arrival).  If *return_exceptions* is True,\n    exceptions in the tasks are treated the same as successful\n    results, and gathered in the result list; otherwise, the first\n    raised exception will be immediately propagated to the returned\n    future.\n\n    Cancellation: if the outer Future is cancelled, all children (that\n    have not completed yet) are also cancelled.  If any child is\n    cancelled, this is treated as if it raised CancelledError --\n    the outer Future is *not* cancelled in this case.  (This is to\n    prevent the cancellation of one child to cause other children to\n    be cancelled.)\n\n    If *return_exceptions* is False, cancelling gather() after it\n    has been marked done won't cancel any submitted awaitables.\n    For instance, gather can be marked done after propagating an\n    exception to the caller, therefore, calling ``gather.cancel()``\n    after catching an exception (raised by one of the awaitables) from\n    gather won't cancel any other awaitables.\n    \"\"\"\n    if not coros_or_futures:\n        loop = events._get_event_loop()\n        outer = loop.create_future()\n        outer.set_result([])\n        return outer\n\n    def _done_callback(fut):\n        nonlocal nfinished\n        nfinished += 1\n\n        if outer is None or outer.done():\n            if not fut.cancelled():\n                # Mark exception retrieved.\n                fut.exception()\n            return\n\n        if not return_exceptions:\n            if fut.cancelled():\n                # Check if 'fut' is cancelled first, as\n                # 'fut.exception()' will *raise* a CancelledError\n                # instead of returning it.\n                exc = fut._make_cancelled_error()\n                outer.set_exception(exc)\n                return\n            else:\n                exc = fut.exception()\n                if exc is not None:\n                    outer.set_exception(exc)\n                    return\n\n        if nfinished == nfuts:\n            # All futures are done; create a list of results\n            # and set it to the 'outer' future.\n            results = []\n\n            for fut in children:\n                if fut.cancelled():\n                    # Check if 'fut' is cancelled first, as 'fut.exception()'\n                    # will *raise* a CancelledError instead of returning it.\n                    # Also, since we're adding the exception return value\n                    # to 'results' instead of raising it, don't bother\n                    # setting __context__.  This also lets us preserve\n                    # calling '_make_cancelled_error()' at most once.\n                    res = exceptions.CancelledError(\n                        '' if fut._cancel_message is None else\n                        fut._cancel_message)\n                else:\n                    res = fut.exception()\n                    if res is None:\n                        res = fut.result()\n                results.append(res)\n\n            if outer._cancel_requested:\n                # If gather is being cancelled we must propagate the\n                # cancellation regardless of *return_exceptions* argument.\n                # See issue 32684.\n                exc = fut._make_cancelled_error()\n                outer.set_exception(exc)\n            else:\n                outer.set_result(results)\n\n    arg_to_fut = {}\n    children = []\n    nfuts = 0\n    nfinished = 0\n    loop = None\n    outer = None  # bpo-46672\n    for arg in coros_or_futures:\n        if arg not in arg_to_fut:\n            fut = _ensure_future(arg, loop=loop)\n            if loop is None:\n                loop = futures._get_loop(fut)\n            if fut is not arg:\n                # 'arg' was not a Future, therefore, 'fut' is a new\n                # Future created specifically for 'arg'.  Since the caller\n                # can't control it, disable the \"destroy pending task\"\n                # warning.\n                fut._log_destroy_pending = False\n\n            nfuts += 1\n            arg_to_fut[arg] = fut\n            fut.add_done_callback(_done_callback)\n\n        else:\n            # There's a duplicate Future object in coros_or_futures.\n            fut = arg_to_fut[arg]\n\n        children.append(fut)\n\n    outer = _GatheringFuture(children, loop=loop)\n    return outer\n\n\ndef shield(arg):\n    \"\"\"Wait for a future, shielding it from cancellation.\n\n    The statement\n\n        task = asyncio.create_task(something())\n        res = await shield(task)\n\n    is exactly equivalent to the statement\n\n        res = await something()\n\n    *except* that if the coroutine containing it is cancelled, the\n    task running in something() is not cancelled.  From the POV of\n    something(), the cancellation did not happen.  But its caller is\n    still cancelled, so the yield-from expression still raises\n    CancelledError.  Note: If something() is cancelled by other means\n    this will still cancel shield().\n\n    If you want to completely ignore cancellation (not recommended)\n    you can combine shield() with a try/except clause, as follows:\n\n        task = asyncio.create_task(something())\n        try:\n            res = await shield(task)\n        except CancelledError:\n            res = None\n\n    Save a reference to tasks passed to this function, to avoid\n    a task disappearing mid-execution. The event loop only keeps\n    weak references to tasks. A task that isn't referenced elsewhere\n    may get garbage collected at any time, even before it's done.\n    \"\"\"\n    inner = _ensure_future(arg)\n    if inner.done():\n        # Shortcut.\n        return inner\n    loop = futures._get_loop(inner)\n    outer = loop.create_future()\n\n    def _inner_done_callback(inner):\n        if outer.cancelled():\n            if not inner.cancelled():\n                # Mark inner's result as retrieved.\n                inner.exception()\n            return\n\n        if inner.cancelled():\n            outer.cancel()\n        else:\n            exc = inner.exception()\n            if exc is not None:\n                outer.set_exception(exc)\n            else:\n                outer.set_result(inner.result())\n\n\n    def _outer_done_callback(outer):\n        if not inner.done():\n            inner.remove_done_callback(_inner_done_callback)\n\n    inner.add_done_callback(_inner_done_callback)\n    outer.add_done_callback(_outer_done_callback)\n    return outer\n\n\ndef run_coroutine_threadsafe(coro, loop):\n    \"\"\"Submit a coroutine object to a given event loop.\n\n    Return a concurrent.futures.Future to access the result.\n    \"\"\"\n    if not coroutines.iscoroutine(coro):\n        raise TypeError('A coroutine object is required')\n    future = concurrent.futures.Future()\n\n    def callback():\n        try:\n            futures._chain_future(ensure_future(coro, loop=loop), future)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            if future.set_running_or_notify_cancel():\n                future.set_exception(exc)\n            raise\n\n    loop.call_soon_threadsafe(callback)\n    return future\n\n\n# WeakSet containing all alive tasks.\n_all_tasks = weakref.WeakSet()\n\n# Dictionary containing tasks that are currently active in\n# all running event loops.  {EventLoop: Task}\n_current_tasks = {}\n\n\ndef _register_task(task):\n    \"\"\"Register a new task in asyncio as executed by loop.\"\"\"\n    _all_tasks.add(task)\n\n\ndef _enter_task(loop, task):\n    current_task = _current_tasks.get(loop)\n    if current_task is not None:\n        raise RuntimeError(f\"Cannot enter into task {task!r} while another \"\n                           f\"task {current_task!r} is being executed.\")\n    _current_tasks[loop] = task\n\n\ndef _leave_task(loop, task):\n    current_task = _current_tasks.get(loop)\n    if current_task is not task:\n        raise RuntimeError(f\"Leaving task {task!r} does not match \"\n                           f\"the current task {current_task!r}.\")\n    del _current_tasks[loop]\n\n\ndef _unregister_task(task):\n    \"\"\"Unregister a task.\"\"\"\n    _all_tasks.discard(task)\n\n\n_py_register_task = _register_task\n_py_unregister_task = _unregister_task\n_py_enter_task = _enter_task\n_py_leave_task = _leave_task\n\n\ntry:\n    from _asyncio import (_register_task, _unregister_task,\n                          _enter_task, _leave_task,\n                          _all_tasks, _current_tasks)\nexcept ImportError:\n    pass\nelse:\n    _c_register_task = _register_task\n    _c_unregister_task = _unregister_task\n    _c_enter_task = _enter_task\n    _c_leave_task = _leave_task\n", 990], "/app/app/providers/base.py": ["from abc import ABC, abstractmethod\nfrom datetime import datetime\nimport logging\nfrom typing import Dict, List, Optional\n\nfrom ..utils.circuit_breaker import (\n    CircuitOpenException,\n    CircuitTimeoutException,\n    circuit_breakers,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass DataProvider(ABC):\n    \"\"\"Base class for data providers with circuit breaker protection.\"\"\"\n\n    def __init__(self, name: str):\n        self.name = name\n        self.is_available = True\n        self.enabled = True\n        self.last_error = None\n        self.last_update = None\n        self.circuit_breaker = circuit_breakers.get_breaker(name)\n\n    @abstractmethod\n    async def get_price(self, symbol: str) -> Optional[Dict]:\n        \"\"\"Get current stock price.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    async def get_historical(self, symbol: str, period: str) -> Optional[Dict]:\n        \"\"\"Get historical stock data.\"\"\"\n        raise NotImplementedError\n\n    async def get_intraday(self, symbol: str, interval: str = \"1m\") -> Optional[Dict]:\n        \"\"\"Get intraday stock data at a given interval. Optional to implement.\"\"\"\n        return None\n\n    async def fetch_bars(self, symbol: str, start, end, interval: str) -> List[Dict]:\n        \"\"\"Return normalized bar list for registry consumers.\"\"\"\n        payload = await self.get_intraday(symbol, interval=interval)\n        if not payload:\n            return []\n\n        data = payload.get(\"data\") or []\n        bars: List[Dict] = []\n        for bar in data:\n            ts = bar.get(\"timestamp\") or bar.get(\"ts\")\n            if ts is None:\n                continue\n            bars.append(\n                {\n                    \"symbol\": symbol.upper(),\n                    \"ts\": datetime.fromisoformat(str(ts)),\n                    \"interval\": interval,\n                    \"o\": float(bar.get(\"open\", 0)),\n                    \"h\": float(bar.get(\"high\", 0)),\n                    \"l\": float(bar.get(\"low\", 0)),\n                    \"c\": float(bar.get(\"close\", 0)),\n                    \"v\": float(bar.get(\"volume\", 0)),\n                    \"status\": bar.get(\"status\", \"final\"),\n                    \"as_of\": datetime.utcnow(),\n                }\n            )\n        return bars\n\n    def mark_unavailable(self, error: str):\n        \"\"\"Mark provider as unavailable.\"\"\"\n        self.is_available = False\n        self.last_error = error\n        self.last_update = datetime.now()\n\n    def mark_available(self):\n        \"\"\"Mark provider as available.\"\"\"\n        self.is_available = True\n        self.last_error = None\n        self.last_update = datetime.now()\n\n    async def get_price_safe(self, symbol: str) -> Optional[Dict]:\n        \"\"\"Get price with circuit breaker protection.\"\"\"\n        try:\n            result = await self.circuit_breaker.call(self._get_price_impl, symbol)\n            self.mark_available()\n            return result\n        except (CircuitOpenException, CircuitTimeoutException) as exc:\n            self.mark_unavailable(str(exc))\n            logger.warning(\"Provider %s failed for %s: %s\", self.name, symbol, exc)\n            return None\n        except Exception as exc:  # pragma: no cover - defensive\n            self.mark_unavailable(str(exc))\n            logger.error(\"Provider %s error for %s: %s\", self.name, symbol, exc)\n            return None\n\n    async def get_historical_safe(self, symbol: str, period: str) -> Optional[Dict]:\n        \"\"\"Get historical data with circuit breaker protection.\"\"\"\n        try:\n            result = await self.circuit_breaker.call(self._get_historical_impl, symbol, period)\n            self.mark_available()\n            return result\n        except (CircuitOpenException, CircuitTimeoutException) as exc:\n            self.mark_unavailable(str(exc))\n            logger.warning(\"Provider %s failed for %s: %s\", self.name, symbol, exc)\n            return None\n        except Exception as exc:  # pragma: no cover - defensive\n            self.mark_unavailable(str(exc))\n            logger.error(\"Provider %s error for %s: %s\", self.name, symbol, exc)\n            return None\n\n    async def get_intraday_safe(self, symbol: str, interval: str = \"1m\") -> Optional[Dict]:\n        \"\"\"Get intraday data with circuit breaker protection.\"\"\"\n        try:\n            result = await self.circuit_breaker.call(self._get_intraday_impl, symbol, interval)\n            self.mark_available()\n            return result\n        except (CircuitOpenException, CircuitTimeoutException) as exc:\n            self.mark_unavailable(str(exc))\n            logger.warning(\"Provider %s failed for %s: %s\", self.name, symbol, exc)\n            return None\n        except Exception as exc:  # pragma: no cover - defensive\n            self.mark_unavailable(str(exc))\n            logger.error(\"Provider %s error for %s: %s\", self.name, symbol, exc)\n            return None\n\n    async def _get_price_impl(self, symbol: str) -> Optional[Dict]:\n        \"\"\"Internal implementation - calls the abstract method.\"\"\"\n        return await self.get_price(symbol)\n\n    async def _get_historical_impl(self, symbol: str, period: str) -> Optional[Dict]:\n        \"\"\"Internal implementation - calls the abstract method.\"\"\"\n        return await self.get_historical(symbol, period)\n\n    async def _get_intraday_impl(self, symbol: str, interval: str = \"1m\") -> Optional[Dict]:\n        \"\"\"Internal implementation - calls the optional method.\"\"\"\n        return await self.get_intraday(symbol, interval)\n\n    def get_provider_stats(self) -> Dict:\n        \"\"\"Get provider statistics including circuit breaker metrics.\"\"\"\n        stats = self.circuit_breaker.get_stats()\n        stats.update(\n            {\n                \"is_available\": self.is_available,\n                \"enabled\": self.enabled,\n                \"last_error\": self.last_error,\n                \"last_update\": self.last_update.isoformat() if self.last_update else None,\n            }\n        )\n        return stats\n", 148], "/app/app/providers/finnhub_provider.py": ["import finnhub\nimport logging\nimport asyncio\nfrom datetime import datetime, timedelta\nfrom typing import Dict, Optional\nfrom .base import DataProvider\n\nlogger = logging.getLogger(__name__)\n\nclass FinnhubProvider(DataProvider):\n    def __init__(self, api_key: str = \"demo\"):\n        super().__init__(\"Finnhub\")\n        self.api_key = api_key\n        self.client = finnhub.Client(api_key=api_key)\n        self.rate_limit_delay = 1.0  # 60 calls/minute = 1 call/second for free tier\n        \n    async def get_price(self, symbol: str) -> Optional[Dict]:\n        \"\"\"Get current stock price from Finnhub\"\"\"\n        try:\n            # Add rate limiting\n            await asyncio.sleep(self.rate_limit_delay)\n            \n            # Get current price quote\n            quote = await asyncio.to_thread(self.client.quote, symbol)\n            \n            if not quote or 'c' not in quote:\n                return None\n            \n            current_price = float(quote['c'])  # Current price\n            previous_close = float(quote['pc'])  # Previous close\n            \n            if current_price == 0:\n                return None\n            \n            change = current_price - previous_close\n            change_percent = (change / previous_close * 100) if previous_close != 0 else 0\n            \n            self.mark_available()\n            return {\n                \"symbol\": symbol.upper(),\n                \"price\": round(current_price, 2),\n                \"change\": round(change, 2),\n                \"change_percent\": round(change_percent, 2),\n                \"high\": round(float(quote.get('h', current_price)), 2),  # Day high\n                \"low\": round(float(quote.get('l', current_price)), 2),   # Day low\n                \"open\": round(float(quote.get('o', current_price)), 2),  # Day open\n                \"previous_close\": round(previous_close, 2),\n                \"timestamp\": datetime.now().isoformat(),\n                \"source\": self.name\n            }\n            \n        except Exception as e:\n            error_msg = f\"Finnhub error for {symbol}: {e}\"\n            self.mark_unavailable(error_msg)\n            logger.error(error_msg)\n            return None\n    \n    async def get_historical(self, symbol: str, period: str = \"1mo\") -> Optional[Dict]:\n        \"\"\"Get historical data from Finnhub\"\"\"\n        try:\n            await asyncio.sleep(self.rate_limit_delay)\n            \n            # Convert period to date range\n            end_date = datetime.now()\n            \n            period_map = {\n                \"1d\": 1,\n                \"5d\": 5,\n                \"1mo\": 30,\n                \"3mo\": 90,\n                \"6mo\": 180,\n                \"1y\": 365\n            }\n            \n            days = period_map.get(period, 30)\n            start_date = end_date - timedelta(days=days)\n            \n            # Convert to Unix timestamps\n            start_ts = int(start_date.timestamp())\n            end_ts = int(end_date.timestamp())\n            \n            # Get candle data (daily resolution)\n            candles = await asyncio.to_thread(\n                self.client.stock_candles, \n                symbol, \n                'D',  # Daily resolution\n                start_ts, \n                end_ts\n            )\n            \n            if not candles or candles['s'] != 'ok':\n                return None\n            \n            data = []\n            timestamps = candles['t']\n            opens = candles['o']\n            highs = candles['h']\n            lows = candles['l']\n            closes = candles['c']\n            volumes = candles['v']\n            \n            for i in range(len(timestamps)):\n                date = datetime.fromtimestamp(timestamps[i])\n                data.append({\n                    \"date\": date.strftime(\"%Y-%m-%d\"),\n                    \"open\": round(float(opens[i]), 2),\n                    \"high\": round(float(highs[i]), 2),\n                    \"low\": round(float(lows[i]), 2),\n                    \"close\": round(float(closes[i]), 2),\n                    \"volume\": int(volumes[i])\n                })\n            \n            return {\n                \"symbol\": symbol.upper(),\n                \"period\": period,\n                \"data\": data,\n                \"source\": self.name\n            }\n            \n        except Exception as e:\n            logger.error(f\"Finnhub historical error for {symbol}: {e}\")\n            return None\n\n    async def get_company_profile(self, symbol: str) -> Optional[Dict]:\n        \"\"\"Get company profile data\"\"\"\n        try:\n            await asyncio.sleep(self.rate_limit_delay)\n            \n            profile = await asyncio.to_thread(self.client.company_profile2, symbol=symbol)\n            \n            if not profile:\n                return None\n                \n            return {\n                \"symbol\": symbol.upper(),\n                \"name\": profile.get('name'),\n                \"country\": profile.get('country'),\n                \"currency\": profile.get('currency'),\n                \"exchange\": profile.get('exchange'),\n                \"ipo\": profile.get('ipo'),\n                \"market_cap\": profile.get('marketCapitalization'),\n                \"shares_outstanding\": profile.get('shareOutstanding'),\n                \"logo\": profile.get('logo'),\n                \"phone\": profile.get('phone'),\n                \"weburl\": profile.get('weburl'),\n                \"industry\": profile.get('finnhubIndustry'),\n                \"source\": self.name\n            }\n            \n        except Exception as e:\n            logger.error(f\"Finnhub company profile error for {symbol}: {e}\")\n            return None\n\n    async def get_news_sentiment(self, symbol: str) -> Optional[Dict]:\n        \"\"\"Get news sentiment for a symbol\"\"\"\n        try:\n            await asyncio.sleep(self.rate_limit_delay)\n            \n            sentiment = await asyncio.to_thread(self.client.news_sentiment, symbol)\n            \n            if not sentiment:\n                return None\n                \n            return {\n                \"symbol\": symbol.upper(),\n                \"sentiment\": {\n                    \"buzz\": sentiment.get('buzz', {}),\n                    \"sentiment\": sentiment.get('sentiment', {}),\n                    \"company_news_score\": sentiment.get('companyNewsScore'),\n                    \"sector_average_bullishness\": sentiment.get('sectorAverageBullishness'),\n                    \"sector_average_news_score\": sentiment.get('sectorAverageNewsScore')\n                },\n                \"source\": self.name\n            }\n            \n        except Exception as e:\n            logger.error(f\"Finnhub news sentiment error for {symbol}: {e}\")\n            return None\n\n    async def get_intraday(self, symbol: str, interval: str = \"1m\") -> Optional[Dict]:\n        \"\"\"Get intraday minute data from Finnhub\"\"\"\n        try:\n            await asyncio.sleep(self.rate_limit_delay)\n\n            end_date = datetime.now()\n            # fetch last trading day window (~6.5h = 390 minutes)\n            start_date = end_date - timedelta(hours=8)\n            start_ts = int(start_date.timestamp())\n            end_ts = int(end_date.timestamp())\n\n            resolution = '1'  # 1-minute\n            candles = await asyncio.to_thread(self.client.stock_candles, symbol, resolution, start_ts, end_ts)\n            if not candles or candles.get('s') != 'ok':\n                return None\n\n            data = []\n            for i, ts in enumerate(candles['t']):\n                date = datetime.fromtimestamp(ts)\n                data.append({\n                    \"timestamp\": date.isoformat(),\n                    \"open\": round(float(candles['o'][i]), 4),\n                    \"high\": round(float(candles['h'][i]), 4),\n                    \"low\": round(float(candles['l'][i]), 4),\n                    \"close\": round(float(candles['c'][i]), 4),\n                    \"volume\": int(candles['v'][i])\n                })\n\n            return {\"symbol\": symbol.upper(), \"interval\": interval, \"data\": data, \"source\": self.name}\n        except Exception as e:\n            logger.error(f\"Finnhub intraday error for {symbol}: {e}\")\n            return None\n", 211], "/usr/local/lib/python3.11/asyncio/futures.py": ["\"\"\"A Future class similar to the one in PEP 3148.\"\"\"\n\n__all__ = (\n    'Future', 'wrap_future', 'isfuture',\n)\n\nimport concurrent.futures\nimport contextvars\nimport logging\nimport sys\nfrom types import GenericAlias\n\nfrom . import base_futures\nfrom . import events\nfrom . import exceptions\nfrom . import format_helpers\n\n\nisfuture = base_futures.isfuture\n\n\n_PENDING = base_futures._PENDING\n_CANCELLED = base_futures._CANCELLED\n_FINISHED = base_futures._FINISHED\n\n\nSTACK_DEBUG = logging.DEBUG - 1  # heavy-duty debugging\n\n\nclass Future:\n    \"\"\"This class is *almost* compatible with concurrent.futures.Future.\n\n    Differences:\n\n    - This class is not thread-safe.\n\n    - result() and exception() do not take a timeout argument and\n      raise an exception when the future isn't done yet.\n\n    - Callbacks registered with add_done_callback() are always called\n      via the event loop's call_soon().\n\n    - This class is not compatible with the wait() and as_completed()\n      methods in the concurrent.futures package.\n\n    (In Python 3.4 or later we may be able to unify the implementations.)\n    \"\"\"\n\n    # Class variables serving as defaults for instance variables.\n    _state = _PENDING\n    _result = None\n    _exception = None\n    _loop = None\n    _source_traceback = None\n    _cancel_message = None\n    # A saved CancelledError for later chaining as an exception context.\n    _cancelled_exc = None\n\n    # This field is used for a dual purpose:\n    # - Its presence is a marker to declare that a class implements\n    #   the Future protocol (i.e. is intended to be duck-type compatible).\n    #   The value must also be not-None, to enable a subclass to declare\n    #   that it is not compatible by setting this to None.\n    # - It is set by __iter__() below so that Task._step() can tell\n    #   the difference between\n    #   `await Future()` or`yield from Future()` (correct) vs.\n    #   `yield Future()` (incorrect).\n    _asyncio_future_blocking = False\n\n    __log_traceback = False\n\n    def __init__(self, *, loop=None):\n        \"\"\"Initialize the future.\n\n        The optional event_loop argument allows explicitly setting the event\n        loop object used by the future. If it's not provided, the future uses\n        the default event loop.\n        \"\"\"\n        if loop is None:\n            self._loop = events._get_event_loop()\n        else:\n            self._loop = loop\n        self._callbacks = []\n        if self._loop.get_debug():\n            self._source_traceback = format_helpers.extract_stack(\n                sys._getframe(1))\n\n    def __repr__(self):\n        return base_futures._future_repr(self)\n\n    def __del__(self):\n        if not self.__log_traceback:\n            # set_exception() was not called, or result() or exception()\n            # has consumed the exception\n            return\n        exc = self._exception\n        context = {\n            'message':\n                f'{self.__class__.__name__} exception was never retrieved',\n            'exception': exc,\n            'future': self,\n        }\n        if self._source_traceback:\n            context['source_traceback'] = self._source_traceback\n        self._loop.call_exception_handler(context)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    @property\n    def _log_traceback(self):\n        return self.__log_traceback\n\n    @_log_traceback.setter\n    def _log_traceback(self, val):\n        if val:\n            raise ValueError('_log_traceback can only be set to False')\n        self.__log_traceback = False\n\n    def get_loop(self):\n        \"\"\"Return the event loop the Future is bound to.\"\"\"\n        loop = self._loop\n        if loop is None:\n            raise RuntimeError(\"Future object is not initialized.\")\n        return loop\n\n    def _make_cancelled_error(self):\n        \"\"\"Create the CancelledError to raise if the Future is cancelled.\n\n        This should only be called once when handling a cancellation since\n        it erases the saved context exception value.\n        \"\"\"\n        if self._cancelled_exc is not None:\n            exc = self._cancelled_exc\n            self._cancelled_exc = None\n            return exc\n\n        if self._cancel_message is None:\n            exc = exceptions.CancelledError()\n        else:\n            exc = exceptions.CancelledError(self._cancel_message)\n        exc.__context__ = self._cancelled_exc\n        # Remove the reference since we don't need this anymore.\n        self._cancelled_exc = None\n        return exc\n\n    def cancel(self, msg=None):\n        \"\"\"Cancel the future and schedule callbacks.\n\n        If the future is already done or cancelled, return False.  Otherwise,\n        change the future's state to cancelled, schedule the callbacks and\n        return True.\n        \"\"\"\n        self.__log_traceback = False\n        if self._state != _PENDING:\n            return False\n        self._state = _CANCELLED\n        self._cancel_message = msg\n        self.__schedule_callbacks()\n        return True\n\n    def __schedule_callbacks(self):\n        \"\"\"Internal: Ask the event loop to call all callbacks.\n\n        The callbacks are scheduled to be called as soon as possible. Also\n        clears the callback list.\n        \"\"\"\n        callbacks = self._callbacks[:]\n        if not callbacks:\n            return\n\n        self._callbacks[:] = []\n        for callback, ctx in callbacks:\n            self._loop.call_soon(callback, self, context=ctx)\n\n    def cancelled(self):\n        \"\"\"Return True if the future was cancelled.\"\"\"\n        return self._state == _CANCELLED\n\n    # Don't implement running(); see http://bugs.python.org/issue18699\n\n    def done(self):\n        \"\"\"Return True if the future is done.\n\n        Done means either that a result / exception are available, or that the\n        future was cancelled.\n        \"\"\"\n        return self._state != _PENDING\n\n    def result(self):\n        \"\"\"Return the result this future represents.\n\n        If the future has been cancelled, raises CancelledError.  If the\n        future's result isn't yet available, raises InvalidStateError.  If\n        the future is done and has an exception set, this exception is raised.\n        \"\"\"\n        if self._state == _CANCELLED:\n            exc = self._make_cancelled_error()\n            raise exc\n        if self._state != _FINISHED:\n            raise exceptions.InvalidStateError('Result is not ready.')\n        self.__log_traceback = False\n        if self._exception is not None:\n            raise self._exception.with_traceback(self._exception_tb)\n        return self._result\n\n    def exception(self):\n        \"\"\"Return the exception that was set on this future.\n\n        The exception (or None if no exception was set) is returned only if\n        the future is done.  If the future has been cancelled, raises\n        CancelledError.  If the future isn't done yet, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state == _CANCELLED:\n            exc = self._make_cancelled_error()\n            raise exc\n        if self._state != _FINISHED:\n            raise exceptions.InvalidStateError('Exception is not set.')\n        self.__log_traceback = False\n        return self._exception\n\n    def add_done_callback(self, fn, *, context=None):\n        \"\"\"Add a callback to be run when the future becomes done.\n\n        The callback is called with a single argument - the future object. If\n        the future is already done when this is called, the callback is\n        scheduled with call_soon.\n        \"\"\"\n        if self._state != _PENDING:\n            self._loop.call_soon(fn, self, context=context)\n        else:\n            if context is None:\n                context = contextvars.copy_context()\n            self._callbacks.append((fn, context))\n\n    # New method not in PEP 3148.\n\n    def remove_done_callback(self, fn):\n        \"\"\"Remove all instances of a callback from the \"call when done\" list.\n\n        Returns the number of callbacks removed.\n        \"\"\"\n        filtered_callbacks = [(f, ctx)\n                              for (f, ctx) in self._callbacks\n                              if f != fn]\n        removed_count = len(self._callbacks) - len(filtered_callbacks)\n        if removed_count:\n            self._callbacks[:] = filtered_callbacks\n        return removed_count\n\n    # So-called internal methods (note: no set_running_or_notify_cancel()).\n\n    def set_result(self, result):\n        \"\"\"Mark the future done and set its result.\n\n        If the future is already done when this method is called, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state != _PENDING:\n            raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n        self._result = result\n        self._state = _FINISHED\n        self.__schedule_callbacks()\n\n    def set_exception(self, exception):\n        \"\"\"Mark the future done and set an exception.\n\n        If the future is already done when this method is called, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state != _PENDING:\n            raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n        if isinstance(exception, type):\n            exception = exception()\n        if type(exception) is StopIteration:\n            raise TypeError(\"StopIteration interacts badly with generators \"\n                            \"and cannot be raised into a Future\")\n        self._exception = exception\n        self._exception_tb = exception.__traceback__\n        self._state = _FINISHED\n        self.__schedule_callbacks()\n        self.__log_traceback = True\n\n    def __await__(self):\n        if not self.done():\n            self._asyncio_future_blocking = True\n            yield self  # This tells Task to wait for completion.\n        if not self.done():\n            raise RuntimeError(\"await wasn't used with future\")\n        return self.result()  # May raise too.\n\n    __iter__ = __await__  # make compatible with 'yield from'.\n\n\n# Needed for testing purposes.\n_PyFuture = Future\n\n\ndef _get_loop(fut):\n    # Tries to call Future.get_loop() if it's available.\n    # Otherwise fallbacks to using the old '_loop' property.\n    try:\n        get_loop = fut.get_loop\n    except AttributeError:\n        pass\n    else:\n        return get_loop()\n    return fut._loop\n\n\ndef _set_result_unless_cancelled(fut, result):\n    \"\"\"Helper setting the result only if the future was not cancelled.\"\"\"\n    if fut.cancelled():\n        return\n    fut.set_result(result)\n\n\ndef _convert_future_exc(exc):\n    exc_class = type(exc)\n    if exc_class is concurrent.futures.CancelledError:\n        return exceptions.CancelledError(*exc.args)\n    elif exc_class is concurrent.futures.TimeoutError:\n        return exceptions.TimeoutError(*exc.args)\n    elif exc_class is concurrent.futures.InvalidStateError:\n        return exceptions.InvalidStateError(*exc.args)\n    else:\n        return exc\n\n\ndef _set_concurrent_future_state(concurrent, source):\n    \"\"\"Copy state from a future to a concurrent.futures.Future.\"\"\"\n    assert source.done()\n    if source.cancelled():\n        concurrent.cancel()\n    if not concurrent.set_running_or_notify_cancel():\n        return\n    exception = source.exception()\n    if exception is not None:\n        concurrent.set_exception(_convert_future_exc(exception))\n    else:\n        result = source.result()\n        concurrent.set_result(result)\n\n\ndef _copy_future_state(source, dest):\n    \"\"\"Internal helper to copy state from another Future.\n\n    The other Future may be a concurrent.futures.Future.\n    \"\"\"\n    assert source.done()\n    if dest.cancelled():\n        return\n    assert not dest.done()\n    if source.cancelled():\n        dest.cancel()\n    else:\n        exception = source.exception()\n        if exception is not None:\n            dest.set_exception(_convert_future_exc(exception))\n        else:\n            result = source.result()\n            dest.set_result(result)\n\n\ndef _chain_future(source, destination):\n    \"\"\"Chain two futures so that when one completes, so does the other.\n\n    The result (or exception) of source will be copied to destination.\n    If destination is cancelled, source gets cancelled too.\n    Compatible with both asyncio.Future and concurrent.futures.Future.\n    \"\"\"\n    if not isfuture(source) and not isinstance(source,\n                                               concurrent.futures.Future):\n        raise TypeError('A future is required for source argument')\n    if not isfuture(destination) and not isinstance(destination,\n                                                    concurrent.futures.Future):\n        raise TypeError('A future is required for destination argument')\n    source_loop = _get_loop(source) if isfuture(source) else None\n    dest_loop = _get_loop(destination) if isfuture(destination) else None\n\n    def _set_state(future, other):\n        if isfuture(future):\n            _copy_future_state(other, future)\n        else:\n            _set_concurrent_future_state(future, other)\n\n    def _call_check_cancel(destination):\n        if destination.cancelled():\n            if source_loop is None or source_loop is dest_loop:\n                source.cancel()\n            else:\n                source_loop.call_soon_threadsafe(source.cancel)\n\n    def _call_set_state(source):\n        if (destination.cancelled() and\n                dest_loop is not None and dest_loop.is_closed()):\n            return\n        if dest_loop is None or dest_loop is source_loop:\n            _set_state(destination, source)\n        else:\n            if dest_loop.is_closed():\n                return\n            dest_loop.call_soon_threadsafe(_set_state, destination, source)\n\n    destination.add_done_callback(_call_check_cancel)\n    source.add_done_callback(_call_set_state)\n\n\ndef wrap_future(future, *, loop=None):\n    \"\"\"Wrap concurrent.futures.Future object.\"\"\"\n    if isfuture(future):\n        return future\n    assert isinstance(future, concurrent.futures.Future), \\\n        f'concurrent.futures.Future is expected, got {future!r}'\n    if loop is None:\n        loop = events._get_event_loop()\n    new_future = loop.create_future()\n    _chain_future(future, new_future)\n    return new_future\n\n\ntry:\n    import _asyncio\nexcept ImportError:\n    pass\nelse:\n    # _CFuture is needed for tests.\n    Future = _CFuture = _asyncio.Future\n", 428], "/usr/local/lib/python3.11/site-packages/uvicorn/server.py": ["from __future__ import annotations\n\nimport asyncio\nimport contextlib\nimport logging\nimport os\nimport platform\nimport signal\nimport socket\nimport sys\nimport threading\nimport time\nfrom collections.abc import Generator, Sequence\nfrom email.utils import formatdate\nfrom types import FrameType\nfrom typing import TYPE_CHECKING, Union\n\nimport click\n\nfrom uvicorn._compat import asyncio_run\nfrom uvicorn.config import Config\n\nif TYPE_CHECKING:\n    from uvicorn.protocols.http.h11_impl import H11Protocol\n    from uvicorn.protocols.http.httptools_impl import HttpToolsProtocol\n    from uvicorn.protocols.websockets.websockets_impl import WebSocketProtocol\n    from uvicorn.protocols.websockets.websockets_sansio_impl import WebSocketsSansIOProtocol\n    from uvicorn.protocols.websockets.wsproto_impl import WSProtocol\n\n    Protocols = Union[H11Protocol, HttpToolsProtocol, WSProtocol, WebSocketProtocol, WebSocketsSansIOProtocol]\n\nHANDLED_SIGNALS = (\n    signal.SIGINT,  # Unix signal 2. Sent by Ctrl+C.\n    signal.SIGTERM,  # Unix signal 15. Sent by `kill <pid>`.\n)\nif sys.platform == \"win32\":  # pragma: py-not-win32\n    HANDLED_SIGNALS += (signal.SIGBREAK,)  # Windows signal 21. Sent by Ctrl+Break.\n\nlogger = logging.getLogger(\"uvicorn.error\")\n\n\nclass ServerState:\n    \"\"\"\n    Shared servers state that is available between all protocol instances.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.total_requests = 0\n        self.connections: set[Protocols] = set()\n        self.tasks: set[asyncio.Task[None]] = set()\n        self.default_headers: list[tuple[bytes, bytes]] = []\n\n\nclass Server:\n    def __init__(self, config: Config) -> None:\n        self.config = config\n        self.server_state = ServerState()\n\n        self.started = False\n        self.should_exit = False\n        self.force_exit = False\n        self.last_notified = 0.0\n\n        self._captured_signals: list[int] = []\n\n    def run(self, sockets: list[socket.socket] | None = None) -> None:\n        return asyncio_run(self.serve(sockets=sockets), loop_factory=self.config.get_loop_factory())\n\n    async def serve(self, sockets: list[socket.socket] | None = None) -> None:\n        with self.capture_signals():\n            await self._serve(sockets)\n\n    async def _serve(self, sockets: list[socket.socket] | None = None) -> None:\n        process_id = os.getpid()\n\n        config = self.config\n        if not config.loaded:\n            config.load()\n\n        self.lifespan = config.lifespan_class(config)\n\n        message = \"Started server process [%d]\"\n        color_message = \"Started server process [\" + click.style(\"%d\", fg=\"cyan\") + \"]\"\n        logger.info(message, process_id, extra={\"color_message\": color_message})\n\n        await self.startup(sockets=sockets)\n        if self.should_exit:\n            return\n        await self.main_loop()\n        await self.shutdown(sockets=sockets)\n\n        message = \"Finished server process [%d]\"\n        color_message = \"Finished server process [\" + click.style(\"%d\", fg=\"cyan\") + \"]\"\n        logger.info(message, process_id, extra={\"color_message\": color_message})\n\n    async def startup(self, sockets: list[socket.socket] | None = None) -> None:\n        await self.lifespan.startup()\n        if self.lifespan.should_exit:\n            self.should_exit = True\n            return\n\n        config = self.config\n\n        def create_protocol(\n            _loop: asyncio.AbstractEventLoop | None = None,\n        ) -> asyncio.Protocol:\n            return config.http_protocol_class(  # type: ignore[call-arg]\n                config=config,\n                server_state=self.server_state,\n                app_state=self.lifespan.state,\n                _loop=_loop,\n            )\n\n        loop = asyncio.get_running_loop()\n\n        listeners: Sequence[socket.SocketType]\n        if sockets is not None:  # pragma: full coverage\n            # Explicitly passed a list of open sockets.\n            # We use this when the server is run from a Gunicorn worker.\n\n            def _share_socket(\n                sock: socket.SocketType,\n            ) -> socket.SocketType:  # pragma py-not-win32\n                # Windows requires the socket be explicitly shared across\n                # multiple workers (processes).\n                from socket import fromshare  # type: ignore[attr-defined]\n\n                sock_data = sock.share(os.getpid())  # type: ignore[attr-defined]\n                return fromshare(sock_data)\n\n            self.servers: list[asyncio.base_events.Server] = []\n            for sock in sockets:\n                is_windows = platform.system() == \"Windows\"\n                if config.workers > 1 and is_windows:  # pragma: py-not-win32\n                    sock = _share_socket(sock)  # type: ignore[assignment]\n                server = await loop.create_server(create_protocol, sock=sock, ssl=config.ssl, backlog=config.backlog)\n                self.servers.append(server)\n            listeners = sockets\n\n        elif config.fd is not None:  # pragma: py-win32\n            # Use an existing socket, from a file descriptor.\n            sock = socket.fromfd(config.fd, socket.AF_UNIX, socket.SOCK_STREAM)\n            server = await loop.create_server(create_protocol, sock=sock, ssl=config.ssl, backlog=config.backlog)\n            assert server.sockets is not None  # mypy\n            listeners = server.sockets\n            self.servers = [server]\n\n        elif config.uds is not None:  # pragma: py-win32\n            # Create a socket using UNIX domain socket.\n            uds_perms = 0o666\n            if os.path.exists(config.uds):\n                uds_perms = os.stat(config.uds).st_mode  # pragma: full coverage\n            server = await loop.create_unix_server(\n                create_protocol, path=config.uds, ssl=config.ssl, backlog=config.backlog\n            )\n            os.chmod(config.uds, uds_perms)\n            assert server.sockets is not None  # mypy\n            listeners = server.sockets\n            self.servers = [server]\n\n        else:\n            # Standard case. Create a socket from a host/port pair.\n            try:\n                server = await loop.create_server(\n                    create_protocol,\n                    host=config.host,\n                    port=config.port,\n                    ssl=config.ssl,\n                    backlog=config.backlog,\n                )\n            except OSError as exc:\n                logger.error(exc)\n                await self.lifespan.shutdown()\n                sys.exit(1)\n\n            assert server.sockets is not None\n            listeners = server.sockets\n            self.servers = [server]\n\n        if sockets is None:\n            self._log_started_message(listeners)\n        else:\n            # We're most likely running multiple workers, so a message has already been\n            # logged by `config.bind_socket()`.\n            pass  # pragma: full coverage\n\n        self.started = True\n\n    def _log_started_message(self, listeners: Sequence[socket.SocketType]) -> None:\n        config = self.config\n\n        if config.fd is not None:  # pragma: py-win32\n            sock = listeners[0]\n            logger.info(\n                \"Uvicorn running on socket %s (Press CTRL+C to quit)\",\n                sock.getsockname(),\n            )\n\n        elif config.uds is not None:  # pragma: py-win32\n            logger.info(\"Uvicorn running on unix socket %s (Press CTRL+C to quit)\", config.uds)\n\n        else:\n            addr_format = \"%s://%s:%d\"\n            host = \"0.0.0.0\" if config.host is None else config.host\n            if \":\" in host:\n                # It's an IPv6 address.\n                addr_format = \"%s://[%s]:%d\"\n\n            port = config.port\n            if port == 0:\n                port = listeners[0].getsockname()[1]\n\n            protocol_name = \"https\" if config.ssl else \"http\"\n            message = f\"Uvicorn running on {addr_format} (Press CTRL+C to quit)\"\n            color_message = \"Uvicorn running on \" + click.style(addr_format, bold=True) + \" (Press CTRL+C to quit)\"\n            logger.info(\n                message,\n                protocol_name,\n                host,\n                port,\n                extra={\"color_message\": color_message},\n            )\n\n    async def main_loop(self) -> None:\n        counter = 0\n        should_exit = await self.on_tick(counter)\n        while not should_exit:\n            counter += 1\n            counter = counter % 864000\n            await asyncio.sleep(0.1)\n            should_exit = await self.on_tick(counter)\n\n    async def on_tick(self, counter: int) -> bool:\n        # Update the default headers, once per second.\n        if counter % 10 == 0:\n            current_time = time.time()\n            current_date = formatdate(current_time, usegmt=True).encode()\n\n            if self.config.date_header:\n                date_header = [(b\"date\", current_date)]\n            else:\n                date_header = []\n\n            self.server_state.default_headers = date_header + self.config.encoded_headers\n\n            # Callback to `callback_notify` once every `timeout_notify` seconds.\n            if self.config.callback_notify is not None:\n                if current_time - self.last_notified > self.config.timeout_notify:  # pragma: full coverage\n                    self.last_notified = current_time\n                    await self.config.callback_notify()\n\n        # Determine if we should exit.\n        if self.should_exit:\n            return True\n\n        max_requests = self.config.limit_max_requests\n        if max_requests is not None and self.server_state.total_requests >= max_requests:\n            logger.warning(f\"Maximum request limit of {max_requests} exceeded. Terminating process.\")\n            return True\n\n        return False\n\n    async def shutdown(self, sockets: list[socket.socket] | None = None) -> None:\n        logger.info(\"Shutting down\")\n\n        # Stop accepting new connections.\n        for server in self.servers:\n            server.close()\n        for sock in sockets or []:\n            sock.close()  # pragma: full coverage\n\n        # Request shutdown on all existing connections.\n        for connection in list(self.server_state.connections):\n            connection.shutdown()\n        await asyncio.sleep(0.1)\n\n        # When 3.10 is not supported anymore, use `async with asyncio.timeout(...):`.\n        try:\n            await asyncio.wait_for(\n                self._wait_tasks_to_complete(),\n                timeout=self.config.timeout_graceful_shutdown,\n            )\n        except asyncio.TimeoutError:\n            logger.error(\n                \"Cancel %s running task(s), timeout graceful shutdown exceeded\",\n                len(self.server_state.tasks),\n            )\n            for t in self.server_state.tasks:\n                t.cancel(msg=\"Task cancelled, timeout graceful shutdown exceeded\")\n\n        # Send the lifespan shutdown event, and wait for application shutdown.\n        if not self.force_exit:\n            await self.lifespan.shutdown()\n\n    async def _wait_tasks_to_complete(self) -> None:\n        # Wait for existing connections to finish sending responses.\n        if self.server_state.connections and not self.force_exit:\n            msg = \"Waiting for connections to close. (CTRL+C to force quit)\"\n            logger.info(msg)\n            while self.server_state.connections and not self.force_exit:\n                await asyncio.sleep(0.1)\n\n        # Wait for existing tasks to complete.\n        if self.server_state.tasks and not self.force_exit:\n            msg = \"Waiting for background tasks to complete. (CTRL+C to force quit)\"\n            logger.info(msg)\n            while self.server_state.tasks and not self.force_exit:\n                await asyncio.sleep(0.1)\n\n        for server in self.servers:\n            await server.wait_closed()\n\n    @contextlib.contextmanager\n    def capture_signals(self) -> Generator[None, None, None]:\n        # Signals can only be listened to from the main thread.\n        if threading.current_thread() is not threading.main_thread():\n            yield\n            return\n        # always use signal.signal, even if loop.add_signal_handler is available\n        # this allows to restore previous signal handlers later on\n        original_handlers = {sig: signal.signal(sig, self.handle_exit) for sig in HANDLED_SIGNALS}\n        try:\n            yield\n        finally:\n            for sig, handler in original_handlers.items():\n                signal.signal(sig, handler)\n        # If we did gracefully shut down due to a signal, try to\n        # trigger the expected behaviour now; multiple signals would be\n        # done LIFO, see https://stackoverflow.com/questions/48434964\n        for captured_signal in reversed(self._captured_signals):\n            signal.raise_signal(captured_signal)\n\n    def handle_exit(self, sig: int, frame: FrameType | None) -> None:\n        self._captured_signals.append(sig)\n        if self.should_exit and sig == signal.SIGINT:\n            self.force_exit = True  # pragma: full coverage\n        else:\n            self.should_exit = True\n", 338], "/usr/local/lib/python3.11/asyncio/streams.py": ["__all__ = (\n    'StreamReader', 'StreamWriter', 'StreamReaderProtocol',\n    'open_connection', 'start_server')\n\nimport collections\nimport socket\nimport sys\nimport warnings\nimport weakref\n\nif hasattr(socket, 'AF_UNIX'):\n    __all__ += ('open_unix_connection', 'start_unix_server')\n\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import format_helpers\nfrom . import protocols\nfrom .log import logger\nfrom .tasks import sleep\n\n\n_DEFAULT_LIMIT = 2 ** 16  # 64 KiB\n\n\nasync def open_connection(host=None, port=None, *,\n                          limit=_DEFAULT_LIMIT, **kwds):\n    \"\"\"A wrapper for create_connection() returning a (reader, writer) pair.\n\n    The reader returned is a StreamReader instance; the writer is a\n    StreamWriter instance.\n\n    The arguments are all the usual arguments to create_connection()\n    except protocol_factory; most common are positional host and port,\n    with various optional keyword arguments following.\n\n    Additional optional keyword arguments are loop (to set the event loop\n    instance to use) and limit (to set the buffer limit passed to the\n    StreamReader).\n\n    (If you want to customize the StreamReader and/or\n    StreamReaderProtocol classes, just copy the code -- there's\n    really nothing special here except some convenience.)\n    \"\"\"\n    loop = events.get_running_loop()\n    reader = StreamReader(limit=limit, loop=loop)\n    protocol = StreamReaderProtocol(reader, loop=loop)\n    transport, _ = await loop.create_connection(\n        lambda: protocol, host, port, **kwds)\n    writer = StreamWriter(transport, protocol, reader, loop)\n    return reader, writer\n\n\nasync def start_server(client_connected_cb, host=None, port=None, *,\n                       limit=_DEFAULT_LIMIT, **kwds):\n    \"\"\"Start a socket server, call back for each client connected.\n\n    The first parameter, `client_connected_cb`, takes two parameters:\n    client_reader, client_writer.  client_reader is a StreamReader\n    object, while client_writer is a StreamWriter object.  This\n    parameter can either be a plain callback function or a coroutine;\n    if it is a coroutine, it will be automatically converted into a\n    Task.\n\n    The rest of the arguments are all the usual arguments to\n    loop.create_server() except protocol_factory; most common are\n    positional host and port, with various optional keyword arguments\n    following.  The return value is the same as loop.create_server().\n\n    Additional optional keyword argument is limit (to set the buffer\n    limit passed to the StreamReader).\n\n    The return value is the same as loop.create_server(), i.e. a\n    Server object which can be used to stop the service.\n    \"\"\"\n    loop = events.get_running_loop()\n\n    def factory():\n        reader = StreamReader(limit=limit, loop=loop)\n        protocol = StreamReaderProtocol(reader, client_connected_cb,\n                                        loop=loop)\n        return protocol\n\n    return await loop.create_server(factory, host, port, **kwds)\n\n\nif hasattr(socket, 'AF_UNIX'):\n    # UNIX Domain Sockets are supported on this platform\n\n    async def open_unix_connection(path=None, *,\n                                   limit=_DEFAULT_LIMIT, **kwds):\n        \"\"\"Similar to `open_connection` but works with UNIX Domain Sockets.\"\"\"\n        loop = events.get_running_loop()\n\n        reader = StreamReader(limit=limit, loop=loop)\n        protocol = StreamReaderProtocol(reader, loop=loop)\n        transport, _ = await loop.create_unix_connection(\n            lambda: protocol, path, **kwds)\n        writer = StreamWriter(transport, protocol, reader, loop)\n        return reader, writer\n\n    async def start_unix_server(client_connected_cb, path=None, *,\n                                limit=_DEFAULT_LIMIT, **kwds):\n        \"\"\"Similar to `start_server` but works with UNIX Domain Sockets.\"\"\"\n        loop = events.get_running_loop()\n\n        def factory():\n            reader = StreamReader(limit=limit, loop=loop)\n            protocol = StreamReaderProtocol(reader, client_connected_cb,\n                                            loop=loop)\n            return protocol\n\n        return await loop.create_unix_server(factory, path, **kwds)\n\n\nclass FlowControlMixin(protocols.Protocol):\n    \"\"\"Reusable flow control logic for StreamWriter.drain().\n\n    This implements the protocol methods pause_writing(),\n    resume_writing() and connection_lost().  If the subclass overrides\n    these it must call the super methods.\n\n    StreamWriter.drain() must wait for _drain_helper() coroutine.\n    \"\"\"\n\n    def __init__(self, loop=None):\n        if loop is None:\n            self._loop = events._get_event_loop(stacklevel=4)\n        else:\n            self._loop = loop\n        self._paused = False\n        self._drain_waiters = collections.deque()\n        self._connection_lost = False\n\n    def pause_writing(self):\n        assert not self._paused\n        self._paused = True\n        if self._loop.get_debug():\n            logger.debug(\"%r pauses writing\", self)\n\n    def resume_writing(self):\n        assert self._paused\n        self._paused = False\n        if self._loop.get_debug():\n            logger.debug(\"%r resumes writing\", self)\n\n        for waiter in self._drain_waiters:\n            if not waiter.done():\n                waiter.set_result(None)\n\n    def connection_lost(self, exc):\n        self._connection_lost = True\n        # Wake up the writer(s) if currently paused.\n        if not self._paused:\n            return\n\n        for waiter in self._drain_waiters:\n            if not waiter.done():\n                if exc is None:\n                    waiter.set_result(None)\n                else:\n                    waiter.set_exception(exc)\n\n    async def _drain_helper(self):\n        if self._connection_lost:\n            raise ConnectionResetError('Connection lost')\n        if not self._paused:\n            return\n        waiter = self._loop.create_future()\n        self._drain_waiters.append(waiter)\n        try:\n            await waiter\n        finally:\n            self._drain_waiters.remove(waiter)\n\n    def _get_close_waiter(self, stream):\n        raise NotImplementedError\n\n\nclass StreamReaderProtocol(FlowControlMixin, protocols.Protocol):\n    \"\"\"Helper class to adapt between Protocol and StreamReader.\n\n    (This is a helper class instead of making StreamReader itself a\n    Protocol subclass, because the StreamReader has other potential\n    uses, and to prevent the user of the StreamReader to accidentally\n    call inappropriate methods of the protocol.)\n    \"\"\"\n\n    _source_traceback = None\n\n    def __init__(self, stream_reader, client_connected_cb=None, loop=None):\n        super().__init__(loop=loop)\n        if stream_reader is not None:\n            self._stream_reader_wr = weakref.ref(stream_reader)\n            self._source_traceback = stream_reader._source_traceback\n        else:\n            self._stream_reader_wr = None\n        if client_connected_cb is not None:\n            # This is a stream created by the `create_server()` function.\n            # Keep a strong reference to the reader until a connection\n            # is established.\n            self._strong_reader = stream_reader\n        self._reject_connection = False\n        self._stream_writer = None\n        self._task = None\n        self._transport = None\n        self._client_connected_cb = client_connected_cb\n        self._over_ssl = False\n        self._closed = self._loop.create_future()\n\n    @property\n    def _stream_reader(self):\n        if self._stream_reader_wr is None:\n            return None\n        return self._stream_reader_wr()\n\n    def _replace_writer(self, writer):\n        loop = self._loop\n        transport = writer.transport\n        self._stream_writer = writer\n        self._transport = transport\n        self._over_ssl = transport.get_extra_info('sslcontext') is not None\n\n    def connection_made(self, transport):\n        if self._reject_connection:\n            context = {\n                'message': ('An open stream was garbage collected prior to '\n                            'establishing network connection; '\n                            'call \"stream.close()\" explicitly.')\n            }\n            if self._source_traceback:\n                context['source_traceback'] = self._source_traceback\n            self._loop.call_exception_handler(context)\n            transport.abort()\n            return\n        self._transport = transport\n        reader = self._stream_reader\n        if reader is not None:\n            reader.set_transport(transport)\n        self._over_ssl = transport.get_extra_info('sslcontext') is not None\n        if self._client_connected_cb is not None:\n            self._stream_writer = StreamWriter(transport, self,\n                                               reader,\n                                               self._loop)\n            res = self._client_connected_cb(reader,\n                                            self._stream_writer)\n            if coroutines.iscoroutine(res):\n                def callback(task):\n                    if task.cancelled():\n                        transport.close()\n                        return\n                    exc = task.exception()\n                    if exc is not None:\n                        self._loop.call_exception_handler({\n                            'message': 'Unhandled exception in client_connected_cb',\n                            'exception': exc,\n                            'transport': transport,\n                        })\n                        transport.close()\n\n                self._task = self._loop.create_task(res)\n                self._task.add_done_callback(callback)\n\n            self._strong_reader = None\n\n    def connection_lost(self, exc):\n        reader = self._stream_reader\n        if reader is not None:\n            if exc is None:\n                reader.feed_eof()\n            else:\n                reader.set_exception(exc)\n        if not self._closed.done():\n            if exc is None:\n                self._closed.set_result(None)\n            else:\n                self._closed.set_exception(exc)\n        super().connection_lost(exc)\n        self._stream_reader_wr = None\n        self._stream_writer = None\n        self._task = None\n        self._transport = None\n\n    def data_received(self, data):\n        reader = self._stream_reader\n        if reader is not None:\n            reader.feed_data(data)\n\n    def eof_received(self):\n        reader = self._stream_reader\n        if reader is not None:\n            reader.feed_eof()\n        if self._over_ssl:\n            # Prevent a warning in SSLProtocol.eof_received:\n            # \"returning true from eof_received()\n            # has no effect when using ssl\"\n            return False\n        return True\n\n    def _get_close_waiter(self, stream):\n        return self._closed\n\n    def __del__(self):\n        # Prevent reports about unhandled exceptions.\n        # Better than self._closed._log_traceback = False hack\n        try:\n            closed = self._closed\n        except AttributeError:\n            pass  # failed constructor\n        else:\n            if closed.done() and not closed.cancelled():\n                closed.exception()\n\n\nclass StreamWriter:\n    \"\"\"Wraps a Transport.\n\n    This exposes write(), writelines(), [can_]write_eof(),\n    get_extra_info() and close().  It adds drain() which returns an\n    optional Future on which you can wait for flow control.  It also\n    adds a transport property which references the Transport\n    directly.\n    \"\"\"\n\n    def __init__(self, transport, protocol, reader, loop):\n        self._transport = transport\n        self._protocol = protocol\n        # drain() expects that the reader has an exception() method\n        assert reader is None or isinstance(reader, StreamReader)\n        self._reader = reader\n        self._loop = loop\n        self._complete_fut = self._loop.create_future()\n        self._complete_fut.set_result(None)\n\n    def __repr__(self):\n        info = [self.__class__.__name__, f'transport={self._transport!r}']\n        if self._reader is not None:\n            info.append(f'reader={self._reader!r}')\n        return '<{}>'.format(' '.join(info))\n\n    @property\n    def transport(self):\n        return self._transport\n\n    def write(self, data):\n        self._transport.write(data)\n\n    def writelines(self, data):\n        self._transport.writelines(data)\n\n    def write_eof(self):\n        return self._transport.write_eof()\n\n    def can_write_eof(self):\n        return self._transport.can_write_eof()\n\n    def close(self):\n        return self._transport.close()\n\n    def is_closing(self):\n        return self._transport.is_closing()\n\n    async def wait_closed(self):\n        await self._protocol._get_close_waiter(self)\n\n    def get_extra_info(self, name, default=None):\n        return self._transport.get_extra_info(name, default)\n\n    async def drain(self):\n        \"\"\"Flush the write buffer.\n\n        The intended use is to write\n\n          w.write(data)\n          await w.drain()\n        \"\"\"\n        if self._reader is not None:\n            exc = self._reader.exception()\n            if exc is not None:\n                raise exc\n        if self._transport.is_closing():\n            # Wait for protocol.connection_lost() call\n            # Raise connection closing error if any,\n            # ConnectionResetError otherwise\n            # Yield to the event loop so connection_lost() may be\n            # called.  Without this, _drain_helper() would return\n            # immediately, and code that calls\n            #     write(...); await drain()\n            # in a loop would never call connection_lost(), so it\n            # would not see an error when the socket is closed.\n            await sleep(0)\n        await self._protocol._drain_helper()\n\n    async def start_tls(self, sslcontext, *,\n                        server_hostname=None,\n                        ssl_handshake_timeout=None):\n        \"\"\"Upgrade an existing stream-based connection to TLS.\"\"\"\n        server_side = self._protocol._client_connected_cb is not None\n        protocol = self._protocol\n        await self.drain()\n        new_transport = await self._loop.start_tls(  # type: ignore\n            self._transport, protocol, sslcontext,\n            server_side=server_side, server_hostname=server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout)\n        self._transport = new_transport\n        protocol._replace_writer(self)\n\n    def __del__(self):\n        if not self._transport.is_closing():\n            if self._loop.is_closed():\n                warnings.warn(\"loop is closed\", ResourceWarning)\n            else:\n                self.close()\n                warnings.warn(f\"unclosed {self!r}\", ResourceWarning)\n\nclass StreamReader:\n\n    _source_traceback = None\n\n    def __init__(self, limit=_DEFAULT_LIMIT, loop=None):\n        # The line length limit is  a security feature;\n        # it also doubles as half the buffer limit.\n\n        if limit <= 0:\n            raise ValueError('Limit cannot be <= 0')\n\n        self._limit = limit\n        if loop is None:\n            self._loop = events._get_event_loop()\n        else:\n            self._loop = loop\n        self._buffer = bytearray()\n        self._eof = False    # Whether we're done.\n        self._waiter = None  # A future used by _wait_for_data()\n        self._exception = None\n        self._transport = None\n        self._paused = False\n        if self._loop.get_debug():\n            self._source_traceback = format_helpers.extract_stack(\n                sys._getframe(1))\n\n    def __repr__(self):\n        info = ['StreamReader']\n        if self._buffer:\n            info.append(f'{len(self._buffer)} bytes')\n        if self._eof:\n            info.append('eof')\n        if self._limit != _DEFAULT_LIMIT:\n            info.append(f'limit={self._limit}')\n        if self._waiter:\n            info.append(f'waiter={self._waiter!r}')\n        if self._exception:\n            info.append(f'exception={self._exception!r}')\n        if self._transport:\n            info.append(f'transport={self._transport!r}')\n        if self._paused:\n            info.append('paused')\n        return '<{}>'.format(' '.join(info))\n\n    def exception(self):\n        return self._exception\n\n    def set_exception(self, exc):\n        self._exception = exc\n\n        waiter = self._waiter\n        if waiter is not None:\n            self._waiter = None\n            if not waiter.cancelled():\n                waiter.set_exception(exc)\n\n    def _wakeup_waiter(self):\n        \"\"\"Wakeup read*() functions waiting for data or EOF.\"\"\"\n        waiter = self._waiter\n        if waiter is not None:\n            self._waiter = None\n            if not waiter.cancelled():\n                waiter.set_result(None)\n\n    def set_transport(self, transport):\n        assert self._transport is None, 'Transport already set'\n        self._transport = transport\n\n    def _maybe_resume_transport(self):\n        if self._paused and len(self._buffer) <= self._limit:\n            self._paused = False\n            self._transport.resume_reading()\n\n    def feed_eof(self):\n        self._eof = True\n        self._wakeup_waiter()\n\n    def at_eof(self):\n        \"\"\"Return True if the buffer is empty and 'feed_eof' was called.\"\"\"\n        return self._eof and not self._buffer\n\n    def feed_data(self, data):\n        assert not self._eof, 'feed_data after feed_eof'\n\n        if not data:\n            return\n\n        self._buffer.extend(data)\n        self._wakeup_waiter()\n\n        if (self._transport is not None and\n                not self._paused and\n                len(self._buffer) > 2 * self._limit):\n            try:\n                self._transport.pause_reading()\n            except NotImplementedError:\n                # The transport can't be paused.\n                # We'll just have to buffer all data.\n                # Forget the transport so we don't keep trying.\n                self._transport = None\n            else:\n                self._paused = True\n\n    async def _wait_for_data(self, func_name):\n        \"\"\"Wait until feed_data() or feed_eof() is called.\n\n        If stream was paused, automatically resume it.\n        \"\"\"\n        # StreamReader uses a future to link the protocol feed_data() method\n        # to a read coroutine. Running two read coroutines at the same time\n        # would have an unexpected behaviour. It would not possible to know\n        # which coroutine would get the next data.\n        if self._waiter is not None:\n            raise RuntimeError(\n                f'{func_name}() called while another coroutine is '\n                f'already waiting for incoming data')\n\n        assert not self._eof, '_wait_for_data after EOF'\n\n        # Waiting for data while paused will make deadlock, so prevent it.\n        # This is essential for readexactly(n) for case when n > self._limit.\n        if self._paused:\n            self._paused = False\n            self._transport.resume_reading()\n\n        self._waiter = self._loop.create_future()\n        try:\n            await self._waiter\n        finally:\n            self._waiter = None\n\n    async def readline(self):\n        \"\"\"Read chunk of data from the stream until newline (b'\\n') is found.\n\n        On success, return chunk that ends with newline. If only partial\n        line can be read due to EOF, return incomplete line without\n        terminating newline. When EOF was reached while no bytes read, empty\n        bytes object is returned.\n\n        If limit is reached, ValueError will be raised. In that case, if\n        newline was found, complete line including newline will be removed\n        from internal buffer. Else, internal buffer will be cleared. Limit is\n        compared against part of the line without newline.\n\n        If stream was paused, this function will automatically resume it if\n        needed.\n        \"\"\"\n        sep = b'\\n'\n        seplen = len(sep)\n        try:\n            line = await self.readuntil(sep)\n        except exceptions.IncompleteReadError as e:\n            return e.partial\n        except exceptions.LimitOverrunError as e:\n            if self._buffer.startswith(sep, e.consumed):\n                del self._buffer[:e.consumed + seplen]\n            else:\n                self._buffer.clear()\n            self._maybe_resume_transport()\n            raise ValueError(e.args[0])\n        return line\n\n    async def readuntil(self, separator=b'\\n'):\n        \"\"\"Read data from the stream until ``separator`` is found.\n\n        On success, the data and separator will be removed from the\n        internal buffer (consumed). Returned data will include the\n        separator at the end.\n\n        Configured stream limit is used to check result. Limit sets the\n        maximal length of data that can be returned, not counting the\n        separator.\n\n        If an EOF occurs and the complete separator is still not found,\n        an IncompleteReadError exception will be raised, and the internal\n        buffer will be reset.  The IncompleteReadError.partial attribute\n        may contain the separator partially.\n\n        If the data cannot be read because of over limit, a\n        LimitOverrunError exception  will be raised, and the data\n        will be left in the internal buffer, so it can be read again.\n        \"\"\"\n        seplen = len(separator)\n        if seplen == 0:\n            raise ValueError('Separator should be at least one-byte string')\n\n        if self._exception is not None:\n            raise self._exception\n\n        # Consume whole buffer except last bytes, which length is\n        # one less than seplen. Let's check corner cases with\n        # separator='SEPARATOR':\n        # * we have received almost complete separator (without last\n        #   byte). i.e buffer='some textSEPARATO'. In this case we\n        #   can safely consume len(separator) - 1 bytes.\n        # * last byte of buffer is first byte of separator, i.e.\n        #   buffer='abcdefghijklmnopqrS'. We may safely consume\n        #   everything except that last byte, but this require to\n        #   analyze bytes of buffer that match partial separator.\n        #   This is slow and/or require FSM. For this case our\n        #   implementation is not optimal, since require rescanning\n        #   of data that is known to not belong to separator. In\n        #   real world, separator will not be so long to notice\n        #   performance problems. Even when reading MIME-encoded\n        #   messages :)\n\n        # `offset` is the number of bytes from the beginning of the buffer\n        # where there is no occurrence of `separator`.\n        offset = 0\n\n        # Loop until we find `separator` in the buffer, exceed the buffer size,\n        # or an EOF has happened.\n        while True:\n            buflen = len(self._buffer)\n\n            # Check if we now have enough data in the buffer for `separator` to\n            # fit.\n            if buflen - offset >= seplen:\n                isep = self._buffer.find(separator, offset)\n\n                if isep != -1:\n                    # `separator` is in the buffer. `isep` will be used later\n                    # to retrieve the data.\n                    break\n\n                # see upper comment for explanation.\n                offset = buflen + 1 - seplen\n                if offset > self._limit:\n                    raise exceptions.LimitOverrunError(\n                        'Separator is not found, and chunk exceed the limit',\n                        offset)\n\n            # Complete message (with full separator) may be present in buffer\n            # even when EOF flag is set. This may happen when the last chunk\n            # adds data which makes separator be found. That's why we check for\n            # EOF *ater* inspecting the buffer.\n            if self._eof:\n                chunk = bytes(self._buffer)\n                self._buffer.clear()\n                raise exceptions.IncompleteReadError(chunk, None)\n\n            # _wait_for_data() will resume reading if stream was paused.\n            await self._wait_for_data('readuntil')\n\n        if isep > self._limit:\n            raise exceptions.LimitOverrunError(\n                'Separator is found, but chunk is longer than limit', isep)\n\n        chunk = self._buffer[:isep + seplen]\n        del self._buffer[:isep + seplen]\n        self._maybe_resume_transport()\n        return bytes(chunk)\n\n    async def read(self, n=-1):\n        \"\"\"Read up to `n` bytes from the stream.\n\n        If `n` is not provided or set to -1,\n        read until EOF, then return all read bytes.\n        If EOF was received and the internal buffer is empty,\n        return an empty bytes object.\n\n        If `n` is 0, return an empty bytes object immediately.\n\n        If `n` is positive, return at most `n` available bytes\n        as soon as at least 1 byte is available in the internal buffer.\n        If EOF is received before any byte is read, return an empty\n        bytes object.\n\n        Returned value is not limited with limit, configured at stream\n        creation.\n\n        If stream was paused, this function will automatically resume it if\n        needed.\n        \"\"\"\n\n        if self._exception is not None:\n            raise self._exception\n\n        if n == 0:\n            return b''\n\n        if n < 0:\n            # This used to just loop creating a new waiter hoping to\n            # collect everything in self._buffer, but that would\n            # deadlock if the subprocess sends more than self.limit\n            # bytes.  So just call self.read(self._limit) until EOF.\n            blocks = []\n            while True:\n                block = await self.read(self._limit)\n                if not block:\n                    break\n                blocks.append(block)\n            return b''.join(blocks)\n\n        if not self._buffer and not self._eof:\n            await self._wait_for_data('read')\n\n        # This will work right even if buffer is less than n bytes\n        data = bytes(self._buffer[:n])\n        del self._buffer[:n]\n\n        self._maybe_resume_transport()\n        return data\n\n    async def readexactly(self, n):\n        \"\"\"Read exactly `n` bytes.\n\n        Raise an IncompleteReadError if EOF is reached before `n` bytes can be\n        read. The IncompleteReadError.partial attribute of the exception will\n        contain the partial read bytes.\n\n        if n is zero, return empty bytes object.\n\n        Returned value is not limited with limit, configured at stream\n        creation.\n\n        If stream was paused, this function will automatically resume it if\n        needed.\n        \"\"\"\n        if n < 0:\n            raise ValueError('readexactly size can not be less than zero')\n\n        if self._exception is not None:\n            raise self._exception\n\n        if n == 0:\n            return b''\n\n        while len(self._buffer) < n:\n            if self._eof:\n                incomplete = bytes(self._buffer)\n                self._buffer.clear()\n                raise exceptions.IncompleteReadError(incomplete, n)\n\n            await self._wait_for_data('readexactly')\n\n        if len(self._buffer) == n:\n            data = bytes(self._buffer)\n            self._buffer.clear()\n        else:\n            data = bytes(self._buffer[:n])\n            del self._buffer[:n]\n        self._maybe_resume_transport()\n        return data\n\n    def __aiter__(self):\n        return self\n\n    async def __anext__(self):\n        val = await self.readline()\n        if val == b'':\n            raise StopAsyncIteration\n        return val\n", 768], "/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py": ["import sys\nfrom abc import ABC\nfrom asyncio import IncompleteReadError, StreamReader, TimeoutError\nfrom typing import Callable, List, Optional, Protocol, Union\n\nif sys.version_info.major >= 3 and sys.version_info.minor >= 11:\n    from asyncio import timeout as async_timeout\nelse:\n    from async_timeout import timeout as async_timeout\n\nfrom ..exceptions import (\n    AskError,\n    AuthenticationError,\n    AuthenticationWrongNumberOfArgsError,\n    BusyLoadingError,\n    ClusterCrossSlotError,\n    ClusterDownError,\n    ConnectionError,\n    ExecAbortError,\n    MasterDownError,\n    ModuleError,\n    MovedError,\n    NoPermissionError,\n    NoScriptError,\n    OutOfMemoryError,\n    ReadOnlyError,\n    RedisError,\n    ResponseError,\n    TryAgainError,\n)\nfrom ..typing import EncodableT\nfrom .encoders import Encoder\nfrom .socket import SERVER_CLOSED_CONNECTION_ERROR, SocketBuffer\n\nMODULE_LOAD_ERROR = \"Error loading the extension. Please check the server logs.\"\nNO_SUCH_MODULE_ERROR = \"Error unloading module: no such module with that name\"\nMODULE_UNLOAD_NOT_POSSIBLE_ERROR = \"Error unloading module: operation not possible.\"\nMODULE_EXPORTS_DATA_TYPES_ERROR = (\n    \"Error unloading module: the module \"\n    \"exports one or more module-side data \"\n    \"types, can't unload\"\n)\n# user send an AUTH cmd to a server without authorization configured\nNO_AUTH_SET_ERROR = {\n    # Redis >= 6.0\n    \"AUTH <password> called without any password \"\n    \"configured for the default user. Are you sure \"\n    \"your configuration is correct?\": AuthenticationError,\n    # Redis < 6.0\n    \"Client sent AUTH, but no password is set\": AuthenticationError,\n}\n\n\nclass BaseParser(ABC):\n    EXCEPTION_CLASSES = {\n        \"ERR\": {\n            \"max number of clients reached\": ConnectionError,\n            \"invalid password\": AuthenticationError,\n            # some Redis server versions report invalid command syntax\n            # in lowercase\n            \"wrong number of arguments \"\n            \"for 'auth' command\": AuthenticationWrongNumberOfArgsError,\n            # some Redis server versions report invalid command syntax\n            # in uppercase\n            \"wrong number of arguments \"\n            \"for 'AUTH' command\": AuthenticationWrongNumberOfArgsError,\n            MODULE_LOAD_ERROR: ModuleError,\n            MODULE_EXPORTS_DATA_TYPES_ERROR: ModuleError,\n            NO_SUCH_MODULE_ERROR: ModuleError,\n            MODULE_UNLOAD_NOT_POSSIBLE_ERROR: ModuleError,\n            **NO_AUTH_SET_ERROR,\n        },\n        \"OOM\": OutOfMemoryError,\n        \"WRONGPASS\": AuthenticationError,\n        \"EXECABORT\": ExecAbortError,\n        \"LOADING\": BusyLoadingError,\n        \"NOSCRIPT\": NoScriptError,\n        \"READONLY\": ReadOnlyError,\n        \"NOAUTH\": AuthenticationError,\n        \"NOPERM\": NoPermissionError,\n        \"ASK\": AskError,\n        \"TRYAGAIN\": TryAgainError,\n        \"MOVED\": MovedError,\n        \"CLUSTERDOWN\": ClusterDownError,\n        \"CROSSSLOT\": ClusterCrossSlotError,\n        \"MASTERDOWN\": MasterDownError,\n    }\n\n    @classmethod\n    def parse_error(cls, response):\n        \"Parse an error response\"\n        error_code = response.split(\" \")[0]\n        if error_code in cls.EXCEPTION_CLASSES:\n            response = response[len(error_code) + 1 :]\n            exception_class = cls.EXCEPTION_CLASSES[error_code]\n            if isinstance(exception_class, dict):\n                exception_class = exception_class.get(response, ResponseError)\n            return exception_class(response)\n        return ResponseError(response)\n\n    def on_disconnect(self):\n        raise NotImplementedError()\n\n    def on_connect(self, connection):\n        raise NotImplementedError()\n\n\nclass _RESPBase(BaseParser):\n    \"\"\"Base class for sync-based resp parsing\"\"\"\n\n    def __init__(self, socket_read_size):\n        self.socket_read_size = socket_read_size\n        self.encoder = None\n        self._sock = None\n        self._buffer = None\n\n    def __del__(self):\n        try:\n            self.on_disconnect()\n        except Exception:\n            pass\n\n    def on_connect(self, connection):\n        \"Called when the socket connects\"\n        self._sock = connection._sock\n        self._buffer = SocketBuffer(\n            self._sock, self.socket_read_size, connection.socket_timeout\n        )\n        self.encoder = connection.encoder\n\n    def on_disconnect(self):\n        \"Called when the socket disconnects\"\n        self._sock = None\n        if self._buffer is not None:\n            self._buffer.close()\n            self._buffer = None\n        self.encoder = None\n\n    def can_read(self, timeout):\n        return self._buffer and self._buffer.can_read(timeout)\n\n\nclass AsyncBaseParser(BaseParser):\n    \"\"\"Base parsing class for the python-backed async parser\"\"\"\n\n    __slots__ = \"_stream\", \"_read_size\"\n\n    def __init__(self, socket_read_size: int):\n        self._stream: Optional[StreamReader] = None\n        self._read_size = socket_read_size\n\n    async def can_read_destructive(self) -> bool:\n        raise NotImplementedError()\n\n    async def read_response(\n        self, disable_decoding: bool = False\n    ) -> Union[EncodableT, ResponseError, None, List[EncodableT]]:\n        raise NotImplementedError()\n\n\n_INVALIDATION_MESSAGE = [b\"invalidate\", \"invalidate\"]\n\n\nclass PushNotificationsParser(Protocol):\n    \"\"\"Protocol defining RESP3-specific parsing functionality\"\"\"\n\n    pubsub_push_handler_func: Callable\n    invalidation_push_handler_func: Optional[Callable] = None\n\n    def handle_pubsub_push_response(self, response):\n        \"\"\"Handle pubsub push responses\"\"\"\n        raise NotImplementedError()\n\n    def handle_push_response(self, response, **kwargs):\n        if response[0] not in _INVALIDATION_MESSAGE:\n            return self.pubsub_push_handler_func(response)\n        if self.invalidation_push_handler_func:\n            return self.invalidation_push_handler_func(response)\n\n    def set_pubsub_push_handler(self, pubsub_push_handler_func):\n        self.pubsub_push_handler_func = pubsub_push_handler_func\n\n    def set_invalidation_push_handler(self, invalidation_push_handler_func):\n        self.invalidation_push_handler_func = invalidation_push_handler_func\n\n\nclass AsyncPushNotificationsParser(Protocol):\n    \"\"\"Protocol defining async RESP3-specific parsing functionality\"\"\"\n\n    pubsub_push_handler_func: Callable\n    invalidation_push_handler_func: Optional[Callable] = None\n\n    async def handle_pubsub_push_response(self, response):\n        \"\"\"Handle pubsub push responses asynchronously\"\"\"\n        raise NotImplementedError()\n\n    async def handle_push_response(self, response, **kwargs):\n        \"\"\"Handle push responses asynchronously\"\"\"\n        if response[0] not in _INVALIDATION_MESSAGE:\n            return await self.pubsub_push_handler_func(response)\n        if self.invalidation_push_handler_func:\n            return await self.invalidation_push_handler_func(response)\n\n    def set_pubsub_push_handler(self, pubsub_push_handler_func):\n        \"\"\"Set the pubsub push handler function\"\"\"\n        self.pubsub_push_handler_func = pubsub_push_handler_func\n\n    def set_invalidation_push_handler(self, invalidation_push_handler_func):\n        \"\"\"Set the invalidation push handler function\"\"\"\n        self.invalidation_push_handler_func = invalidation_push_handler_func\n\n\nclass _AsyncRESPBase(AsyncBaseParser):\n    \"\"\"Base class for async resp parsing\"\"\"\n\n    __slots__ = AsyncBaseParser.__slots__ + (\"encoder\", \"_buffer\", \"_pos\", \"_chunks\")\n\n    def __init__(self, socket_read_size: int):\n        super().__init__(socket_read_size)\n        self.encoder: Optional[Encoder] = None\n        self._buffer = b\"\"\n        self._chunks = []\n        self._pos = 0\n\n    def _clear(self):\n        self._buffer = b\"\"\n        self._chunks.clear()\n\n    def on_connect(self, connection):\n        \"\"\"Called when the stream connects\"\"\"\n        self._stream = connection._reader\n        if self._stream is None:\n            raise RedisError(\"Buffer is closed.\")\n        self.encoder = connection.encoder\n        self._clear()\n        self._connected = True\n\n    def on_disconnect(self):\n        \"\"\"Called when the stream disconnects\"\"\"\n        self._connected = False\n\n    async def can_read_destructive(self) -> bool:\n        if not self._connected:\n            raise RedisError(\"Buffer is closed.\")\n        if self._buffer:\n            return True\n        try:\n            async with async_timeout(0):\n                return self._stream.at_eof()\n        except TimeoutError:\n            return False\n\n    async def _read(self, length: int) -> bytes:\n        \"\"\"\n        Read `length` bytes of data.  These are assumed to be followed\n        by a '\\r\\n' terminator which is subsequently discarded.\n        \"\"\"\n        want = length + 2\n        end = self._pos + want\n        if len(self._buffer) >= end:\n            result = self._buffer[self._pos : end - 2]\n        else:\n            tail = self._buffer[self._pos :]\n            try:\n                data = await self._stream.readexactly(want - len(tail))\n            except IncompleteReadError as error:\n                raise ConnectionError(SERVER_CLOSED_CONNECTION_ERROR) from error\n            result = (tail + data)[:-2]\n            self._chunks.append(data)\n        self._pos += want\n        return result\n\n    async def _readline(self) -> bytes:\n        \"\"\"\n        read an unknown number of bytes up to the next '\\r\\n'\n        line separator, which is discarded.\n        \"\"\"\n        found = self._buffer.find(b\"\\r\\n\", self._pos)\n        if found >= 0:\n            result = self._buffer[self._pos : found]\n        else:\n            tail = self._buffer[self._pos :]\n            data = await self._stream.readline()\n            if not data.endswith(b\"\\r\\n\"):\n                raise ConnectionError(SERVER_CLOSED_CONNECTION_ERROR)\n            result = (tail + data)[:-2]\n            self._chunks.append(data)\n        self._pos += len(result) + 2\n        return result\n", 289], "/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py": ["from typing import Any, Union\n\nfrom ..exceptions import ConnectionError, InvalidResponse, ResponseError\nfrom ..typing import EncodableT\nfrom .base import _AsyncRESPBase, _RESPBase\nfrom .socket import SERVER_CLOSED_CONNECTION_ERROR\n\n\nclass _RESP2Parser(_RESPBase):\n    \"\"\"RESP2 protocol implementation\"\"\"\n\n    def read_response(self, disable_decoding=False):\n        pos = self._buffer.get_pos() if self._buffer else None\n        try:\n            result = self._read_response(disable_decoding=disable_decoding)\n        except BaseException:\n            if self._buffer:\n                self._buffer.rewind(pos)\n            raise\n        else:\n            self._buffer.purge()\n            return result\n\n    def _read_response(self, disable_decoding=False):\n        raw = self._buffer.readline()\n        if not raw:\n            raise ConnectionError(SERVER_CLOSED_CONNECTION_ERROR)\n\n        byte, response = raw[:1], raw[1:]\n\n        # server returned an error\n        if byte == b\"-\":\n            response = response.decode(\"utf-8\", errors=\"replace\")\n            error = self.parse_error(response)\n            # if the error is a ConnectionError, raise immediately so the user\n            # is notified\n            if isinstance(error, ConnectionError):\n                raise error\n            # otherwise, we're dealing with a ResponseError that might belong\n            # inside a pipeline response. the connection's read_response()\n            # and/or the pipeline's execute() will raise this error if\n            # necessary, so just return the exception instance here.\n            return error\n        # single value\n        elif byte == b\"+\":\n            pass\n        # int value\n        elif byte == b\":\":\n            return int(response)\n        # bulk response\n        elif byte == b\"$\" and response == b\"-1\":\n            return None\n        elif byte == b\"$\":\n            response = self._buffer.read(int(response))\n        # multi-bulk response\n        elif byte == b\"*\" and response == b\"-1\":\n            return None\n        elif byte == b\"*\":\n            response = [\n                self._read_response(disable_decoding=disable_decoding)\n                for i in range(int(response))\n            ]\n        else:\n            raise InvalidResponse(f\"Protocol Error: {raw!r}\")\n\n        if disable_decoding is False:\n            response = self.encoder.decode(response)\n        return response\n\n\nclass _AsyncRESP2Parser(_AsyncRESPBase):\n    \"\"\"Async class for the RESP2 protocol\"\"\"\n\n    async def read_response(self, disable_decoding: bool = False):\n        if not self._connected:\n            raise ConnectionError(SERVER_CLOSED_CONNECTION_ERROR)\n        if self._chunks:\n            # augment parsing buffer with previously read data\n            self._buffer += b\"\".join(self._chunks)\n            self._chunks.clear()\n        self._pos = 0\n        response = await self._read_response(disable_decoding=disable_decoding)\n        # Successfully parsing a response allows us to clear our parsing buffer\n        self._clear()\n        return response\n\n    async def _read_response(\n        self, disable_decoding: bool = False\n    ) -> Union[EncodableT, ResponseError, None]:\n        raw = await self._readline()\n        response: Any\n        byte, response = raw[:1], raw[1:]\n\n        # server returned an error\n        if byte == b\"-\":\n            response = response.decode(\"utf-8\", errors=\"replace\")\n            error = self.parse_error(response)\n            # if the error is a ConnectionError, raise immediately so the user\n            # is notified\n            if isinstance(error, ConnectionError):\n                self._clear()  # Successful parse\n                raise error\n            # otherwise, we're dealing with a ResponseError that might belong\n            # inside a pipeline response. the connection's read_response()\n            # and/or the pipeline's execute() will raise this error if\n            # necessary, so just return the exception instance here.\n            return error\n        # single value\n        elif byte == b\"+\":\n            pass\n        # int value\n        elif byte == b\":\":\n            return int(response)\n        # bulk response\n        elif byte == b\"$\" and response == b\"-1\":\n            return None\n        elif byte == b\"$\":\n            response = await self._read(int(response))\n        # multi-bulk response\n        elif byte == b\"*\" and response == b\"-1\":\n            return None\n        elif byte == b\"*\":\n            response = [\n                (await self._read_response(disable_decoding))\n                for _ in range(int(response))  # noqa\n            ]\n        else:\n            raise InvalidResponse(f\"Protocol Error: {raw!r}\")\n\n        if disable_decoding is False:\n            response = self.encoder.decode(response)\n        return response\n", 132], "/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py": ["import asyncio\nimport copy\nimport enum\nimport inspect\nimport socket\nimport sys\nimport warnings\nimport weakref\nfrom abc import abstractmethod\nfrom itertools import chain\nfrom types import MappingProxyType\nfrom typing import (\n    Any,\n    Callable,\n    Iterable,\n    List,\n    Mapping,\n    Optional,\n    Protocol,\n    Set,\n    Tuple,\n    Type,\n    TypedDict,\n    TypeVar,\n    Union,\n)\nfrom urllib.parse import ParseResult, parse_qs, unquote, urlparse\n\nfrom ..utils import SSL_AVAILABLE\n\nif SSL_AVAILABLE:\n    import ssl\n    from ssl import SSLContext, TLSVersion\nelse:\n    ssl = None\n    TLSVersion = None\n    SSLContext = None\n\nfrom ..auth.token import TokenInterface\nfrom ..event import AsyncAfterConnectionReleasedEvent, EventDispatcher\nfrom ..utils import deprecated_args, format_error_message\n\n# the functionality is available in 3.11.x but has a major issue before\n# 3.11.3. See https://github.com/redis/redis-py/issues/2633\nif sys.version_info >= (3, 11, 3):\n    from asyncio import timeout as async_timeout\nelse:\n    from async_timeout import timeout as async_timeout\n\nfrom redis.asyncio.retry import Retry\nfrom redis.backoff import NoBackoff\nfrom redis.connection import DEFAULT_RESP_VERSION\nfrom redis.credentials import CredentialProvider, UsernamePasswordCredentialProvider\nfrom redis.exceptions import (\n    AuthenticationError,\n    AuthenticationWrongNumberOfArgsError,\n    ConnectionError,\n    DataError,\n    RedisError,\n    ResponseError,\n    TimeoutError,\n)\nfrom redis.typing import EncodableT\nfrom redis.utils import HIREDIS_AVAILABLE, get_lib_version, str_if_bytes\n\nfrom .._parsers import (\n    BaseParser,\n    Encoder,\n    _AsyncHiredisParser,\n    _AsyncRESP2Parser,\n    _AsyncRESP3Parser,\n)\n\nSYM_STAR = b\"*\"\nSYM_DOLLAR = b\"$\"\nSYM_CRLF = b\"\\r\\n\"\nSYM_LF = b\"\\n\"\nSYM_EMPTY = b\"\"\n\n\nclass _Sentinel(enum.Enum):\n    sentinel = object()\n\n\nSENTINEL = _Sentinel.sentinel\n\n\nDefaultParser: Type[Union[_AsyncRESP2Parser, _AsyncRESP3Parser, _AsyncHiredisParser]]\nif HIREDIS_AVAILABLE:\n    DefaultParser = _AsyncHiredisParser\nelse:\n    DefaultParser = _AsyncRESP2Parser\n\n\nclass ConnectCallbackProtocol(Protocol):\n    def __call__(self, connection: \"AbstractConnection\"): ...\n\n\nclass AsyncConnectCallbackProtocol(Protocol):\n    async def __call__(self, connection: \"AbstractConnection\"): ...\n\n\nConnectCallbackT = Union[ConnectCallbackProtocol, AsyncConnectCallbackProtocol]\n\n\nclass AbstractConnection:\n    \"\"\"Manages communication to and from a Redis server\"\"\"\n\n    __slots__ = (\n        \"db\",\n        \"username\",\n        \"client_name\",\n        \"lib_name\",\n        \"lib_version\",\n        \"credential_provider\",\n        \"password\",\n        \"socket_timeout\",\n        \"socket_connect_timeout\",\n        \"redis_connect_func\",\n        \"retry_on_timeout\",\n        \"retry_on_error\",\n        \"health_check_interval\",\n        \"next_health_check\",\n        \"last_active_at\",\n        \"encoder\",\n        \"ssl_context\",\n        \"protocol\",\n        \"_reader\",\n        \"_writer\",\n        \"_parser\",\n        \"_connect_callbacks\",\n        \"_buffer_cutoff\",\n        \"_lock\",\n        \"_socket_read_size\",\n        \"__dict__\",\n    )\n\n    def __init__(\n        self,\n        *,\n        db: Union[str, int] = 0,\n        password: Optional[str] = None,\n        socket_timeout: Optional[float] = None,\n        socket_connect_timeout: Optional[float] = None,\n        retry_on_timeout: bool = False,\n        retry_on_error: Union[list, _Sentinel] = SENTINEL,\n        encoding: str = \"utf-8\",\n        encoding_errors: str = \"strict\",\n        decode_responses: bool = False,\n        parser_class: Type[BaseParser] = DefaultParser,\n        socket_read_size: int = 65536,\n        health_check_interval: float = 0,\n        client_name: Optional[str] = None,\n        lib_name: Optional[str] = \"redis-py\",\n        lib_version: Optional[str] = get_lib_version(),\n        username: Optional[str] = None,\n        retry: Optional[Retry] = None,\n        redis_connect_func: Optional[ConnectCallbackT] = None,\n        encoder_class: Type[Encoder] = Encoder,\n        credential_provider: Optional[CredentialProvider] = None,\n        protocol: Optional[int] = 2,\n        event_dispatcher: Optional[EventDispatcher] = None,\n    ):\n        if (username or password) and credential_provider is not None:\n            raise DataError(\n                \"'username' and 'password' cannot be passed along with 'credential_\"\n                \"provider'. Please provide only one of the following arguments: \\n\"\n                \"1. 'password' and (optional) 'username'\\n\"\n                \"2. 'credential_provider'\"\n            )\n        if event_dispatcher is None:\n            self._event_dispatcher = EventDispatcher()\n        else:\n            self._event_dispatcher = event_dispatcher\n        self.db = db\n        self.client_name = client_name\n        self.lib_name = lib_name\n        self.lib_version = lib_version\n        self.credential_provider = credential_provider\n        self.password = password\n        self.username = username\n        self.socket_timeout = socket_timeout\n        if socket_connect_timeout is None:\n            socket_connect_timeout = socket_timeout\n        self.socket_connect_timeout = socket_connect_timeout\n        self.retry_on_timeout = retry_on_timeout\n        if retry_on_error is SENTINEL:\n            retry_on_error = []\n        if retry_on_timeout:\n            retry_on_error.append(TimeoutError)\n            retry_on_error.append(socket.timeout)\n            retry_on_error.append(asyncio.TimeoutError)\n        self.retry_on_error = retry_on_error\n        if retry or retry_on_error:\n            if not retry:\n                self.retry = Retry(NoBackoff(), 1)\n            else:\n                # deep-copy the Retry object as it is mutable\n                self.retry = copy.deepcopy(retry)\n            # Update the retry's supported errors with the specified errors\n            self.retry.update_supported_errors(retry_on_error)\n        else:\n            self.retry = Retry(NoBackoff(), 0)\n        self.health_check_interval = health_check_interval\n        self.next_health_check: float = -1\n        self.encoder = encoder_class(encoding, encoding_errors, decode_responses)\n        self.redis_connect_func = redis_connect_func\n        self._reader: Optional[asyncio.StreamReader] = None\n        self._writer: Optional[asyncio.StreamWriter] = None\n        self._socket_read_size = socket_read_size\n        self.set_parser(parser_class)\n        self._connect_callbacks: List[weakref.WeakMethod[ConnectCallbackT]] = []\n        self._buffer_cutoff = 6000\n        self._re_auth_token: Optional[TokenInterface] = None\n\n        try:\n            p = int(protocol)\n        except TypeError:\n            p = DEFAULT_RESP_VERSION\n        except ValueError:\n            raise ConnectionError(\"protocol must be an integer\")\n        finally:\n            if p < 2 or p > 3:\n                raise ConnectionError(\"protocol must be either 2 or 3\")\n            self.protocol = protocol\n\n    def __del__(self, _warnings: Any = warnings):\n        # For some reason, the individual streams don't get properly garbage\n        # collected and therefore produce no resource warnings.  We add one\n        # here, in the same style as those from the stdlib.\n        if getattr(self, \"_writer\", None):\n            _warnings.warn(\n                f\"unclosed Connection {self!r}\", ResourceWarning, source=self\n            )\n\n            try:\n                asyncio.get_running_loop()\n                self._close()\n            except RuntimeError:\n                # No actions been taken if pool already closed.\n                pass\n\n    def _close(self):\n        \"\"\"\n        Internal method to silently close the connection without waiting\n        \"\"\"\n        if self._writer:\n            self._writer.close()\n            self._writer = self._reader = None\n\n    def __repr__(self):\n        repr_args = \",\".join((f\"{k}={v}\" for k, v in self.repr_pieces()))\n        return f\"<{self.__class__.__module__}.{self.__class__.__name__}({repr_args})>\"\n\n    @abstractmethod\n    def repr_pieces(self):\n        pass\n\n    @property\n    def is_connected(self):\n        return self._reader is not None and self._writer is not None\n\n    def register_connect_callback(self, callback):\n        \"\"\"\n        Register a callback to be called when the connection is established either\n        initially or reconnected.  This allows listeners to issue commands that\n        are ephemeral to the connection, for example pub/sub subscription or\n        key tracking.  The callback must be a _method_ and will be kept as\n        a weak reference.\n        \"\"\"\n        wm = weakref.WeakMethod(callback)\n        if wm not in self._connect_callbacks:\n            self._connect_callbacks.append(wm)\n\n    def deregister_connect_callback(self, callback):\n        \"\"\"\n        De-register a previously registered callback.  It will no-longer receive\n        notifications on connection events.  Calling this is not required when the\n        listener goes away, since the callbacks are kept as weak methods.\n        \"\"\"\n        try:\n            self._connect_callbacks.remove(weakref.WeakMethod(callback))\n        except ValueError:\n            pass\n\n    def set_parser(self, parser_class: Type[BaseParser]) -> None:\n        \"\"\"\n        Creates a new instance of parser_class with socket size:\n        _socket_read_size and assigns it to the parser for the connection\n        :param parser_class: The required parser class\n        \"\"\"\n        self._parser = parser_class(socket_read_size=self._socket_read_size)\n\n    async def connect(self):\n        \"\"\"Connects to the Redis server if not already connected\"\"\"\n        await self.connect_check_health(check_health=True)\n\n    async def connect_check_health(\n        self, check_health: bool = True, retry_socket_connect: bool = True\n    ):\n        if self.is_connected:\n            return\n        try:\n            if retry_socket_connect:\n                await self.retry.call_with_retry(\n                    lambda: self._connect(), lambda error: self.disconnect()\n                )\n            else:\n                await self._connect()\n        except asyncio.CancelledError:\n            raise  # in 3.7 and earlier, this is an Exception, not BaseException\n        except (socket.timeout, asyncio.TimeoutError):\n            raise TimeoutError(\"Timeout connecting to server\")\n        except OSError as e:\n            raise ConnectionError(self._error_message(e))\n        except Exception as exc:\n            raise ConnectionError(exc) from exc\n\n        try:\n            if not self.redis_connect_func:\n                # Use the default on_connect function\n                await self.on_connect_check_health(check_health=check_health)\n            else:\n                # Use the passed function redis_connect_func\n                (\n                    await self.redis_connect_func(self)\n                    if asyncio.iscoroutinefunction(self.redis_connect_func)\n                    else self.redis_connect_func(self)\n                )\n        except RedisError:\n            # clean up after any error in on_connect\n            await self.disconnect()\n            raise\n\n        # run any user callbacks. right now the only internal callback\n        # is for pubsub channel/pattern resubscription\n        # first, remove any dead weakrefs\n        self._connect_callbacks = [ref for ref in self._connect_callbacks if ref()]\n        for ref in self._connect_callbacks:\n            callback = ref()\n            task = callback(self)\n            if task and inspect.isawaitable(task):\n                await task\n\n    @abstractmethod\n    async def _connect(self):\n        pass\n\n    @abstractmethod\n    def _host_error(self) -> str:\n        pass\n\n    def _error_message(self, exception: BaseException) -> str:\n        return format_error_message(self._host_error(), exception)\n\n    def get_protocol(self):\n        return self.protocol\n\n    async def on_connect(self) -> None:\n        \"\"\"Initialize the connection, authenticate and select a database\"\"\"\n        await self.on_connect_check_health(check_health=True)\n\n    async def on_connect_check_health(self, check_health: bool = True) -> None:\n        self._parser.on_connect(self)\n        parser = self._parser\n\n        auth_args = None\n        # if credential provider or username and/or password are set, authenticate\n        if self.credential_provider or (self.username or self.password):\n            cred_provider = (\n                self.credential_provider\n                or UsernamePasswordCredentialProvider(self.username, self.password)\n            )\n            auth_args = await cred_provider.get_credentials_async()\n\n            # if resp version is specified and we have auth args,\n            # we need to send them via HELLO\n        if auth_args and self.protocol not in [2, \"2\"]:\n            if isinstance(self._parser, _AsyncRESP2Parser):\n                self.set_parser(_AsyncRESP3Parser)\n                # update cluster exception classes\n                self._parser.EXCEPTION_CLASSES = parser.EXCEPTION_CLASSES\n                self._parser.on_connect(self)\n            if len(auth_args) == 1:\n                auth_args = [\"default\", auth_args[0]]\n            # avoid checking health here -- PING will fail if we try\n            # to check the health prior to the AUTH\n            await self.send_command(\n                \"HELLO\", self.protocol, \"AUTH\", *auth_args, check_health=False\n            )\n            response = await self.read_response()\n            if response.get(b\"proto\") != int(self.protocol) and response.get(\n                \"proto\"\n            ) != int(self.protocol):\n                raise ConnectionError(\"Invalid RESP version\")\n        # avoid checking health here -- PING will fail if we try\n        # to check the health prior to the AUTH\n        elif auth_args:\n            await self.send_command(\"AUTH\", *auth_args, check_health=False)\n\n            try:\n                auth_response = await self.read_response()\n            except AuthenticationWrongNumberOfArgsError:\n                # a username and password were specified but the Redis\n                # server seems to be < 6.0.0 which expects a single password\n                # arg. retry auth with just the password.\n                # https://github.com/andymccurdy/redis-py/issues/1274\n                await self.send_command(\"AUTH\", auth_args[-1], check_health=False)\n                auth_response = await self.read_response()\n\n            if str_if_bytes(auth_response) != \"OK\":\n                raise AuthenticationError(\"Invalid Username or Password\")\n\n        # if resp version is specified, switch to it\n        elif self.protocol not in [2, \"2\"]:\n            if isinstance(self._parser, _AsyncRESP2Parser):\n                self.set_parser(_AsyncRESP3Parser)\n                # update cluster exception classes\n                self._parser.EXCEPTION_CLASSES = parser.EXCEPTION_CLASSES\n                self._parser.on_connect(self)\n            await self.send_command(\"HELLO\", self.protocol, check_health=check_health)\n            response = await self.read_response()\n            # if response.get(b\"proto\") != self.protocol and response.get(\n            #     \"proto\"\n            # ) != self.protocol:\n            #     raise ConnectionError(\"Invalid RESP version\")\n\n        # if a client_name is given, set it\n        if self.client_name:\n            await self.send_command(\n                \"CLIENT\",\n                \"SETNAME\",\n                self.client_name,\n                check_health=check_health,\n            )\n            if str_if_bytes(await self.read_response()) != \"OK\":\n                raise ConnectionError(\"Error setting client name\")\n\n        # set the library name and version, pipeline for lower startup latency\n        if self.lib_name:\n            await self.send_command(\n                \"CLIENT\",\n                \"SETINFO\",\n                \"LIB-NAME\",\n                self.lib_name,\n                check_health=check_health,\n            )\n        if self.lib_version:\n            await self.send_command(\n                \"CLIENT\",\n                \"SETINFO\",\n                \"LIB-VER\",\n                self.lib_version,\n                check_health=check_health,\n            )\n        # if a database is specified, switch to it. Also pipeline this\n        if self.db:\n            await self.send_command(\"SELECT\", self.db, check_health=check_health)\n\n        # read responses from pipeline\n        for _ in (sent for sent in (self.lib_name, self.lib_version) if sent):\n            try:\n                await self.read_response()\n            except ResponseError:\n                pass\n\n        if self.db:\n            if str_if_bytes(await self.read_response()) != \"OK\":\n                raise ConnectionError(\"Invalid Database\")\n\n    async def disconnect(self, nowait: bool = False) -> None:\n        \"\"\"Disconnects from the Redis server\"\"\"\n        try:\n            async with async_timeout(self.socket_connect_timeout):\n                self._parser.on_disconnect()\n                if not self.is_connected:\n                    return\n                try:\n                    self._writer.close()  # type: ignore[union-attr]\n                    # wait for close to finish, except when handling errors and\n                    # forcefully disconnecting.\n                    if not nowait:\n                        await self._writer.wait_closed()  # type: ignore[union-attr]\n                except OSError:\n                    pass\n                finally:\n                    self._reader = None\n                    self._writer = None\n        except asyncio.TimeoutError:\n            raise TimeoutError(\n                f\"Timed out closing connection after {self.socket_connect_timeout}\"\n            ) from None\n\n    async def _send_ping(self):\n        \"\"\"Send PING, expect PONG in return\"\"\"\n        await self.send_command(\"PING\", check_health=False)\n        if str_if_bytes(await self.read_response()) != \"PONG\":\n            raise ConnectionError(\"Bad response from PING health check\")\n\n    async def _ping_failed(self, error):\n        \"\"\"Function to call when PING fails\"\"\"\n        await self.disconnect()\n\n    async def check_health(self):\n        \"\"\"Check the health of the connection with a PING/PONG\"\"\"\n        if (\n            self.health_check_interval\n            and asyncio.get_running_loop().time() > self.next_health_check\n        ):\n            await self.retry.call_with_retry(self._send_ping, self._ping_failed)\n\n    async def _send_packed_command(self, command: Iterable[bytes]) -> None:\n        self._writer.writelines(command)\n        await self._writer.drain()\n\n    async def send_packed_command(\n        self, command: Union[bytes, str, Iterable[bytes]], check_health: bool = True\n    ) -> None:\n        if not self.is_connected:\n            await self.connect_check_health(check_health=False)\n        if check_health:\n            await self.check_health()\n\n        try:\n            if isinstance(command, str):\n                command = command.encode()\n            if isinstance(command, bytes):\n                command = [command]\n            if self.socket_timeout:\n                await asyncio.wait_for(\n                    self._send_packed_command(command), self.socket_timeout\n                )\n            else:\n                self._writer.writelines(command)\n                await self._writer.drain()\n        except asyncio.TimeoutError:\n            await self.disconnect(nowait=True)\n            raise TimeoutError(\"Timeout writing to socket\") from None\n        except OSError as e:\n            await self.disconnect(nowait=True)\n            if len(e.args) == 1:\n                err_no, errmsg = \"UNKNOWN\", e.args[0]\n            else:\n                err_no = e.args[0]\n                errmsg = e.args[1]\n            raise ConnectionError(\n                f\"Error {err_no} while writing to socket. {errmsg}.\"\n            ) from e\n        except BaseException:\n            # BaseExceptions can be raised when a socket send operation is not\n            # finished, e.g. due to a timeout.  Ideally, a caller could then re-try\n            # to send un-sent data. However, the send_packed_command() API\n            # does not support it so there is no point in keeping the connection open.\n            await self.disconnect(nowait=True)\n            raise\n\n    async def send_command(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Pack and send a command to the Redis server\"\"\"\n        await self.send_packed_command(\n            self.pack_command(*args), check_health=kwargs.get(\"check_health\", True)\n        )\n\n    async def can_read_destructive(self):\n        \"\"\"Poll the socket to see if there's data that can be read.\"\"\"\n        try:\n            return await self._parser.can_read_destructive()\n        except OSError as e:\n            await self.disconnect(nowait=True)\n            host_error = self._host_error()\n            raise ConnectionError(f\"Error while reading from {host_error}: {e.args}\")\n\n    async def read_response(\n        self,\n        disable_decoding: bool = False,\n        timeout: Optional[float] = None,\n        *,\n        disconnect_on_error: bool = True,\n        push_request: Optional[bool] = False,\n    ):\n        \"\"\"Read the response from a previously sent command\"\"\"\n        read_timeout = timeout if timeout is not None else self.socket_timeout\n        host_error = self._host_error()\n        try:\n            if read_timeout is not None and self.protocol in [\"3\", 3]:\n                async with async_timeout(read_timeout):\n                    response = await self._parser.read_response(\n                        disable_decoding=disable_decoding, push_request=push_request\n                    )\n            elif read_timeout is not None:\n                async with async_timeout(read_timeout):\n                    response = await self._parser.read_response(\n                        disable_decoding=disable_decoding\n                    )\n            elif self.protocol in [\"3\", 3]:\n                response = await self._parser.read_response(\n                    disable_decoding=disable_decoding, push_request=push_request\n                )\n            else:\n                response = await self._parser.read_response(\n                    disable_decoding=disable_decoding\n                )\n        except asyncio.TimeoutError:\n            if timeout is not None:\n                # user requested timeout, return None. Operation can be retried\n                return None\n            # it was a self.socket_timeout error.\n            if disconnect_on_error:\n                await self.disconnect(nowait=True)\n            raise TimeoutError(f\"Timeout reading from {host_error}\")\n        except OSError as e:\n            if disconnect_on_error:\n                await self.disconnect(nowait=True)\n            raise ConnectionError(f\"Error while reading from {host_error} : {e.args}\")\n        except BaseException:\n            # Also by default close in case of BaseException.  A lot of code\n            # relies on this behaviour when doing Command/Response pairs.\n            # See #1128.\n            if disconnect_on_error:\n                await self.disconnect(nowait=True)\n            raise\n\n        if self.health_check_interval:\n            next_time = asyncio.get_running_loop().time() + self.health_check_interval\n            self.next_health_check = next_time\n\n        if isinstance(response, ResponseError):\n            raise response from None\n        return response\n\n    def pack_command(self, *args: EncodableT) -> List[bytes]:\n        \"\"\"Pack a series of arguments into the Redis protocol\"\"\"\n        output = []\n        # the client might have included 1 or more literal arguments in\n        # the command name, e.g., 'CONFIG GET'. The Redis server expects these\n        # arguments to be sent separately, so split the first argument\n        # manually. These arguments should be bytestrings so that they are\n        # not encoded.\n        assert not isinstance(args[0], float)\n        if isinstance(args[0], str):\n            args = tuple(args[0].encode().split()) + args[1:]\n        elif b\" \" in args[0]:\n            args = tuple(args[0].split()) + args[1:]\n\n        buff = SYM_EMPTY.join((SYM_STAR, str(len(args)).encode(), SYM_CRLF))\n\n        buffer_cutoff = self._buffer_cutoff\n        for arg in map(self.encoder.encode, args):\n            # to avoid large string mallocs, chunk the command into the\n            # output list if we're sending large values or memoryviews\n            arg_length = len(arg)\n            if (\n                len(buff) > buffer_cutoff\n                or arg_length > buffer_cutoff\n                or isinstance(arg, memoryview)\n            ):\n                buff = SYM_EMPTY.join(\n                    (buff, SYM_DOLLAR, str(arg_length).encode(), SYM_CRLF)\n                )\n                output.append(buff)\n                output.append(arg)\n                buff = SYM_CRLF\n            else:\n                buff = SYM_EMPTY.join(\n                    (\n                        buff,\n                        SYM_DOLLAR,\n                        str(arg_length).encode(),\n                        SYM_CRLF,\n                        arg,\n                        SYM_CRLF,\n                    )\n                )\n        output.append(buff)\n        return output\n\n    def pack_commands(self, commands: Iterable[Iterable[EncodableT]]) -> List[bytes]:\n        \"\"\"Pack multiple commands into the Redis protocol\"\"\"\n        output: List[bytes] = []\n        pieces: List[bytes] = []\n        buffer_length = 0\n        buffer_cutoff = self._buffer_cutoff\n\n        for cmd in commands:\n            for chunk in self.pack_command(*cmd):\n                chunklen = len(chunk)\n                if (\n                    buffer_length > buffer_cutoff\n                    or chunklen > buffer_cutoff\n                    or isinstance(chunk, memoryview)\n                ):\n                    if pieces:\n                        output.append(SYM_EMPTY.join(pieces))\n                    buffer_length = 0\n                    pieces = []\n\n                if chunklen > buffer_cutoff or isinstance(chunk, memoryview):\n                    output.append(chunk)\n                else:\n                    pieces.append(chunk)\n                    buffer_length += chunklen\n\n        if pieces:\n            output.append(SYM_EMPTY.join(pieces))\n        return output\n\n    def _socket_is_empty(self):\n        \"\"\"Check if the socket is empty\"\"\"\n        return len(self._reader._buffer) == 0\n\n    async def process_invalidation_messages(self):\n        while not self._socket_is_empty():\n            await self.read_response(push_request=True)\n\n    def set_re_auth_token(self, token: TokenInterface):\n        self._re_auth_token = token\n\n    async def re_auth(self):\n        if self._re_auth_token is not None:\n            await self.send_command(\n                \"AUTH\",\n                self._re_auth_token.try_get(\"oid\"),\n                self._re_auth_token.get_value(),\n            )\n            await self.read_response()\n            self._re_auth_token = None\n\n\nclass Connection(AbstractConnection):\n    \"Manages TCP communication to and from a Redis server\"\n\n    def __init__(\n        self,\n        *,\n        host: str = \"localhost\",\n        port: Union[str, int] = 6379,\n        socket_keepalive: bool = False,\n        socket_keepalive_options: Optional[Mapping[int, Union[int, bytes]]] = None,\n        socket_type: int = 0,\n        **kwargs,\n    ):\n        self.host = host\n        self.port = int(port)\n        self.socket_keepalive = socket_keepalive\n        self.socket_keepalive_options = socket_keepalive_options or {}\n        self.socket_type = socket_type\n        super().__init__(**kwargs)\n\n    def repr_pieces(self):\n        pieces = [(\"host\", self.host), (\"port\", self.port), (\"db\", self.db)]\n        if self.client_name:\n            pieces.append((\"client_name\", self.client_name))\n        return pieces\n\n    def _connection_arguments(self) -> Mapping:\n        return {\"host\": self.host, \"port\": self.port}\n\n    async def _connect(self):\n        \"\"\"Create a TCP socket connection\"\"\"\n        async with async_timeout(self.socket_connect_timeout):\n            reader, writer = await asyncio.open_connection(\n                **self._connection_arguments()\n            )\n        self._reader = reader\n        self._writer = writer\n        sock = writer.transport.get_extra_info(\"socket\")\n        if sock:\n            sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n            try:\n                # TCP_KEEPALIVE\n                if self.socket_keepalive:\n                    sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)\n                    for k, v in self.socket_keepalive_options.items():\n                        sock.setsockopt(socket.SOL_TCP, k, v)\n\n            except (OSError, TypeError):\n                # `socket_keepalive_options` might contain invalid options\n                # causing an error. Do not leave the connection open.\n                writer.close()\n                raise\n\n    def _host_error(self) -> str:\n        return f\"{self.host}:{self.port}\"\n\n\nclass SSLConnection(Connection):\n    \"\"\"Manages SSL connections to and from the Redis server(s).\n    This class extends the Connection class, adding SSL functionality, and making\n    use of ssl.SSLContext (https://docs.python.org/3/library/ssl.html#ssl.SSLContext)\n    \"\"\"\n\n    def __init__(\n        self,\n        ssl_keyfile: Optional[str] = None,\n        ssl_certfile: Optional[str] = None,\n        ssl_cert_reqs: Union[str, ssl.VerifyMode] = \"required\",\n        ssl_ca_certs: Optional[str] = None,\n        ssl_ca_data: Optional[str] = None,\n        ssl_check_hostname: bool = True,\n        ssl_min_version: Optional[TLSVersion] = None,\n        ssl_ciphers: Optional[str] = None,\n        **kwargs,\n    ):\n        if not SSL_AVAILABLE:\n            raise RedisError(\"Python wasn't built with SSL support\")\n\n        self.ssl_context: RedisSSLContext = RedisSSLContext(\n            keyfile=ssl_keyfile,\n            certfile=ssl_certfile,\n            cert_reqs=ssl_cert_reqs,\n            ca_certs=ssl_ca_certs,\n            ca_data=ssl_ca_data,\n            check_hostname=ssl_check_hostname,\n            min_version=ssl_min_version,\n            ciphers=ssl_ciphers,\n        )\n        super().__init__(**kwargs)\n\n    def _connection_arguments(self) -> Mapping:\n        kwargs = super()._connection_arguments()\n        kwargs[\"ssl\"] = self.ssl_context.get()\n        return kwargs\n\n    @property\n    def keyfile(self):\n        return self.ssl_context.keyfile\n\n    @property\n    def certfile(self):\n        return self.ssl_context.certfile\n\n    @property\n    def cert_reqs(self):\n        return self.ssl_context.cert_reqs\n\n    @property\n    def ca_certs(self):\n        return self.ssl_context.ca_certs\n\n    @property\n    def ca_data(self):\n        return self.ssl_context.ca_data\n\n    @property\n    def check_hostname(self):\n        return self.ssl_context.check_hostname\n\n    @property\n    def min_version(self):\n        return self.ssl_context.min_version\n\n\nclass RedisSSLContext:\n    __slots__ = (\n        \"keyfile\",\n        \"certfile\",\n        \"cert_reqs\",\n        \"ca_certs\",\n        \"ca_data\",\n        \"context\",\n        \"check_hostname\",\n        \"min_version\",\n        \"ciphers\",\n    )\n\n    def __init__(\n        self,\n        keyfile: Optional[str] = None,\n        certfile: Optional[str] = None,\n        cert_reqs: Optional[Union[str, ssl.VerifyMode]] = None,\n        ca_certs: Optional[str] = None,\n        ca_data: Optional[str] = None,\n        check_hostname: bool = False,\n        min_version: Optional[TLSVersion] = None,\n        ciphers: Optional[str] = None,\n    ):\n        if not SSL_AVAILABLE:\n            raise RedisError(\"Python wasn't built with SSL support\")\n\n        self.keyfile = keyfile\n        self.certfile = certfile\n        if cert_reqs is None:\n            cert_reqs = ssl.CERT_NONE\n        elif isinstance(cert_reqs, str):\n            CERT_REQS = {  # noqa: N806\n                \"none\": ssl.CERT_NONE,\n                \"optional\": ssl.CERT_OPTIONAL,\n                \"required\": ssl.CERT_REQUIRED,\n            }\n            if cert_reqs not in CERT_REQS:\n                raise RedisError(\n                    f\"Invalid SSL Certificate Requirements Flag: {cert_reqs}\"\n                )\n            cert_reqs = CERT_REQS[cert_reqs]\n        self.cert_reqs = cert_reqs\n        self.ca_certs = ca_certs\n        self.ca_data = ca_data\n        self.check_hostname = (\n            check_hostname if self.cert_reqs != ssl.CERT_NONE else False\n        )\n        self.min_version = min_version\n        self.ciphers = ciphers\n        self.context: Optional[SSLContext] = None\n\n    def get(self) -> SSLContext:\n        if not self.context:\n            context = ssl.create_default_context()\n            context.check_hostname = self.check_hostname\n            context.verify_mode = self.cert_reqs\n            if self.certfile and self.keyfile:\n                context.load_cert_chain(certfile=self.certfile, keyfile=self.keyfile)\n            if self.ca_certs or self.ca_data:\n                context.load_verify_locations(cafile=self.ca_certs, cadata=self.ca_data)\n            if self.min_version is not None:\n                context.minimum_version = self.min_version\n            if self.ciphers is not None:\n                context.set_ciphers(self.ciphers)\n            self.context = context\n        return self.context\n\n\nclass UnixDomainSocketConnection(AbstractConnection):\n    \"Manages UDS communication to and from a Redis server\"\n\n    def __init__(self, *, path: str = \"\", **kwargs):\n        self.path = path\n        super().__init__(**kwargs)\n\n    def repr_pieces(self) -> Iterable[Tuple[str, Union[str, int]]]:\n        pieces = [(\"path\", self.path), (\"db\", self.db)]\n        if self.client_name:\n            pieces.append((\"client_name\", self.client_name))\n        return pieces\n\n    async def _connect(self):\n        async with async_timeout(self.socket_connect_timeout):\n            reader, writer = await asyncio.open_unix_connection(path=self.path)\n        self._reader = reader\n        self._writer = writer\n        await self.on_connect()\n\n    def _host_error(self) -> str:\n        return self.path\n\n\nFALSE_STRINGS = (\"0\", \"F\", \"FALSE\", \"N\", \"NO\")\n\n\ndef to_bool(value) -> Optional[bool]:\n    if value is None or value == \"\":\n        return None\n    if isinstance(value, str) and value.upper() in FALSE_STRINGS:\n        return False\n    return bool(value)\n\n\nURL_QUERY_ARGUMENT_PARSERS: Mapping[str, Callable[..., object]] = MappingProxyType(\n    {\n        \"db\": int,\n        \"socket_timeout\": float,\n        \"socket_connect_timeout\": float,\n        \"socket_keepalive\": to_bool,\n        \"retry_on_timeout\": to_bool,\n        \"max_connections\": int,\n        \"health_check_interval\": int,\n        \"ssl_check_hostname\": to_bool,\n        \"timeout\": float,\n    }\n)\n\n\nclass ConnectKwargs(TypedDict, total=False):\n    username: str\n    password: str\n    connection_class: Type[AbstractConnection]\n    host: str\n    port: int\n    db: int\n    path: str\n\n\ndef parse_url(url: str) -> ConnectKwargs:\n    parsed: ParseResult = urlparse(url)\n    kwargs: ConnectKwargs = {}\n\n    for name, value_list in parse_qs(parsed.query).items():\n        if value_list and len(value_list) > 0:\n            value = unquote(value_list[0])\n            parser = URL_QUERY_ARGUMENT_PARSERS.get(name)\n            if parser:\n                try:\n                    kwargs[name] = parser(value)\n                except (TypeError, ValueError):\n                    raise ValueError(f\"Invalid value for '{name}' in connection URL.\")\n            else:\n                kwargs[name] = value\n\n    if parsed.username:\n        kwargs[\"username\"] = unquote(parsed.username)\n    if parsed.password:\n        kwargs[\"password\"] = unquote(parsed.password)\n\n    # We only support redis://, rediss:// and unix:// schemes.\n    if parsed.scheme == \"unix\":\n        if parsed.path:\n            kwargs[\"path\"] = unquote(parsed.path)\n        kwargs[\"connection_class\"] = UnixDomainSocketConnection\n\n    elif parsed.scheme in (\"redis\", \"rediss\"):\n        if parsed.hostname:\n            kwargs[\"host\"] = unquote(parsed.hostname)\n        if parsed.port:\n            kwargs[\"port\"] = int(parsed.port)\n\n        # If there's a path argument, use it as the db argument if a\n        # querystring value wasn't specified\n        if parsed.path and \"db\" not in kwargs:\n            try:\n                kwargs[\"db\"] = int(unquote(parsed.path).replace(\"/\", \"\"))\n            except (AttributeError, ValueError):\n                pass\n\n        if parsed.scheme == \"rediss\":\n            kwargs[\"connection_class\"] = SSLConnection\n    else:\n        valid_schemes = \"redis://, rediss://, unix://\"\n        raise ValueError(\n            f\"Redis URL must specify one of the following schemes ({valid_schemes})\"\n        )\n\n    return kwargs\n\n\n_CP = TypeVar(\"_CP\", bound=\"ConnectionPool\")\n\n\nclass ConnectionPool:\n    \"\"\"\n    Create a connection pool. ``If max_connections`` is set, then this\n    object raises :py:class:`~redis.ConnectionError` when the pool's\n    limit is reached.\n\n    By default, TCP connections are created unless ``connection_class``\n    is specified. Use :py:class:`~redis.UnixDomainSocketConnection` for\n    unix sockets.\n    :py:class:`~redis.SSLConnection` can be used for SSL enabled connections.\n\n    Any additional keyword arguments are passed to the constructor of\n    ``connection_class``.\n    \"\"\"\n\n    @classmethod\n    def from_url(cls: Type[_CP], url: str, **kwargs) -> _CP:\n        \"\"\"\n        Return a connection pool configured from the given URL.\n\n        For example::\n\n            redis://[[username]:[password]]@localhost:6379/0\n            rediss://[[username]:[password]]@localhost:6379/0\n            unix://[username@]/path/to/socket.sock?db=0[&password=password]\n\n        Three URL schemes are supported:\n\n        - `redis://` creates a TCP socket connection. See more at:\n          <https://www.iana.org/assignments/uri-schemes/prov/redis>\n        - `rediss://` creates a SSL wrapped TCP socket connection. See more at:\n          <https://www.iana.org/assignments/uri-schemes/prov/rediss>\n        - ``unix://``: creates a Unix Domain Socket connection.\n\n        The username, password, hostname, path and all querystring values\n        are passed through urllib.parse.unquote in order to replace any\n        percent-encoded values with their corresponding characters.\n\n        There are several ways to specify a database number. The first value\n        found will be used:\n\n        1. A ``db`` querystring option, e.g. redis://localhost?db=0\n\n        2. If using the redis:// or rediss:// schemes, the path argument\n               of the url, e.g. redis://localhost/0\n\n        3. A ``db`` keyword argument to this function.\n\n        If none of these options are specified, the default db=0 is used.\n\n        All querystring options are cast to their appropriate Python types.\n        Boolean arguments can be specified with string values \"True\"/\"False\"\n        or \"Yes\"/\"No\". Values that cannot be properly cast cause a\n        ``ValueError`` to be raised. Once parsed, the querystring arguments\n        and keyword arguments are passed to the ``ConnectionPool``'s\n        class initializer. In the case of conflicting arguments, querystring\n        arguments always win.\n        \"\"\"\n        url_options = parse_url(url)\n        kwargs.update(url_options)\n        return cls(**kwargs)\n\n    def __init__(\n        self,\n        connection_class: Type[AbstractConnection] = Connection,\n        max_connections: Optional[int] = None,\n        **connection_kwargs,\n    ):\n        max_connections = max_connections or 2**31\n        if not isinstance(max_connections, int) or max_connections < 0:\n            raise ValueError('\"max_connections\" must be a positive integer')\n\n        self.connection_class = connection_class\n        self.connection_kwargs = connection_kwargs\n        self.max_connections = max_connections\n\n        self._available_connections: List[AbstractConnection] = []\n        self._in_use_connections: Set[AbstractConnection] = set()\n        self.encoder_class = self.connection_kwargs.get(\"encoder_class\", Encoder)\n        self._lock = asyncio.Lock()\n        self._event_dispatcher = self.connection_kwargs.get(\"event_dispatcher\", None)\n        if self._event_dispatcher is None:\n            self._event_dispatcher = EventDispatcher()\n\n    def __repr__(self):\n        conn_kwargs = \",\".join([f\"{k}={v}\" for k, v in self.connection_kwargs.items()])\n        return (\n            f\"<{self.__class__.__module__}.{self.__class__.__name__}\"\n            f\"(<{self.connection_class.__module__}.{self.connection_class.__name__}\"\n            f\"({conn_kwargs})>)>\"\n        )\n\n    def reset(self):\n        self._available_connections = []\n        self._in_use_connections = weakref.WeakSet()\n\n    def can_get_connection(self) -> bool:\n        \"\"\"Return True if a connection can be retrieved from the pool.\"\"\"\n        return (\n            self._available_connections\n            or len(self._in_use_connections) < self.max_connections\n        )\n\n    @deprecated_args(\n        args_to_warn=[\"*\"],\n        reason=\"Use get_connection() without args instead\",\n        version=\"5.3.0\",\n    )\n    async def get_connection(self, command_name=None, *keys, **options):\n        async with self._lock:\n            \"\"\"Get a connected connection from the pool\"\"\"\n            connection = self.get_available_connection()\n            try:\n                await self.ensure_connection(connection)\n            except BaseException:\n                await self.release(connection)\n                raise\n\n        return connection\n\n    def get_available_connection(self):\n        \"\"\"Get a connection from the pool, without making sure it is connected\"\"\"\n        try:\n            connection = self._available_connections.pop()\n        except IndexError:\n            if len(self._in_use_connections) >= self.max_connections:\n                raise ConnectionError(\"Too many connections\") from None\n            connection = self.make_connection()\n        self._in_use_connections.add(connection)\n        return connection\n\n    def get_encoder(self):\n        \"\"\"Return an encoder based on encoding settings\"\"\"\n        kwargs = self.connection_kwargs\n        return self.encoder_class(\n            encoding=kwargs.get(\"encoding\", \"utf-8\"),\n            encoding_errors=kwargs.get(\"encoding_errors\", \"strict\"),\n            decode_responses=kwargs.get(\"decode_responses\", False),\n        )\n\n    def make_connection(self):\n        \"\"\"Create a new connection.  Can be overridden by child classes.\"\"\"\n        return self.connection_class(**self.connection_kwargs)\n\n    async def ensure_connection(self, connection: AbstractConnection):\n        \"\"\"Ensure that the connection object is connected and valid\"\"\"\n        await connection.connect()\n        # connections that the pool provides should be ready to send\n        # a command. if not, the connection was either returned to the\n        # pool before all data has been read or the socket has been\n        # closed. either way, reconnect and verify everything is good.\n        try:\n            if await connection.can_read_destructive():\n                raise ConnectionError(\"Connection has data\") from None\n        except (ConnectionError, TimeoutError, OSError):\n            await connection.disconnect()\n            await connection.connect()\n            if await connection.can_read_destructive():\n                raise ConnectionError(\"Connection not ready\") from None\n\n    async def release(self, connection: AbstractConnection):\n        \"\"\"Releases the connection back to the pool\"\"\"\n        # Connections should always be returned to the correct pool,\n        # not doing so is an error that will cause an exception here.\n        self._in_use_connections.remove(connection)\n        self._available_connections.append(connection)\n        await self._event_dispatcher.dispatch_async(\n            AsyncAfterConnectionReleasedEvent(connection)\n        )\n\n    async def disconnect(self, inuse_connections: bool = True):\n        \"\"\"\n        Disconnects connections in the pool\n\n        If ``inuse_connections`` is True, disconnect connections that are\n        current in use, potentially by other tasks. Otherwise only disconnect\n        connections that are idle in the pool.\n        \"\"\"\n        if inuse_connections:\n            connections: Iterable[AbstractConnection] = chain(\n                self._available_connections, self._in_use_connections\n            )\n        else:\n            connections = self._available_connections\n        resp = await asyncio.gather(\n            *(connection.disconnect() for connection in connections),\n            return_exceptions=True,\n        )\n        exc = next((r for r in resp if isinstance(r, BaseException)), None)\n        if exc:\n            raise exc\n\n    async def aclose(self) -> None:\n        \"\"\"Close the pool, disconnecting all connections\"\"\"\n        await self.disconnect()\n\n    def set_retry(self, retry: \"Retry\") -> None:\n        for conn in self._available_connections:\n            conn.retry = retry\n        for conn in self._in_use_connections:\n            conn.retry = retry\n\n    async def re_auth_callback(self, token: TokenInterface):\n        async with self._lock:\n            for conn in self._available_connections:\n                await conn.retry.call_with_retry(\n                    lambda: conn.send_command(\n                        \"AUTH\", token.try_get(\"oid\"), token.get_value()\n                    ),\n                    lambda error: self._mock(error),\n                )\n                await conn.retry.call_with_retry(\n                    lambda: conn.read_response(), lambda error: self._mock(error)\n                )\n            for conn in self._in_use_connections:\n                conn.set_re_auth_token(token)\n\n    async def _mock(self, error: RedisError):\n        \"\"\"\n        Dummy functions, needs to be passed as error callback to retry object.\n        :param error:\n        :return:\n        \"\"\"\n        pass\n\n\nclass BlockingConnectionPool(ConnectionPool):\n    \"\"\"\n    A blocking connection pool::\n\n        >>> from redis.asyncio import Redis, BlockingConnectionPool\n        >>> client = Redis.from_pool(BlockingConnectionPool())\n\n    It performs the same function as the default\n    :py:class:`~redis.asyncio.ConnectionPool` implementation, in that,\n    it maintains a pool of reusable connections that can be shared by\n    multiple async redis clients.\n\n    The difference is that, in the event that a client tries to get a\n    connection from the pool when all of connections are in use, rather than\n    raising a :py:class:`~redis.ConnectionError` (as the default\n    :py:class:`~redis.asyncio.ConnectionPool` implementation does), it\n    blocks the current `Task` for a specified number of seconds until\n    a connection becomes available.\n\n    Use ``max_connections`` to increase / decrease the pool size::\n\n        >>> pool = BlockingConnectionPool(max_connections=10)\n\n    Use ``timeout`` to tell it either how many seconds to wait for a connection\n    to become available, or to block forever:\n\n        >>> # Block forever.\n        >>> pool = BlockingConnectionPool(timeout=None)\n\n        >>> # Raise a ``ConnectionError`` after five seconds if a connection is\n        >>> # not available.\n        >>> pool = BlockingConnectionPool(timeout=5)\n    \"\"\"\n\n    def __init__(\n        self,\n        max_connections: int = 50,\n        timeout: Optional[int] = 20,\n        connection_class: Type[AbstractConnection] = Connection,\n        queue_class: Type[asyncio.Queue] = asyncio.LifoQueue,  # deprecated\n        **connection_kwargs,\n    ):\n        super().__init__(\n            connection_class=connection_class,\n            max_connections=max_connections,\n            **connection_kwargs,\n        )\n        self._condition = asyncio.Condition()\n        self.timeout = timeout\n\n    @deprecated_args(\n        args_to_warn=[\"*\"],\n        reason=\"Use get_connection() without args instead\",\n        version=\"5.3.0\",\n    )\n    async def get_connection(self, command_name=None, *keys, **options):\n        \"\"\"Gets a connection from the pool, blocking until one is available\"\"\"\n        try:\n            async with self._condition:\n                async with async_timeout(self.timeout):\n                    await self._condition.wait_for(self.can_get_connection)\n                    connection = super().get_available_connection()\n        except asyncio.TimeoutError as err:\n            raise ConnectionError(\"No connection available.\") from err\n\n        # We now perform the connection check outside of the lock.\n        try:\n            await self.ensure_connection(connection)\n            return connection\n        except BaseException:\n            await self.release(connection)\n            raise\n\n    async def release(self, connection: AbstractConnection):\n        \"\"\"Releases the connection back to the pool.\"\"\"\n        async with self._condition:\n            await super().release(connection)\n            self._condition.notify()\n", 1339], "/usr/local/lib/python3.11/site-packages/redis/client.py": ["import copy\nimport re\nimport threading\nimport time\nfrom itertools import chain\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    List,\n    Mapping,\n    Optional,\n    Set,\n    Type,\n    Union,\n)\n\nfrom redis._parsers.encoders import Encoder\nfrom redis._parsers.helpers import (\n    _RedisCallbacks,\n    _RedisCallbacksRESP2,\n    _RedisCallbacksRESP3,\n    bool_ok,\n)\nfrom redis.backoff import ExponentialWithJitterBackoff\nfrom redis.cache import CacheConfig, CacheInterface\nfrom redis.commands import (\n    CoreCommands,\n    RedisModuleCommands,\n    SentinelCommands,\n    list_or_args,\n)\nfrom redis.commands.core import Script\nfrom redis.connection import (\n    AbstractConnection,\n    Connection,\n    ConnectionPool,\n    SSLConnection,\n    UnixDomainSocketConnection,\n)\nfrom redis.credentials import CredentialProvider\nfrom redis.event import (\n    AfterPooledConnectionsInstantiationEvent,\n    AfterPubSubConnectionInstantiationEvent,\n    AfterSingleConnectionInstantiationEvent,\n    ClientType,\n    EventDispatcher,\n)\nfrom redis.exceptions import (\n    ConnectionError,\n    ExecAbortError,\n    PubSubError,\n    RedisError,\n    ResponseError,\n    WatchError,\n)\nfrom redis.lock import Lock\nfrom redis.retry import Retry\nfrom redis.utils import (\n    _set_info_logger,\n    deprecated_args,\n    get_lib_version,\n    safe_str,\n    str_if_bytes,\n    truncate_text,\n)\n\nif TYPE_CHECKING:\n    import ssl\n\n    import OpenSSL\n\nSYM_EMPTY = b\"\"\nEMPTY_RESPONSE = \"EMPTY_RESPONSE\"\n\n# some responses (ie. dump) are binary, and just meant to never be decoded\nNEVER_DECODE = \"NEVER_DECODE\"\n\n\nclass CaseInsensitiveDict(dict):\n    \"Case insensitive dict implementation. Assumes string keys only.\"\n\n    def __init__(self, data: Dict[str, str]) -> None:\n        for k, v in data.items():\n            self[k.upper()] = v\n\n    def __contains__(self, k):\n        return super().__contains__(k.upper())\n\n    def __delitem__(self, k):\n        super().__delitem__(k.upper())\n\n    def __getitem__(self, k):\n        return super().__getitem__(k.upper())\n\n    def get(self, k, default=None):\n        return super().get(k.upper(), default)\n\n    def __setitem__(self, k, v):\n        super().__setitem__(k.upper(), v)\n\n    def update(self, data):\n        data = CaseInsensitiveDict(data)\n        super().update(data)\n\n\nclass AbstractRedis:\n    pass\n\n\nclass Redis(RedisModuleCommands, CoreCommands, SentinelCommands):\n    \"\"\"\n    Implementation of the Redis protocol.\n\n    This abstract class provides a Python interface to all Redis commands\n    and an implementation of the Redis protocol.\n\n    Pipelines derive from this, implementing how\n    the commands are sent and received to the Redis server. Based on\n    configuration, an instance will either use a ConnectionPool, or\n    Connection object to talk to redis.\n\n    It is not safe to pass PubSub or Pipeline objects between threads.\n    \"\"\"\n\n    @classmethod\n    def from_url(cls, url: str, **kwargs) -> \"Redis\":\n        \"\"\"\n        Return a Redis client object configured from the given URL\n\n        For example::\n\n            redis://[[username]:[password]]@localhost:6379/0\n            rediss://[[username]:[password]]@localhost:6379/0\n            unix://[username@]/path/to/socket.sock?db=0[&password=password]\n\n        Three URL schemes are supported:\n\n        - `redis://` creates a TCP socket connection. See more at:\n          <https://www.iana.org/assignments/uri-schemes/prov/redis>\n        - `rediss://` creates a SSL wrapped TCP socket connection. See more at:\n          <https://www.iana.org/assignments/uri-schemes/prov/rediss>\n        - ``unix://``: creates a Unix Domain Socket connection.\n\n        The username, password, hostname, path and all querystring values\n        are passed through urllib.parse.unquote in order to replace any\n        percent-encoded values with their corresponding characters.\n\n        There are several ways to specify a database number. The first value\n        found will be used:\n\n            1. A ``db`` querystring option, e.g. redis://localhost?db=0\n            2. If using the redis:// or rediss:// schemes, the path argument\n               of the url, e.g. redis://localhost/0\n            3. A ``db`` keyword argument to this function.\n\n        If none of these options are specified, the default db=0 is used.\n\n        All querystring options are cast to their appropriate Python types.\n        Boolean arguments can be specified with string values \"True\"/\"False\"\n        or \"Yes\"/\"No\". Values that cannot be properly cast cause a\n        ``ValueError`` to be raised. Once parsed, the querystring arguments\n        and keyword arguments are passed to the ``ConnectionPool``'s\n        class initializer. In the case of conflicting arguments, querystring\n        arguments always win.\n\n        \"\"\"\n        single_connection_client = kwargs.pop(\"single_connection_client\", False)\n        connection_pool = ConnectionPool.from_url(url, **kwargs)\n        client = cls(\n            connection_pool=connection_pool,\n            single_connection_client=single_connection_client,\n        )\n        client.auto_close_connection_pool = True\n        return client\n\n    @classmethod\n    def from_pool(\n        cls: Type[\"Redis\"],\n        connection_pool: ConnectionPool,\n    ) -> \"Redis\":\n        \"\"\"\n        Return a Redis client from the given connection pool.\n        The Redis client will take ownership of the connection pool and\n        close it when the Redis client is closed.\n        \"\"\"\n        client = cls(\n            connection_pool=connection_pool,\n        )\n        client.auto_close_connection_pool = True\n        return client\n\n    @deprecated_args(\n        args_to_warn=[\"retry_on_timeout\"],\n        reason=\"TimeoutError is included by default.\",\n        version=\"6.0.0\",\n    )\n    def __init__(\n        self,\n        host: str = \"localhost\",\n        port: int = 6379,\n        db: int = 0,\n        password: Optional[str] = None,\n        socket_timeout: Optional[float] = None,\n        socket_connect_timeout: Optional[float] = None,\n        socket_keepalive: Optional[bool] = None,\n        socket_keepalive_options: Optional[Mapping[int, Union[int, bytes]]] = None,\n        connection_pool: Optional[ConnectionPool] = None,\n        unix_socket_path: Optional[str] = None,\n        encoding: str = \"utf-8\",\n        encoding_errors: str = \"strict\",\n        decode_responses: bool = False,\n        retry_on_timeout: bool = False,\n        retry: Retry = Retry(\n            backoff=ExponentialWithJitterBackoff(base=1, cap=10), retries=3\n        ),\n        retry_on_error: Optional[List[Type[Exception]]] = None,\n        ssl: bool = False,\n        ssl_keyfile: Optional[str] = None,\n        ssl_certfile: Optional[str] = None,\n        ssl_cert_reqs: Union[str, \"ssl.VerifyMode\"] = \"required\",\n        ssl_ca_certs: Optional[str] = None,\n        ssl_ca_path: Optional[str] = None,\n        ssl_ca_data: Optional[str] = None,\n        ssl_check_hostname: bool = True,\n        ssl_password: Optional[str] = None,\n        ssl_validate_ocsp: bool = False,\n        ssl_validate_ocsp_stapled: bool = False,\n        ssl_ocsp_context: Optional[\"OpenSSL.SSL.Context\"] = None,\n        ssl_ocsp_expected_cert: Optional[str] = None,\n        ssl_min_version: Optional[\"ssl.TLSVersion\"] = None,\n        ssl_ciphers: Optional[str] = None,\n        max_connections: Optional[int] = None,\n        single_connection_client: bool = False,\n        health_check_interval: int = 0,\n        client_name: Optional[str] = None,\n        lib_name: Optional[str] = \"redis-py\",\n        lib_version: Optional[str] = get_lib_version(),\n        username: Optional[str] = None,\n        redis_connect_func: Optional[Callable[[], None]] = None,\n        credential_provider: Optional[CredentialProvider] = None,\n        protocol: Optional[int] = 2,\n        cache: Optional[CacheInterface] = None,\n        cache_config: Optional[CacheConfig] = None,\n        event_dispatcher: Optional[EventDispatcher] = None,\n    ) -> None:\n        \"\"\"\n        Initialize a new Redis client.\n\n        To specify a retry policy for specific errors, you have two options:\n\n        1. Set the `retry_on_error` to a list of the error/s to retry on, and\n        you can also set `retry` to a valid `Retry` object(in case the default\n        one is not appropriate) - with this approach the retries will be triggered\n        on the default errors specified in the Retry object enriched with the\n        errors specified in `retry_on_error`.\n\n        2. Define a `Retry` object with configured 'supported_errors' and set\n        it to the `retry` parameter - with this approach you completely redefine\n        the errors on which retries will happen.\n\n        `retry_on_timeout` is deprecated - please include the TimeoutError\n        either in the Retry object or in the `retry_on_error` list.\n\n        When 'connection_pool' is provided - the retry configuration of the\n        provided pool will be used.\n\n        Args:\n\n        single_connection_client:\n            if `True`, connection pool is not used. In that case `Redis`\n            instance use is not thread safe.\n        \"\"\"\n        if event_dispatcher is None:\n            self._event_dispatcher = EventDispatcher()\n        else:\n            self._event_dispatcher = event_dispatcher\n        if not connection_pool:\n            if not retry_on_error:\n                retry_on_error = []\n            kwargs = {\n                \"db\": db,\n                \"username\": username,\n                \"password\": password,\n                \"socket_timeout\": socket_timeout,\n                \"encoding\": encoding,\n                \"encoding_errors\": encoding_errors,\n                \"decode_responses\": decode_responses,\n                \"retry_on_error\": retry_on_error,\n                \"retry\": copy.deepcopy(retry),\n                \"max_connections\": max_connections,\n                \"health_check_interval\": health_check_interval,\n                \"client_name\": client_name,\n                \"lib_name\": lib_name,\n                \"lib_version\": lib_version,\n                \"redis_connect_func\": redis_connect_func,\n                \"credential_provider\": credential_provider,\n                \"protocol\": protocol,\n            }\n            # based on input, setup appropriate connection args\n            if unix_socket_path is not None:\n                kwargs.update(\n                    {\n                        \"path\": unix_socket_path,\n                        \"connection_class\": UnixDomainSocketConnection,\n                    }\n                )\n            else:\n                # TCP specific options\n                kwargs.update(\n                    {\n                        \"host\": host,\n                        \"port\": port,\n                        \"socket_connect_timeout\": socket_connect_timeout,\n                        \"socket_keepalive\": socket_keepalive,\n                        \"socket_keepalive_options\": socket_keepalive_options,\n                    }\n                )\n\n                if ssl:\n                    kwargs.update(\n                        {\n                            \"connection_class\": SSLConnection,\n                            \"ssl_keyfile\": ssl_keyfile,\n                            \"ssl_certfile\": ssl_certfile,\n                            \"ssl_cert_reqs\": ssl_cert_reqs,\n                            \"ssl_ca_certs\": ssl_ca_certs,\n                            \"ssl_ca_data\": ssl_ca_data,\n                            \"ssl_check_hostname\": ssl_check_hostname,\n                            \"ssl_password\": ssl_password,\n                            \"ssl_ca_path\": ssl_ca_path,\n                            \"ssl_validate_ocsp_stapled\": ssl_validate_ocsp_stapled,\n                            \"ssl_validate_ocsp\": ssl_validate_ocsp,\n                            \"ssl_ocsp_context\": ssl_ocsp_context,\n                            \"ssl_ocsp_expected_cert\": ssl_ocsp_expected_cert,\n                            \"ssl_min_version\": ssl_min_version,\n                            \"ssl_ciphers\": ssl_ciphers,\n                        }\n                    )\n                if (cache_config or cache) and protocol in [3, \"3\"]:\n                    kwargs.update(\n                        {\n                            \"cache\": cache,\n                            \"cache_config\": cache_config,\n                        }\n                    )\n            connection_pool = ConnectionPool(**kwargs)\n            self._event_dispatcher.dispatch(\n                AfterPooledConnectionsInstantiationEvent(\n                    [connection_pool], ClientType.SYNC, credential_provider\n                )\n            )\n            self.auto_close_connection_pool = True\n        else:\n            self.auto_close_connection_pool = False\n            self._event_dispatcher.dispatch(\n                AfterPooledConnectionsInstantiationEvent(\n                    [connection_pool], ClientType.SYNC, credential_provider\n                )\n            )\n\n        self.connection_pool = connection_pool\n\n        if (cache_config or cache) and self.connection_pool.get_protocol() not in [\n            3,\n            \"3\",\n        ]:\n            raise RedisError(\"Client caching is only supported with RESP version 3\")\n\n        self.single_connection_lock = threading.RLock()\n        self.connection = None\n        self._single_connection_client = single_connection_client\n        if self._single_connection_client:\n            self.connection = self.connection_pool.get_connection()\n            self._event_dispatcher.dispatch(\n                AfterSingleConnectionInstantiationEvent(\n                    self.connection, ClientType.SYNC, self.single_connection_lock\n                )\n            )\n\n        self.response_callbacks = CaseInsensitiveDict(_RedisCallbacks)\n\n        if self.connection_pool.connection_kwargs.get(\"protocol\") in [\"3\", 3]:\n            self.response_callbacks.update(_RedisCallbacksRESP3)\n        else:\n            self.response_callbacks.update(_RedisCallbacksRESP2)\n\n    def __repr__(self) -> str:\n        return (\n            f\"<{type(self).__module__}.{type(self).__name__}\"\n            f\"({repr(self.connection_pool)})>\"\n        )\n\n    def get_encoder(self) -> \"Encoder\":\n        \"\"\"Get the connection pool's encoder\"\"\"\n        return self.connection_pool.get_encoder()\n\n    def get_connection_kwargs(self) -> Dict:\n        \"\"\"Get the connection's key-word arguments\"\"\"\n        return self.connection_pool.connection_kwargs\n\n    def get_retry(self) -> Optional[Retry]:\n        return self.get_connection_kwargs().get(\"retry\")\n\n    def set_retry(self, retry: Retry) -> None:\n        self.get_connection_kwargs().update({\"retry\": retry})\n        self.connection_pool.set_retry(retry)\n\n    def set_response_callback(self, command: str, callback: Callable) -> None:\n        \"\"\"Set a custom Response Callback\"\"\"\n        self.response_callbacks[command] = callback\n\n    def load_external_module(self, funcname, func) -> None:\n        \"\"\"\n        This function can be used to add externally defined redis modules,\n        and their namespaces to the redis client.\n\n        funcname - A string containing the name of the function to create\n        func - The function, being added to this class.\n\n        ex: Assume that one has a custom redis module named foomod that\n        creates command named 'foo.dothing' and 'foo.anotherthing' in redis.\n        To load function functions into this namespace:\n\n        from redis import Redis\n        from foomodule import F\n        r = Redis()\n        r.load_external_module(\"foo\", F)\n        r.foo().dothing('your', 'arguments')\n\n        For a concrete example see the reimport of the redisjson module in\n        tests/test_connection.py::test_loading_external_modules\n        \"\"\"\n        setattr(self, funcname, func)\n\n    def pipeline(self, transaction=True, shard_hint=None) -> \"Pipeline\":\n        \"\"\"\n        Return a new pipeline object that can queue multiple commands for\n        later execution. ``transaction`` indicates whether all commands\n        should be executed atomically. Apart from making a group of operations\n        atomic, pipelines are useful for reducing the back-and-forth overhead\n        between the client and server.\n        \"\"\"\n        return Pipeline(\n            self.connection_pool, self.response_callbacks, transaction, shard_hint\n        )\n\n    def transaction(\n        self, func: Callable[[\"Pipeline\"], None], *watches, **kwargs\n    ) -> Union[List[Any], Any, None]:\n        \"\"\"\n        Convenience method for executing the callable `func` as a transaction\n        while watching all keys specified in `watches`. The 'func' callable\n        should expect a single argument which is a Pipeline object.\n        \"\"\"\n        shard_hint = kwargs.pop(\"shard_hint\", None)\n        value_from_callable = kwargs.pop(\"value_from_callable\", False)\n        watch_delay = kwargs.pop(\"watch_delay\", None)\n        with self.pipeline(True, shard_hint) as pipe:\n            while True:\n                try:\n                    if watches:\n                        pipe.watch(*watches)\n                    func_value = func(pipe)\n                    exec_value = pipe.execute()\n                    return func_value if value_from_callable else exec_value\n                except WatchError:\n                    if watch_delay is not None and watch_delay > 0:\n                        time.sleep(watch_delay)\n                    continue\n\n    def lock(\n        self,\n        name: str,\n        timeout: Optional[float] = None,\n        sleep: float = 0.1,\n        blocking: bool = True,\n        blocking_timeout: Optional[float] = None,\n        lock_class: Union[None, Any] = None,\n        thread_local: bool = True,\n        raise_on_release_error: bool = True,\n    ):\n        \"\"\"\n        Return a new Lock object using key ``name`` that mimics\n        the behavior of threading.Lock.\n\n        If specified, ``timeout`` indicates a maximum life for the lock.\n        By default, it will remain locked until release() is called.\n\n        ``sleep`` indicates the amount of time to sleep per loop iteration\n        when the lock is in blocking mode and another client is currently\n        holding the lock.\n\n        ``blocking`` indicates whether calling ``acquire`` should block until\n        the lock has been acquired or to fail immediately, causing ``acquire``\n        to return False and the lock not being acquired. Defaults to True.\n        Note this value can be overridden by passing a ``blocking``\n        argument to ``acquire``.\n\n        ``blocking_timeout`` indicates the maximum amount of time in seconds to\n        spend trying to acquire the lock. A value of ``None`` indicates\n        continue trying forever. ``blocking_timeout`` can be specified as a\n        float or integer, both representing the number of seconds to wait.\n\n        ``lock_class`` forces the specified lock implementation. Note that as\n        of redis-py 3.0, the only lock class we implement is ``Lock`` (which is\n        a Lua-based lock). So, it's unlikely you'll need this parameter, unless\n        you have created your own custom lock class.\n\n        ``thread_local`` indicates whether the lock token is placed in\n        thread-local storage. By default, the token is placed in thread local\n        storage so that a thread only sees its token, not a token set by\n        another thread. Consider the following timeline:\n\n            time: 0, thread-1 acquires `my-lock`, with a timeout of 5 seconds.\n                     thread-1 sets the token to \"abc\"\n            time: 1, thread-2 blocks trying to acquire `my-lock` using the\n                     Lock instance.\n            time: 5, thread-1 has not yet completed. redis expires the lock\n                     key.\n            time: 5, thread-2 acquired `my-lock` now that it's available.\n                     thread-2 sets the token to \"xyz\"\n            time: 6, thread-1 finishes its work and calls release(). if the\n                     token is *not* stored in thread local storage, then\n                     thread-1 would see the token value as \"xyz\" and would be\n                     able to successfully release the thread-2's lock.\n\n        ``raise_on_release_error`` indicates whether to raise an exception when\n        the lock is no longer owned when exiting the context manager. By default,\n        this is True, meaning an exception will be raised. If False, the warning\n        will be logged and the exception will be suppressed.\n\n        In some use cases it's necessary to disable thread local storage. For\n        example, if you have code where one thread acquires a lock and passes\n        that lock instance to a worker thread to release later. If thread\n        local storage isn't disabled in this case, the worker thread won't see\n        the token set by the thread that acquired the lock. Our assumption\n        is that these cases aren't common and as such default to using\n        thread local storage.\"\"\"\n        if lock_class is None:\n            lock_class = Lock\n        return lock_class(\n            self,\n            name,\n            timeout=timeout,\n            sleep=sleep,\n            blocking=blocking,\n            blocking_timeout=blocking_timeout,\n            thread_local=thread_local,\n            raise_on_release_error=raise_on_release_error,\n        )\n\n    def pubsub(self, **kwargs):\n        \"\"\"\n        Return a Publish/Subscribe object. With this object, you can\n        subscribe to channels and listen for messages that get published to\n        them.\n        \"\"\"\n        return PubSub(\n            self.connection_pool, event_dispatcher=self._event_dispatcher, **kwargs\n        )\n\n    def monitor(self):\n        return Monitor(self.connection_pool)\n\n    def client(self):\n        return self.__class__(\n            connection_pool=self.connection_pool, single_connection_client=True\n        )\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n\n    def __del__(self):\n        try:\n            self.close()\n        except Exception:\n            pass\n\n    def close(self) -> None:\n        # In case a connection property does not yet exist\n        # (due to a crash earlier in the Redis() constructor), return\n        # immediately as there is nothing to clean-up.\n        if not hasattr(self, \"connection\"):\n            return\n\n        conn = self.connection\n        if conn:\n            self.connection = None\n            self.connection_pool.release(conn)\n\n        if self.auto_close_connection_pool:\n            self.connection_pool.disconnect()\n\n    def _send_command_parse_response(self, conn, command_name, *args, **options):\n        \"\"\"\n        Send a command and parse the response\n        \"\"\"\n        conn.send_command(*args, **options)\n        return self.parse_response(conn, command_name, **options)\n\n    def _close_connection(self, conn) -> None:\n        \"\"\"\n        Close the connection before retrying.\n\n        The supported exceptions are already checked in the\n        retry object so we don't need to do it here.\n\n        After we disconnect the connection, it will try to reconnect and\n        do a health check as part of the send_command logic(on connection level).\n        \"\"\"\n\n        conn.disconnect()\n\n    # COMMAND EXECUTION AND PROTOCOL PARSING\n    def execute_command(self, *args, **options):\n        return self._execute_command(*args, **options)\n\n    def _execute_command(self, *args, **options):\n        \"\"\"Execute a command and return a parsed response\"\"\"\n        pool = self.connection_pool\n        command_name = args[0]\n        conn = self.connection or pool.get_connection()\n\n        if self._single_connection_client:\n            self.single_connection_lock.acquire()\n        try:\n            return conn.retry.call_with_retry(\n                lambda: self._send_command_parse_response(\n                    conn, command_name, *args, **options\n                ),\n                lambda _: self._close_connection(conn),\n            )\n        finally:\n            if self._single_connection_client:\n                self.single_connection_lock.release()\n            if not self.connection:\n                pool.release(conn)\n\n    def parse_response(self, connection, command_name, **options):\n        \"\"\"Parses a response from the Redis server\"\"\"\n        try:\n            if NEVER_DECODE in options:\n                response = connection.read_response(disable_decoding=True)\n                options.pop(NEVER_DECODE)\n            else:\n                response = connection.read_response()\n        except ResponseError:\n            if EMPTY_RESPONSE in options:\n                return options[EMPTY_RESPONSE]\n            raise\n\n        if EMPTY_RESPONSE in options:\n            options.pop(EMPTY_RESPONSE)\n\n        # Remove keys entry, it needs only for cache.\n        options.pop(\"keys\", None)\n\n        if command_name in self.response_callbacks:\n            return self.response_callbacks[command_name](response, **options)\n        return response\n\n    def get_cache(self) -> Optional[CacheInterface]:\n        return self.connection_pool.cache\n\n\nStrictRedis = Redis\n\n\nclass Monitor:\n    \"\"\"\n    Monitor is useful for handling the MONITOR command to the redis server.\n    next_command() method returns one command from monitor\n    listen() method yields commands from monitor.\n    \"\"\"\n\n    monitor_re = re.compile(r\"\\[(\\d+) (.*?)\\] (.*)\")\n    command_re = re.compile(r'\"(.*?)(?<!\\\\)\"')\n\n    def __init__(self, connection_pool):\n        self.connection_pool = connection_pool\n        self.connection = self.connection_pool.get_connection()\n\n    def __enter__(self):\n        self.connection.send_command(\"MONITOR\")\n        # check that monitor returns 'OK', but don't return it to user\n        response = self.connection.read_response()\n        if not bool_ok(response):\n            raise RedisError(f\"MONITOR failed: {response}\")\n        return self\n\n    def __exit__(self, *args):\n        self.connection.disconnect()\n        self.connection_pool.release(self.connection)\n\n    def next_command(self):\n        \"\"\"Parse the response from a monitor command\"\"\"\n        response = self.connection.read_response()\n        if isinstance(response, bytes):\n            response = self.connection.encoder.decode(response, force=True)\n        command_time, command_data = response.split(\" \", 1)\n        m = self.monitor_re.match(command_data)\n        db_id, client_info, command = m.groups()\n        command = \" \".join(self.command_re.findall(command))\n        # Redis escapes double quotes because each piece of the command\n        # string is surrounded by double quotes. We don't have that\n        # requirement so remove the escaping and leave the quote.\n        command = command.replace('\\\\\"', '\"')\n\n        if client_info == \"lua\":\n            client_address = \"lua\"\n            client_port = \"\"\n            client_type = \"lua\"\n        elif client_info.startswith(\"unix\"):\n            client_address = \"unix\"\n            client_port = client_info[5:]\n            client_type = \"unix\"\n        else:\n            # use rsplit as ipv6 addresses contain colons\n            client_address, client_port = client_info.rsplit(\":\", 1)\n            client_type = \"tcp\"\n        return {\n            \"time\": float(command_time),\n            \"db\": int(db_id),\n            \"client_address\": client_address,\n            \"client_port\": client_port,\n            \"client_type\": client_type,\n            \"command\": command,\n        }\n\n    def listen(self):\n        \"\"\"Listen for commands coming to the server.\"\"\"\n        while True:\n            yield self.next_command()\n\n\nclass PubSub:\n    \"\"\"\n    PubSub provides publish, subscribe and listen support to Redis channels.\n\n    After subscribing to one or more channels, the listen() method will block\n    until a message arrives on one of the subscribed channels. That message\n    will be returned and it's safe to start listening again.\n    \"\"\"\n\n    PUBLISH_MESSAGE_TYPES = (\"message\", \"pmessage\", \"smessage\")\n    UNSUBSCRIBE_MESSAGE_TYPES = (\"unsubscribe\", \"punsubscribe\", \"sunsubscribe\")\n    HEALTH_CHECK_MESSAGE = \"redis-py-health-check\"\n\n    def __init__(\n        self,\n        connection_pool,\n        shard_hint=None,\n        ignore_subscribe_messages: bool = False,\n        encoder: Optional[\"Encoder\"] = None,\n        push_handler_func: Union[None, Callable[[str], None]] = None,\n        event_dispatcher: Optional[\"EventDispatcher\"] = None,\n    ):\n        self.connection_pool = connection_pool\n        self.shard_hint = shard_hint\n        self.ignore_subscribe_messages = ignore_subscribe_messages\n        self.connection = None\n        self.subscribed_event = threading.Event()\n        # we need to know the encoding options for this connection in order\n        # to lookup channel and pattern names for callback handlers.\n        self.encoder = encoder\n        self.push_handler_func = push_handler_func\n        if event_dispatcher is None:\n            self._event_dispatcher = EventDispatcher()\n        else:\n            self._event_dispatcher = event_dispatcher\n\n        self._lock = threading.RLock()\n        if self.encoder is None:\n            self.encoder = self.connection_pool.get_encoder()\n        self.health_check_response_b = self.encoder.encode(self.HEALTH_CHECK_MESSAGE)\n        if self.encoder.decode_responses:\n            self.health_check_response = [\"pong\", self.HEALTH_CHECK_MESSAGE]\n        else:\n            self.health_check_response = [b\"pong\", self.health_check_response_b]\n        if self.push_handler_func is None:\n            _set_info_logger()\n        self.reset()\n\n    def __enter__(self) -> \"PubSub\":\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback) -> None:\n        self.reset()\n\n    def __del__(self) -> None:\n        try:\n            # if this object went out of scope prior to shutting down\n            # subscriptions, close the connection manually before\n            # returning it to the connection pool\n            self.reset()\n        except Exception:\n            pass\n\n    def reset(self) -> None:\n        if self.connection:\n            self.connection.disconnect()\n            self.connection.deregister_connect_callback(self.on_connect)\n            self.connection_pool.release(self.connection)\n            self.connection = None\n        self.health_check_response_counter = 0\n        self.channels = {}\n        self.pending_unsubscribe_channels = set()\n        self.shard_channels = {}\n        self.pending_unsubscribe_shard_channels = set()\n        self.patterns = {}\n        self.pending_unsubscribe_patterns = set()\n        self.subscribed_event.clear()\n\n    def close(self) -> None:\n        self.reset()\n\n    def on_connect(self, connection) -> None:\n        \"Re-subscribe to any channels and patterns previously subscribed to\"\n        # NOTE: for python3, we can't pass bytestrings as keyword arguments\n        # so we need to decode channel/pattern names back to unicode strings\n        # before passing them to [p]subscribe.\n        self.pending_unsubscribe_channels.clear()\n        self.pending_unsubscribe_patterns.clear()\n        self.pending_unsubscribe_shard_channels.clear()\n        if self.channels:\n            channels = {\n                self.encoder.decode(k, force=True): v for k, v in self.channels.items()\n            }\n            self.subscribe(**channels)\n        if self.patterns:\n            patterns = {\n                self.encoder.decode(k, force=True): v for k, v in self.patterns.items()\n            }\n            self.psubscribe(**patterns)\n        if self.shard_channels:\n            shard_channels = {\n                self.encoder.decode(k, force=True): v\n                for k, v in self.shard_channels.items()\n            }\n            self.ssubscribe(**shard_channels)\n\n    @property\n    def subscribed(self) -> bool:\n        \"\"\"Indicates if there are subscriptions to any channels or patterns\"\"\"\n        return self.subscribed_event.is_set()\n\n    def execute_command(self, *args):\n        \"\"\"Execute a publish/subscribe command\"\"\"\n\n        # NOTE: don't parse the response in this function -- it could pull a\n        # legitimate message off the stack if the connection is already\n        # subscribed to one or more channels\n\n        if self.connection is None:\n            self.connection = self.connection_pool.get_connection()\n            # register a callback that re-subscribes to any channels we\n            # were listening to when we were disconnected\n            self.connection.register_connect_callback(self.on_connect)\n            if self.push_handler_func is not None:\n                self.connection._parser.set_pubsub_push_handler(self.push_handler_func)\n            self._event_dispatcher.dispatch(\n                AfterPubSubConnectionInstantiationEvent(\n                    self.connection, self.connection_pool, ClientType.SYNC, self._lock\n                )\n            )\n        connection = self.connection\n        kwargs = {\"check_health\": not self.subscribed}\n        if not self.subscribed:\n            self.clean_health_check_responses()\n        with self._lock:\n            self._execute(connection, connection.send_command, *args, **kwargs)\n\n    def clean_health_check_responses(self) -> None:\n        \"\"\"\n        If any health check responses are present, clean them\n        \"\"\"\n        ttl = 10\n        conn = self.connection\n        while self.health_check_response_counter > 0 and ttl > 0:\n            if self._execute(conn, conn.can_read, timeout=conn.socket_timeout):\n                response = self._execute(conn, conn.read_response)\n                if self.is_health_check_response(response):\n                    self.health_check_response_counter -= 1\n                else:\n                    raise PubSubError(\n                        \"A non health check response was cleaned by \"\n                        \"execute_command: {}\".format(response)\n                    )\n            ttl -= 1\n\n    def _reconnect(self, conn) -> None:\n        \"\"\"\n        The supported exceptions are already checked in the\n        retry object so we don't need to do it here.\n\n        In this error handler we are trying to reconnect to the server.\n        \"\"\"\n        conn.disconnect()\n        conn.connect()\n\n    def _execute(self, conn, command, *args, **kwargs):\n        \"\"\"\n        Connect manually upon disconnection. If the Redis server is down,\n        this will fail and raise a ConnectionError as desired.\n        After reconnection, the ``on_connect`` callback should have been\n        called by the # connection to resubscribe us to any channels and\n        patterns we were previously listening to\n        \"\"\"\n        return conn.retry.call_with_retry(\n            lambda: command(*args, **kwargs),\n            lambda _: self._reconnect(conn),\n        )\n\n    def parse_response(self, block=True, timeout=0):\n        \"\"\"Parse the response from a publish/subscribe command\"\"\"\n        conn = self.connection\n        if conn is None:\n            raise RuntimeError(\n                \"pubsub connection not set: \"\n                \"did you forget to call subscribe() or psubscribe()?\"\n            )\n\n        self.check_health()\n\n        def try_read():\n            if not block:\n                if not conn.can_read(timeout=timeout):\n                    return None\n            else:\n                conn.connect()\n            return conn.read_response(disconnect_on_error=False, push_request=True)\n\n        response = self._execute(conn, try_read)\n\n        if self.is_health_check_response(response):\n            # ignore the health check message as user might not expect it\n            self.health_check_response_counter -= 1\n            return None\n        return response\n\n    def is_health_check_response(self, response) -> bool:\n        \"\"\"\n        Check if the response is a health check response.\n        If there are no subscriptions redis responds to PING command with a\n        bulk response, instead of a multi-bulk with \"pong\" and the response.\n        \"\"\"\n        return response in [\n            self.health_check_response,  # If there was a subscription\n            self.health_check_response_b,  # If there wasn't\n        ]\n\n    def check_health(self) -> None:\n        conn = self.connection\n        if conn is None:\n            raise RuntimeError(\n                \"pubsub connection not set: \"\n                \"did you forget to call subscribe() or psubscribe()?\"\n            )\n\n        if conn.health_check_interval and time.monotonic() > conn.next_health_check:\n            conn.send_command(\"PING\", self.HEALTH_CHECK_MESSAGE, check_health=False)\n            self.health_check_response_counter += 1\n\n    def _normalize_keys(self, data) -> Dict:\n        \"\"\"\n        normalize channel/pattern names to be either bytes or strings\n        based on whether responses are automatically decoded. this saves us\n        from coercing the value for each message coming in.\n        \"\"\"\n        encode = self.encoder.encode\n        decode = self.encoder.decode\n        return {decode(encode(k)): v for k, v in data.items()}\n\n    def psubscribe(self, *args, **kwargs):\n        \"\"\"\n        Subscribe to channel patterns. Patterns supplied as keyword arguments\n        expect a pattern name as the key and a callable as the value. A\n        pattern's callable will be invoked automatically when a message is\n        received on that pattern rather than producing a message via\n        ``listen()``.\n        \"\"\"\n        if args:\n            args = list_or_args(args[0], args[1:])\n        new_patterns = dict.fromkeys(args)\n        new_patterns.update(kwargs)\n        ret_val = self.execute_command(\"PSUBSCRIBE\", *new_patterns.keys())\n        # update the patterns dict AFTER we send the command. we don't want to\n        # subscribe twice to these patterns, once for the command and again\n        # for the reconnection.\n        new_patterns = self._normalize_keys(new_patterns)\n        self.patterns.update(new_patterns)\n        if not self.subscribed:\n            # Set the subscribed_event flag to True\n            self.subscribed_event.set()\n            # Clear the health check counter\n            self.health_check_response_counter = 0\n        self.pending_unsubscribe_patterns.difference_update(new_patterns)\n        return ret_val\n\n    def punsubscribe(self, *args):\n        \"\"\"\n        Unsubscribe from the supplied patterns. If empty, unsubscribe from\n        all patterns.\n        \"\"\"\n        if args:\n            args = list_or_args(args[0], args[1:])\n            patterns = self._normalize_keys(dict.fromkeys(args))\n        else:\n            patterns = self.patterns\n        self.pending_unsubscribe_patterns.update(patterns)\n        return self.execute_command(\"PUNSUBSCRIBE\", *args)\n\n    def subscribe(self, *args, **kwargs):\n        \"\"\"\n        Subscribe to channels. Channels supplied as keyword arguments expect\n        a channel name as the key and a callable as the value. A channel's\n        callable will be invoked automatically when a message is received on\n        that channel rather than producing a message via ``listen()`` or\n        ``get_message()``.\n        \"\"\"\n        if args:\n            args = list_or_args(args[0], args[1:])\n        new_channels = dict.fromkeys(args)\n        new_channels.update(kwargs)\n        ret_val = self.execute_command(\"SUBSCRIBE\", *new_channels.keys())\n        # update the channels dict AFTER we send the command. we don't want to\n        # subscribe twice to these channels, once for the command and again\n        # for the reconnection.\n        new_channels = self._normalize_keys(new_channels)\n        self.channels.update(new_channels)\n        if not self.subscribed:\n            # Set the subscribed_event flag to True\n            self.subscribed_event.set()\n            # Clear the health check counter\n            self.health_check_response_counter = 0\n        self.pending_unsubscribe_channels.difference_update(new_channels)\n        return ret_val\n\n    def unsubscribe(self, *args):\n        \"\"\"\n        Unsubscribe from the supplied channels. If empty, unsubscribe from\n        all channels\n        \"\"\"\n        if args:\n            args = list_or_args(args[0], args[1:])\n            channels = self._normalize_keys(dict.fromkeys(args))\n        else:\n            channels = self.channels\n        self.pending_unsubscribe_channels.update(channels)\n        return self.execute_command(\"UNSUBSCRIBE\", *args)\n\n    def ssubscribe(self, *args, target_node=None, **kwargs):\n        \"\"\"\n        Subscribes the client to the specified shard channels.\n        Channels supplied as keyword arguments expect a channel name as the key\n        and a callable as the value. A channel's callable will be invoked automatically\n        when a message is received on that channel rather than producing a message via\n        ``listen()`` or ``get_sharded_message()``.\n        \"\"\"\n        if args:\n            args = list_or_args(args[0], args[1:])\n        new_s_channels = dict.fromkeys(args)\n        new_s_channels.update(kwargs)\n        ret_val = self.execute_command(\"SSUBSCRIBE\", *new_s_channels.keys())\n        # update the s_channels dict AFTER we send the command. we don't want to\n        # subscribe twice to these channels, once for the command and again\n        # for the reconnection.\n        new_s_channels = self._normalize_keys(new_s_channels)\n        self.shard_channels.update(new_s_channels)\n        if not self.subscribed:\n            # Set the subscribed_event flag to True\n            self.subscribed_event.set()\n            # Clear the health check counter\n            self.health_check_response_counter = 0\n        self.pending_unsubscribe_shard_channels.difference_update(new_s_channels)\n        return ret_val\n\n    def sunsubscribe(self, *args, target_node=None):\n        \"\"\"\n        Unsubscribe from the supplied shard_channels. If empty, unsubscribe from\n        all shard_channels\n        \"\"\"\n        if args:\n            args = list_or_args(args[0], args[1:])\n            s_channels = self._normalize_keys(dict.fromkeys(args))\n        else:\n            s_channels = self.shard_channels\n        self.pending_unsubscribe_shard_channels.update(s_channels)\n        return self.execute_command(\"SUNSUBSCRIBE\", *args)\n\n    def listen(self):\n        \"Listen for messages on channels this client has been subscribed to\"\n        while self.subscribed:\n            response = self.handle_message(self.parse_response(block=True))\n            if response is not None:\n                yield response\n\n    def get_message(\n        self, ignore_subscribe_messages: bool = False, timeout: float = 0.0\n    ):\n        \"\"\"\n        Get the next message if one is available, otherwise None.\n\n        If timeout is specified, the system will wait for `timeout` seconds\n        before returning. Timeout should be specified as a floating point\n        number, or None, to wait indefinitely.\n        \"\"\"\n        if not self.subscribed:\n            # Wait for subscription\n            start_time = time.monotonic()\n            if self.subscribed_event.wait(timeout) is True:\n                # The connection was subscribed during the timeout time frame.\n                # The timeout should be adjusted based on the time spent\n                # waiting for the subscription\n                time_spent = time.monotonic() - start_time\n                timeout = max(0.0, timeout - time_spent)\n            else:\n                # The connection isn't subscribed to any channels or patterns,\n                # so no messages are available\n                return None\n\n        response = self.parse_response(block=(timeout is None), timeout=timeout)\n        if response:\n            return self.handle_message(response, ignore_subscribe_messages)\n        return None\n\n    get_sharded_message = get_message\n\n    def ping(self, message: Union[str, None] = None) -> bool:\n        \"\"\"\n        Ping the Redis server\n        \"\"\"\n        args = [\"PING\", message] if message is not None else [\"PING\"]\n        return self.execute_command(*args)\n\n    def handle_message(self, response, ignore_subscribe_messages=False):\n        \"\"\"\n        Parses a pub/sub message. If the channel or pattern was subscribed to\n        with a message handler, the handler is invoked instead of a parsed\n        message being returned.\n        \"\"\"\n        if response is None:\n            return None\n        if isinstance(response, bytes):\n            response = [b\"pong\", response] if response != b\"PONG\" else [b\"pong\", b\"\"]\n        message_type = str_if_bytes(response[0])\n        if message_type == \"pmessage\":\n            message = {\n                \"type\": message_type,\n                \"pattern\": response[1],\n                \"channel\": response[2],\n                \"data\": response[3],\n            }\n        elif message_type == \"pong\":\n            message = {\n                \"type\": message_type,\n                \"pattern\": None,\n                \"channel\": None,\n                \"data\": response[1],\n            }\n        else:\n            message = {\n                \"type\": message_type,\n                \"pattern\": None,\n                \"channel\": response[1],\n                \"data\": response[2],\n            }\n\n        # if this is an unsubscribe message, remove it from memory\n        if message_type in self.UNSUBSCRIBE_MESSAGE_TYPES:\n            if message_type == \"punsubscribe\":\n                pattern = response[1]\n                if pattern in self.pending_unsubscribe_patterns:\n                    self.pending_unsubscribe_patterns.remove(pattern)\n                    self.patterns.pop(pattern, None)\n            elif message_type == \"sunsubscribe\":\n                s_channel = response[1]\n                if s_channel in self.pending_unsubscribe_shard_channels:\n                    self.pending_unsubscribe_shard_channels.remove(s_channel)\n                    self.shard_channels.pop(s_channel, None)\n            else:\n                channel = response[1]\n                if channel in self.pending_unsubscribe_channels:\n                    self.pending_unsubscribe_channels.remove(channel)\n                    self.channels.pop(channel, None)\n            if not self.channels and not self.patterns and not self.shard_channels:\n                # There are no subscriptions anymore, set subscribed_event flag\n                # to false\n                self.subscribed_event.clear()\n\n        if message_type in self.PUBLISH_MESSAGE_TYPES:\n            # if there's a message handler, invoke it\n            if message_type == \"pmessage\":\n                handler = self.patterns.get(message[\"pattern\"], None)\n            elif message_type == \"smessage\":\n                handler = self.shard_channels.get(message[\"channel\"], None)\n            else:\n                handler = self.channels.get(message[\"channel\"], None)\n            if handler:\n                handler(message)\n                return None\n        elif message_type != \"pong\":\n            # this is a subscribe/unsubscribe message. ignore if we don't\n            # want them\n            if ignore_subscribe_messages or self.ignore_subscribe_messages:\n                return None\n\n        return message\n\n    def run_in_thread(\n        self,\n        sleep_time: float = 0.0,\n        daemon: bool = False,\n        exception_handler: Optional[Callable] = None,\n    ) -> \"PubSubWorkerThread\":\n        for channel, handler in self.channels.items():\n            if handler is None:\n                raise PubSubError(f\"Channel: '{channel}' has no handler registered\")\n        for pattern, handler in self.patterns.items():\n            if handler is None:\n                raise PubSubError(f\"Pattern: '{pattern}' has no handler registered\")\n        for s_channel, handler in self.shard_channels.items():\n            if handler is None:\n                raise PubSubError(\n                    f\"Shard Channel: '{s_channel}' has no handler registered\"\n                )\n\n        thread = PubSubWorkerThread(\n            self, sleep_time, daemon=daemon, exception_handler=exception_handler\n        )\n        thread.start()\n        return thread\n\n\nclass PubSubWorkerThread(threading.Thread):\n    def __init__(\n        self,\n        pubsub,\n        sleep_time: float,\n        daemon: bool = False,\n        exception_handler: Union[\n            Callable[[Exception, \"PubSub\", \"PubSubWorkerThread\"], None], None\n        ] = None,\n    ):\n        super().__init__()\n        self.daemon = daemon\n        self.pubsub = pubsub\n        self.sleep_time = sleep_time\n        self.exception_handler = exception_handler\n        self._running = threading.Event()\n\n    def run(self) -> None:\n        if self._running.is_set():\n            return\n        self._running.set()\n        pubsub = self.pubsub\n        sleep_time = self.sleep_time\n        while self._running.is_set():\n            try:\n                pubsub.get_message(ignore_subscribe_messages=True, timeout=sleep_time)\n            except BaseException as e:\n                if self.exception_handler is None:\n                    raise\n                self.exception_handler(e, pubsub, self)\n        pubsub.close()\n\n    def stop(self) -> None:\n        # trip the flag so the run loop exits. the run loop will\n        # close the pubsub connection, which disconnects the socket\n        # and returns the connection to the pool.\n        self._running.clear()\n\n\nclass Pipeline(Redis):\n    \"\"\"\n    Pipelines provide a way to transmit multiple commands to the Redis server\n    in one transmission.  This is convenient for batch processing, such as\n    saving all the values in a list to Redis.\n\n    All commands executed within a pipeline(when running in transactional mode,\n    which is the default behavior) are wrapped with MULTI and EXEC\n    calls. This guarantees all commands executed in the pipeline will be\n    executed atomically.\n\n    Any command raising an exception does *not* halt the execution of\n    subsequent commands in the pipeline. Instead, the exception is caught\n    and its instance is placed into the response list returned by execute().\n    Code iterating over the response list should be able to deal with an\n    instance of an exception as a potential value. In general, these will be\n    ResponseError exceptions, such as those raised when issuing a command\n    on a key of a different datatype.\n    \"\"\"\n\n    UNWATCH_COMMANDS = {\"DISCARD\", \"EXEC\", \"UNWATCH\"}\n\n    def __init__(\n        self,\n        connection_pool: ConnectionPool,\n        response_callbacks,\n        transaction,\n        shard_hint,\n    ):\n        self.connection_pool = connection_pool\n        self.connection: Optional[Connection] = None\n        self.response_callbacks = response_callbacks\n        self.transaction = transaction\n        self.shard_hint = shard_hint\n        self.watching = False\n        self.command_stack = []\n        self.scripts: Set[Script] = set()\n        self.explicit_transaction = False\n\n    def __enter__(self) -> \"Pipeline\":\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.reset()\n\n    def __del__(self):\n        try:\n            self.reset()\n        except Exception:\n            pass\n\n    def __len__(self) -> int:\n        return len(self.command_stack)\n\n    def __bool__(self) -> bool:\n        \"\"\"Pipeline instances should always evaluate to True\"\"\"\n        return True\n\n    def reset(self) -> None:\n        self.command_stack = []\n        self.scripts = set()\n        # make sure to reset the connection state in the event that we were\n        # watching something\n        if self.watching and self.connection:\n            try:\n                # call this manually since our unwatch or\n                # immediate_execute_command methods can call reset()\n                self.connection.send_command(\"UNWATCH\")\n                self.connection.read_response()\n            except ConnectionError:\n                # disconnect will also remove any previous WATCHes\n                self.connection.disconnect()\n        # clean up the other instance attributes\n        self.watching = False\n        self.explicit_transaction = False\n        # we can safely return the connection to the pool here since we're\n        # sure we're no longer WATCHing anything\n        if self.connection:\n            self.connection_pool.release(self.connection)\n            self.connection = None\n\n    def close(self) -> None:\n        \"\"\"Close the pipeline\"\"\"\n        self.reset()\n\n    def multi(self) -> None:\n        \"\"\"\n        Start a transactional block of the pipeline after WATCH commands\n        are issued. End the transactional block with `execute`.\n        \"\"\"\n        if self.explicit_transaction:\n            raise RedisError(\"Cannot issue nested calls to MULTI\")\n        if self.command_stack:\n            raise RedisError(\n                \"Commands without an initial WATCH have already been issued\"\n            )\n        self.explicit_transaction = True\n\n    def execute_command(self, *args, **kwargs):\n        if (self.watching or args[0] == \"WATCH\") and not self.explicit_transaction:\n            return self.immediate_execute_command(*args, **kwargs)\n        return self.pipeline_execute_command(*args, **kwargs)\n\n    def _disconnect_reset_raise_on_watching(\n        self,\n        conn: AbstractConnection,\n        error: Exception,\n    ) -> None:\n        \"\"\"\n        Close the connection reset watching state and\n        raise an exception if we were watching.\n\n        The supported exceptions are already checked in the\n        retry object so we don't need to do it here.\n\n        After we disconnect the connection, it will try to reconnect and\n        do a health check as part of the send_command logic(on connection level).\n        \"\"\"\n        conn.disconnect()\n\n        # if we were already watching a variable, the watch is no longer\n        # valid since this connection has died. raise a WatchError, which\n        # indicates the user should retry this transaction.\n        if self.watching:\n            self.reset()\n            raise WatchError(\n                f\"A {type(error).__name__} occurred while watching one or more keys\"\n            )\n\n    def immediate_execute_command(self, *args, **options):\n        \"\"\"\n        Execute a command immediately, but don't auto-retry on the supported\n        errors for retry if we're already WATCHing a variable.\n        Used when issuing WATCH or subsequent commands retrieving their values but before\n        MULTI is called.\n        \"\"\"\n        command_name = args[0]\n        conn = self.connection\n        # if this is the first call, we need a connection\n        if not conn:\n            conn = self.connection_pool.get_connection()\n            self.connection = conn\n\n        return conn.retry.call_with_retry(\n            lambda: self._send_command_parse_response(\n                conn, command_name, *args, **options\n            ),\n            lambda error: self._disconnect_reset_raise_on_watching(conn, error),\n        )\n\n    def pipeline_execute_command(self, *args, **options) -> \"Pipeline\":\n        \"\"\"\n        Stage a command to be executed when execute() is next called\n\n        Returns the current Pipeline object back so commands can be\n        chained together, such as:\n\n        pipe = pipe.set('foo', 'bar').incr('baz').decr('bang')\n\n        At some other point, you can then run: pipe.execute(),\n        which will execute all commands queued in the pipe.\n        \"\"\"\n        self.command_stack.append((args, options))\n        return self\n\n    def _execute_transaction(\n        self, connection: Connection, commands, raise_on_error\n    ) -> List:\n        cmds = chain([((\"MULTI\",), {})], commands, [((\"EXEC\",), {})])\n        all_cmds = connection.pack_commands(\n            [args for args, options in cmds if EMPTY_RESPONSE not in options]\n        )\n        connection.send_packed_command(all_cmds)\n        errors = []\n\n        # parse off the response for MULTI\n        # NOTE: we need to handle ResponseErrors here and continue\n        # so that we read all the additional command messages from\n        # the socket\n        try:\n            self.parse_response(connection, \"_\")\n        except ResponseError as e:\n            errors.append((0, e))\n\n        # and all the other commands\n        for i, command in enumerate(commands):\n            if EMPTY_RESPONSE in command[1]:\n                errors.append((i, command[1][EMPTY_RESPONSE]))\n            else:\n                try:\n                    self.parse_response(connection, \"_\")\n                except ResponseError as e:\n                    self.annotate_exception(e, i + 1, command[0])\n                    errors.append((i, e))\n\n        # parse the EXEC.\n        try:\n            response = self.parse_response(connection, \"_\")\n        except ExecAbortError:\n            if errors:\n                raise errors[0][1]\n            raise\n\n        # EXEC clears any watched keys\n        self.watching = False\n\n        if response is None:\n            raise WatchError(\"Watched variable changed.\")\n\n        # put any parse errors into the response\n        for i, e in errors:\n            response.insert(i, e)\n\n        if len(response) != len(commands):\n            self.connection.disconnect()\n            raise ResponseError(\n                \"Wrong number of response items from pipeline execution\"\n            )\n\n        # find any errors in the response and raise if necessary\n        if raise_on_error:\n            self.raise_first_error(commands, response)\n\n        # We have to run response callbacks manually\n        data = []\n        for r, cmd in zip(response, commands):\n            if not isinstance(r, Exception):\n                args, options = cmd\n                # Remove keys entry, it needs only for cache.\n                options.pop(\"keys\", None)\n                command_name = args[0]\n                if command_name in self.response_callbacks:\n                    r = self.response_callbacks[command_name](r, **options)\n            data.append(r)\n        return data\n\n    def _execute_pipeline(self, connection, commands, raise_on_error):\n        # build up all commands into a single request to increase network perf\n        all_cmds = connection.pack_commands([args for args, _ in commands])\n        connection.send_packed_command(all_cmds)\n\n        response = []\n        for args, options in commands:\n            try:\n                response.append(self.parse_response(connection, args[0], **options))\n            except ResponseError as e:\n                response.append(e)\n\n        if raise_on_error:\n            self.raise_first_error(commands, response)\n        return response\n\n    def raise_first_error(self, commands, response):\n        for i, r in enumerate(response):\n            if isinstance(r, ResponseError):\n                self.annotate_exception(r, i + 1, commands[i][0])\n                raise r\n\n    def annotate_exception(self, exception, number, command):\n        cmd = \" \".join(map(safe_str, command))\n        msg = (\n            f\"Command # {number} ({truncate_text(cmd)}) of pipeline \"\n            f\"caused error: {exception.args[0]}\"\n        )\n        exception.args = (msg,) + exception.args[1:]\n\n    def parse_response(self, connection, command_name, **options):\n        result = Redis.parse_response(self, connection, command_name, **options)\n        if command_name in self.UNWATCH_COMMANDS:\n            self.watching = False\n        elif command_name == \"WATCH\":\n            self.watching = True\n        return result\n\n    def load_scripts(self):\n        # make sure all scripts that are about to be run on this pipeline exist\n        scripts = list(self.scripts)\n        immediate = self.immediate_execute_command\n        shas = [s.sha for s in scripts]\n        # we can't use the normal script_* methods because they would just\n        # get buffered in the pipeline.\n        exists = immediate(\"SCRIPT EXISTS\", *shas)\n        if not all(exists):\n            for s, exist in zip(scripts, exists):\n                if not exist:\n                    s.sha = immediate(\"SCRIPT LOAD\", s.script)\n\n    def _disconnect_raise_on_watching(\n        self,\n        conn: AbstractConnection,\n        error: Exception,\n    ) -> None:\n        \"\"\"\n        Close the connection, raise an exception if we were watching.\n\n        The supported exceptions are already checked in the\n        retry object so we don't need to do it here.\n\n        After we disconnect the connection, it will try to reconnect and\n        do a health check as part of the send_command logic(on connection level).\n        \"\"\"\n        conn.disconnect()\n        # if we were watching a variable, the watch is no longer valid\n        # since this connection has died. raise a WatchError, which\n        # indicates the user should retry this transaction.\n        if self.watching:\n            raise WatchError(\n                f\"A {type(error).__name__} occurred while watching one or more keys\"\n            )\n\n    def execute(self, raise_on_error: bool = True) -> List[Any]:\n        \"\"\"Execute all the commands in the current pipeline\"\"\"\n        stack = self.command_stack\n        if not stack and not self.watching:\n            return []\n        if self.scripts:\n            self.load_scripts()\n        if self.transaction or self.explicit_transaction:\n            execute = self._execute_transaction\n        else:\n            execute = self._execute_pipeline\n\n        conn = self.connection\n        if not conn:\n            conn = self.connection_pool.get_connection()\n            # assign to self.connection so reset() releases the connection\n            # back to the pool after we're done\n            self.connection = conn\n\n        try:\n            return conn.retry.call_with_retry(\n                lambda: execute(conn, stack, raise_on_error),\n                lambda error: self._disconnect_raise_on_watching(conn, error),\n            )\n        finally:\n            self.reset()\n\n    def discard(self):\n        \"\"\"\n        Flushes all previously queued commands\n        See: https://redis.io/commands/DISCARD\n        \"\"\"\n        self.execute_command(\"DISCARD\")\n\n    def watch(self, *names):\n        \"\"\"Watches the values at keys ``names``\"\"\"\n        if self.explicit_transaction:\n            raise RedisError(\"Cannot issue a WATCH after a MULTI\")\n        return self.execute_command(\"WATCH\", *names)\n\n    def unwatch(self) -> bool:\n        \"\"\"Unwatches all previously specified keys\"\"\"\n        return self.watching and self.execute_command(\"UNWATCH\") or True\n", 1631], "/usr/local/lib/python3.11/typing.py": ["\"\"\"\nThe typing module: Support for gradual typing as defined by PEP 484 and subsequent PEPs.\n\nAmong other things, the module includes the following:\n* Generic, Protocol, and internal machinery to support generic aliases.\n  All subscripted types like X[int], Union[int, str] are generic aliases.\n* Various \"special forms\" that have unique meanings in type annotations:\n  NoReturn, Never, ClassVar, Self, Concatenate, Unpack, and others.\n* Classes whose instances can be type arguments to generic classes and functions:\n  TypeVar, ParamSpec, TypeVarTuple.\n* Public helper functions: get_type_hints, overload, cast, final, and others.\n* Several protocols to support duck-typing:\n  SupportsFloat, SupportsIndex, SupportsAbs, and others.\n* Special types: NewType, NamedTuple, TypedDict.\n* Deprecated wrapper submodules for re and io related types.\n* Deprecated aliases for builtin types and collections.abc ABCs.\n\nAny name not present in __all__ is an implementation detail\nthat may be changed without notice. Use at your own risk!\n\"\"\"\n\nfrom abc import abstractmethod, ABCMeta\nimport collections\nfrom collections import defaultdict\nimport collections.abc\nimport contextlib\nimport functools\nimport operator\nimport re as stdlib_re  # Avoid confusion with the re we export.\nimport sys\nimport types\nimport warnings\nfrom types import WrapperDescriptorType, MethodWrapperType, MethodDescriptorType, GenericAlias\n\n\ntry:\n    from _typing import _idfunc\nexcept ImportError:\n    def _idfunc(_, x):\n        return x\n\n# Please keep __all__ alphabetized within each category.\n__all__ = [\n    # Super-special typing primitives.\n    'Annotated',\n    'Any',\n    'Callable',\n    'ClassVar',\n    'Concatenate',\n    'Final',\n    'ForwardRef',\n    'Generic',\n    'Literal',\n    'Optional',\n    'ParamSpec',\n    'Protocol',\n    'Tuple',\n    'Type',\n    'TypeVar',\n    'TypeVarTuple',\n    'Union',\n\n    # ABCs (from collections.abc).\n    'AbstractSet',  # collections.abc.Set.\n    'ByteString',\n    'Container',\n    'ContextManager',\n    'Hashable',\n    'ItemsView',\n    'Iterable',\n    'Iterator',\n    'KeysView',\n    'Mapping',\n    'MappingView',\n    'MutableMapping',\n    'MutableSequence',\n    'MutableSet',\n    'Sequence',\n    'Sized',\n    'ValuesView',\n    'Awaitable',\n    'AsyncIterator',\n    'AsyncIterable',\n    'Coroutine',\n    'Collection',\n    'AsyncGenerator',\n    'AsyncContextManager',\n\n    # Structural checks, a.k.a. protocols.\n    'Reversible',\n    'SupportsAbs',\n    'SupportsBytes',\n    'SupportsComplex',\n    'SupportsFloat',\n    'SupportsIndex',\n    'SupportsInt',\n    'SupportsRound',\n\n    # Concrete collection types.\n    'ChainMap',\n    'Counter',\n    'Deque',\n    'Dict',\n    'DefaultDict',\n    'List',\n    'OrderedDict',\n    'Set',\n    'FrozenSet',\n    'NamedTuple',  # Not really a type.\n    'TypedDict',  # Not really a type.\n    'Generator',\n\n    # Other concrete types.\n    'BinaryIO',\n    'IO',\n    'Match',\n    'Pattern',\n    'TextIO',\n\n    # One-off things.\n    'AnyStr',\n    'assert_type',\n    'assert_never',\n    'cast',\n    'clear_overloads',\n    'dataclass_transform',\n    'final',\n    'get_args',\n    'get_origin',\n    'get_overloads',\n    'get_type_hints',\n    'is_typeddict',\n    'LiteralString',\n    'Never',\n    'NewType',\n    'no_type_check',\n    'no_type_check_decorator',\n    'NoReturn',\n    'NotRequired',\n    'overload',\n    'ParamSpecArgs',\n    'ParamSpecKwargs',\n    'Required',\n    'reveal_type',\n    'runtime_checkable',\n    'Self',\n    'Text',\n    'TYPE_CHECKING',\n    'TypeAlias',\n    'TypeGuard',\n    'Unpack',\n]\n\n# The pseudo-submodules 're' and 'io' are part of the public\n# namespace, but excluded from __all__ because they might stomp on\n# legitimate imports of those modules.\n\n\ndef _type_convert(arg, module=None, *, allow_special_forms=False):\n    \"\"\"For converting None to type(None), and strings to ForwardRef.\"\"\"\n    if arg is None:\n        return type(None)\n    if isinstance(arg, str):\n        return ForwardRef(arg, module=module, is_class=allow_special_forms)\n    return arg\n\n\ndef _type_check(arg, msg, is_argument=True, module=None, *, allow_special_forms=False):\n    \"\"\"Check that the argument is a type, and return it (internal helper).\n\n    As a special case, accept None and return type(None) instead. Also wrap strings\n    into ForwardRef instances. Consider several corner cases, for example plain\n    special forms like Union are not valid, while Union[int, str] is OK, etc.\n    The msg argument is a human-readable error message, e.g.::\n\n        \"Union[arg, ...]: arg should be a type.\"\n\n    We append the repr() of the actual value (truncated to 100 chars).\n    \"\"\"\n    invalid_generic_forms = (Generic, Protocol)\n    if not allow_special_forms:\n        invalid_generic_forms += (ClassVar,)\n        if is_argument:\n            invalid_generic_forms += (Final,)\n\n    arg = _type_convert(arg, module=module, allow_special_forms=allow_special_forms)\n    if (isinstance(arg, _GenericAlias) and\n            arg.__origin__ in invalid_generic_forms):\n        raise TypeError(f\"{arg} is not valid as type argument\")\n    if arg in (Any, LiteralString, NoReturn, Never, Self, TypeAlias):\n        return arg\n    if allow_special_forms and arg in (ClassVar, Final):\n        return arg\n    if isinstance(arg, _SpecialForm) or arg in (Generic, Protocol):\n        raise TypeError(f\"Plain {arg} is not valid as type argument\")\n    if type(arg) is tuple:\n        raise TypeError(f\"{msg} Got {arg!r:.100}.\")\n    return arg\n\n\ndef _is_param_expr(arg):\n    return arg is ... or isinstance(arg,\n            (tuple, list, ParamSpec, _ConcatenateGenericAlias))\n\n\ndef _should_unflatten_callable_args(typ, args):\n    \"\"\"Internal helper for munging collections.abc.Callable's __args__.\n\n    The canonical representation for a Callable's __args__ flattens the\n    argument types, see https://github.com/python/cpython/issues/86361.\n\n    For example::\n\n        >>> import collections.abc\n        >>> P = ParamSpec('P')\n        >>> collections.abc.Callable[[int, int], str].__args__ == (int, int, str)\n        True\n        >>> collections.abc.Callable[P, str].__args__ == (P, str)\n        True\n\n    As a result, if we need to reconstruct the Callable from its __args__,\n    we need to unflatten it.\n    \"\"\"\n    return (\n        typ.__origin__ is collections.abc.Callable\n        and not (len(args) == 2 and _is_param_expr(args[0]))\n    )\n\n\ndef _type_repr(obj):\n    \"\"\"Return the repr() of an object, special-casing types (internal helper).\n\n    If obj is a type, we return a shorter version than the default\n    type.__repr__, based on the module and qualified name, which is\n    typically enough to uniquely identify a type.  For everything\n    else, we fall back on repr(obj).\n    \"\"\"\n    if isinstance(obj, types.GenericAlias):\n        return repr(obj)\n    if isinstance(obj, type):\n        if obj.__module__ == 'builtins':\n            return obj.__qualname__\n        return f'{obj.__module__}.{obj.__qualname__}'\n    if obj is ...:\n        return('...')\n    if isinstance(obj, types.FunctionType):\n        return obj.__name__\n    return repr(obj)\n\n\ndef _collect_parameters(args):\n    \"\"\"Collect all type variables and parameter specifications in args\n    in order of first appearance (lexicographic order).\n\n    For example::\n\n        >>> P = ParamSpec('P')\n        >>> T = TypeVar('T')\n        >>> _collect_parameters((T, Callable[P, T]))\n        (~T, ~P)\n    \"\"\"\n    parameters = []\n    for t in args:\n        if isinstance(t, type):\n            # We don't want __parameters__ descriptor of a bare Python class.\n            pass\n        elif isinstance(t, tuple):\n            # `t` might be a tuple, when `ParamSpec` is substituted with\n            # `[T, int]`, or `[int, *Ts]`, etc.\n            for x in t:\n                for collected in _collect_parameters([x]):\n                    if collected not in parameters:\n                        parameters.append(collected)\n        elif hasattr(t, '__typing_subst__'):\n            if t not in parameters:\n                parameters.append(t)\n        else:\n            for x in getattr(t, '__parameters__', ()):\n                if x not in parameters:\n                    parameters.append(x)\n    return tuple(parameters)\n\n\ndef _check_generic(cls, parameters, elen):\n    \"\"\"Check correct count for parameters of a generic cls (internal helper).\n\n    This gives a nice error message in case of count mismatch.\n    \"\"\"\n    if not elen:\n        raise TypeError(f\"{cls} is not a generic class\")\n    alen = len(parameters)\n    if alen != elen:\n        raise TypeError(f\"Too {'many' if alen > elen else 'few'} arguments for {cls};\"\n                        f\" actual {alen}, expected {elen}\")\n\ndef _unpack_args(args):\n    newargs = []\n    for arg in args:\n        subargs = getattr(arg, '__typing_unpacked_tuple_args__', None)\n        if subargs is not None and not (subargs and subargs[-1] is ...):\n            newargs.extend(subargs)\n        else:\n            newargs.append(arg)\n    return newargs\n\ndef _deduplicate(params, *, unhashable_fallback=False):\n    # Weed out strict duplicates, preserving the first of each occurrence.\n    try:\n        return dict.fromkeys(params)\n    except TypeError:\n        if not unhashable_fallback:\n            raise\n        # Happens for cases like `Annotated[dict, {'x': IntValidator()}]`\n        return _deduplicate_unhashable(params)\n\ndef _deduplicate_unhashable(unhashable_params):\n    new_unhashable = []\n    for t in unhashable_params:\n        if t not in new_unhashable:\n            new_unhashable.append(t)\n    return new_unhashable\n\ndef _compare_args_orderless(first_args, second_args):\n    first_unhashable = _deduplicate_unhashable(first_args)\n    second_unhashable = _deduplicate_unhashable(second_args)\n    t = list(second_unhashable)\n    try:\n        for elem in first_unhashable:\n            t.remove(elem)\n    except ValueError:\n        return False\n    return not t\n\ndef _remove_dups_flatten(parameters):\n    \"\"\"Internal helper for Union creation and substitution.\n\n    Flatten Unions among parameters, then remove duplicates.\n    \"\"\"\n    # Flatten out Union[Union[...], ...].\n    params = []\n    for p in parameters:\n        if isinstance(p, (_UnionGenericAlias, types.UnionType)):\n            params.extend(p.__args__)\n        else:\n            params.append(p)\n\n    return tuple(_deduplicate(params, unhashable_fallback=True))\n\n\ndef _flatten_literal_params(parameters):\n    \"\"\"Internal helper for Literal creation: flatten Literals among parameters.\"\"\"\n    params = []\n    for p in parameters:\n        if isinstance(p, _LiteralGenericAlias):\n            params.extend(p.__args__)\n        else:\n            params.append(p)\n    return tuple(params)\n\n\n_cleanups = []\n\n\ndef _tp_cache(func=None, /, *, typed=False):\n    \"\"\"Internal wrapper caching __getitem__ of generic types.\n\n    For non-hashable arguments, the original function is used as a fallback.\n    \"\"\"\n    def decorator(func):\n        cached = functools.lru_cache(typed=typed)(func)\n        _cleanups.append(cached.cache_clear)\n\n        @functools.wraps(func)\n        def inner(*args, **kwds):\n            try:\n                return cached(*args, **kwds)\n            except TypeError:\n                pass  # All real errors (not unhashable args) are raised below.\n            return func(*args, **kwds)\n        return inner\n\n    if func is not None:\n        return decorator(func)\n\n    return decorator\n\ndef _eval_type(t, globalns, localns, recursive_guard=frozenset()):\n    \"\"\"Evaluate all forward references in the given type t.\n\n    For use of globalns and localns see the docstring for get_type_hints().\n    recursive_guard is used to prevent infinite recursion with a recursive\n    ForwardRef.\n    \"\"\"\n    if isinstance(t, ForwardRef):\n        return t._evaluate(globalns, localns, recursive_guard)\n    if isinstance(t, (_GenericAlias, GenericAlias, types.UnionType)):\n        if isinstance(t, GenericAlias):\n            args = tuple(\n                ForwardRef(arg) if isinstance(arg, str) else arg\n                for arg in t.__args__\n            )\n            is_unpacked = t.__unpacked__\n            if _should_unflatten_callable_args(t, args):\n                t = t.__origin__[(args[:-1], args[-1])]\n            else:\n                t = t.__origin__[args]\n            if is_unpacked:\n                t = Unpack[t]\n        ev_args = tuple(_eval_type(a, globalns, localns, recursive_guard) for a in t.__args__)\n        if ev_args == t.__args__:\n            return t\n        if isinstance(t, GenericAlias):\n            return GenericAlias(t.__origin__, ev_args)\n        if isinstance(t, types.UnionType):\n            return functools.reduce(operator.or_, ev_args)\n        else:\n            return t.copy_with(ev_args)\n    return t\n\n\nclass _Final:\n    \"\"\"Mixin to prohibit subclassing.\"\"\"\n\n    __slots__ = ('__weakref__',)\n\n    def __init_subclass__(cls, /, *args, **kwds):\n        if '_root' not in kwds:\n            raise TypeError(\"Cannot subclass special typing classes\")\n\nclass _Immutable:\n    \"\"\"Mixin to indicate that object should not be copied.\"\"\"\n\n    __slots__ = ()\n\n    def __copy__(self):\n        return self\n\n    def __deepcopy__(self, memo):\n        return self\n\n\nclass _NotIterable:\n    \"\"\"Mixin to prevent iteration, without being compatible with Iterable.\n\n    That is, we could do::\n\n        def __iter__(self): raise TypeError()\n\n    But this would make users of this mixin duck type-compatible with\n    collections.abc.Iterable - isinstance(foo, Iterable) would be True.\n\n    Luckily, we can instead prevent iteration by setting __iter__ to None, which\n    is treated specially.\n    \"\"\"\n\n    __slots__ = ()\n    __iter__ = None\n\n\n# Internal indicator of special typing constructs.\n# See __doc__ instance attribute for specific docs.\nclass _SpecialForm(_Final, _NotIterable, _root=True):\n    __slots__ = ('_name', '__doc__', '_getitem')\n\n    def __init__(self, getitem):\n        self._getitem = getitem\n        self._name = getitem.__name__\n        self.__doc__ = getitem.__doc__\n\n    def __getattr__(self, item):\n        if item in {'__name__', '__qualname__'}:\n            return self._name\n\n        raise AttributeError(item)\n\n    def __mro_entries__(self, bases):\n        raise TypeError(f\"Cannot subclass {self!r}\")\n\n    def __repr__(self):\n        return 'typing.' + self._name\n\n    def __reduce__(self):\n        return self._name\n\n    def __call__(self, *args, **kwds):\n        raise TypeError(f\"Cannot instantiate {self!r}\")\n\n    def __or__(self, other):\n        return Union[self, other]\n\n    def __ror__(self, other):\n        return Union[other, self]\n\n    def __instancecheck__(self, obj):\n        raise TypeError(f\"{self} cannot be used with isinstance()\")\n\n    def __subclasscheck__(self, cls):\n        raise TypeError(f\"{self} cannot be used with issubclass()\")\n\n    @_tp_cache\n    def __getitem__(self, parameters):\n        return self._getitem(self, parameters)\n\n\nclass _LiteralSpecialForm(_SpecialForm, _root=True):\n    def __getitem__(self, parameters):\n        if not isinstance(parameters, tuple):\n            parameters = (parameters,)\n        return self._getitem(self, *parameters)\n\n\nclass _AnyMeta(type):\n    def __instancecheck__(self, obj):\n        if self is Any:\n            raise TypeError(\"typing.Any cannot be used with isinstance()\")\n        return super().__instancecheck__(obj)\n\n    def __repr__(self):\n        if self is Any:\n            return \"typing.Any\"\n        return super().__repr__()  # respect to subclasses\n\n\nclass Any(metaclass=_AnyMeta):\n    \"\"\"Special type indicating an unconstrained type.\n\n    - Any is compatible with every type.\n    - Any assumed to have all methods.\n    - All values assumed to be instances of Any.\n\n    Note that all the above statements are true from the point of view of\n    static type checkers. At runtime, Any should not be used with instance\n    checks.\n    \"\"\"\n\n    def __new__(cls, *args, **kwargs):\n        if cls is Any:\n            raise TypeError(\"Any cannot be instantiated\")\n        return super().__new__(cls)\n\n\n@_SpecialForm\ndef NoReturn(self, parameters):\n    \"\"\"Special type indicating functions that never return.\n\n    Example::\n\n        from typing import NoReturn\n\n        def stop() -> NoReturn:\n            raise Exception('no way')\n\n    NoReturn can also be used as a bottom type, a type that\n    has no values. Starting in Python 3.11, the Never type should\n    be used for this concept instead. Type checkers should treat the two\n    equivalently.\n    \"\"\"\n    raise TypeError(f\"{self} is not subscriptable\")\n\n# This is semantically identical to NoReturn, but it is implemented\n# separately so that type checkers can distinguish between the two\n# if they want.\n@_SpecialForm\ndef Never(self, parameters):\n    \"\"\"The bottom type, a type that has no members.\n\n    This can be used to define a function that should never be\n    called, or a function that never returns::\n\n        from typing import Never\n\n        def never_call_me(arg: Never) -> None:\n            pass\n\n        def int_or_str(arg: int | str) -> None:\n            never_call_me(arg)  # type checker error\n            match arg:\n                case int():\n                    print(\"It's an int\")\n                case str():\n                    print(\"It's a str\")\n                case _:\n                    never_call_me(arg)  # OK, arg is of type Never\n    \"\"\"\n    raise TypeError(f\"{self} is not subscriptable\")\n\n\n@_SpecialForm\ndef Self(self, parameters):\n    \"\"\"Used to spell the type of \"self\" in classes.\n\n    Example::\n\n        from typing import Self\n\n        class Foo:\n            def return_self(self) -> Self:\n                ...\n                return self\n\n    This is especially useful for:\n        - classmethods that are used as alternative constructors\n        - annotating an `__enter__` method which returns self\n    \"\"\"\n    raise TypeError(f\"{self} is not subscriptable\")\n\n\n@_SpecialForm\ndef LiteralString(self, parameters):\n    \"\"\"Represents an arbitrary literal string.\n\n    Example::\n\n        from typing import LiteralString\n\n        def run_query(sql: LiteralString) -> None:\n            ...\n\n        def caller(arbitrary_string: str, literal_string: LiteralString) -> None:\n            run_query(\"SELECT * FROM students\")  # OK\n            run_query(literal_string)  # OK\n            run_query(\"SELECT * FROM \" + literal_string)  # OK\n            run_query(arbitrary_string)  # type checker error\n            run_query(  # type checker error\n                f\"SELECT * FROM students WHERE name = {arbitrary_string}\"\n            )\n\n    Only string literals and other LiteralStrings are compatible\n    with LiteralString. This provides a tool to help prevent\n    security issues such as SQL injection.\n    \"\"\"\n    raise TypeError(f\"{self} is not subscriptable\")\n\n\n@_SpecialForm\ndef ClassVar(self, parameters):\n    \"\"\"Special type construct to mark class variables.\n\n    An annotation wrapped in ClassVar indicates that a given\n    attribute is intended to be used as a class variable and\n    should not be set on instances of that class.\n\n    Usage::\n\n        class Starship:\n            stats: ClassVar[dict[str, int]] = {} # class variable\n            damage: int = 10                     # instance variable\n\n    ClassVar accepts only types and cannot be further subscribed.\n\n    Note that ClassVar is not a class itself, and should not\n    be used with isinstance() or issubclass().\n    \"\"\"\n    item = _type_check(parameters, f'{self} accepts only single type.')\n    return _GenericAlias(self, (item,))\n\n@_SpecialForm\ndef Final(self, parameters):\n    \"\"\"Special typing construct to indicate final names to type checkers.\n\n    A final name cannot be re-assigned or overridden in a subclass.\n\n    For example::\n\n        MAX_SIZE: Final = 9000\n        MAX_SIZE += 1  # Error reported by type checker\n\n        class Connection:\n            TIMEOUT: Final[int] = 10\n\n        class FastConnector(Connection):\n            TIMEOUT = 1  # Error reported by type checker\n\n    There is no runtime checking of these properties.\n    \"\"\"\n    item = _type_check(parameters, f'{self} accepts only single type.')\n    return _GenericAlias(self, (item,))\n\n@_SpecialForm\ndef Union(self, parameters):\n    \"\"\"Union type; Union[X, Y] means either X or Y.\n\n    On Python 3.10 and higher, the | operator\n    can also be used to denote unions;\n    X | Y means the same thing to the type checker as Union[X, Y].\n\n    To define a union, use e.g. Union[int, str]. Details:\n    - The arguments must be types and there must be at least one.\n    - None as an argument is a special case and is replaced by\n      type(None).\n    - Unions of unions are flattened, e.g.::\n\n        assert Union[Union[int, str], float] == Union[int, str, float]\n\n    - Unions of a single argument vanish, e.g.::\n\n        assert Union[int] == int  # The constructor actually returns int\n\n    - Redundant arguments are skipped, e.g.::\n\n        assert Union[int, str, int] == Union[int, str]\n\n    - When comparing unions, the argument order is ignored, e.g.::\n\n        assert Union[int, str] == Union[str, int]\n\n    - You cannot subclass or instantiate a union.\n    - You can use Optional[X] as a shorthand for Union[X, None].\n    \"\"\"\n    if parameters == ():\n        raise TypeError(\"Cannot take a Union of no types.\")\n    if not isinstance(parameters, tuple):\n        parameters = (parameters,)\n    msg = \"Union[arg, ...]: each arg must be a type.\"\n    parameters = tuple(_type_check(p, msg) for p in parameters)\n    parameters = _remove_dups_flatten(parameters)\n    if len(parameters) == 1:\n        return parameters[0]\n    if len(parameters) == 2 and type(None) in parameters:\n        return _UnionGenericAlias(self, parameters, name=\"Optional\")\n    return _UnionGenericAlias(self, parameters)\n\n@_SpecialForm\ndef Optional(self, parameters):\n    \"\"\"Optional[X] is equivalent to Union[X, None].\"\"\"\n    arg = _type_check(parameters, f\"{self} requires a single type.\")\n    return Union[arg, type(None)]\n\n@_LiteralSpecialForm\n@_tp_cache(typed=True)\ndef Literal(self, *parameters):\n    \"\"\"Special typing form to define literal types (a.k.a. value types).\n\n    This form can be used to indicate to type checkers that the corresponding\n    variable or function parameter has a value equivalent to the provided\n    literal (or one of several literals)::\n\n        def validate_simple(data: Any) -> Literal[True]:  # always returns True\n            ...\n\n        MODE = Literal['r', 'rb', 'w', 'wb']\n        def open_helper(file: str, mode: MODE) -> str:\n            ...\n\n        open_helper('/some/path', 'r')  # Passes type check\n        open_helper('/other/path', 'typo')  # Error in type checker\n\n    Literal[...] cannot be subclassed. At runtime, an arbitrary value\n    is allowed as type argument to Literal[...], but type checkers may\n    impose restrictions.\n    \"\"\"\n    # There is no '_type_check' call because arguments to Literal[...] are\n    # values, not types.\n    parameters = _flatten_literal_params(parameters)\n\n    try:\n        parameters = tuple(p for p, _ in _deduplicate(list(_value_and_type_iter(parameters))))\n    except TypeError:  # unhashable parameters\n        pass\n\n    return _LiteralGenericAlias(self, parameters)\n\n\n@_SpecialForm\ndef TypeAlias(self, parameters):\n    \"\"\"Special form for marking type aliases.\n\n    Use TypeAlias to indicate that an assignment should\n    be recognized as a proper type alias definition by type\n    checkers.\n\n    For example::\n\n        Predicate: TypeAlias = Callable[..., bool]\n\n    It's invalid when used anywhere except as in the example above.\n    \"\"\"\n    raise TypeError(f\"{self} is not subscriptable\")\n\n\n@_SpecialForm\ndef Concatenate(self, parameters):\n    \"\"\"Special form for annotating higher-order functions.\n\n    ``Concatenate`` can be used in conjunction with ``ParamSpec`` and\n    ``Callable`` to represent a higher-order function which adds, removes or\n    transforms the parameters of a callable.\n\n    For example::\n\n        Callable[Concatenate[int, P], int]\n\n    See PEP 612 for detailed information.\n    \"\"\"\n    if parameters == ():\n        raise TypeError(\"Cannot take a Concatenate of no types.\")\n    if not isinstance(parameters, tuple):\n        parameters = (parameters,)\n    if not (parameters[-1] is ... or isinstance(parameters[-1], ParamSpec)):\n        raise TypeError(\"The last parameter to Concatenate should be a \"\n                        \"ParamSpec variable or ellipsis.\")\n    msg = \"Concatenate[arg, ...]: each arg must be a type.\"\n    parameters = (*(_type_check(p, msg) for p in parameters[:-1]), parameters[-1])\n    return _ConcatenateGenericAlias(self, parameters,\n                                    _paramspec_tvars=True)\n\n\n@_SpecialForm\ndef TypeGuard(self, parameters):\n    \"\"\"Special typing construct for marking user-defined type guard functions.\n\n    ``TypeGuard`` can be used to annotate the return type of a user-defined\n    type guard function.  ``TypeGuard`` only accepts a single type argument.\n    At runtime, functions marked this way should return a boolean.\n\n    ``TypeGuard`` aims to benefit *type narrowing* -- a technique used by static\n    type checkers to determine a more precise type of an expression within a\n    program's code flow.  Usually type narrowing is done by analyzing\n    conditional code flow and applying the narrowing to a block of code.  The\n    conditional expression here is sometimes referred to as a \"type guard\".\n\n    Sometimes it would be convenient to use a user-defined boolean function\n    as a type guard.  Such a function should use ``TypeGuard[...]`` as its\n    return type to alert static type checkers to this intention.\n\n    Using  ``-> TypeGuard`` tells the static type checker that for a given\n    function:\n\n    1. The return value is a boolean.\n    2. If the return value is ``True``, the type of its argument\n       is the type inside ``TypeGuard``.\n\n       For example::\n\n           def is_str(val: Union[str, float]):\n               # \"isinstance\" type guard\n               if isinstance(val, str):\n                   # Type of ``val`` is narrowed to ``str``\n                   ...\n               else:\n                   # Else, type of ``val`` is narrowed to ``float``.\n                   ...\n\n    Strict type narrowing is not enforced -- ``TypeB`` need not be a narrower\n    form of ``TypeA`` (it can even be a wider form) and this may lead to\n    type-unsafe results.  The main reason is to allow for things like\n    narrowing ``List[object]`` to ``List[str]`` even though the latter is not\n    a subtype of the former, since ``List`` is invariant.  The responsibility of\n    writing type-safe type guards is left to the user.\n\n    ``TypeGuard`` also works with type variables.  For more information, see\n    PEP 647 (User-Defined Type Guards).\n    \"\"\"\n    item = _type_check(parameters, f'{self} accepts only single type.')\n    return _GenericAlias(self, (item,))\n\n\nclass ForwardRef(_Final, _root=True):\n    \"\"\"Internal wrapper to hold a forward reference.\"\"\"\n\n    __slots__ = ('__forward_arg__', '__forward_code__',\n                 '__forward_evaluated__', '__forward_value__',\n                 '__forward_is_argument__', '__forward_is_class__',\n                 '__forward_module__')\n\n    def __init__(self, arg, is_argument=True, module=None, *, is_class=False):\n        if not isinstance(arg, str):\n            raise TypeError(f\"Forward reference must be a string -- got {arg!r}\")\n\n        # If we do `def f(*args: *Ts)`, then we'll have `arg = '*Ts'`.\n        # Unfortunately, this isn't a valid expression on its own, so we\n        # do the unpacking manually.\n        if arg.startswith('*'):\n            arg_to_compile = f'({arg},)[0]'  # E.g. (*Ts,)[0] or (*tuple[int, int],)[0]\n        else:\n            arg_to_compile = arg\n        try:\n            code = compile(arg_to_compile, '<string>', 'eval')\n        except SyntaxError:\n            raise SyntaxError(f\"Forward reference must be an expression -- got {arg!r}\")\n\n        self.__forward_arg__ = arg\n        self.__forward_code__ = code\n        self.__forward_evaluated__ = False\n        self.__forward_value__ = None\n        self.__forward_is_argument__ = is_argument\n        self.__forward_is_class__ = is_class\n        self.__forward_module__ = module\n\n    def _evaluate(self, globalns, localns, recursive_guard):\n        if self.__forward_arg__ in recursive_guard:\n            return self\n        if not self.__forward_evaluated__ or localns is not globalns:\n            if globalns is None and localns is None:\n                globalns = localns = {}\n            elif globalns is None:\n                globalns = localns\n            elif localns is None:\n                localns = globalns\n            if self.__forward_module__ is not None:\n                globalns = getattr(\n                    sys.modules.get(self.__forward_module__, None), '__dict__', globalns\n                )\n            type_ = _type_check(\n                eval(self.__forward_code__, globalns, localns),\n                \"Forward references must evaluate to types.\",\n                is_argument=self.__forward_is_argument__,\n                allow_special_forms=self.__forward_is_class__,\n            )\n            self.__forward_value__ = _eval_type(\n                type_, globalns, localns, recursive_guard | {self.__forward_arg__}\n            )\n            self.__forward_evaluated__ = True\n        return self.__forward_value__\n\n    def __eq__(self, other):\n        if not isinstance(other, ForwardRef):\n            return NotImplemented\n        if self.__forward_evaluated__ and other.__forward_evaluated__:\n            return (self.__forward_arg__ == other.__forward_arg__ and\n                    self.__forward_value__ == other.__forward_value__)\n        return (self.__forward_arg__ == other.__forward_arg__ and\n                self.__forward_module__ == other.__forward_module__)\n\n    def __hash__(self):\n        return hash((self.__forward_arg__, self.__forward_module__))\n\n    def __or__(self, other):\n        return Union[self, other]\n\n    def __ror__(self, other):\n        return Union[other, self]\n\n    def __repr__(self):\n        if self.__forward_module__ is None:\n            module_repr = ''\n        else:\n            module_repr = f', module={self.__forward_module__!r}'\n        return f'ForwardRef({self.__forward_arg__!r}{module_repr})'\n\n\ndef _is_unpacked_typevartuple(x: Any) -> bool:\n    return ((not isinstance(x, type)) and\n            getattr(x, '__typing_is_unpacked_typevartuple__', False))\n\n\ndef _is_typevar_like(x: Any) -> bool:\n    return isinstance(x, (TypeVar, ParamSpec)) or _is_unpacked_typevartuple(x)\n\n\nclass _PickleUsingNameMixin:\n    \"\"\"Mixin enabling pickling based on self.__name__.\"\"\"\n\n    def __reduce__(self):\n        return self.__name__\n\n\nclass _BoundVarianceMixin:\n    \"\"\"Mixin giving __init__ bound and variance arguments.\n\n    This is used by TypeVar and ParamSpec, which both employ the notions of\n    a type 'bound' (restricting type arguments to be a subtype of some\n    specified type) and type 'variance' (determining subtype relations between\n    generic types).\n    \"\"\"\n    def __init__(self, bound, covariant, contravariant):\n        \"\"\"Used to setup TypeVars and ParamSpec's bound, covariant and\n        contravariant attributes.\n        \"\"\"\n        if covariant and contravariant:\n            raise ValueError(\"Bivariant types are not supported.\")\n        self.__covariant__ = bool(covariant)\n        self.__contravariant__ = bool(contravariant)\n        if bound:\n            self.__bound__ = _type_check(bound, \"Bound must be a type.\")\n        else:\n            self.__bound__ = None\n\n    def __or__(self, right):\n        return Union[self, right]\n\n    def __ror__(self, left):\n        return Union[left, self]\n\n    def __repr__(self):\n        if self.__covariant__:\n            prefix = '+'\n        elif self.__contravariant__:\n            prefix = '-'\n        else:\n            prefix = '~'\n        return prefix + self.__name__\n\n\nclass TypeVar(_Final, _Immutable, _BoundVarianceMixin, _PickleUsingNameMixin,\n              _root=True):\n    \"\"\"Type variable.\n\n    Usage::\n\n      T = TypeVar('T')  # Can be anything\n      A = TypeVar('A', str, bytes)  # Must be str or bytes\n\n    Type variables exist primarily for the benefit of static type\n    checkers.  They serve as the parameters for generic types as well\n    as for generic function definitions.  See class Generic for more\n    information on generic types.  Generic functions work as follows:\n\n      def repeat(x: T, n: int) -> List[T]:\n          '''Return a list containing n references to x.'''\n          return [x]*n\n\n      def longest(x: A, y: A) -> A:\n          '''Return the longest of two strings.'''\n          return x if len(x) >= len(y) else y\n\n    The latter example's signature is essentially the overloading\n    of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n    that if the arguments are instances of some subclass of str,\n    the return type is still plain str.\n\n    At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n\n    Type variables defined with covariant=True or contravariant=True\n    can be used to declare covariant or contravariant generic types.\n    See PEP 484 for more details. By default generic types are invariant\n    in all type variables.\n\n    Type variables can be introspected. e.g.:\n\n      T.__name__ == 'T'\n      T.__constraints__ == ()\n      T.__covariant__ == False\n      T.__contravariant__ = False\n      A.__constraints__ == (str, bytes)\n\n    Note that only type variables defined in global scope can be pickled.\n    \"\"\"\n\n    def __init__(self, name, *constraints, bound=None,\n                 covariant=False, contravariant=False):\n        self.__name__ = name\n        super().__init__(bound, covariant, contravariant)\n        if constraints and bound is not None:\n            raise TypeError(\"Constraints cannot be combined with bound=...\")\n        if constraints and len(constraints) == 1:\n            raise TypeError(\"A single constraint is not allowed\")\n        msg = \"TypeVar(name, constraint, ...): constraints must be types.\"\n        self.__constraints__ = tuple(_type_check(t, msg) for t in constraints)\n        def_mod = _caller()\n        if def_mod != 'typing':\n            self.__module__ = def_mod\n\n    def __typing_subst__(self, arg):\n        msg = \"Parameters to generic types must be types.\"\n        arg = _type_check(arg, msg, is_argument=True)\n        if ((isinstance(arg, _GenericAlias) and arg.__origin__ is Unpack) or\n            (isinstance(arg, GenericAlias) and getattr(arg, '__unpacked__', False))):\n            raise TypeError(f\"{arg} is not valid as type argument\")\n        return arg\n\n\nclass TypeVarTuple(_Final, _Immutable, _PickleUsingNameMixin, _root=True):\n    \"\"\"Type variable tuple.\n\n    Usage:\n\n      Ts = TypeVarTuple('Ts')  # Can be given any name\n\n    Just as a TypeVar (type variable) is a placeholder for a single type,\n    a TypeVarTuple is a placeholder for an *arbitrary* number of types. For\n    example, if we define a generic class using a TypeVarTuple:\n\n      class C(Generic[*Ts]): ...\n\n    Then we can parameterize that class with an arbitrary number of type\n    arguments:\n\n      C[int]       # Fine\n      C[int, str]  # Also fine\n      C[()]        # Even this is fine\n\n    For more details, see PEP 646.\n\n    Note that only TypeVarTuples defined in global scope can be pickled.\n    \"\"\"\n\n    def __init__(self, name):\n        self.__name__ = name\n\n        # Used for pickling.\n        def_mod = _caller()\n        if def_mod != 'typing':\n            self.__module__ = def_mod\n\n    def __iter__(self):\n        yield Unpack[self]\n\n    def __repr__(self):\n        return self.__name__\n\n    def __typing_subst__(self, arg):\n        raise TypeError(\"Substitution of bare TypeVarTuple is not supported\")\n\n    def __typing_prepare_subst__(self, alias, args):\n        params = alias.__parameters__\n        typevartuple_index = params.index(self)\n        for param in params[typevartuple_index + 1:]:\n            if isinstance(param, TypeVarTuple):\n                raise TypeError(f\"More than one TypeVarTuple parameter in {alias}\")\n\n        alen = len(args)\n        plen = len(params)\n        left = typevartuple_index\n        right = plen - typevartuple_index - 1\n        var_tuple_index = None\n        fillarg = None\n        for k, arg in enumerate(args):\n            if not isinstance(arg, type):\n                subargs = getattr(arg, '__typing_unpacked_tuple_args__', None)\n                if subargs and len(subargs) == 2 and subargs[-1] is ...:\n                    if var_tuple_index is not None:\n                        raise TypeError(\"More than one unpacked arbitrary-length tuple argument\")\n                    var_tuple_index = k\n                    fillarg = subargs[0]\n        if var_tuple_index is not None:\n            left = min(left, var_tuple_index)\n            right = min(right, alen - var_tuple_index - 1)\n        elif left + right > alen:\n            raise TypeError(f\"Too few arguments for {alias};\"\n                            f\" actual {alen}, expected at least {plen-1}\")\n\n        return (\n            *args[:left],\n            *([fillarg]*(typevartuple_index - left)),\n            tuple(args[left: alen - right]),\n            *([fillarg]*(plen - right - left - typevartuple_index - 1)),\n            *args[alen - right:],\n        )\n\n\nclass ParamSpecArgs(_Final, _Immutable, _root=True):\n    \"\"\"The args for a ParamSpec object.\n\n    Given a ParamSpec object P, P.args is an instance of ParamSpecArgs.\n\n    ParamSpecArgs objects have a reference back to their ParamSpec:\n\n       P.args.__origin__ is P\n\n    This type is meant for runtime introspection and has no special meaning to\n    static type checkers.\n    \"\"\"\n    def __init__(self, origin):\n        self.__origin__ = origin\n\n    def __repr__(self):\n        return f\"{self.__origin__.__name__}.args\"\n\n    def __eq__(self, other):\n        if not isinstance(other, ParamSpecArgs):\n            return NotImplemented\n        return self.__origin__ == other.__origin__\n\n\nclass ParamSpecKwargs(_Final, _Immutable, _root=True):\n    \"\"\"The kwargs for a ParamSpec object.\n\n    Given a ParamSpec object P, P.kwargs is an instance of ParamSpecKwargs.\n\n    ParamSpecKwargs objects have a reference back to their ParamSpec:\n\n       P.kwargs.__origin__ is P\n\n    This type is meant for runtime introspection and has no special meaning to\n    static type checkers.\n    \"\"\"\n    def __init__(self, origin):\n        self.__origin__ = origin\n\n    def __repr__(self):\n        return f\"{self.__origin__.__name__}.kwargs\"\n\n    def __eq__(self, other):\n        if not isinstance(other, ParamSpecKwargs):\n            return NotImplemented\n        return self.__origin__ == other.__origin__\n\n\nclass ParamSpec(_Final, _Immutable, _BoundVarianceMixin, _PickleUsingNameMixin,\n                _root=True):\n    \"\"\"Parameter specification variable.\n\n    Usage::\n\n       P = ParamSpec('P')\n\n    Parameter specification variables exist primarily for the benefit of static\n    type checkers.  They are used to forward the parameter types of one\n    callable to another callable, a pattern commonly found in higher order\n    functions and decorators.  They are only valid when used in ``Concatenate``,\n    or as the first argument to ``Callable``, or as parameters for user-defined\n    Generics.  See class Generic for more information on generic types.  An\n    example for annotating a decorator::\n\n       T = TypeVar('T')\n       P = ParamSpec('P')\n\n       def add_logging(f: Callable[P, T]) -> Callable[P, T]:\n           '''A type-safe decorator to add logging to a function.'''\n           def inner(*args: P.args, **kwargs: P.kwargs) -> T:\n               logging.info(f'{f.__name__} was called')\n               return f(*args, **kwargs)\n           return inner\n\n       @add_logging\n       def add_two(x: float, y: float) -> float:\n           '''Add two numbers together.'''\n           return x + y\n\n    Parameter specification variables can be introspected. e.g.:\n\n       P.__name__ == 'P'\n\n    Note that only parameter specification variables defined in global scope can\n    be pickled.\n    \"\"\"\n\n    @property\n    def args(self):\n        return ParamSpecArgs(self)\n\n    @property\n    def kwargs(self):\n        return ParamSpecKwargs(self)\n\n    def __init__(self, name, *, bound=None, covariant=False, contravariant=False):\n        self.__name__ = name\n        super().__init__(bound, covariant, contravariant)\n        def_mod = _caller()\n        if def_mod != 'typing':\n            self.__module__ = def_mod\n\n    def __typing_subst__(self, arg):\n        if isinstance(arg, (list, tuple)):\n            arg = tuple(_type_check(a, \"Expected a type.\") for a in arg)\n        elif not _is_param_expr(arg):\n            raise TypeError(f\"Expected a list of types, an ellipsis, \"\n                            f\"ParamSpec, or Concatenate. Got {arg}\")\n        return arg\n\n    def __typing_prepare_subst__(self, alias, args):\n        params = alias.__parameters__\n        i = params.index(self)\n        if i >= len(args):\n            raise TypeError(f\"Too few arguments for {alias}\")\n        # Special case where Z[[int, str, bool]] == Z[int, str, bool] in PEP 612.\n        if len(params) == 1 and not _is_param_expr(args[0]):\n            assert i == 0\n            args = (args,)\n        # Convert lists to tuples to help other libraries cache the results.\n        elif isinstance(args[i], list):\n            args = (*args[:i], tuple(args[i]), *args[i+1:])\n        return args\n\ndef _is_dunder(attr):\n    return attr.startswith('__') and attr.endswith('__')\n\nclass _BaseGenericAlias(_Final, _root=True):\n    \"\"\"The central part of the internal API.\n\n    This represents a generic version of type 'origin' with type arguments 'params'.\n    There are two kind of these aliases: user defined and special. The special ones\n    are wrappers around builtin collections and ABCs in collections.abc. These must\n    have 'name' always set. If 'inst' is False, then the alias can't be instantiated;\n    this is used by e.g. typing.List and typing.Dict.\n    \"\"\"\n\n    def __init__(self, origin, *, inst=True, name=None):\n        self._inst = inst\n        self._name = name\n        self.__origin__ = origin\n        self.__slots__ = None  # This is not documented.\n\n    def __call__(self, *args, **kwargs):\n        if not self._inst:\n            raise TypeError(f\"Type {self._name} cannot be instantiated; \"\n                            f\"use {self.__origin__.__name__}() instead\")\n        result = self.__origin__(*args, **kwargs)\n        try:\n            result.__orig_class__ = self\n        # Some objects raise TypeError (or something even more exotic)\n        # if you try to set attributes on them; we guard against that here\n        except Exception:\n            pass\n        return result\n\n    def __mro_entries__(self, bases):\n        res = []\n        if self.__origin__ not in bases:\n            res.append(self.__origin__)\n        i = bases.index(self)\n        for b in bases[i+1:]:\n            if isinstance(b, _BaseGenericAlias) or issubclass(b, Generic):\n                break\n        else:\n            res.append(Generic)\n        return tuple(res)\n\n    def __getattr__(self, attr):\n        if attr in {'__name__', '__qualname__'}:\n            return self._name or self.__origin__.__name__\n\n        # We are careful for copy and pickle.\n        # Also for simplicity we don't relay any dunder names\n        if '__origin__' in self.__dict__ and not _is_dunder(attr):\n            return getattr(self.__origin__, attr)\n        raise AttributeError(attr)\n\n    def __setattr__(self, attr, val):\n        if _is_dunder(attr) or attr in {'_name', '_inst', '_nparams',\n                                        '_paramspec_tvars'}:\n            super().__setattr__(attr, val)\n        else:\n            setattr(self.__origin__, attr, val)\n\n    def __instancecheck__(self, obj):\n        return self.__subclasscheck__(type(obj))\n\n    def __subclasscheck__(self, cls):\n        raise TypeError(\"Subscripted generics cannot be used with\"\n                        \" class and instance checks\")\n\n    def __dir__(self):\n        return list(set(super().__dir__()\n                + [attr for attr in dir(self.__origin__) if not _is_dunder(attr)]))\n\n\n# Special typing constructs Union, Optional, Generic, Callable and Tuple\n# use three special attributes for internal bookkeeping of generic types:\n# * __parameters__ is a tuple of unique free type parameters of a generic\n#   type, for example, Dict[T, T].__parameters__ == (T,);\n# * __origin__ keeps a reference to a type that was subscripted,\n#   e.g., Union[T, int].__origin__ == Union, or the non-generic version of\n#   the type.\n# * __args__ is a tuple of all arguments used in subscripting,\n#   e.g., Dict[T, int].__args__ == (T, int).\n\n\nclass _GenericAlias(_BaseGenericAlias, _root=True):\n    # The type of parameterized generics.\n    #\n    # That is, for example, `type(List[int])` is `_GenericAlias`.\n    #\n    # Objects which are instances of this class include:\n    # * Parameterized container types, e.g. `Tuple[int]`, `List[int]`.\n    #  * Note that native container types, e.g. `tuple`, `list`, use\n    #    `types.GenericAlias` instead.\n    # * Parameterized classes:\n    #     T = TypeVar('T')\n    #     class C(Generic[T]): pass\n    #     # C[int] is a _GenericAlias\n    # * `Callable` aliases, generic `Callable` aliases, and\n    #   parameterized `Callable` aliases:\n    #     T = TypeVar('T')\n    #     # _CallableGenericAlias inherits from _GenericAlias.\n    #     A = Callable[[], None]  # _CallableGenericAlias\n    #     B = Callable[[T], None]  # _CallableGenericAlias\n    #     C = B[int]  # _CallableGenericAlias\n    # * Parameterized `Final`, `ClassVar` and `TypeGuard`:\n    #     # All _GenericAlias\n    #     Final[int]\n    #     ClassVar[float]\n    #     TypeVar[bool]\n\n    def __init__(self, origin, args, *, inst=True, name=None,\n                 _paramspec_tvars=False):\n        super().__init__(origin, inst=inst, name=name)\n        if not isinstance(args, tuple):\n            args = (args,)\n        self.__args__ = tuple(... if a is _TypingEllipsis else\n                              a for a in args)\n        self.__parameters__ = _collect_parameters(args)\n        self._paramspec_tvars = _paramspec_tvars\n        if not name:\n            self.__module__ = origin.__module__\n\n    def __eq__(self, other):\n        if not isinstance(other, _GenericAlias):\n            return NotImplemented\n        return (self.__origin__ == other.__origin__\n                and self.__args__ == other.__args__)\n\n    def __hash__(self):\n        return hash((self.__origin__, self.__args__))\n\n    def __or__(self, right):\n        return Union[self, right]\n\n    def __ror__(self, left):\n        return Union[left, self]\n\n    @_tp_cache\n    def __getitem__(self, args):\n        # Parameterizes an already-parameterized object.\n        #\n        # For example, we arrive here doing something like:\n        #   T1 = TypeVar('T1')\n        #   T2 = TypeVar('T2')\n        #   T3 = TypeVar('T3')\n        #   class A(Generic[T1]): pass\n        #   B = A[T2]  # B is a _GenericAlias\n        #   C = B[T3]  # Invokes _GenericAlias.__getitem__\n        #\n        # We also arrive here when parameterizing a generic `Callable` alias:\n        #   T = TypeVar('T')\n        #   C = Callable[[T], None]\n        #   C[int]  # Invokes _GenericAlias.__getitem__\n\n        if self.__origin__ in (Generic, Protocol):\n            # Can't subscript Generic[...] or Protocol[...].\n            raise TypeError(f\"Cannot subscript already-subscripted {self}\")\n        if not self.__parameters__:\n            raise TypeError(f\"{self} is not a generic class\")\n\n        # Preprocess `args`.\n        if not isinstance(args, tuple):\n            args = (args,)\n        args = tuple(_type_convert(p) for p in args)\n        args = _unpack_args(args)\n        new_args = self._determine_new_args(args)\n        r = self.copy_with(new_args)\n        return r\n\n    def _determine_new_args(self, args):\n        # Determines new __args__ for __getitem__.\n        #\n        # For example, suppose we had:\n        #   T1 = TypeVar('T1')\n        #   T2 = TypeVar('T2')\n        #   class A(Generic[T1, T2]): pass\n        #   T3 = TypeVar('T3')\n        #   B = A[int, T3]\n        #   C = B[str]\n        # `B.__args__` is `(int, T3)`, so `C.__args__` should be `(int, str)`.\n        # Unfortunately, this is harder than it looks, because if `T3` is\n        # anything more exotic than a plain `TypeVar`, we need to consider\n        # edge cases.\n\n        params = self.__parameters__\n        # In the example above, this would be {T3: str}\n        for param in params:\n            prepare = getattr(param, '__typing_prepare_subst__', None)\n            if prepare is not None:\n                args = prepare(self, args)\n        alen = len(args)\n        plen = len(params)\n        if alen != plen:\n            raise TypeError(f\"Too {'many' if alen > plen else 'few'} arguments for {self};\"\n                            f\" actual {alen}, expected {plen}\")\n        new_arg_by_param = dict(zip(params, args))\n        return tuple(self._make_substitution(self.__args__, new_arg_by_param))\n\n    def _make_substitution(self, args, new_arg_by_param):\n        \"\"\"Create a list of new type arguments.\"\"\"\n        new_args = []\n        for old_arg in args:\n            if isinstance(old_arg, type):\n                new_args.append(old_arg)\n                continue\n\n            substfunc = getattr(old_arg, '__typing_subst__', None)\n            if substfunc:\n                new_arg = substfunc(new_arg_by_param[old_arg])\n            else:\n                subparams = getattr(old_arg, '__parameters__', ())\n                if not subparams:\n                    new_arg = old_arg\n                else:\n                    subargs = []\n                    for x in subparams:\n                        if isinstance(x, TypeVarTuple):\n                            subargs.extend(new_arg_by_param[x])\n                        else:\n                            subargs.append(new_arg_by_param[x])\n                    new_arg = old_arg[tuple(subargs)]\n\n            if self.__origin__ == collections.abc.Callable and isinstance(new_arg, tuple):\n                # Consider the following `Callable`.\n                #   C = Callable[[int], str]\n                # Here, `C.__args__` should be (int, str) - NOT ([int], str).\n                # That means that if we had something like...\n                #   P = ParamSpec('P')\n                #   T = TypeVar('T')\n                #   C = Callable[P, T]\n                #   D = C[[int, str], float]\n                # ...we need to be careful; `new_args` should end up as\n                # `(int, str, float)` rather than `([int, str], float)`.\n                new_args.extend(new_arg)\n            elif _is_unpacked_typevartuple(old_arg):\n                # Consider the following `_GenericAlias`, `B`:\n                #   class A(Generic[*Ts]): ...\n                #   B = A[T, *Ts]\n                # If we then do:\n                #   B[float, int, str]\n                # The `new_arg` corresponding to `T` will be `float`, and the\n                # `new_arg` corresponding to `*Ts` will be `(int, str)`. We\n                # should join all these types together in a flat list\n                # `(float, int, str)` - so again, we should `extend`.\n                new_args.extend(new_arg)\n            elif isinstance(old_arg, tuple):\n                # Corner case:\n                #    P = ParamSpec('P')\n                #    T = TypeVar('T')\n                #    class Base(Generic[P]): ...\n                # Can be substituted like this:\n                #    X = Base[[int, T]]\n                # In this case, `old_arg` will be a tuple:\n                new_args.append(\n                    tuple(self._make_substitution(old_arg, new_arg_by_param)),\n                )\n            else:\n                new_args.append(new_arg)\n        return new_args\n\n    def copy_with(self, args):\n        return self.__class__(self.__origin__, args, name=self._name, inst=self._inst,\n                              _paramspec_tvars=self._paramspec_tvars)\n\n    def __repr__(self):\n        if self._name:\n            name = 'typing.' + self._name\n        else:\n            name = _type_repr(self.__origin__)\n        if self.__args__:\n            args = \", \".join([_type_repr(a) for a in self.__args__])\n        else:\n            # To ensure the repr is eval-able.\n            args = \"()\"\n        return f'{name}[{args}]'\n\n    def __reduce__(self):\n        if self._name:\n            origin = globals()[self._name]\n        else:\n            origin = self.__origin__\n        args = tuple(self.__args__)\n        if len(args) == 1 and not isinstance(args[0], tuple):\n            args, = args\n        return operator.getitem, (origin, args)\n\n    def __mro_entries__(self, bases):\n        if isinstance(self.__origin__, _SpecialForm):\n            raise TypeError(f\"Cannot subclass {self!r}\")\n\n        if self._name:  # generic version of an ABC or built-in class\n            return super().__mro_entries__(bases)\n        if self.__origin__ is Generic:\n            if Protocol in bases:\n                return ()\n            i = bases.index(self)\n            for b in bases[i+1:]:\n                if isinstance(b, _BaseGenericAlias) and b is not self:\n                    return ()\n        return (self.__origin__,)\n\n    def __iter__(self):\n        yield Unpack[self]\n\n\n# _nparams is the number of accepted parameters, e.g. 0 for Hashable,\n# 1 for List and 2 for Dict.  It may be -1 if variable number of\n# parameters are accepted (needs custom __getitem__).\n\nclass _SpecialGenericAlias(_NotIterable, _BaseGenericAlias, _root=True):\n    def __init__(self, origin, nparams, *, inst=True, name=None):\n        if name is None:\n            name = origin.__name__\n        super().__init__(origin, inst=inst, name=name)\n        self._nparams = nparams\n        if origin.__module__ == 'builtins':\n            self.__doc__ = f'A generic version of {origin.__qualname__}.'\n        else:\n            self.__doc__ = f'A generic version of {origin.__module__}.{origin.__qualname__}.'\n\n    @_tp_cache\n    def __getitem__(self, params):\n        if not isinstance(params, tuple):\n            params = (params,)\n        msg = \"Parameters to generic types must be types.\"\n        params = tuple(_type_check(p, msg) for p in params)\n        _check_generic(self, params, self._nparams)\n        return self.copy_with(params)\n\n    def copy_with(self, params):\n        return _GenericAlias(self.__origin__, params,\n                             name=self._name, inst=self._inst)\n\n    def __repr__(self):\n        return 'typing.' + self._name\n\n    def __subclasscheck__(self, cls):\n        if isinstance(cls, _SpecialGenericAlias):\n            return issubclass(cls.__origin__, self.__origin__)\n        if not isinstance(cls, _GenericAlias):\n            return issubclass(cls, self.__origin__)\n        return super().__subclasscheck__(cls)\n\n    def __reduce__(self):\n        return self._name\n\n    def __or__(self, right):\n        return Union[self, right]\n\n    def __ror__(self, left):\n        return Union[left, self]\n\nclass _CallableGenericAlias(_NotIterable, _GenericAlias, _root=True):\n    def __repr__(self):\n        assert self._name == 'Callable'\n        args = self.__args__\n        if len(args) == 2 and _is_param_expr(args[0]):\n            return super().__repr__()\n        return (f'typing.Callable'\n                f'[[{\", \".join([_type_repr(a) for a in args[:-1]])}], '\n                f'{_type_repr(args[-1])}]')\n\n    def __reduce__(self):\n        args = self.__args__\n        if not (len(args) == 2 and _is_param_expr(args[0])):\n            args = list(args[:-1]), args[-1]\n        return operator.getitem, (Callable, args)\n\n\nclass _CallableType(_SpecialGenericAlias, _root=True):\n    def copy_with(self, params):\n        return _CallableGenericAlias(self.__origin__, params,\n                                     name=self._name, inst=self._inst,\n                                     _paramspec_tvars=True)\n\n    def __getitem__(self, params):\n        if not isinstance(params, tuple) or len(params) != 2:\n            raise TypeError(\"Callable must be used as \"\n                            \"Callable[[arg, ...], result].\")\n        args, result = params\n        # This relaxes what args can be on purpose to allow things like\n        # PEP 612 ParamSpec.  Responsibility for whether a user is using\n        # Callable[...] properly is deferred to static type checkers.\n        if isinstance(args, list):\n            params = (tuple(args), result)\n        else:\n            params = (args, result)\n        return self.__getitem_inner__(params)\n\n    @_tp_cache\n    def __getitem_inner__(self, params):\n        args, result = params\n        msg = \"Callable[args, result]: result must be a type.\"\n        result = _type_check(result, msg)\n        if args is Ellipsis:\n            return self.copy_with((_TypingEllipsis, result))\n        if not isinstance(args, tuple):\n            args = (args,)\n        args = tuple(_type_convert(arg) for arg in args)\n        params = args + (result,)\n        return self.copy_with(params)\n\n\nclass _TupleType(_SpecialGenericAlias, _root=True):\n    @_tp_cache\n    def __getitem__(self, params):\n        if not isinstance(params, tuple):\n            params = (params,)\n        if len(params) >= 2 and params[-1] is ...:\n            msg = \"Tuple[t, ...]: t must be a type.\"\n            params = tuple(_type_check(p, msg) for p in params[:-1])\n            return self.copy_with((*params, _TypingEllipsis))\n        msg = \"Tuple[t0, t1, ...]: each t must be a type.\"\n        params = tuple(_type_check(p, msg) for p in params)\n        return self.copy_with(params)\n\n\nclass _UnionGenericAlias(_NotIterable, _GenericAlias, _root=True):\n    def copy_with(self, params):\n        return Union[params]\n\n    def __eq__(self, other):\n        if not isinstance(other, (_UnionGenericAlias, types.UnionType)):\n            return NotImplemented\n        try:  # fast path\n            return set(self.__args__) == set(other.__args__)\n        except TypeError:  # not hashable, slow path\n            return _compare_args_orderless(self.__args__, other.__args__)\n\n    def __hash__(self):\n        return hash(frozenset(self.__args__))\n\n    def __repr__(self):\n        args = self.__args__\n        if len(args) == 2:\n            if args[0] is type(None):\n                return f'typing.Optional[{_type_repr(args[1])}]'\n            elif args[1] is type(None):\n                return f'typing.Optional[{_type_repr(args[0])}]'\n        return super().__repr__()\n\n    def __instancecheck__(self, obj):\n        return self.__subclasscheck__(type(obj))\n\n    def __subclasscheck__(self, cls):\n        for arg in self.__args__:\n            if issubclass(cls, arg):\n                return True\n\n    def __reduce__(self):\n        func, (origin, args) = super().__reduce__()\n        return func, (Union, args)\n\n\ndef _value_and_type_iter(parameters):\n    return ((p, type(p)) for p in parameters)\n\n\nclass _LiteralGenericAlias(_GenericAlias, _root=True):\n    def __eq__(self, other):\n        if not isinstance(other, _LiteralGenericAlias):\n            return NotImplemented\n\n        return set(_value_and_type_iter(self.__args__)) == set(_value_and_type_iter(other.__args__))\n\n    def __hash__(self):\n        return hash(frozenset(_value_and_type_iter(self.__args__)))\n\n\nclass _ConcatenateGenericAlias(_GenericAlias, _root=True):\n    def copy_with(self, params):\n        if isinstance(params[-1], (list, tuple)):\n            return (*params[:-1], *params[-1])\n        if isinstance(params[-1], _ConcatenateGenericAlias):\n            params = (*params[:-1], *params[-1].__args__)\n        return super().copy_with(params)\n\n\n@_SpecialForm\ndef Unpack(self, parameters):\n    \"\"\"Type unpack operator.\n\n    The type unpack operator takes the child types from some container type,\n    such as `tuple[int, str]` or a `TypeVarTuple`, and 'pulls them out'.\n\n    For example::\n\n        # For some generic class `Foo`:\n        Foo[Unpack[tuple[int, str]]]  # Equivalent to Foo[int, str]\n\n        Ts = TypeVarTuple('Ts')\n        # Specifies that `Bar` is generic in an arbitrary number of types.\n        # (Think of `Ts` as a tuple of an arbitrary number of individual\n        #  `TypeVar`s, which the `Unpack` is 'pulling out' directly into the\n        #  `Generic[]`.)\n        class Bar(Generic[Unpack[Ts]]): ...\n        Bar[int]  # Valid\n        Bar[int, str]  # Also valid\n\n    From Python 3.11, this can also be done using the `*` operator::\n\n        Foo[*tuple[int, str]]\n        class Bar(Generic[*Ts]): ...\n\n    Note that there is only some runtime checking of this operator. Not\n    everything the runtime allows may be accepted by static type checkers.\n\n    For more information, see PEP 646.\n    \"\"\"\n    item = _type_check(parameters, f'{self} accepts only single type.')\n    return _UnpackGenericAlias(origin=self, args=(item,))\n\n\nclass _UnpackGenericAlias(_GenericAlias, _root=True):\n    def __repr__(self):\n        # `Unpack` only takes one argument, so __args__ should contain only\n        # a single item.\n        return '*' + repr(self.__args__[0])\n\n    def __getitem__(self, args):\n        if self.__typing_is_unpacked_typevartuple__:\n            return args\n        return super().__getitem__(args)\n\n    @property\n    def __typing_unpacked_tuple_args__(self):\n        assert self.__origin__ is Unpack\n        assert len(self.__args__) == 1\n        arg, = self.__args__\n        if isinstance(arg, _GenericAlias):\n            assert arg.__origin__ is tuple\n            return arg.__args__\n        return None\n\n    @property\n    def __typing_is_unpacked_typevartuple__(self):\n        assert self.__origin__ is Unpack\n        assert len(self.__args__) == 1\n        return isinstance(self.__args__[0], TypeVarTuple)\n\n\nclass Generic:\n    \"\"\"Abstract base class for generic types.\n\n    A generic type is typically declared by inheriting from\n    this class parameterized with one or more type variables.\n    For example, a generic mapping type might be defined as::\n\n      class Mapping(Generic[KT, VT]):\n          def __getitem__(self, key: KT) -> VT:\n              ...\n          # Etc.\n\n    This class can then be used as follows::\n\n      def lookup_name(mapping: Mapping[KT, VT], key: KT, default: VT) -> VT:\n          try:\n              return mapping[key]\n          except KeyError:\n              return default\n    \"\"\"\n    __slots__ = ()\n    _is_protocol = False\n\n    @_tp_cache\n    def __class_getitem__(cls, params):\n        \"\"\"Parameterizes a generic class.\n\n        At least, parameterizing a generic class is the *main* thing this method\n        does. For example, for some generic class `Foo`, this is called when we\n        do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n\n        However, note that this method is also called when defining generic\n        classes in the first place with `class Foo(Generic[T]): ...`.\n        \"\"\"\n        if not isinstance(params, tuple):\n            params = (params,)\n\n        params = tuple(_type_convert(p) for p in params)\n        if cls in (Generic, Protocol):\n            # Generic and Protocol can only be subscripted with unique type variables.\n            if not params:\n                raise TypeError(\n                    f\"Parameter list to {cls.__qualname__}[...] cannot be empty\"\n                )\n            if not all(_is_typevar_like(p) for p in params):\n                raise TypeError(\n                    f\"Parameters to {cls.__name__}[...] must all be type variables \"\n                    f\"or parameter specification variables.\")\n            if len(set(params)) != len(params):\n                raise TypeError(\n                    f\"Parameters to {cls.__name__}[...] must all be unique\")\n        else:\n            # Subscripting a regular Generic subclass.\n            for param in cls.__parameters__:\n                prepare = getattr(param, '__typing_prepare_subst__', None)\n                if prepare is not None:\n                    params = prepare(cls, params)\n            _check_generic(cls, params, len(cls.__parameters__))\n\n            new_args = []\n            for param, new_arg in zip(cls.__parameters__, params):\n                if isinstance(param, TypeVarTuple):\n                    new_args.extend(new_arg)\n                else:\n                    new_args.append(new_arg)\n            params = tuple(new_args)\n\n        return _GenericAlias(cls, params,\n                             _paramspec_tvars=True)\n\n    def __init_subclass__(cls, *args, **kwargs):\n        super().__init_subclass__(*args, **kwargs)\n        tvars = []\n        if '__orig_bases__' in cls.__dict__:\n            error = Generic in cls.__orig_bases__\n        else:\n            error = (Generic in cls.__bases__ and\n                        cls.__name__ != 'Protocol' and\n                        type(cls) != _TypedDictMeta)\n        if error:\n            raise TypeError(\"Cannot inherit from plain Generic\")\n        if '__orig_bases__' in cls.__dict__:\n            tvars = _collect_parameters(cls.__orig_bases__)\n            # Look for Generic[T1, ..., Tn].\n            # If found, tvars must be a subset of it.\n            # If not found, tvars is it.\n            # Also check for and reject plain Generic,\n            # and reject multiple Generic[...].\n            gvars = None\n            for base in cls.__orig_bases__:\n                if (isinstance(base, _GenericAlias) and\n                        base.__origin__ is Generic):\n                    if gvars is not None:\n                        raise TypeError(\n                            \"Cannot inherit from Generic[...] multiple times.\")\n                    gvars = base.__parameters__\n            if gvars is not None:\n                tvarset = set(tvars)\n                gvarset = set(gvars)\n                if not tvarset <= gvarset:\n                    s_vars = ', '.join(str(t) for t in tvars if t not in gvarset)\n                    s_args = ', '.join(str(g) for g in gvars)\n                    raise TypeError(f\"Some type variables ({s_vars}) are\"\n                                    f\" not listed in Generic[{s_args}]\")\n                tvars = gvars\n        cls.__parameters__ = tuple(tvars)\n\n\nclass _TypingEllipsis:\n    \"\"\"Internal placeholder for ... (ellipsis).\"\"\"\n\n\n_TYPING_INTERNALS = ['__parameters__', '__orig_bases__',  '__orig_class__',\n                     '_is_protocol', '_is_runtime_protocol', '__final__']\n\n_SPECIAL_NAMES = ['__abstractmethods__', '__annotations__', '__dict__', '__doc__',\n                  '__init__', '__module__', '__new__', '__slots__',\n                  '__subclasshook__', '__weakref__', '__class_getitem__']\n\n# These special attributes will be not collected as protocol members.\nEXCLUDED_ATTRIBUTES = _TYPING_INTERNALS + _SPECIAL_NAMES + ['_MutableMapping__marker']\n\n\ndef _get_protocol_attrs(cls):\n    \"\"\"Collect protocol members from a protocol class objects.\n\n    This includes names actually defined in the class dictionary, as well\n    as names that appear in annotations. Special names (above) are skipped.\n    \"\"\"\n    attrs = set()\n    for base in cls.__mro__[:-1]:  # without object\n        if base.__name__ in ('Protocol', 'Generic'):\n            continue\n        annotations = getattr(base, '__annotations__', {})\n        for attr in list(base.__dict__.keys()) + list(annotations.keys()):\n            if not attr.startswith('_abc_') and attr not in EXCLUDED_ATTRIBUTES:\n                attrs.add(attr)\n    return attrs\n\n\ndef _is_callable_members_only(cls):\n    # PEP 544 prohibits using issubclass() with protocols that have non-method members.\n    return all(callable(getattr(cls, attr, None)) for attr in _get_protocol_attrs(cls))\n\n\ndef _no_init_or_replace_init(self, *args, **kwargs):\n    cls = type(self)\n\n    if cls._is_protocol:\n        raise TypeError('Protocols cannot be instantiated')\n\n    # Already using a custom `__init__`. No need to calculate correct\n    # `__init__` to call. This can lead to RecursionError. See bpo-45121.\n    if cls.__init__ is not _no_init_or_replace_init:\n        return\n\n    # Initially, `__init__` of a protocol subclass is set to `_no_init_or_replace_init`.\n    # The first instantiation of the subclass will call `_no_init_or_replace_init` which\n    # searches for a proper new `__init__` in the MRO. The new `__init__`\n    # replaces the subclass' old `__init__` (ie `_no_init_or_replace_init`). Subsequent\n    # instantiation of the protocol subclass will thus use the new\n    # `__init__` and no longer call `_no_init_or_replace_init`.\n    for base in cls.__mro__:\n        init = base.__dict__.get('__init__', _no_init_or_replace_init)\n        if init is not _no_init_or_replace_init:\n            cls.__init__ = init\n            break\n    else:\n        # should not happen\n        cls.__init__ = object.__init__\n\n    cls.__init__(self, *args, **kwargs)\n\n\ndef _caller(depth=1, default='__main__'):\n    try:\n        return sys._getframe(depth + 1).f_globals.get('__name__', default)\n    except (AttributeError, ValueError):  # For platforms without _getframe()\n        return None\n\n\ndef _allow_reckless_class_checks(depth=3):\n    \"\"\"Allow instance and class checks for special stdlib modules.\n\n    The abc and functools modules indiscriminately call isinstance() and\n    issubclass() on the whole MRO of a user class, which may contain protocols.\n    \"\"\"\n    return _caller(depth) in {'abc', 'functools', None}\n\n\n_PROTO_ALLOWLIST = {\n    'collections.abc': [\n        'Callable', 'Awaitable', 'Iterable', 'Iterator', 'AsyncIterable',\n        'Hashable', 'Sized', 'Container', 'Collection', 'Reversible',\n    ],\n    'contextlib': ['AbstractContextManager', 'AbstractAsyncContextManager'],\n}\n\n\nclass _ProtocolMeta(ABCMeta):\n    # This metaclass is really unfortunate and exists only because of\n    # the lack of __instancehook__.\n    def __instancecheck__(cls, instance):\n        # We need this method for situations where attributes are\n        # assigned in __init__.\n        if (\n            getattr(cls, '_is_protocol', False) and\n            not getattr(cls, '_is_runtime_protocol', False) and\n            not _allow_reckless_class_checks(depth=2)\n        ):\n            raise TypeError(\"Instance and class checks can only be used with\"\n                            \" @runtime_checkable protocols\")\n\n        if ((not getattr(cls, '_is_protocol', False) or\n                _is_callable_members_only(cls)) and\n                issubclass(instance.__class__, cls)):\n            return True\n        if cls._is_protocol:\n            if all(hasattr(instance, attr) and\n                    # All *methods* can be blocked by setting them to None.\n                    (not callable(getattr(cls, attr, None)) or\n                     getattr(instance, attr) is not None)\n                    for attr in _get_protocol_attrs(cls)):\n                return True\n        return super().__instancecheck__(instance)\n\n\nclass Protocol(Generic, metaclass=_ProtocolMeta):\n    \"\"\"Base class for protocol classes.\n\n    Protocol classes are defined as::\n\n        class Proto(Protocol):\n            def meth(self) -> int:\n                ...\n\n    Such classes are primarily used with static type checkers that recognize\n    structural subtyping (static duck-typing).\n\n    For example::\n\n        class C:\n            def meth(self) -> int:\n                return 0\n\n        def func(x: Proto) -> int:\n            return x.meth()\n\n        func(C())  # Passes static type check\n\n    See PEP 544 for details. Protocol classes decorated with\n    @typing.runtime_checkable act as simple-minded runtime protocols that check\n    only the presence of given attributes, ignoring their type signatures.\n    Protocol classes can be generic, they are defined as::\n\n        class GenProto(Protocol[T]):\n            def meth(self) -> T:\n                ...\n    \"\"\"\n\n    __slots__ = ()\n    _is_protocol = True\n    _is_runtime_protocol = False\n\n    def __init_subclass__(cls, *args, **kwargs):\n        super().__init_subclass__(*args, **kwargs)\n\n        # Determine if this is a protocol or a concrete subclass.\n        if not cls.__dict__.get('_is_protocol', False):\n            cls._is_protocol = any(b is Protocol for b in cls.__bases__)\n\n        # Set (or override) the protocol subclass hook.\n        def _proto_hook(other):\n            if not cls.__dict__.get('_is_protocol', False):\n                return NotImplemented\n\n            # First, perform various sanity checks.\n            if not getattr(cls, '_is_runtime_protocol', False):\n                if _allow_reckless_class_checks():\n                    return NotImplemented\n                raise TypeError(\"Instance and class checks can only be used with\"\n                                \" @runtime_checkable protocols\")\n            if not _is_callable_members_only(cls):\n                if _allow_reckless_class_checks():\n                    return NotImplemented\n                raise TypeError(\"Protocols with non-method members\"\n                                \" don't support issubclass()\")\n            if not isinstance(other, type):\n                # Same error message as for issubclass(1, int).\n                raise TypeError('issubclass() arg 1 must be a class')\n\n            # Second, perform the actual structural compatibility check.\n            for attr in _get_protocol_attrs(cls):\n                for base in other.__mro__:\n                    # Check if the members appears in the class dictionary...\n                    if attr in base.__dict__:\n                        if base.__dict__[attr] is None:\n                            return NotImplemented\n                        break\n\n                    # ...or in annotations, if it is a sub-protocol.\n                    annotations = getattr(base, '__annotations__', {})\n                    if (isinstance(annotations, collections.abc.Mapping) and\n                            attr in annotations and\n                            issubclass(other, Generic) and other._is_protocol):\n                        break\n                else:\n                    return NotImplemented\n            return True\n\n        if '__subclasshook__' not in cls.__dict__:\n            cls.__subclasshook__ = _proto_hook\n\n        # We have nothing more to do for non-protocols...\n        if not cls._is_protocol:\n            return\n\n        # ... otherwise check consistency of bases, and prohibit instantiation.\n        for base in cls.__bases__:\n            if not (base in (object, Generic) or\n                    base.__module__ in _PROTO_ALLOWLIST and\n                    base.__name__ in _PROTO_ALLOWLIST[base.__module__] or\n                    issubclass(base, Generic) and base._is_protocol):\n                raise TypeError('Protocols can only inherit from other'\n                                ' protocols, got %r' % base)\n        if cls.__init__ is Protocol.__init__:\n            cls.__init__ = _no_init_or_replace_init\n\n\nclass _AnnotatedAlias(_NotIterable, _GenericAlias, _root=True):\n    \"\"\"Runtime representation of an annotated type.\n\n    At its core 'Annotated[t, dec1, dec2, ...]' is an alias for the type 't'\n    with extra annotations. The alias behaves like a normal typing alias.\n    Instantiating is the same as instantiating the underlying type; binding\n    it to types is also the same.\n\n    The metadata itself is stored in a '__metadata__' attribute as a tuple.\n    \"\"\"\n\n    def __init__(self, origin, metadata):\n        if isinstance(origin, _AnnotatedAlias):\n            metadata = origin.__metadata__ + metadata\n            origin = origin.__origin__\n        super().__init__(origin, origin)\n        self.__metadata__ = metadata\n\n    def copy_with(self, params):\n        assert len(params) == 1\n        new_type = params[0]\n        return _AnnotatedAlias(new_type, self.__metadata__)\n\n    def __repr__(self):\n        return \"typing.Annotated[{}, {}]\".format(\n            _type_repr(self.__origin__),\n            \", \".join(repr(a) for a in self.__metadata__)\n        )\n\n    def __reduce__(self):\n        return operator.getitem, (\n            Annotated, (self.__origin__,) + self.__metadata__\n        )\n\n    def __eq__(self, other):\n        if not isinstance(other, _AnnotatedAlias):\n            return NotImplemented\n        return (self.__origin__ == other.__origin__\n                and self.__metadata__ == other.__metadata__)\n\n    def __hash__(self):\n        return hash((self.__origin__, self.__metadata__))\n\n    def __getattr__(self, attr):\n        if attr in {'__name__', '__qualname__'}:\n            return 'Annotated'\n        return super().__getattr__(attr)\n\n\nclass Annotated:\n    \"\"\"Add context-specific metadata to a type.\n\n    Example: Annotated[int, runtime_check.Unsigned] indicates to the\n    hypothetical runtime_check module that this type is an unsigned int.\n    Every other consumer of this type can ignore this metadata and treat\n    this type as int.\n\n    The first argument to Annotated must be a valid type.\n\n    Details:\n\n    - It's an error to call `Annotated` with less than two arguments.\n    - Access the metadata via the ``__metadata__`` attribute::\n\n        assert Annotated[int, '$'].__metadata__ == ('$',)\n\n    - Nested Annotated types are flattened::\n\n        assert Annotated[Annotated[T, Ann1, Ann2], Ann3] == Annotated[T, Ann1, Ann2, Ann3]\n\n    - Instantiating an annotated type is equivalent to instantiating the\n    underlying type::\n\n        assert Annotated[C, Ann1](5) == C(5)\n\n    - Annotated can be used as a generic type alias::\n\n        Optimized: TypeAlias = Annotated[T, runtime.Optimize()]\n        assert Optimized[int] == Annotated[int, runtime.Optimize()]\n\n        OptimizedList: TypeAlias = Annotated[list[T], runtime.Optimize()]\n        assert OptimizedList[int] == Annotated[list[int], runtime.Optimize()]\n\n    - Annotated cannot be used with an unpacked TypeVarTuple::\n\n        Variadic: TypeAlias = Annotated[*Ts, Ann1]  # NOT valid\n\n      This would be equivalent to::\n\n        Annotated[T1, T2, T3, ..., Ann1]\n\n      where T1, T2 etc. are TypeVars, which would be invalid, because\n      only one type should be passed to Annotated.\n    \"\"\"\n\n    __slots__ = ()\n\n    def __new__(cls, *args, **kwargs):\n        raise TypeError(\"Type Annotated cannot be instantiated.\")\n\n    def __class_getitem__(cls, params):\n        if not isinstance(params, tuple):\n            params = (params,)\n        return cls._class_getitem_inner(cls, *params)\n\n    @_tp_cache(typed=True)\n    def _class_getitem_inner(cls, *params):\n        if len(params) < 2:\n            raise TypeError(\"Annotated[...] should be used \"\n                            \"with at least two arguments (a type and an \"\n                            \"annotation).\")\n        if _is_unpacked_typevartuple(params[0]):\n            raise TypeError(\"Annotated[...] should not be used with an \"\n                            \"unpacked TypeVarTuple\")\n        msg = \"Annotated[t, ...]: t must be a type.\"\n        origin = _type_check(params[0], msg, allow_special_forms=True)\n        metadata = tuple(params[1:])\n        return _AnnotatedAlias(origin, metadata)\n\n    def __init_subclass__(cls, *args, **kwargs):\n        raise TypeError(\n            \"Cannot subclass {}.Annotated\".format(cls.__module__)\n        )\n\n\ndef runtime_checkable(cls):\n    \"\"\"Mark a protocol class as a runtime protocol.\n\n    Such protocol can be used with isinstance() and issubclass().\n    Raise TypeError if applied to a non-protocol class.\n    This allows a simple-minded structural check very similar to\n    one trick ponies in collections.abc such as Iterable.\n\n    For example::\n\n        @runtime_checkable\n        class Closable(Protocol):\n            def close(self): ...\n\n        assert isinstance(open('/some/file'), Closable)\n\n    Warning: this will check only the presence of the required methods,\n    not their type signatures!\n    \"\"\"\n    if not issubclass(cls, Generic) or not cls._is_protocol:\n        raise TypeError('@runtime_checkable can be only applied to protocol classes,'\n                        ' got %r' % cls)\n    cls._is_runtime_protocol = True\n    return cls\n\n\ndef cast(typ, val):\n    \"\"\"Cast a value to a type.\n\n    This returns the value unchanged.  To the type checker this\n    signals that the return value has the designated type, but at\n    runtime we intentionally don't check anything (we want this\n    to be as fast as possible).\n    \"\"\"\n    return val\n\n\ndef assert_type(val, typ, /):\n    \"\"\"Ask a static type checker to confirm that the value is of the given type.\n\n    At runtime this does nothing: it returns the first argument unchanged with no\n    checks or side effects, no matter the actual type of the argument.\n\n    When a static type checker encounters a call to assert_type(), it\n    emits an error if the value is not of the specified type::\n\n        def greet(name: str) -> None:\n            assert_type(name, str)  # OK\n            assert_type(name, int)  # type checker error\n    \"\"\"\n    return val\n\n\n_allowed_types = (types.FunctionType, types.BuiltinFunctionType,\n                  types.MethodType, types.ModuleType,\n                  WrapperDescriptorType, MethodWrapperType, MethodDescriptorType)\n\n\ndef get_type_hints(obj, globalns=None, localns=None, include_extras=False):\n    \"\"\"Return type hints for an object.\n\n    This is often the same as obj.__annotations__, but it handles\n    forward references encoded as string literals and recursively replaces all\n    'Annotated[T, ...]' with 'T' (unless 'include_extras=True').\n\n    The argument may be a module, class, method, or function. The annotations\n    are returned as a dictionary. For classes, annotations include also\n    inherited members.\n\n    TypeError is raised if the argument is not of a type that can contain\n    annotations, and an empty dictionary is returned if no annotations are\n    present.\n\n    BEWARE -- the behavior of globalns and localns is counterintuitive\n    (unless you are familiar with how eval() and exec() work).  The\n    search order is locals first, then globals.\n\n    - If no dict arguments are passed, an attempt is made to use the\n      globals from obj (or the respective module's globals for classes),\n      and these are also used as the locals.  If the object does not appear\n      to have globals, an empty dictionary is used.  For classes, the search\n      order is globals first then locals.\n\n    - If one dict argument is passed, it is used for both globals and\n      locals.\n\n    - If two dict arguments are passed, they specify globals and\n      locals, respectively.\n    \"\"\"\n    if getattr(obj, '__no_type_check__', None):\n        return {}\n    # Classes require a special treatment.\n    if isinstance(obj, type):\n        hints = {}\n        for base in reversed(obj.__mro__):\n            if globalns is None:\n                base_globals = getattr(sys.modules.get(base.__module__, None), '__dict__', {})\n            else:\n                base_globals = globalns\n            ann = base.__dict__.get('__annotations__', {})\n            if isinstance(ann, types.GetSetDescriptorType):\n                ann = {}\n            base_locals = dict(vars(base)) if localns is None else localns\n            if localns is None and globalns is None:\n                # This is surprising, but required.  Before Python 3.10,\n                # get_type_hints only evaluated the globalns of\n                # a class.  To maintain backwards compatibility, we reverse\n                # the globalns and localns order so that eval() looks into\n                # *base_globals* first rather than *base_locals*.\n                # This only affects ForwardRefs.\n                base_globals, base_locals = base_locals, base_globals\n            for name, value in ann.items():\n                if value is None:\n                    value = type(None)\n                if isinstance(value, str):\n                    value = ForwardRef(value, is_argument=False, is_class=True)\n                value = _eval_type(value, base_globals, base_locals)\n                hints[name] = value\n        return hints if include_extras else {k: _strip_annotations(t) for k, t in hints.items()}\n\n    if globalns is None:\n        if isinstance(obj, types.ModuleType):\n            globalns = obj.__dict__\n        else:\n            nsobj = obj\n            # Find globalns for the unwrapped object.\n            while hasattr(nsobj, '__wrapped__'):\n                nsobj = nsobj.__wrapped__\n            globalns = getattr(nsobj, '__globals__', {})\n        if localns is None:\n            localns = globalns\n    elif localns is None:\n        localns = globalns\n    hints = getattr(obj, '__annotations__', None)\n    if hints is None:\n        # Return empty annotations for something that _could_ have them.\n        if isinstance(obj, _allowed_types):\n            return {}\n        else:\n            raise TypeError('{!r} is not a module, class, method, '\n                            'or function.'.format(obj))\n    hints = dict(hints)\n    for name, value in hints.items():\n        if value is None:\n            value = type(None)\n        if isinstance(value, str):\n            # class-level forward refs were handled above, this must be either\n            # a module-level annotation or a function argument annotation\n            value = ForwardRef(\n                value,\n                is_argument=not isinstance(obj, types.ModuleType),\n                is_class=False,\n            )\n        hints[name] = _eval_type(value, globalns, localns)\n    return hints if include_extras else {k: _strip_annotations(t) for k, t in hints.items()}\n\n\ndef _strip_annotations(t):\n    \"\"\"Strip the annotations from a given type.\"\"\"\n    if isinstance(t, _AnnotatedAlias):\n        return _strip_annotations(t.__origin__)\n    if hasattr(t, \"__origin__\") and t.__origin__ in (Required, NotRequired):\n        return _strip_annotations(t.__args__[0])\n    if isinstance(t, _GenericAlias):\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\n        if stripped_args == t.__args__:\n            return t\n        return t.copy_with(stripped_args)\n    if isinstance(t, GenericAlias):\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\n        if stripped_args == t.__args__:\n            return t\n        return GenericAlias(t.__origin__, stripped_args)\n    if isinstance(t, types.UnionType):\n        stripped_args = tuple(_strip_annotations(a) for a in t.__args__)\n        if stripped_args == t.__args__:\n            return t\n        return functools.reduce(operator.or_, stripped_args)\n\n    return t\n\n\ndef get_origin(tp):\n    \"\"\"Get the unsubscripted version of a type.\n\n    This supports generic types, Callable, Tuple, Union, Literal, Final, ClassVar,\n    Annotated, and others. Return None for unsupported types.\n\n    Examples::\n\n        >>> P = ParamSpec('P')\n        >>> assert get_origin(Literal[42]) is Literal\n        >>> assert get_origin(int) is None\n        >>> assert get_origin(ClassVar[int]) is ClassVar\n        >>> assert get_origin(Generic) is Generic\n        >>> assert get_origin(Generic[T]) is Generic\n        >>> assert get_origin(Union[T, int]) is Union\n        >>> assert get_origin(List[Tuple[T, T]][int]) is list\n        >>> assert get_origin(P.args) is P\n    \"\"\"\n    if isinstance(tp, _AnnotatedAlias):\n        return Annotated\n    if isinstance(tp, (_BaseGenericAlias, GenericAlias,\n                       ParamSpecArgs, ParamSpecKwargs)):\n        return tp.__origin__\n    if tp is Generic:\n        return Generic\n    if isinstance(tp, types.UnionType):\n        return types.UnionType\n    return None\n\n\ndef get_args(tp):\n    \"\"\"Get type arguments with all substitutions performed.\n\n    For unions, basic simplifications used by Union constructor are performed.\n\n    Examples::\n\n        >>> T = TypeVar('T')\n        >>> assert get_args(Dict[str, int]) == (str, int)\n        >>> assert get_args(int) == ()\n        >>> assert get_args(Union[int, Union[T, int], str][int]) == (int, str)\n        >>> assert get_args(Union[int, Tuple[T, int]][str]) == (int, Tuple[str, int])\n        >>> assert get_args(Callable[[], T][int]) == ([], int)\n    \"\"\"\n    if isinstance(tp, _AnnotatedAlias):\n        return (tp.__origin__,) + tp.__metadata__\n    if isinstance(tp, (_GenericAlias, GenericAlias)):\n        res = tp.__args__\n        if _should_unflatten_callable_args(tp, res):\n            res = (list(res[:-1]), res[-1])\n        return res\n    if isinstance(tp, types.UnionType):\n        return tp.__args__\n    return ()\n\n\ndef is_typeddict(tp):\n    \"\"\"Check if an annotation is a TypedDict class.\n\n    For example::\n\n        >>> from typing import TypedDict\n        >>> class Film(TypedDict):\n        ...     title: str\n        ...     year: int\n        ...\n        >>> is_typeddict(Film)\n        True\n        >>> is_typeddict(dict)\n        False\n    \"\"\"\n    return isinstance(tp, _TypedDictMeta)\n\n\n_ASSERT_NEVER_REPR_MAX_LENGTH = 100\n\n\ndef assert_never(arg: Never, /) -> Never:\n    \"\"\"Statically assert that a line of code is unreachable.\n\n    Example::\n\n        def int_or_str(arg: int | str) -> None:\n            match arg:\n                case int():\n                    print(\"It's an int\")\n                case str():\n                    print(\"It's a str\")\n                case _:\n                    assert_never(arg)\n\n    If a type checker finds that a call to assert_never() is\n    reachable, it will emit an error.\n\n    At runtime, this throws an exception when called.\n    \"\"\"\n    value = repr(arg)\n    if len(value) > _ASSERT_NEVER_REPR_MAX_LENGTH:\n        value = value[:_ASSERT_NEVER_REPR_MAX_LENGTH] + '...'\n    raise AssertionError(f\"Expected code to be unreachable, but got: {value}\")\n\n\ndef no_type_check(arg):\n    \"\"\"Decorator to indicate that annotations are not type hints.\n\n    The argument must be a class or function; if it is a class, it\n    applies recursively to all methods and classes defined in that class\n    (but not to methods defined in its superclasses or subclasses).\n\n    This mutates the function(s) or class(es) in place.\n    \"\"\"\n    if isinstance(arg, type):\n        for key in dir(arg):\n            obj = getattr(arg, key)\n            if (\n                not hasattr(obj, '__qualname__')\n                or obj.__qualname__ != f'{arg.__qualname__}.{obj.__name__}'\n                or getattr(obj, '__module__', None) != arg.__module__\n            ):\n                # We only modify objects that are defined in this type directly.\n                # If classes / methods are nested in multiple layers,\n                # we will modify them when processing their direct holders.\n                continue\n            # Instance, class, and static methods:\n            if isinstance(obj, types.FunctionType):\n                obj.__no_type_check__ = True\n            if isinstance(obj, types.MethodType):\n                obj.__func__.__no_type_check__ = True\n            # Nested types:\n            if isinstance(obj, type):\n                no_type_check(obj)\n    try:\n        arg.__no_type_check__ = True\n    except TypeError:  # built-in classes\n        pass\n    return arg\n\n\ndef no_type_check_decorator(decorator):\n    \"\"\"Decorator to give another decorator the @no_type_check effect.\n\n    This wraps the decorator with something that wraps the decorated\n    function in @no_type_check.\n    \"\"\"\n    @functools.wraps(decorator)\n    def wrapped_decorator(*args, **kwds):\n        func = decorator(*args, **kwds)\n        func = no_type_check(func)\n        return func\n\n    return wrapped_decorator\n\n\ndef _overload_dummy(*args, **kwds):\n    \"\"\"Helper for @overload to raise when called.\"\"\"\n    raise NotImplementedError(\n        \"You should not call an overloaded function. \"\n        \"A series of @overload-decorated functions \"\n        \"outside a stub module should always be followed \"\n        \"by an implementation that is not @overload-ed.\")\n\n\n# {module: {qualname: {firstlineno: func}}}\n_overload_registry = defaultdict(functools.partial(defaultdict, dict))\n\n\ndef overload(func):\n    \"\"\"Decorator for overloaded functions/methods.\n\n    In a stub file, place two or more stub definitions for the same\n    function in a row, each decorated with @overload.\n\n    For example::\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n\n    In a non-stub file (i.e. a regular .py file), do the same but\n    follow it with an implementation.  The implementation should *not*\n    be decorated with @overload::\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n        def utf8(value):\n            ...  # implementation goes here\n\n    The overloads for a function can be retrieved at runtime using the\n    get_overloads() function.\n    \"\"\"\n    # classmethod and staticmethod\n    f = getattr(func, \"__func__\", func)\n    try:\n        _overload_registry[f.__module__][f.__qualname__][f.__code__.co_firstlineno] = func\n    except AttributeError:\n        # Not a normal function; ignore.\n        pass\n    return _overload_dummy\n\n\ndef get_overloads(func):\n    \"\"\"Return all defined overloads for *func* as a sequence.\"\"\"\n    # classmethod and staticmethod\n    f = getattr(func, \"__func__\", func)\n    if f.__module__ not in _overload_registry:\n        return []\n    mod_dict = _overload_registry[f.__module__]\n    if f.__qualname__ not in mod_dict:\n        return []\n    return list(mod_dict[f.__qualname__].values())\n\n\ndef clear_overloads():\n    \"\"\"Clear all overloads in the registry.\"\"\"\n    _overload_registry.clear()\n\n\ndef final(f):\n    \"\"\"Decorator to indicate final methods and final classes.\n\n    Use this decorator to indicate to type checkers that the decorated\n    method cannot be overridden, and decorated class cannot be subclassed.\n\n    For example::\n\n        class Base:\n            @final\n            def done(self) -> None:\n                ...\n        class Sub(Base):\n            def done(self) -> None:  # Error reported by type checker\n                ...\n\n        @final\n        class Leaf:\n            ...\n        class Other(Leaf):  # Error reported by type checker\n            ...\n\n    There is no runtime checking of these properties. The decorator\n    attempts to set the ``__final__`` attribute to ``True`` on the decorated\n    object to allow runtime introspection.\n    \"\"\"\n    try:\n        f.__final__ = True\n    except (AttributeError, TypeError):\n        # Skip the attribute silently if it is not writable.\n        # AttributeError happens if the object has __slots__ or a\n        # read-only property, TypeError if it's a builtin class.\n        pass\n    return f\n\n\n# Some unconstrained type variables.  These are used by the container types.\n# (These are not for export.)\nT = TypeVar('T')  # Any type.\nKT = TypeVar('KT')  # Key type.\nVT = TypeVar('VT')  # Value type.\nT_co = TypeVar('T_co', covariant=True)  # Any type covariant containers.\nV_co = TypeVar('V_co', covariant=True)  # Any type covariant containers.\nVT_co = TypeVar('VT_co', covariant=True)  # Value type covariant containers.\nT_contra = TypeVar('T_contra', contravariant=True)  # Ditto contravariant.\n# Internal type variable used for Type[].\nCT_co = TypeVar('CT_co', covariant=True, bound=type)\n\n# A useful type variable with constraints.  This represents string types.\n# (This one *is* for export!)\nAnyStr = TypeVar('AnyStr', bytes, str)\n\n\n# Various ABCs mimicking those in collections.abc.\n_alias = _SpecialGenericAlias\n\nHashable = _alias(collections.abc.Hashable, 0)  # Not generic.\nAwaitable = _alias(collections.abc.Awaitable, 1)\nCoroutine = _alias(collections.abc.Coroutine, 3)\nAsyncIterable = _alias(collections.abc.AsyncIterable, 1)\nAsyncIterator = _alias(collections.abc.AsyncIterator, 1)\nIterable = _alias(collections.abc.Iterable, 1)\nIterator = _alias(collections.abc.Iterator, 1)\nReversible = _alias(collections.abc.Reversible, 1)\nSized = _alias(collections.abc.Sized, 0)  # Not generic.\nContainer = _alias(collections.abc.Container, 1)\nCollection = _alias(collections.abc.Collection, 1)\nCallable = _CallableType(collections.abc.Callable, 2)\nCallable.__doc__ = \\\n    \"\"\"Deprecated alias to collections.abc.Callable.\n\n    Callable[[int], str] signifies a function that takes a single\n    parameter of type int and returns a str.\n\n    The subscription syntax must always be used with exactly two\n    values: the argument list and the return type.\n    The argument list must be a list of types, a ParamSpec,\n    Concatenate or ellipsis. The return type must be a single type.\n\n    There is no syntax to indicate optional or keyword arguments;\n    such function types are rarely used as callback types.\n    \"\"\"\nAbstractSet = _alias(collections.abc.Set, 1, name='AbstractSet')\nMutableSet = _alias(collections.abc.MutableSet, 1)\n# NOTE: Mapping is only covariant in the value type.\nMapping = _alias(collections.abc.Mapping, 2)\nMutableMapping = _alias(collections.abc.MutableMapping, 2)\nSequence = _alias(collections.abc.Sequence, 1)\nMutableSequence = _alias(collections.abc.MutableSequence, 1)\nByteString = _alias(collections.abc.ByteString, 0)  # Not generic\n# Tuple accepts variable number of parameters.\nTuple = _TupleType(tuple, -1, inst=False, name='Tuple')\nTuple.__doc__ = \\\n    \"\"\"Deprecated alias to builtins.tuple.\n\n    Tuple[X, Y] is the cross-product type of X and Y.\n\n    Example: Tuple[T1, T2] is a tuple of two elements corresponding\n    to type variables T1 and T2.  Tuple[int, float, str] is a tuple\n    of an int, a float and a string.\n\n    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].\n    \"\"\"\nList = _alias(list, 1, inst=False, name='List')\nDeque = _alias(collections.deque, 1, name='Deque')\nSet = _alias(set, 1, inst=False, name='Set')\nFrozenSet = _alias(frozenset, 1, inst=False, name='FrozenSet')\nMappingView = _alias(collections.abc.MappingView, 1)\nKeysView = _alias(collections.abc.KeysView, 1)\nItemsView = _alias(collections.abc.ItemsView, 2)\nValuesView = _alias(collections.abc.ValuesView, 1)\nContextManager = _alias(contextlib.AbstractContextManager, 1, name='ContextManager')\nAsyncContextManager = _alias(contextlib.AbstractAsyncContextManager, 1, name='AsyncContextManager')\nDict = _alias(dict, 2, inst=False, name='Dict')\nDefaultDict = _alias(collections.defaultdict, 2, name='DefaultDict')\nOrderedDict = _alias(collections.OrderedDict, 2)\nCounter = _alias(collections.Counter, 1)\nChainMap = _alias(collections.ChainMap, 2)\nGenerator = _alias(collections.abc.Generator, 3)\nAsyncGenerator = _alias(collections.abc.AsyncGenerator, 2)\nType = _alias(type, 1, inst=False, name='Type')\nType.__doc__ = \\\n    \"\"\"Deprecated alias to builtins.type.\n\n    builtins.type or typing.Type can be used to annotate class objects.\n    For example, suppose we have the following classes::\n\n        class User: ...  # Abstract base for User classes\n        class BasicUser(User): ...\n        class ProUser(User): ...\n        class TeamUser(User): ...\n\n    And a function that takes a class argument that's a subclass of\n    User and returns an instance of the corresponding class::\n\n        U = TypeVar('U', bound=User)\n        def new_user(user_class: Type[U]) -> U:\n            user = user_class()\n            # (Here we could write the user object to a database)\n            return user\n\n        joe = new_user(BasicUser)\n\n    At this point the type checker knows that joe has type BasicUser.\n    \"\"\"\n\n\n@runtime_checkable\nclass SupportsInt(Protocol):\n    \"\"\"An ABC with one abstract method __int__.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __int__(self) -> int:\n        pass\n\n\n@runtime_checkable\nclass SupportsFloat(Protocol):\n    \"\"\"An ABC with one abstract method __float__.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __float__(self) -> float:\n        pass\n\n\n@runtime_checkable\nclass SupportsComplex(Protocol):\n    \"\"\"An ABC with one abstract method __complex__.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __complex__(self) -> complex:\n        pass\n\n\n@runtime_checkable\nclass SupportsBytes(Protocol):\n    \"\"\"An ABC with one abstract method __bytes__.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __bytes__(self) -> bytes:\n        pass\n\n\n@runtime_checkable\nclass SupportsIndex(Protocol):\n    \"\"\"An ABC with one abstract method __index__.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __index__(self) -> int:\n        pass\n\n\n@runtime_checkable\nclass SupportsAbs(Protocol[T_co]):\n    \"\"\"An ABC with one abstract method __abs__ that is covariant in its return type.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __abs__(self) -> T_co:\n        pass\n\n\n@runtime_checkable\nclass SupportsRound(Protocol[T_co]):\n    \"\"\"An ABC with one abstract method __round__ that is covariant in its return type.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __round__(self, ndigits: int = 0) -> T_co:\n        pass\n\n\ndef _make_nmtuple(name, types, module, defaults = ()):\n    fields = [n for n, t in types]\n    types = {n: _type_check(t, f\"field {n} annotation must be a type\")\n             for n, t in types}\n    nm_tpl = collections.namedtuple(name, fields,\n                                    defaults=defaults, module=module)\n    nm_tpl.__annotations__ = nm_tpl.__new__.__annotations__ = types\n    return nm_tpl\n\n\n# attributes prohibited to set in NamedTuple class syntax\n_prohibited = frozenset({'__new__', '__init__', '__slots__', '__getnewargs__',\n                         '_fields', '_field_defaults',\n                         '_make', '_replace', '_asdict', '_source'})\n\n_special = frozenset({'__module__', '__name__', '__annotations__'})\n\n\nclass NamedTupleMeta(type):\n    def __new__(cls, typename, bases, ns):\n        assert _NamedTuple in bases\n        for base in bases:\n            if base is not _NamedTuple and base is not Generic:\n                raise TypeError(\n                    'can only inherit from a NamedTuple type and Generic')\n        bases = tuple(tuple if base is _NamedTuple else base for base in bases)\n        types = ns.get('__annotations__', {})\n        default_names = []\n        for field_name in types:\n            if field_name in ns:\n                default_names.append(field_name)\n            elif default_names:\n                raise TypeError(f\"Non-default namedtuple field {field_name} \"\n                                f\"cannot follow default field\"\n                                f\"{'s' if len(default_names) > 1 else ''} \"\n                                f\"{', '.join(default_names)}\")\n        nm_tpl = _make_nmtuple(typename, types.items(),\n                               defaults=[ns[n] for n in default_names],\n                               module=ns['__module__'])\n        nm_tpl.__bases__ = bases\n        if Generic in bases:\n            class_getitem = Generic.__class_getitem__.__func__\n            nm_tpl.__class_getitem__ = classmethod(class_getitem)\n        # update from user namespace without overriding special namedtuple attributes\n        for key in ns:\n            if key in _prohibited:\n                raise AttributeError(\"Cannot overwrite NamedTuple attribute \" + key)\n            elif key not in _special and key not in nm_tpl._fields:\n                setattr(nm_tpl, key, ns[key])\n        if Generic in bases:\n            nm_tpl.__init_subclass__()\n        return nm_tpl\n\n\ndef NamedTuple(typename, fields=None, /, **kwargs):\n    \"\"\"Typed version of namedtuple.\n\n    Usage::\n\n        class Employee(NamedTuple):\n            name: str\n            id: int\n\n    This is equivalent to::\n\n        Employee = collections.namedtuple('Employee', ['name', 'id'])\n\n    The resulting class has an extra __annotations__ attribute, giving a\n    dict that maps field names to types.  (The field names are also in\n    the _fields attribute, which is part of the namedtuple API.)\n    An alternative equivalent functional syntax is also accepted::\n\n        Employee = NamedTuple('Employee', [('name', str), ('id', int)])\n    \"\"\"\n    if fields is None:\n        fields = kwargs.items()\n    elif kwargs:\n        raise TypeError(\"Either list of fields or keywords\"\n                        \" can be provided to NamedTuple, not both\")\n    return _make_nmtuple(typename, fields, module=_caller())\n\n_NamedTuple = type.__new__(NamedTupleMeta, 'NamedTuple', (), {})\n\ndef _namedtuple_mro_entries(bases):\n    assert NamedTuple in bases\n    return (_NamedTuple,)\n\nNamedTuple.__mro_entries__ = _namedtuple_mro_entries\n\n\nclass _TypedDictMeta(type):\n    def __new__(cls, name, bases, ns, total=True):\n        \"\"\"Create a new typed dict class object.\n\n        This method is called when TypedDict is subclassed,\n        or when TypedDict is instantiated. This way\n        TypedDict supports all three syntax forms described in its docstring.\n        Subclasses and instances of TypedDict return actual dictionaries.\n        \"\"\"\n        for base in bases:\n            if type(base) is not _TypedDictMeta and base is not Generic:\n                raise TypeError('cannot inherit from both a TypedDict type '\n                                'and a non-TypedDict base class')\n\n        if any(issubclass(b, Generic) for b in bases):\n            generic_base = (Generic,)\n        else:\n            generic_base = ()\n\n        tp_dict = type.__new__(_TypedDictMeta, name, (*generic_base, dict), ns)\n\n        annotations = {}\n        own_annotations = ns.get('__annotations__', {})\n        msg = \"TypedDict('Name', {f0: t0, f1: t1, ...}); each t must be a type\"\n        own_annotations = {\n            n: _type_check(tp, msg, module=tp_dict.__module__)\n            for n, tp in own_annotations.items()\n        }\n        required_keys = set()\n        optional_keys = set()\n\n        for base in bases:\n            annotations.update(base.__dict__.get('__annotations__', {}))\n\n            base_required = base.__dict__.get('__required_keys__', set())\n            required_keys |= base_required\n            optional_keys -= base_required\n\n            base_optional = base.__dict__.get('__optional_keys__', set())\n            required_keys -= base_optional\n            optional_keys |= base_optional\n\n        annotations.update(own_annotations)\n        for annotation_key, annotation_type in own_annotations.items():\n            annotation_origin = get_origin(annotation_type)\n            if annotation_origin is Annotated:\n                annotation_args = get_args(annotation_type)\n                if annotation_args:\n                    annotation_type = annotation_args[0]\n                    annotation_origin = get_origin(annotation_type)\n\n            if annotation_origin is Required:\n                is_required = True\n            elif annotation_origin is NotRequired:\n                is_required = False\n            else:\n                is_required = total\n\n            if is_required:\n                required_keys.add(annotation_key)\n                optional_keys.discard(annotation_key)\n            else:\n                optional_keys.add(annotation_key)\n                required_keys.discard(annotation_key)\n\n        assert required_keys.isdisjoint(optional_keys), (\n            f\"Required keys overlap with optional keys in {name}:\"\n            f\" {required_keys=}, {optional_keys=}\"\n        )\n        tp_dict.__annotations__ = annotations\n        tp_dict.__required_keys__ = frozenset(required_keys)\n        tp_dict.__optional_keys__ = frozenset(optional_keys)\n        if not hasattr(tp_dict, '__total__'):\n            tp_dict.__total__ = total\n        return tp_dict\n\n    __call__ = dict  # static method\n\n    def __subclasscheck__(cls, other):\n        # Typed dicts are only for static structural subtyping.\n        raise TypeError('TypedDict does not support instance and class checks')\n\n    __instancecheck__ = __subclasscheck__\n\n\ndef TypedDict(typename, fields=None, /, *, total=True, **kwargs):\n    \"\"\"A simple typed namespace. At runtime it is equivalent to a plain dict.\n\n    TypedDict creates a dictionary type such that a type checker will expect all\n    instances to have a certain set of keys, where each key is\n    associated with a value of a consistent type. This expectation\n    is not checked at runtime.\n\n    Usage::\n\n        >>> class Point2D(TypedDict):\n        ...     x: int\n        ...     y: int\n        ...     label: str\n        ...\n        >>> a: Point2D = {'x': 1, 'y': 2, 'label': 'good'}  # OK\n        >>> b: Point2D = {'z': 3, 'label': 'bad'}           # Fails type check\n        >>> Point2D(x=1, y=2, label='first') == dict(x=1, y=2, label='first')\n        True\n\n    The type info can be accessed via the Point2D.__annotations__ dict, and\n    the Point2D.__required_keys__ and Point2D.__optional_keys__ frozensets.\n    TypedDict supports an additional equivalent form::\n\n        Point2D = TypedDict('Point2D', {'x': int, 'y': int, 'label': str})\n\n    By default, all keys must be present in a TypedDict. It is possible\n    to override this by specifying totality::\n\n        class Point2D(TypedDict, total=False):\n            x: int\n            y: int\n\n    This means that a Point2D TypedDict can have any of the keys omitted. A type\n    checker is only expected to support a literal False or True as the value of\n    the total argument. True is the default, and makes all items defined in the\n    class body be required.\n\n    The Required and NotRequired special forms can also be used to mark\n    individual keys as being required or not required::\n\n        class Point2D(TypedDict):\n            x: int               # the \"x\" key must always be present (Required is the default)\n            y: NotRequired[int]  # the \"y\" key can be omitted\n\n    See PEP 655 for more details on Required and NotRequired.\n    \"\"\"\n    if fields is None:\n        fields = kwargs\n    elif kwargs:\n        raise TypeError(\"TypedDict takes either a dict or keyword arguments,\"\n                        \" but not both\")\n    if kwargs:\n        warnings.warn(\n            \"The kwargs-based syntax for TypedDict definitions is deprecated \"\n            \"in Python 3.11, will be removed in Python 3.13, and may not be \"\n            \"understood by third-party type checkers.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    ns = {'__annotations__': dict(fields)}\n    module = _caller()\n    if module is not None:\n        # Setting correct module is necessary to make typed dict classes pickleable.\n        ns['__module__'] = module\n\n    return _TypedDictMeta(typename, (), ns, total=total)\n\n_TypedDict = type.__new__(_TypedDictMeta, 'TypedDict', (), {})\nTypedDict.__mro_entries__ = lambda bases: (_TypedDict,)\n\n\n@_SpecialForm\ndef Required(self, parameters):\n    \"\"\"Special typing construct to mark a TypedDict key as required.\n\n    This is mainly useful for total=False TypedDicts.\n\n    For example::\n\n        class Movie(TypedDict, total=False):\n            title: Required[str]\n            year: int\n\n        m = Movie(\n            title='The Matrix',  # typechecker error if key is omitted\n            year=1999,\n        )\n\n    There is no runtime checking that a required key is actually provided\n    when instantiating a related TypedDict.\n    \"\"\"\n    item = _type_check(parameters, f'{self._name} accepts only a single type.')\n    return _GenericAlias(self, (item,))\n\n\n@_SpecialForm\ndef NotRequired(self, parameters):\n    \"\"\"Special typing construct to mark a TypedDict key as potentially missing.\n\n    For example::\n\n        class Movie(TypedDict):\n            title: str\n            year: NotRequired[int]\n\n        m = Movie(\n            title='The Matrix',  # typechecker error if key is omitted\n            year=1999,\n        )\n    \"\"\"\n    item = _type_check(parameters, f'{self._name} accepts only a single type.')\n    return _GenericAlias(self, (item,))\n\n\nclass NewType:\n    \"\"\"NewType creates simple unique types with almost zero runtime overhead.\n\n    NewType(name, tp) is considered a subtype of tp\n    by static type checkers. At runtime, NewType(name, tp) returns\n    a dummy callable that simply returns its argument.\n\n    Usage::\n\n        UserId = NewType('UserId', int)\n\n        def name_by_id(user_id: UserId) -> str:\n            ...\n\n        UserId('user')          # Fails type check\n\n        name_by_id(42)          # Fails type check\n        name_by_id(UserId(42))  # OK\n\n        num = UserId(5) + 1     # type: int\n    \"\"\"\n\n    __call__ = _idfunc\n\n    def __init__(self, name, tp):\n        self.__qualname__ = name\n        if '.' in name:\n            name = name.rpartition('.')[-1]\n        self.__name__ = name\n        self.__supertype__ = tp\n        def_mod = _caller()\n        if def_mod != 'typing':\n            self.__module__ = def_mod\n\n    def __mro_entries__(self, bases):\n        # We defined __mro_entries__ to get a better error message\n        # if a user attempts to subclass a NewType instance. bpo-46170\n        superclass_name = self.__name__\n\n        class Dummy:\n            def __init_subclass__(cls):\n                subclass_name = cls.__name__\n                raise TypeError(\n                    f\"Cannot subclass an instance of NewType. Perhaps you were looking for: \"\n                    f\"`{subclass_name} = NewType({subclass_name!r}, {superclass_name})`\"\n                )\n\n        return (Dummy,)\n\n    def __repr__(self):\n        return f'{self.__module__}.{self.__qualname__}'\n\n    def __reduce__(self):\n        return self.__qualname__\n\n    def __or__(self, other):\n        return Union[self, other]\n\n    def __ror__(self, other):\n        return Union[other, self]\n\n\n# Python-version-specific alias (Python 2: unicode; Python 3: str)\nText = str\n\n\n# Constant that's True when type checking, but False here.\nTYPE_CHECKING = False\n\n\nclass IO(Generic[AnyStr]):\n    \"\"\"Generic base class for TextIO and BinaryIO.\n\n    This is an abstract, generic version of the return of open().\n\n    NOTE: This does not distinguish between the different possible\n    classes (text vs. binary, read vs. write vs. read/write,\n    append-only, unbuffered).  The TextIO and BinaryIO subclasses\n    below capture the distinctions between text vs. binary, which is\n    pervasive in the interface; however we currently do not offer a\n    way to track the other distinctions in the type system.\n    \"\"\"\n\n    __slots__ = ()\n\n    @property\n    @abstractmethod\n    def mode(self) -> str:\n        pass\n\n    @property\n    @abstractmethod\n    def name(self) -> str:\n        pass\n\n    @abstractmethod\n    def close(self) -> None:\n        pass\n\n    @property\n    @abstractmethod\n    def closed(self) -> bool:\n        pass\n\n    @abstractmethod\n    def fileno(self) -> int:\n        pass\n\n    @abstractmethod\n    def flush(self) -> None:\n        pass\n\n    @abstractmethod\n    def isatty(self) -> bool:\n        pass\n\n    @abstractmethod\n    def read(self, n: int = -1) -> AnyStr:\n        pass\n\n    @abstractmethod\n    def readable(self) -> bool:\n        pass\n\n    @abstractmethod\n    def readline(self, limit: int = -1) -> AnyStr:\n        pass\n\n    @abstractmethod\n    def readlines(self, hint: int = -1) -> List[AnyStr]:\n        pass\n\n    @abstractmethod\n    def seek(self, offset: int, whence: int = 0) -> int:\n        pass\n\n    @abstractmethod\n    def seekable(self) -> bool:\n        pass\n\n    @abstractmethod\n    def tell(self) -> int:\n        pass\n\n    @abstractmethod\n    def truncate(self, size: int = None) -> int:\n        pass\n\n    @abstractmethod\n    def writable(self) -> bool:\n        pass\n\n    @abstractmethod\n    def write(self, s: AnyStr) -> int:\n        pass\n\n    @abstractmethod\n    def writelines(self, lines: List[AnyStr]) -> None:\n        pass\n\n    @abstractmethod\n    def __enter__(self) -> 'IO[AnyStr]':\n        pass\n\n    @abstractmethod\n    def __exit__(self, type, value, traceback) -> None:\n        pass\n\n\nclass BinaryIO(IO[bytes]):\n    \"\"\"Typed version of the return of open() in binary mode.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def write(self, s: Union[bytes, bytearray]) -> int:\n        pass\n\n    @abstractmethod\n    def __enter__(self) -> 'BinaryIO':\n        pass\n\n\nclass TextIO(IO[str]):\n    \"\"\"Typed version of the return of open() in text mode.\"\"\"\n\n    __slots__ = ()\n\n    @property\n    @abstractmethod\n    def buffer(self) -> BinaryIO:\n        pass\n\n    @property\n    @abstractmethod\n    def encoding(self) -> str:\n        pass\n\n    @property\n    @abstractmethod\n    def errors(self) -> Optional[str]:\n        pass\n\n    @property\n    @abstractmethod\n    def line_buffering(self) -> bool:\n        pass\n\n    @property\n    @abstractmethod\n    def newlines(self) -> Any:\n        pass\n\n    @abstractmethod\n    def __enter__(self) -> 'TextIO':\n        pass\n\n\nclass _DeprecatedType(type):\n    def __getattribute__(cls, name):\n        if name not in {\"__dict__\", \"__module__\", \"__doc__\"} and name in cls.__dict__:\n            warnings.warn(\n                f\"{cls.__name__} is deprecated, import directly \"\n                f\"from typing instead. {cls.__name__} will be removed \"\n                \"in Python 3.12.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        return super().__getattribute__(name)\n\n\nclass io(metaclass=_DeprecatedType):\n    \"\"\"Wrapper namespace for IO generic classes.\"\"\"\n\n    __all__ = ['IO', 'TextIO', 'BinaryIO']\n    IO = IO\n    TextIO = TextIO\n    BinaryIO = BinaryIO\n\n\nio.__name__ = __name__ + '.io'\nsys.modules[io.__name__] = io\n\nPattern = _alias(stdlib_re.Pattern, 1)\nMatch = _alias(stdlib_re.Match, 1)\n\nclass re(metaclass=_DeprecatedType):\n    \"\"\"Wrapper namespace for re type aliases.\"\"\"\n\n    __all__ = ['Pattern', 'Match']\n    Pattern = Pattern\n    Match = Match\n\n\nre.__name__ = __name__ + '.re'\nsys.modules[re.__name__] = re\n\n\ndef reveal_type(obj: T, /) -> T:\n    \"\"\"Ask a static type checker to reveal the inferred type of an expression.\n\n    When a static type checker encounters a call to ``reveal_type()``,\n    it will emit the inferred type of the argument::\n\n        x: int = 1\n        reveal_type(x)\n\n    Running a static type checker (e.g., mypy) on this example\n    will produce output similar to 'Revealed type is \"builtins.int\"'.\n\n    At runtime, the function prints the runtime type of the\n    argument and returns the argument unchanged.\n    \"\"\"\n    print(f\"Runtime type is {type(obj).__name__!r}\", file=sys.stderr)\n    return obj\n\n\ndef dataclass_transform(\n    *,\n    eq_default: bool = True,\n    order_default: bool = False,\n    kw_only_default: bool = False,\n    field_specifiers: tuple[type[Any] | Callable[..., Any], ...] = (),\n    **kwargs: Any,\n) -> Callable[[T], T]:\n    \"\"\"Decorator to mark an object as providing dataclass-like behaviour.\n\n    The decorator can be applied to a function, class, or metaclass.\n\n    Example usage with a decorator function::\n\n        T = TypeVar(\"T\")\n\n        @dataclass_transform()\n        def create_model(cls: type[T]) -> type[T]:\n            ...\n            return cls\n\n        @create_model\n        class CustomerModel:\n            id: int\n            name: str\n\n    On a base class::\n\n        @dataclass_transform()\n        class ModelBase: ...\n\n        class CustomerModel(ModelBase):\n            id: int\n            name: str\n\n    On a metaclass::\n\n        @dataclass_transform()\n        class ModelMeta(type): ...\n\n        class ModelBase(metaclass=ModelMeta): ...\n\n        class CustomerModel(ModelBase):\n            id: int\n            name: str\n\n    The ``CustomerModel`` classes defined above will\n    be treated by type checkers similarly to classes created with\n    ``@dataclasses.dataclass``.\n    For example, type checkers will assume these classes have\n    ``__init__`` methods that accept ``id`` and ``name``.\n\n    The arguments to this decorator can be used to customize this behavior:\n    - ``eq_default`` indicates whether the ``eq`` parameter is assumed to be\n        ``True`` or ``False`` if it is omitted by the caller.\n    - ``order_default`` indicates whether the ``order`` parameter is\n        assumed to be True or False if it is omitted by the caller.\n    - ``kw_only_default`` indicates whether the ``kw_only`` parameter is\n        assumed to be True or False if it is omitted by the caller.\n    - ``field_specifiers`` specifies a static list of supported classes\n        or functions that describe fields, similar to ``dataclasses.field()``.\n    - Arbitrary other keyword arguments are accepted in order to allow for\n        possible future extensions.\n\n    At runtime, this decorator records its arguments in the\n    ``__dataclass_transform__`` attribute on the decorated object.\n    It has no other runtime effect.\n\n    See PEP 681 for more details.\n    \"\"\"\n    def decorator(cls_or_fn):\n        cls_or_fn.__dataclass_transform__ = {\n            \"eq_default\": eq_default,\n            \"order_default\": order_default,\n            \"kw_only_default\": kw_only_default,\n            \"field_specifiers\": field_specifiers,\n            \"kwargs\": kwargs,\n        }\n        return cls_or_fn\n    return decorator\n", 3538], "/usr/local/lib/python3.11/site-packages/redis/_parsers/helpers.py": ["import datetime\n\nfrom redis.utils import str_if_bytes\n\n\ndef timestamp_to_datetime(response):\n    \"Converts a unix timestamp to a Python datetime object\"\n    if not response:\n        return None\n    try:\n        response = int(response)\n    except ValueError:\n        return None\n    return datetime.datetime.fromtimestamp(response)\n\n\ndef parse_debug_object(response):\n    \"Parse the results of Redis's DEBUG OBJECT command into a Python dict\"\n    # The 'type' of the object is the first item in the response, but isn't\n    # prefixed with a name\n    response = str_if_bytes(response)\n    response = \"type:\" + response\n    response = dict(kv.split(\":\") for kv in response.split())\n\n    # parse some expected int values from the string response\n    # note: this cmd isn't spec'd so these may not appear in all redis versions\n    int_fields = (\"refcount\", \"serializedlength\", \"lru\", \"lru_seconds_idle\")\n    for field in int_fields:\n        if field in response:\n            response[field] = int(response[field])\n\n    return response\n\n\ndef parse_info(response):\n    \"\"\"Parse the result of Redis's INFO command into a Python dict\"\"\"\n    info = {}\n    response = str_if_bytes(response)\n\n    def get_value(value):\n        if \",\" not in value and \"=\" not in value:\n            try:\n                if \".\" in value:\n                    return float(value)\n                else:\n                    return int(value)\n            except ValueError:\n                return value\n        elif \"=\" not in value:\n            return [get_value(v) for v in value.split(\",\") if v]\n        else:\n            sub_dict = {}\n            for item in value.split(\",\"):\n                if not item:\n                    continue\n                if \"=\" in item:\n                    k, v = item.rsplit(\"=\", 1)\n                    sub_dict[k] = get_value(v)\n                else:\n                    sub_dict[item] = True\n            return sub_dict\n\n    for line in response.splitlines():\n        if line and not line.startswith(\"#\"):\n            if line.find(\":\") != -1:\n                # Split, the info fields keys and values.\n                # Note that the value may contain ':'. but the 'host:'\n                # pseudo-command is the only case where the key contains ':'\n                key, value = line.split(\":\", 1)\n                if key == \"cmdstat_host\":\n                    key, value = line.rsplit(\":\", 1)\n\n                if key == \"module\":\n                    # Hardcode a list for key 'modules' since there could be\n                    # multiple lines that started with 'module'\n                    info.setdefault(\"modules\", []).append(get_value(value))\n                else:\n                    info[key] = get_value(value)\n            else:\n                # if the line isn't splittable, append it to the \"__raw__\" key\n                info.setdefault(\"__raw__\", []).append(line)\n\n    return info\n\n\ndef parse_memory_stats(response, **kwargs):\n    \"\"\"Parse the results of MEMORY STATS\"\"\"\n    stats = pairs_to_dict(response, decode_keys=True, decode_string_values=True)\n    for key, value in stats.items():\n        if key.startswith(\"db.\") and isinstance(value, list):\n            stats[key] = pairs_to_dict(\n                value, decode_keys=True, decode_string_values=True\n            )\n    return stats\n\n\nSENTINEL_STATE_TYPES = {\n    \"can-failover-its-master\": int,\n    \"config-epoch\": int,\n    \"down-after-milliseconds\": int,\n    \"failover-timeout\": int,\n    \"info-refresh\": int,\n    \"last-hello-message\": int,\n    \"last-ok-ping-reply\": int,\n    \"last-ping-reply\": int,\n    \"last-ping-sent\": int,\n    \"master-link-down-time\": int,\n    \"master-port\": int,\n    \"num-other-sentinels\": int,\n    \"num-slaves\": int,\n    \"o-down-time\": int,\n    \"pending-commands\": int,\n    \"parallel-syncs\": int,\n    \"port\": int,\n    \"quorum\": int,\n    \"role-reported-time\": int,\n    \"s-down-time\": int,\n    \"slave-priority\": int,\n    \"slave-repl-offset\": int,\n    \"voted-leader-epoch\": int,\n}\n\n\ndef parse_sentinel_state(item):\n    result = pairs_to_dict_typed(item, SENTINEL_STATE_TYPES)\n    flags = set(result[\"flags\"].split(\",\"))\n    for name, flag in (\n        (\"is_master\", \"master\"),\n        (\"is_slave\", \"slave\"),\n        (\"is_sdown\", \"s_down\"),\n        (\"is_odown\", \"o_down\"),\n        (\"is_sentinel\", \"sentinel\"),\n        (\"is_disconnected\", \"disconnected\"),\n        (\"is_master_down\", \"master_down\"),\n    ):\n        result[name] = flag in flags\n    return result\n\n\ndef parse_sentinel_master(response):\n    return parse_sentinel_state(map(str_if_bytes, response))\n\n\ndef parse_sentinel_state_resp3(response):\n    result = {}\n    for key in response:\n        try:\n            value = SENTINEL_STATE_TYPES[key](str_if_bytes(response[key]))\n            result[str_if_bytes(key)] = value\n        except Exception:\n            result[str_if_bytes(key)] = response[str_if_bytes(key)]\n    flags = set(result[\"flags\"].split(\",\"))\n    result[\"flags\"] = flags\n    return result\n\n\ndef parse_sentinel_masters(response):\n    result = {}\n    for item in response:\n        state = parse_sentinel_state(map(str_if_bytes, item))\n        result[state[\"name\"]] = state\n    return result\n\n\ndef parse_sentinel_masters_resp3(response):\n    return [parse_sentinel_state(master) for master in response]\n\n\ndef parse_sentinel_slaves_and_sentinels(response):\n    return [parse_sentinel_state(map(str_if_bytes, item)) for item in response]\n\n\ndef parse_sentinel_slaves_and_sentinels_resp3(response):\n    return [parse_sentinel_state_resp3(item) for item in response]\n\n\ndef parse_sentinel_get_master(response):\n    return response and (response[0], int(response[1])) or None\n\n\ndef pairs_to_dict(response, decode_keys=False, decode_string_values=False):\n    \"\"\"Create a dict given a list of key/value pairs\"\"\"\n    if response is None:\n        return {}\n    if decode_keys or decode_string_values:\n        # the iter form is faster, but I don't know how to make that work\n        # with a str_if_bytes() map\n        keys = response[::2]\n        if decode_keys:\n            keys = map(str_if_bytes, keys)\n        values = response[1::2]\n        if decode_string_values:\n            values = map(str_if_bytes, values)\n        return dict(zip(keys, values))\n    else:\n        it = iter(response)\n        return dict(zip(it, it))\n\n\ndef pairs_to_dict_typed(response, type_info):\n    it = iter(response)\n    result = {}\n    for key, value in zip(it, it):\n        if key in type_info:\n            try:\n                value = type_info[key](value)\n            except Exception:\n                # if for some reason the value can't be coerced, just use\n                # the string value\n                pass\n        result[key] = value\n    return result\n\n\ndef zset_score_pairs(response, **options):\n    \"\"\"\n    If ``withscores`` is specified in the options, return the response as\n    a list of (value, score) pairs\n    \"\"\"\n    if not response or not options.get(\"withscores\"):\n        return response\n    score_cast_func = options.get(\"score_cast_func\", float)\n    it = iter(response)\n    return list(zip(it, map(score_cast_func, it)))\n\n\ndef sort_return_tuples(response, **options):\n    \"\"\"\n    If ``groups`` is specified, return the response as a list of\n    n-element tuples with n being the value found in options['groups']\n    \"\"\"\n    if not response or not options.get(\"groups\"):\n        return response\n    n = options[\"groups\"]\n    return list(zip(*[response[i::n] for i in range(n)]))\n\n\ndef parse_stream_list(response):\n    if response is None:\n        return None\n    data = []\n    for r in response:\n        if r is not None:\n            data.append((r[0], pairs_to_dict(r[1])))\n        else:\n            data.append((None, None))\n    return data\n\n\ndef pairs_to_dict_with_str_keys(response):\n    return pairs_to_dict(response, decode_keys=True)\n\n\ndef parse_list_of_dicts(response):\n    return list(map(pairs_to_dict_with_str_keys, response))\n\n\ndef parse_xclaim(response, **options):\n    if options.get(\"parse_justid\", False):\n        return response\n    return parse_stream_list(response)\n\n\ndef parse_xautoclaim(response, **options):\n    if options.get(\"parse_justid\", False):\n        return response[1]\n    response[1] = parse_stream_list(response[1])\n    return response\n\n\ndef parse_xinfo_stream(response, **options):\n    if isinstance(response, list):\n        data = pairs_to_dict(response, decode_keys=True)\n    else:\n        data = {str_if_bytes(k): v for k, v in response.items()}\n    if not options.get(\"full\", False):\n        first = data.get(\"first-entry\")\n        if first is not None and first[0] is not None:\n            data[\"first-entry\"] = (first[0], pairs_to_dict(first[1]))\n        last = data[\"last-entry\"]\n        if last is not None and last[0] is not None:\n            data[\"last-entry\"] = (last[0], pairs_to_dict(last[1]))\n    else:\n        data[\"entries\"] = {_id: pairs_to_dict(entry) for _id, entry in data[\"entries\"]}\n        if len(data[\"groups\"]) > 0 and isinstance(data[\"groups\"][0], list):\n            data[\"groups\"] = [\n                pairs_to_dict(group, decode_keys=True) for group in data[\"groups\"]\n            ]\n            for g in data[\"groups\"]:\n                if g[\"consumers\"] and g[\"consumers\"][0] is not None:\n                    g[\"consumers\"] = [\n                        pairs_to_dict(c, decode_keys=True) for c in g[\"consumers\"]\n                    ]\n        else:\n            data[\"groups\"] = [\n                {str_if_bytes(k): v for k, v in group.items()}\n                for group in data[\"groups\"]\n            ]\n    return data\n\n\ndef parse_xread(response):\n    if response is None:\n        return []\n    return [[r[0], parse_stream_list(r[1])] for r in response]\n\n\ndef parse_xread_resp3(response):\n    if response is None:\n        return {}\n    return {key: [parse_stream_list(value)] for key, value in response.items()}\n\n\ndef parse_xpending(response, **options):\n    if options.get(\"parse_detail\", False):\n        return parse_xpending_range(response)\n    consumers = [{\"name\": n, \"pending\": int(p)} for n, p in response[3] or []]\n    return {\n        \"pending\": response[0],\n        \"min\": response[1],\n        \"max\": response[2],\n        \"consumers\": consumers,\n    }\n\n\ndef parse_xpending_range(response):\n    k = (\"message_id\", \"consumer\", \"time_since_delivered\", \"times_delivered\")\n    return [dict(zip(k, r)) for r in response]\n\n\ndef float_or_none(response):\n    if response is None:\n        return None\n    return float(response)\n\n\ndef bool_ok(response, **options):\n    return str_if_bytes(response) == \"OK\"\n\n\ndef parse_zadd(response, **options):\n    if response is None:\n        return None\n    if options.get(\"as_score\"):\n        return float(response)\n    return int(response)\n\n\ndef parse_client_list(response, **options):\n    clients = []\n    for c in str_if_bytes(response).splitlines():\n        # Values might contain '='\n        clients.append(dict(pair.split(\"=\", 1) for pair in c.split(\" \")))\n    return clients\n\n\ndef parse_config_get(response, **options):\n    response = [str_if_bytes(i) if i is not None else None for i in response]\n    return response and pairs_to_dict(response) or {}\n\n\ndef parse_scan(response, **options):\n    cursor, r = response\n    return int(cursor), r\n\n\ndef parse_hscan(response, **options):\n    cursor, r = response\n    no_values = options.get(\"no_values\", False)\n    if no_values:\n        payload = r or []\n    else:\n        payload = r and pairs_to_dict(r) or {}\n    return int(cursor), payload\n\n\ndef parse_zscan(response, **options):\n    score_cast_func = options.get(\"score_cast_func\", float)\n    cursor, r = response\n    it = iter(r)\n    return int(cursor), list(zip(it, map(score_cast_func, it)))\n\n\ndef parse_zmscore(response, **options):\n    # zmscore: list of scores (double precision floating point number) or nil\n    return [float(score) if score is not None else None for score in response]\n\n\ndef parse_slowlog_get(response, **options):\n    space = \" \" if options.get(\"decode_responses\", False) else b\" \"\n\n    def parse_item(item):\n        result = {\"id\": item[0], \"start_time\": int(item[1]), \"duration\": int(item[2])}\n        # Redis Enterprise injects another entry at index [3], which has\n        # the complexity info (i.e. the value N in case the command has\n        # an O(N) complexity) instead of the command.\n        if isinstance(item[3], list):\n            result[\"command\"] = space.join(item[3])\n\n            # These fields are optional, depends on environment.\n            if len(item) >= 6:\n                result[\"client_address\"] = item[4]\n                result[\"client_name\"] = item[5]\n        else:\n            result[\"complexity\"] = item[3]\n            result[\"command\"] = space.join(item[4])\n\n            # These fields are optional, depends on environment.\n            if len(item) >= 7:\n                result[\"client_address\"] = item[5]\n                result[\"client_name\"] = item[6]\n\n        return result\n\n    return [parse_item(item) for item in response]\n\n\ndef parse_stralgo(response, **options):\n    \"\"\"\n    Parse the response from `STRALGO` command.\n    Without modifiers the returned value is string.\n    When LEN is given the command returns the length of the result\n    (i.e integer).\n    When IDX is given the command returns a dictionary with the LCS\n    length and all the ranges in both the strings, start and end\n    offset for each string, where there are matches.\n    When WITHMATCHLEN is given, each array representing a match will\n    also have the length of the match at the beginning of the array.\n    \"\"\"\n    if options.get(\"len\", False):\n        return int(response)\n    if options.get(\"idx\", False):\n        if options.get(\"withmatchlen\", False):\n            matches = [\n                [(int(match[-1]))] + list(map(tuple, match[:-1]))\n                for match in response[1]\n            ]\n        else:\n            matches = [list(map(tuple, match)) for match in response[1]]\n        return {\n            str_if_bytes(response[0]): matches,\n            str_if_bytes(response[2]): int(response[3]),\n        }\n    return str_if_bytes(response)\n\n\ndef parse_cluster_info(response, **options):\n    response = str_if_bytes(response)\n    return dict(line.split(\":\") for line in response.splitlines() if line)\n\n\ndef _parse_node_line(line):\n    line_items = line.split(\" \")\n    node_id, addr, flags, master_id, ping, pong, epoch, connected = line.split(\" \")[:8]\n    ip = addr.split(\"@\")[0]\n    hostname = addr.split(\"@\")[1].split(\",\")[1] if \"@\" in addr and \",\" in addr else \"\"\n    node_dict = {\n        \"node_id\": node_id,\n        \"hostname\": hostname,\n        \"flags\": flags,\n        \"master_id\": master_id,\n        \"last_ping_sent\": ping,\n        \"last_pong_rcvd\": pong,\n        \"epoch\": epoch,\n        \"slots\": [],\n        \"migrations\": [],\n        \"connected\": True if connected == \"connected\" else False,\n    }\n    if len(line_items) >= 9:\n        slots, migrations = _parse_slots(line_items[8:])\n        node_dict[\"slots\"], node_dict[\"migrations\"] = slots, migrations\n    return ip, node_dict\n\n\ndef _parse_slots(slot_ranges):\n    slots, migrations = [], []\n    for s_range in slot_ranges:\n        if \"->-\" in s_range:\n            slot_id, dst_node_id = s_range[1:-1].split(\"->-\", 1)\n            migrations.append(\n                {\"slot\": slot_id, \"node_id\": dst_node_id, \"state\": \"migrating\"}\n            )\n        elif \"-<-\" in s_range:\n            slot_id, src_node_id = s_range[1:-1].split(\"-<-\", 1)\n            migrations.append(\n                {\"slot\": slot_id, \"node_id\": src_node_id, \"state\": \"importing\"}\n            )\n        else:\n            s_range = [sl for sl in s_range.split(\"-\")]\n            slots.append(s_range)\n\n    return slots, migrations\n\n\ndef parse_cluster_nodes(response, **options):\n    \"\"\"\n    @see: https://redis.io/commands/cluster-nodes  # string / bytes\n    @see: https://redis.io/commands/cluster-replicas # list of string / bytes\n    \"\"\"\n    if isinstance(response, (str, bytes)):\n        response = response.splitlines()\n    return dict(_parse_node_line(str_if_bytes(node)) for node in response)\n\n\ndef parse_geosearch_generic(response, **options):\n    \"\"\"\n    Parse the response of 'GEOSEARCH', GEORADIUS' and 'GEORADIUSBYMEMBER'\n    commands according to 'withdist', 'withhash' and 'withcoord' labels.\n    \"\"\"\n    try:\n        if options[\"store\"] or options[\"store_dist\"]:\n            # `store` and `store_dist` cant be combined\n            # with other command arguments.\n            # relevant to 'GEORADIUS' and 'GEORADIUSBYMEMBER'\n            return response\n    except KeyError:  # it means the command was sent via execute_command\n        return response\n\n    if not isinstance(response, list):\n        response_list = [response]\n    else:\n        response_list = response\n\n    if not options[\"withdist\"] and not options[\"withcoord\"] and not options[\"withhash\"]:\n        # just a bunch of places\n        return response_list\n\n    cast = {\n        \"withdist\": float,\n        \"withcoord\": lambda ll: (float(ll[0]), float(ll[1])),\n        \"withhash\": int,\n    }\n\n    # zip all output results with each casting function to get\n    # the properly native Python value.\n    f = [lambda x: x]\n    f += [cast[o] for o in [\"withdist\", \"withhash\", \"withcoord\"] if options[o]]\n    return [list(map(lambda fv: fv[0](fv[1]), zip(f, r))) for r in response_list]\n\n\ndef parse_command(response, **options):\n    commands = {}\n    for command in response:\n        cmd_dict = {}\n        cmd_name = str_if_bytes(command[0])\n        cmd_dict[\"name\"] = cmd_name\n        cmd_dict[\"arity\"] = int(command[1])\n        cmd_dict[\"flags\"] = [str_if_bytes(flag) for flag in command[2]]\n        cmd_dict[\"first_key_pos\"] = command[3]\n        cmd_dict[\"last_key_pos\"] = command[4]\n        cmd_dict[\"step_count\"] = command[5]\n        if len(command) > 7:\n            cmd_dict[\"tips\"] = command[7]\n            cmd_dict[\"key_specifications\"] = command[8]\n            cmd_dict[\"subcommands\"] = command[9]\n        commands[cmd_name] = cmd_dict\n    return commands\n\n\ndef parse_command_resp3(response, **options):\n    commands = {}\n    for command in response:\n        cmd_dict = {}\n        cmd_name = str_if_bytes(command[0])\n        cmd_dict[\"name\"] = cmd_name\n        cmd_dict[\"arity\"] = command[1]\n        cmd_dict[\"flags\"] = {str_if_bytes(flag) for flag in command[2]}\n        cmd_dict[\"first_key_pos\"] = command[3]\n        cmd_dict[\"last_key_pos\"] = command[4]\n        cmd_dict[\"step_count\"] = command[5]\n        cmd_dict[\"acl_categories\"] = command[6]\n        if len(command) > 7:\n            cmd_dict[\"tips\"] = command[7]\n            cmd_dict[\"key_specifications\"] = command[8]\n            cmd_dict[\"subcommands\"] = command[9]\n\n        commands[cmd_name] = cmd_dict\n    return commands\n\n\ndef parse_pubsub_numsub(response, **options):\n    return list(zip(response[0::2], response[1::2]))\n\n\ndef parse_client_kill(response, **options):\n    if isinstance(response, int):\n        return response\n    return str_if_bytes(response) == \"OK\"\n\n\ndef parse_acl_getuser(response, **options):\n    if response is None:\n        return None\n    if isinstance(response, list):\n        data = pairs_to_dict(response, decode_keys=True)\n    else:\n        data = {str_if_bytes(key): value for key, value in response.items()}\n\n    # convert everything but user-defined data in 'keys' to native strings\n    data[\"flags\"] = list(map(str_if_bytes, data[\"flags\"]))\n    data[\"passwords\"] = list(map(str_if_bytes, data[\"passwords\"]))\n    data[\"commands\"] = str_if_bytes(data[\"commands\"])\n    if isinstance(data[\"keys\"], str) or isinstance(data[\"keys\"], bytes):\n        data[\"keys\"] = list(str_if_bytes(data[\"keys\"]).split(\" \"))\n    if data[\"keys\"] == [\"\"]:\n        data[\"keys\"] = []\n    if \"channels\" in data:\n        if isinstance(data[\"channels\"], str) or isinstance(data[\"channels\"], bytes):\n            data[\"channels\"] = list(str_if_bytes(data[\"channels\"]).split(\" \"))\n        if data[\"channels\"] == [\"\"]:\n            data[\"channels\"] = []\n    if \"selectors\" in data:\n        if data[\"selectors\"] != [] and isinstance(data[\"selectors\"][0], list):\n            data[\"selectors\"] = [\n                list(map(str_if_bytes, selector)) for selector in data[\"selectors\"]\n            ]\n        elif data[\"selectors\"] != []:\n            data[\"selectors\"] = [\n                {str_if_bytes(k): str_if_bytes(v) for k, v in selector.items()}\n                for selector in data[\"selectors\"]\n            ]\n\n    # split 'commands' into separate 'categories' and 'commands' lists\n    commands, categories = [], []\n    for command in data[\"commands\"].split(\" \"):\n        categories.append(command) if \"@\" in command else commands.append(command)\n\n    data[\"commands\"] = commands\n    data[\"categories\"] = categories\n    data[\"enabled\"] = \"on\" in data[\"flags\"]\n    return data\n\n\ndef parse_acl_log(response, **options):\n    if response is None:\n        return None\n    if isinstance(response, list):\n        data = []\n        for log in response:\n            log_data = pairs_to_dict(log, True, True)\n            client_info = log_data.get(\"client-info\", \"\")\n            log_data[\"client-info\"] = parse_client_info(client_info)\n\n            # float() is lossy comparing to the \"double\" in C\n            log_data[\"age-seconds\"] = float(log_data[\"age-seconds\"])\n            data.append(log_data)\n    else:\n        data = bool_ok(response)\n    return data\n\n\ndef parse_client_info(value):\n    \"\"\"\n    Parsing client-info in ACL Log in following format.\n    \"key1=value1 key2=value2 key3=value3\"\n    \"\"\"\n    client_info = {}\n    for info in str_if_bytes(value).strip().split():\n        key, value = info.split(\"=\")\n        client_info[key] = value\n\n    # Those fields are defined as int in networking.c\n    for int_key in {\n        \"id\",\n        \"age\",\n        \"idle\",\n        \"db\",\n        \"sub\",\n        \"psub\",\n        \"multi\",\n        \"qbuf\",\n        \"qbuf-free\",\n        \"obl\",\n        \"argv-mem\",\n        \"oll\",\n        \"omem\",\n        \"tot-mem\",\n    }:\n        if int_key in client_info:\n            client_info[int_key] = int(client_info[int_key])\n    return client_info\n\n\ndef parse_set_result(response, **options):\n    \"\"\"\n    Handle SET result since GET argument is available since Redis 6.2.\n    Parsing SET result into:\n    - BOOL\n    - String when GET argument is used\n    \"\"\"\n    if options.get(\"get\"):\n        # Redis will return a getCommand result.\n        # See `setGenericCommand` in t_string.c\n        return response\n    return response and str_if_bytes(response) == \"OK\"\n\n\ndef string_keys_to_dict(key_string, callback):\n    return dict.fromkeys(key_string.split(), callback)\n\n\n_RedisCallbacks = {\n    **string_keys_to_dict(\n        \"AUTH COPY EXPIRE EXPIREAT HEXISTS HMSET MOVE MSETNX PERSIST PSETEX \"\n        \"PEXPIRE PEXPIREAT RENAMENX SETEX SETNX SMOVE\",\n        bool,\n    ),\n    **string_keys_to_dict(\"HINCRBYFLOAT INCRBYFLOAT\", float),\n    **string_keys_to_dict(\n        \"ASKING FLUSHALL FLUSHDB LSET LTRIM MSET PFMERGE READONLY READWRITE \"\n        \"RENAME SAVE SELECT SHUTDOWN SLAVEOF SWAPDB WATCH UNWATCH\",\n        bool_ok,\n    ),\n    **string_keys_to_dict(\"XREAD XREADGROUP\", parse_xread),\n    **string_keys_to_dict(\n        \"GEORADIUS GEORADIUSBYMEMBER GEOSEARCH\",\n        parse_geosearch_generic,\n    ),\n    **string_keys_to_dict(\"XRANGE XREVRANGE\", parse_stream_list),\n    \"ACL GETUSER\": parse_acl_getuser,\n    \"ACL LOAD\": bool_ok,\n    \"ACL LOG\": parse_acl_log,\n    \"ACL SETUSER\": bool_ok,\n    \"ACL SAVE\": bool_ok,\n    \"CLIENT INFO\": parse_client_info,\n    \"CLIENT KILL\": parse_client_kill,\n    \"CLIENT LIST\": parse_client_list,\n    \"CLIENT PAUSE\": bool_ok,\n    \"CLIENT SETINFO\": bool_ok,\n    \"CLIENT SETNAME\": bool_ok,\n    \"CLIENT UNBLOCK\": bool,\n    \"CLUSTER ADDSLOTS\": bool_ok,\n    \"CLUSTER ADDSLOTSRANGE\": bool_ok,\n    \"CLUSTER DELSLOTS\": bool_ok,\n    \"CLUSTER DELSLOTSRANGE\": bool_ok,\n    \"CLUSTER FAILOVER\": bool_ok,\n    \"CLUSTER FORGET\": bool_ok,\n    \"CLUSTER INFO\": parse_cluster_info,\n    \"CLUSTER MEET\": bool_ok,\n    \"CLUSTER NODES\": parse_cluster_nodes,\n    \"CLUSTER REPLICAS\": parse_cluster_nodes,\n    \"CLUSTER REPLICATE\": bool_ok,\n    \"CLUSTER RESET\": bool_ok,\n    \"CLUSTER SAVECONFIG\": bool_ok,\n    \"CLUSTER SET-CONFIG-EPOCH\": bool_ok,\n    \"CLUSTER SETSLOT\": bool_ok,\n    \"CLUSTER SLAVES\": parse_cluster_nodes,\n    \"COMMAND\": parse_command,\n    \"CONFIG RESETSTAT\": bool_ok,\n    \"CONFIG SET\": bool_ok,\n    \"FUNCTION DELETE\": bool_ok,\n    \"FUNCTION FLUSH\": bool_ok,\n    \"FUNCTION RESTORE\": bool_ok,\n    \"GEODIST\": float_or_none,\n    \"HSCAN\": parse_hscan,\n    \"INFO\": parse_info,\n    \"LASTSAVE\": timestamp_to_datetime,\n    \"MEMORY PURGE\": bool_ok,\n    \"MODULE LOAD\": bool,\n    \"MODULE UNLOAD\": bool,\n    \"PING\": lambda r: str_if_bytes(r) == \"PONG\",\n    \"PUBSUB NUMSUB\": parse_pubsub_numsub,\n    \"PUBSUB SHARDNUMSUB\": parse_pubsub_numsub,\n    \"QUIT\": bool_ok,\n    \"SET\": parse_set_result,\n    \"SCAN\": parse_scan,\n    \"SCRIPT EXISTS\": lambda r: list(map(bool, r)),\n    \"SCRIPT FLUSH\": bool_ok,\n    \"SCRIPT KILL\": bool_ok,\n    \"SCRIPT LOAD\": str_if_bytes,\n    \"SENTINEL CKQUORUM\": bool_ok,\n    \"SENTINEL FAILOVER\": bool_ok,\n    \"SENTINEL FLUSHCONFIG\": bool_ok,\n    \"SENTINEL GET-MASTER-ADDR-BY-NAME\": parse_sentinel_get_master,\n    \"SENTINEL MONITOR\": bool_ok,\n    \"SENTINEL RESET\": bool_ok,\n    \"SENTINEL REMOVE\": bool_ok,\n    \"SENTINEL SET\": bool_ok,\n    \"SLOWLOG GET\": parse_slowlog_get,\n    \"SLOWLOG RESET\": bool_ok,\n    \"SORT\": sort_return_tuples,\n    \"SSCAN\": parse_scan,\n    \"TIME\": lambda x: (int(x[0]), int(x[1])),\n    \"XAUTOCLAIM\": parse_xautoclaim,\n    \"XCLAIM\": parse_xclaim,\n    \"XGROUP CREATE\": bool_ok,\n    \"XGROUP DESTROY\": bool,\n    \"XGROUP SETID\": bool_ok,\n    \"XINFO STREAM\": parse_xinfo_stream,\n    \"XPENDING\": parse_xpending,\n    \"ZSCAN\": parse_zscan,\n}\n\n\n_RedisCallbacksRESP2 = {\n    **string_keys_to_dict(\n        \"SDIFF SINTER SMEMBERS SUNION\", lambda r: r and set(r) or set()\n    ),\n    **string_keys_to_dict(\n        \"ZDIFF ZINTER ZPOPMAX ZPOPMIN ZRANGE ZRANGEBYSCORE ZRANK ZREVRANGE \"\n        \"ZREVRANGEBYSCORE ZREVRANK ZUNION\",\n        zset_score_pairs,\n    ),\n    **string_keys_to_dict(\"ZINCRBY ZSCORE\", float_or_none),\n    **string_keys_to_dict(\"BGREWRITEAOF BGSAVE\", lambda r: True),\n    **string_keys_to_dict(\"BLPOP BRPOP\", lambda r: r and tuple(r) or None),\n    **string_keys_to_dict(\n        \"BZPOPMAX BZPOPMIN\", lambda r: r and (r[0], r[1], float(r[2])) or None\n    ),\n    \"ACL CAT\": lambda r: list(map(str_if_bytes, r)),\n    \"ACL GENPASS\": str_if_bytes,\n    \"ACL HELP\": lambda r: list(map(str_if_bytes, r)),\n    \"ACL LIST\": lambda r: list(map(str_if_bytes, r)),\n    \"ACL USERS\": lambda r: list(map(str_if_bytes, r)),\n    \"ACL WHOAMI\": str_if_bytes,\n    \"CLIENT GETNAME\": str_if_bytes,\n    \"CLIENT TRACKINGINFO\": lambda r: list(map(str_if_bytes, r)),\n    \"CLUSTER GETKEYSINSLOT\": lambda r: list(map(str_if_bytes, r)),\n    \"COMMAND GETKEYS\": lambda r: list(map(str_if_bytes, r)),\n    \"CONFIG GET\": parse_config_get,\n    \"DEBUG OBJECT\": parse_debug_object,\n    \"GEOHASH\": lambda r: list(map(str_if_bytes, r)),\n    \"GEOPOS\": lambda r: list(\n        map(lambda ll: (float(ll[0]), float(ll[1])) if ll is not None else None, r)\n    ),\n    \"HGETALL\": lambda r: r and pairs_to_dict(r) or {},\n    \"MEMORY STATS\": parse_memory_stats,\n    \"MODULE LIST\": lambda r: [pairs_to_dict(m) for m in r],\n    \"RESET\": str_if_bytes,\n    \"SENTINEL MASTER\": parse_sentinel_master,\n    \"SENTINEL MASTERS\": parse_sentinel_masters,\n    \"SENTINEL SENTINELS\": parse_sentinel_slaves_and_sentinels,\n    \"SENTINEL SLAVES\": parse_sentinel_slaves_and_sentinels,\n    \"STRALGO\": parse_stralgo,\n    \"XINFO CONSUMERS\": parse_list_of_dicts,\n    \"XINFO GROUPS\": parse_list_of_dicts,\n    \"ZADD\": parse_zadd,\n    \"ZMSCORE\": parse_zmscore,\n}\n\n\n_RedisCallbacksRESP3 = {\n    **string_keys_to_dict(\n        \"SDIFF SINTER SMEMBERS SUNION\", lambda r: r and set(r) or set()\n    ),\n    **string_keys_to_dict(\n        \"ZRANGE ZINTER ZPOPMAX ZPOPMIN ZRANGEBYSCORE ZREVRANGE ZREVRANGEBYSCORE \"\n        \"ZUNION HGETALL XREADGROUP\",\n        lambda r, **kwargs: r,\n    ),\n    **string_keys_to_dict(\"XREAD XREADGROUP\", parse_xread_resp3),\n    \"ACL LOG\": lambda r: (\n        [\n            {str_if_bytes(key): str_if_bytes(value) for key, value in x.items()}\n            for x in r\n        ]\n        if isinstance(r, list)\n        else bool_ok(r)\n    ),\n    \"COMMAND\": parse_command_resp3,\n    \"CONFIG GET\": lambda r: {\n        str_if_bytes(key) if key is not None else None: (\n            str_if_bytes(value) if value is not None else None\n        )\n        for key, value in r.items()\n    },\n    \"MEMORY STATS\": lambda r: {str_if_bytes(key): value for key, value in r.items()},\n    \"SENTINEL MASTER\": parse_sentinel_state_resp3,\n    \"SENTINEL MASTERS\": parse_sentinel_masters_resp3,\n    \"SENTINEL SENTINELS\": parse_sentinel_slaves_and_sentinels_resp3,\n    \"SENTINEL SLAVES\": parse_sentinel_slaves_and_sentinels_resp3,\n    \"STRALGO\": lambda r, **options: (\n        {str_if_bytes(key): str_if_bytes(value) for key, value in r.items()}\n        if isinstance(r, dict)\n        else str_if_bytes(r)\n    ),\n    \"XINFO CONSUMERS\": lambda r: [\n        {str_if_bytes(key): value for key, value in x.items()} for x in r\n    ],\n    \"XINFO GROUPS\": lambda r: [\n        {str_if_bytes(key): value for key, value in d.items()} for d in r\n    ],\n}\n", 883], "<frozen abc>": ["# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Abstract Base Classes (ABCs) according to PEP 3119.\"\"\"\n\n\ndef abstractmethod(funcobj):\n    \"\"\"A decorator indicating abstract methods.\n\n    Requires that the metaclass is ABCMeta or derived from it.  A\n    class that has a metaclass derived from ABCMeta cannot be\n    instantiated unless all of its abstract methods are overridden.\n    The abstract methods can be called using any of the normal\n    'super' call mechanisms.  abstractmethod() may be used to declare\n    abstract methods for properties and descriptors.\n\n    Usage:\n\n        class C(metaclass=ABCMeta):\n            @abstractmethod\n            def my_abstract_method(self, arg1, arg2, argN):\n                ...\n    \"\"\"\n    funcobj.__isabstractmethod__ = True\n    return funcobj\n\n\nclass abstractclassmethod(classmethod):\n    \"\"\"A decorator indicating abstract classmethods.\n\n    Deprecated, use 'classmethod' with 'abstractmethod' instead:\n\n        class C(ABC):\n            @classmethod\n            @abstractmethod\n            def my_abstract_classmethod(cls, ...):\n                ...\n\n    \"\"\"\n\n    __isabstractmethod__ = True\n\n    def __init__(self, callable):\n        callable.__isabstractmethod__ = True\n        super().__init__(callable)\n\n\nclass abstractstaticmethod(staticmethod):\n    \"\"\"A decorator indicating abstract staticmethods.\n\n    Deprecated, use 'staticmethod' with 'abstractmethod' instead:\n\n        class C(ABC):\n            @staticmethod\n            @abstractmethod\n            def my_abstract_staticmethod(...):\n                ...\n\n    \"\"\"\n\n    __isabstractmethod__ = True\n\n    def __init__(self, callable):\n        callable.__isabstractmethod__ = True\n        super().__init__(callable)\n\n\nclass abstractproperty(property):\n    \"\"\"A decorator indicating abstract properties.\n\n    Deprecated, use 'property' with 'abstractmethod' instead:\n\n        class C(ABC):\n            @property\n            @abstractmethod\n            def my_abstract_property(self):\n                ...\n\n    \"\"\"\n\n    __isabstractmethod__ = True\n\n\ntry:\n    from _abc import (get_cache_token, _abc_init, _abc_register,\n                      _abc_instancecheck, _abc_subclasscheck, _get_dump,\n                      _reset_registry, _reset_caches)\nexcept ImportError:\n    from _py_abc import ABCMeta, get_cache_token\n    ABCMeta.__module__ = 'abc'\nelse:\n    class ABCMeta(type):\n        \"\"\"Metaclass for defining Abstract Base Classes (ABCs).\n\n        Use this metaclass to create an ABC.  An ABC can be subclassed\n        directly, and then acts as a mix-in class.  You can also register\n        unrelated concrete classes (even built-in classes) and unrelated\n        ABCs as 'virtual subclasses' -- these and their descendants will\n        be considered subclasses of the registering ABC by the built-in\n        issubclass() function, but the registering ABC won't show up in\n        their MRO (Method Resolution Order) nor will method\n        implementations defined by the registering ABC be callable (not\n        even via super()).\n        \"\"\"\n        def __new__(mcls, name, bases, namespace, /, **kwargs):\n            cls = super().__new__(mcls, name, bases, namespace, **kwargs)\n            _abc_init(cls)\n            return cls\n\n        def register(cls, subclass):\n            \"\"\"Register a virtual subclass of an ABC.\n\n            Returns the subclass, to allow usage as a class decorator.\n            \"\"\"\n            return _abc_register(cls, subclass)\n\n        def __instancecheck__(cls, instance):\n            \"\"\"Override for isinstance(instance, cls).\"\"\"\n            return _abc_instancecheck(cls, instance)\n\n        def __subclasscheck__(cls, subclass):\n            \"\"\"Override for issubclass(subclass, cls).\"\"\"\n            return _abc_subclasscheck(cls, subclass)\n\n        def _dump_registry(cls, file=None):\n            \"\"\"Debug helper to print the ABC registry.\"\"\"\n            print(f\"Class: {cls.__module__}.{cls.__qualname__}\", file=file)\n            print(f\"Inv. counter: {get_cache_token()}\", file=file)\n            (_abc_registry, _abc_cache, _abc_negative_cache,\n             _abc_negative_cache_version) = _get_dump(cls)\n            print(f\"_abc_registry: {_abc_registry!r}\", file=file)\n            print(f\"_abc_cache: {_abc_cache!r}\", file=file)\n            print(f\"_abc_negative_cache: {_abc_negative_cache!r}\", file=file)\n            print(f\"_abc_negative_cache_version: {_abc_negative_cache_version!r}\",\n                  file=file)\n\n        def _abc_registry_clear(cls):\n            \"\"\"Clear the registry (for debugging or testing).\"\"\"\n            _reset_registry(cls)\n\n        def _abc_caches_clear(cls):\n            \"\"\"Clear the caches (for debugging or testing).\"\"\"\n            _reset_caches(cls)\n\n\ndef update_abstractmethods(cls):\n    \"\"\"Recalculate the set of abstract methods of an abstract class.\n\n    If a class has had one of its abstract methods implemented after the\n    class was created, the method will not be considered implemented until\n    this function is called. Alternatively, if a new abstract method has been\n    added to the class, it will only be considered an abstract method of the\n    class after this function is called.\n\n    This function should be called before any use is made of the class,\n    usually in class decorators that add methods to the subject class.\n\n    Returns cls, to allow usage as a class decorator.\n\n    If cls is not an instance of ABCMeta, does nothing.\n    \"\"\"\n    if not hasattr(cls, '__abstractmethods__'):\n        # We check for __abstractmethods__ here because cls might by a C\n        # implementation or a python implementation (especially during\n        # testing), and we want to handle both cases.\n        return cls\n\n    abstracts = set()\n    # Check the existing abstract methods of the parents, keep only the ones\n    # that are not implemented.\n    for scls in cls.__bases__:\n        for name in getattr(scls, '__abstractmethods__', ()):\n            value = getattr(cls, name, None)\n            if getattr(value, \"__isabstractmethod__\", False):\n                abstracts.add(name)\n    # Also add any other newly added abstract methods.\n    for name, value in cls.__dict__.items():\n        if getattr(value, \"__isabstractmethod__\", False):\n            abstracts.add(name)\n    cls.__abstractmethods__ = frozenset(abstracts)\n    return cls\n\n\nclass ABC(metaclass=ABCMeta):\n    \"\"\"Helper class that provides a standard way to create an ABC using\n    inheritance.\n    \"\"\"\n    __slots__ = ()\n", 188], "/usr/local/lib/python3.11/inspect.py": ["\"\"\"Get useful information from live Python objects.\n\nThis module encapsulates the interface provided by the internal special\nattributes (co_*, im_*, tb_*, etc.) in a friendlier fashion.\nIt also provides some help for examining source code and class layout.\n\nHere are some of the useful functions provided by this module:\n\n    ismodule(), isclass(), ismethod(), isfunction(), isgeneratorfunction(),\n        isgenerator(), istraceback(), isframe(), iscode(), isbuiltin(),\n        isroutine() - check object types\n    getmembers() - get members of an object that satisfy a given condition\n\n    getfile(), getsourcefile(), getsource() - find an object's source code\n    getdoc(), getcomments() - get documentation on an object\n    getmodule() - determine the module that an object came from\n    getclasstree() - arrange classes so as to represent their hierarchy\n\n    getargvalues(), getcallargs() - get info about function arguments\n    getfullargspec() - same, with support for Python 3 features\n    formatargvalues() - format an argument spec\n    getouterframes(), getinnerframes() - get info about frames\n    currentframe() - get the current stack frame\n    stack(), trace() - get info about frames on the stack or in a traceback\n\n    signature() - get a Signature object for the callable\n\n    get_annotations() - safely compute an object's annotations\n\"\"\"\n\n# This module is in the public domain.  No warranties.\n\n__author__ = ('Ka-Ping Yee <ping@lfw.org>',\n              'Yury Selivanov <yselivanov@sprymix.com>')\n\n__all__ = [\n    \"ArgInfo\",\n    \"Arguments\",\n    \"Attribute\",\n    \"BlockFinder\",\n    \"BoundArguments\",\n    \"CORO_CLOSED\",\n    \"CORO_CREATED\",\n    \"CORO_RUNNING\",\n    \"CORO_SUSPENDED\",\n    \"CO_ASYNC_GENERATOR\",\n    \"CO_COROUTINE\",\n    \"CO_GENERATOR\",\n    \"CO_ITERABLE_COROUTINE\",\n    \"CO_NESTED\",\n    \"CO_NEWLOCALS\",\n    \"CO_NOFREE\",\n    \"CO_OPTIMIZED\",\n    \"CO_VARARGS\",\n    \"CO_VARKEYWORDS\",\n    \"ClassFoundException\",\n    \"ClosureVars\",\n    \"EndOfBlock\",\n    \"FrameInfo\",\n    \"FullArgSpec\",\n    \"GEN_CLOSED\",\n    \"GEN_CREATED\",\n    \"GEN_RUNNING\",\n    \"GEN_SUSPENDED\",\n    \"Parameter\",\n    \"Signature\",\n    \"TPFLAGS_IS_ABSTRACT\",\n    \"Traceback\",\n    \"classify_class_attrs\",\n    \"cleandoc\",\n    \"currentframe\",\n    \"findsource\",\n    \"formatannotation\",\n    \"formatannotationrelativeto\",\n    \"formatargvalues\",\n    \"get_annotations\",\n    \"getabsfile\",\n    \"getargs\",\n    \"getargvalues\",\n    \"getattr_static\",\n    \"getblock\",\n    \"getcallargs\",\n    \"getclasstree\",\n    \"getclosurevars\",\n    \"getcomments\",\n    \"getcoroutinelocals\",\n    \"getcoroutinestate\",\n    \"getdoc\",\n    \"getfile\",\n    \"getframeinfo\",\n    \"getfullargspec\",\n    \"getgeneratorlocals\",\n    \"getgeneratorstate\",\n    \"getinnerframes\",\n    \"getlineno\",\n    \"getmembers\",\n    \"getmembers_static\",\n    \"getmodule\",\n    \"getmodulename\",\n    \"getmro\",\n    \"getouterframes\",\n    \"getsource\",\n    \"getsourcefile\",\n    \"getsourcelines\",\n    \"indentsize\",\n    \"isabstract\",\n    \"isasyncgen\",\n    \"isasyncgenfunction\",\n    \"isawaitable\",\n    \"isbuiltin\",\n    \"isclass\",\n    \"iscode\",\n    \"iscoroutine\",\n    \"iscoroutinefunction\",\n    \"isdatadescriptor\",\n    \"isframe\",\n    \"isfunction\",\n    \"isgenerator\",\n    \"isgeneratorfunction\",\n    \"isgetsetdescriptor\",\n    \"ismemberdescriptor\",\n    \"ismethod\",\n    \"ismethoddescriptor\",\n    \"ismethodwrapper\",\n    \"ismodule\",\n    \"isroutine\",\n    \"istraceback\",\n    \"signature\",\n    \"stack\",\n    \"trace\",\n    \"unwrap\",\n    \"walktree\",\n]\n\n\nimport abc\nimport ast\nimport dis\nimport collections.abc\nimport enum\nimport importlib.machinery\nimport itertools\nimport linecache\nimport os\nimport re\nimport sys\nimport tokenize\nimport token\nimport types\nimport functools\nimport builtins\nfrom keyword import iskeyword\nfrom operator import attrgetter\nfrom collections import namedtuple, OrderedDict\n\n# Create constants for the compiler flags in Include/code.h\n# We try to get them from dis to avoid duplication\nmod_dict = globals()\nfor k, v in dis.COMPILER_FLAG_NAMES.items():\n    mod_dict[\"CO_\" + v] = k\ndel k, v, mod_dict\n\n# See Include/object.h\nTPFLAGS_IS_ABSTRACT = 1 << 20\n\n\ndef get_annotations(obj, *, globals=None, locals=None, eval_str=False):\n    \"\"\"Compute the annotations dict for an object.\n\n    obj may be a callable, class, or module.\n    Passing in an object of any other type raises TypeError.\n\n    Returns a dict.  get_annotations() returns a new dict every time\n    it's called; calling it twice on the same object will return two\n    different but equivalent dicts.\n\n    This function handles several details for you:\n\n      * If eval_str is true, values of type str will\n        be un-stringized using eval().  This is intended\n        for use with stringized annotations\n        (\"from __future__ import annotations\").\n      * If obj doesn't have an annotations dict, returns an\n        empty dict.  (Functions and methods always have an\n        annotations dict; classes, modules, and other types of\n        callables may not.)\n      * Ignores inherited annotations on classes.  If a class\n        doesn't have its own annotations dict, returns an empty dict.\n      * All accesses to object members and dict values are done\n        using getattr() and dict.get() for safety.\n      * Always, always, always returns a freshly-created dict.\n\n    eval_str controls whether or not values of type str are replaced\n    with the result of calling eval() on those values:\n\n      * If eval_str is true, eval() is called on values of type str.\n      * If eval_str is false (the default), values of type str are unchanged.\n\n    globals and locals are passed in to eval(); see the documentation\n    for eval() for more information.  If either globals or locals is\n    None, this function may replace that value with a context-specific\n    default, contingent on type(obj):\n\n      * If obj is a module, globals defaults to obj.__dict__.\n      * If obj is a class, globals defaults to\n        sys.modules[obj.__module__].__dict__ and locals\n        defaults to the obj class namespace.\n      * If obj is a callable, globals defaults to obj.__globals__,\n        although if obj is a wrapped function (using\n        functools.update_wrapper()) it is first unwrapped.\n    \"\"\"\n    if isinstance(obj, type):\n        # class\n        obj_dict = getattr(obj, '__dict__', None)\n        if obj_dict and hasattr(obj_dict, 'get'):\n            ann = obj_dict.get('__annotations__', None)\n            if isinstance(ann, types.GetSetDescriptorType):\n                ann = None\n        else:\n            ann = None\n\n        obj_globals = None\n        module_name = getattr(obj, '__module__', None)\n        if module_name:\n            module = sys.modules.get(module_name, None)\n            if module:\n                obj_globals = getattr(module, '__dict__', None)\n        obj_locals = dict(vars(obj))\n        unwrap = obj\n    elif isinstance(obj, types.ModuleType):\n        # module\n        ann = getattr(obj, '__annotations__', None)\n        obj_globals = getattr(obj, '__dict__')\n        obj_locals = None\n        unwrap = None\n    elif callable(obj):\n        # this includes types.Function, types.BuiltinFunctionType,\n        # types.BuiltinMethodType, functools.partial, functools.singledispatch,\n        # \"class funclike\" from Lib/test/test_inspect... on and on it goes.\n        ann = getattr(obj, '__annotations__', None)\n        obj_globals = getattr(obj, '__globals__', None)\n        obj_locals = None\n        unwrap = obj\n    else:\n        raise TypeError(f\"{obj!r} is not a module, class, or callable.\")\n\n    if ann is None:\n        return {}\n\n    if not isinstance(ann, dict):\n        raise ValueError(f\"{obj!r}.__annotations__ is neither a dict nor None\")\n\n    if not ann:\n        return {}\n\n    if not eval_str:\n        return dict(ann)\n\n    if unwrap is not None:\n        while True:\n            if hasattr(unwrap, '__wrapped__'):\n                unwrap = unwrap.__wrapped__\n                continue\n            if isinstance(unwrap, functools.partial):\n                unwrap = unwrap.func\n                continue\n            break\n        if hasattr(unwrap, \"__globals__\"):\n            obj_globals = unwrap.__globals__\n\n    if globals is None:\n        globals = obj_globals\n    if locals is None:\n        locals = obj_locals\n\n    return_value = {key:\n        value if not isinstance(value, str) else eval(value, globals, locals)\n        for key, value in ann.items() }\n    return return_value\n\n\n# ----------------------------------------------------------- type-checking\ndef ismodule(object):\n    \"\"\"Return true if the object is a module.\n\n    Module objects provide these attributes:\n        __cached__      pathname to byte compiled file\n        __doc__         documentation string\n        __file__        filename (missing for built-in modules)\"\"\"\n    return isinstance(object, types.ModuleType)\n\ndef isclass(object):\n    \"\"\"Return true if the object is a class.\n\n    Class objects provide these attributes:\n        __doc__         documentation string\n        __module__      name of module in which this class was defined\"\"\"\n    return isinstance(object, type)\n\ndef ismethod(object):\n    \"\"\"Return true if the object is an instance method.\n\n    Instance method objects provide these attributes:\n        __doc__         documentation string\n        __name__        name with which this method was defined\n        __func__        function object containing implementation of method\n        __self__        instance to which this method is bound\"\"\"\n    return isinstance(object, types.MethodType)\n\ndef ismethoddescriptor(object):\n    \"\"\"Return true if the object is a method descriptor.\n\n    But not if ismethod() or isclass() or isfunction() are true.\n\n    This is new in Python 2.2, and, for example, is true of int.__add__.\n    An object passing this test has a __get__ attribute but not a __set__\n    attribute, but beyond that the set of attributes varies.  __name__ is\n    usually sensible, and __doc__ often is.\n\n    Methods implemented via descriptors that also pass one of the other\n    tests return false from the ismethoddescriptor() test, simply because\n    the other tests promise more -- you can, e.g., count on having the\n    __func__ attribute (etc) when an object passes ismethod().\"\"\"\n    if isclass(object) or ismethod(object) or isfunction(object):\n        # mutual exclusion\n        return False\n    tp = type(object)\n    return hasattr(tp, \"__get__\") and not hasattr(tp, \"__set__\")\n\ndef isdatadescriptor(object):\n    \"\"\"Return true if the object is a data descriptor.\n\n    Data descriptors have a __set__ or a __delete__ attribute.  Examples are\n    properties (defined in Python) and getsets and members (defined in C).\n    Typically, data descriptors will also have __name__ and __doc__ attributes\n    (properties, getsets, and members have both of these attributes), but this\n    is not guaranteed.\"\"\"\n    if isclass(object) or ismethod(object) or isfunction(object):\n        # mutual exclusion\n        return False\n    tp = type(object)\n    return hasattr(tp, \"__set__\") or hasattr(tp, \"__delete__\")\n\nif hasattr(types, 'MemberDescriptorType'):\n    # CPython and equivalent\n    def ismemberdescriptor(object):\n        \"\"\"Return true if the object is a member descriptor.\n\n        Member descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return isinstance(object, types.MemberDescriptorType)\nelse:\n    # Other implementations\n    def ismemberdescriptor(object):\n        \"\"\"Return true if the object is a member descriptor.\n\n        Member descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return False\n\nif hasattr(types, 'GetSetDescriptorType'):\n    # CPython and equivalent\n    def isgetsetdescriptor(object):\n        \"\"\"Return true if the object is a getset descriptor.\n\n        getset descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return isinstance(object, types.GetSetDescriptorType)\nelse:\n    # Other implementations\n    def isgetsetdescriptor(object):\n        \"\"\"Return true if the object is a getset descriptor.\n\n        getset descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return False\n\ndef isfunction(object):\n    \"\"\"Return true if the object is a user-defined function.\n\n    Function objects provide these attributes:\n        __doc__         documentation string\n        __name__        name with which this function was defined\n        __code__        code object containing compiled function bytecode\n        __defaults__    tuple of any default values for arguments\n        __globals__     global namespace in which this function was defined\n        __annotations__ dict of parameter annotations\n        __kwdefaults__  dict of keyword only parameters with defaults\"\"\"\n    return isinstance(object, types.FunctionType)\n\ndef _has_code_flag(f, flag):\n    \"\"\"Return true if ``f`` is a function (or a method or functools.partial\n    wrapper wrapping a function) whose code object has the given ``flag``\n    set in its flags.\"\"\"\n    while ismethod(f):\n        f = f.__func__\n    f = functools._unwrap_partial(f)\n    if not (isfunction(f) or _signature_is_functionlike(f)):\n        return False\n    return bool(f.__code__.co_flags & flag)\n\ndef isgeneratorfunction(obj):\n    \"\"\"Return true if the object is a user-defined generator function.\n\n    Generator function objects provide the same attributes as functions.\n    See help(isfunction) for a list of attributes.\"\"\"\n    return _has_code_flag(obj, CO_GENERATOR)\n\ndef iscoroutinefunction(obj):\n    \"\"\"Return true if the object is a coroutine function.\n\n    Coroutine functions are defined with \"async def\" syntax.\n    \"\"\"\n    return _has_code_flag(obj, CO_COROUTINE)\n\ndef isasyncgenfunction(obj):\n    \"\"\"Return true if the object is an asynchronous generator function.\n\n    Asynchronous generator functions are defined with \"async def\"\n    syntax and have \"yield\" expressions in their body.\n    \"\"\"\n    return _has_code_flag(obj, CO_ASYNC_GENERATOR)\n\ndef isasyncgen(object):\n    \"\"\"Return true if the object is an asynchronous generator.\"\"\"\n    return isinstance(object, types.AsyncGeneratorType)\n\ndef isgenerator(object):\n    \"\"\"Return true if the object is a generator.\n\n    Generator objects provide these attributes:\n        __iter__        defined to support iteration over container\n        close           raises a new GeneratorExit exception inside the\n                        generator to terminate the iteration\n        gi_code         code object\n        gi_frame        frame object or possibly None once the generator has\n                        been exhausted\n        gi_running      set to 1 when generator is executing, 0 otherwise\n        next            return the next item from the container\n        send            resumes the generator and \"sends\" a value that becomes\n                        the result of the current yield-expression\n        throw           used to raise an exception inside the generator\"\"\"\n    return isinstance(object, types.GeneratorType)\n\ndef iscoroutine(object):\n    \"\"\"Return true if the object is a coroutine.\"\"\"\n    return isinstance(object, types.CoroutineType)\n\ndef isawaitable(object):\n    \"\"\"Return true if object can be passed to an ``await`` expression.\"\"\"\n    return (isinstance(object, types.CoroutineType) or\n            isinstance(object, types.GeneratorType) and\n                bool(object.gi_code.co_flags & CO_ITERABLE_COROUTINE) or\n            isinstance(object, collections.abc.Awaitable))\n\ndef istraceback(object):\n    \"\"\"Return true if the object is a traceback.\n\n    Traceback objects provide these attributes:\n        tb_frame        frame object at this level\n        tb_lasti        index of last attempted instruction in bytecode\n        tb_lineno       current line number in Python source code\n        tb_next         next inner traceback object (called by this level)\"\"\"\n    return isinstance(object, types.TracebackType)\n\ndef isframe(object):\n    \"\"\"Return true if the object is a frame object.\n\n    Frame objects provide these attributes:\n        f_back          next outer frame object (this frame's caller)\n        f_builtins      built-in namespace seen by this frame\n        f_code          code object being executed in this frame\n        f_globals       global namespace seen by this frame\n        f_lasti         index of last attempted instruction in bytecode\n        f_lineno        current line number in Python source code\n        f_locals        local namespace seen by this frame\n        f_trace         tracing function for this frame, or None\"\"\"\n    return isinstance(object, types.FrameType)\n\ndef iscode(object):\n    \"\"\"Return true if the object is a code object.\n\n    Code objects provide these attributes:\n        co_argcount         number of arguments (not including *, ** args\n                            or keyword only arguments)\n        co_code             string of raw compiled bytecode\n        co_cellvars         tuple of names of cell variables\n        co_consts           tuple of constants used in the bytecode\n        co_filename         name of file in which this code object was created\n        co_firstlineno      number of first line in Python source code\n        co_flags            bitmap: 1=optimized | 2=newlocals | 4=*arg | 8=**arg\n                            | 16=nested | 32=generator | 64=nofree | 128=coroutine\n                            | 256=iterable_coroutine | 512=async_generator\n        co_freevars         tuple of names of free variables\n        co_posonlyargcount  number of positional only arguments\n        co_kwonlyargcount   number of keyword only arguments (not including ** arg)\n        co_lnotab           encoded mapping of line numbers to bytecode indices\n        co_name             name with which this code object was defined\n        co_names            tuple of names other than arguments and function locals\n        co_nlocals          number of local variables\n        co_stacksize        virtual machine stack space required\n        co_varnames         tuple of names of arguments and local variables\"\"\"\n    return isinstance(object, types.CodeType)\n\ndef isbuiltin(object):\n    \"\"\"Return true if the object is a built-in function or method.\n\n    Built-in functions and methods provide these attributes:\n        __doc__         documentation string\n        __name__        original name of this function or method\n        __self__        instance to which a method is bound, or None\"\"\"\n    return isinstance(object, types.BuiltinFunctionType)\n\ndef ismethodwrapper(object):\n    \"\"\"Return true if the object is a method wrapper.\"\"\"\n    return isinstance(object, types.MethodWrapperType)\n\ndef isroutine(object):\n    \"\"\"Return true if the object is any kind of function or method.\"\"\"\n    return (isbuiltin(object)\n            or isfunction(object)\n            or ismethod(object)\n            or ismethoddescriptor(object)\n            or ismethodwrapper(object))\n\ndef isabstract(object):\n    \"\"\"Return true if the object is an abstract base class (ABC).\"\"\"\n    if not isinstance(object, type):\n        return False\n    if object.__flags__ & TPFLAGS_IS_ABSTRACT:\n        return True\n    if not issubclass(type(object), abc.ABCMeta):\n        return False\n    if hasattr(object, '__abstractmethods__'):\n        # It looks like ABCMeta.__new__ has finished running;\n        # TPFLAGS_IS_ABSTRACT should have been accurate.\n        return False\n    # It looks like ABCMeta.__new__ has not finished running yet; we're\n    # probably in __init_subclass__. We'll look for abstractmethods manually.\n    for name, value in object.__dict__.items():\n        if getattr(value, \"__isabstractmethod__\", False):\n            return True\n    for base in object.__bases__:\n        for name in getattr(base, \"__abstractmethods__\", ()):\n            value = getattr(object, name, None)\n            if getattr(value, \"__isabstractmethod__\", False):\n                return True\n    return False\n\ndef _getmembers(object, predicate, getter):\n    results = []\n    processed = set()\n    names = dir(object)\n    if isclass(object):\n        mro = (object,) + getmro(object)\n        # add any DynamicClassAttributes to the list of names if object is a class;\n        # this may result in duplicate entries if, for example, a virtual\n        # attribute with the same name as a DynamicClassAttribute exists\n        try:\n            for base in object.__bases__:\n                for k, v in base.__dict__.items():\n                    if isinstance(v, types.DynamicClassAttribute):\n                        names.append(k)\n        except AttributeError:\n            pass\n    else:\n        mro = ()\n    for key in names:\n        # First try to get the value via getattr.  Some descriptors don't\n        # like calling their __get__ (see bug #1785), so fall back to\n        # looking in the __dict__.\n        try:\n            value = getter(object, key)\n            # handle the duplicate key\n            if key in processed:\n                raise AttributeError\n        except AttributeError:\n            for base in mro:\n                if key in base.__dict__:\n                    value = base.__dict__[key]\n                    break\n            else:\n                # could be a (currently) missing slot member, or a buggy\n                # __dir__; discard and move on\n                continue\n        if not predicate or predicate(value):\n            results.append((key, value))\n        processed.add(key)\n    results.sort(key=lambda pair: pair[0])\n    return results\n\ndef getmembers(object, predicate=None):\n    \"\"\"Return all members of an object as (name, value) pairs sorted by name.\n    Optionally, only return members that satisfy a given predicate.\"\"\"\n    return _getmembers(object, predicate, getattr)\n\ndef getmembers_static(object, predicate=None):\n    \"\"\"Return all members of an object as (name, value) pairs sorted by name\n    without triggering dynamic lookup via the descriptor protocol,\n    __getattr__ or __getattribute__. Optionally, only return members that\n    satisfy a given predicate.\n\n    Note: this function may not be able to retrieve all members\n       that getmembers can fetch (like dynamically created attributes)\n       and may find members that getmembers can't (like descriptors\n       that raise AttributeError). It can also return descriptor objects\n       instead of instance members in some cases.\n    \"\"\"\n    return _getmembers(object, predicate, getattr_static)\n\nAttribute = namedtuple('Attribute', 'name kind defining_class object')\n\ndef classify_class_attrs(cls):\n    \"\"\"Return list of attribute-descriptor tuples.\n\n    For each name in dir(cls), the return list contains a 4-tuple\n    with these elements:\n\n        0. The name (a string).\n\n        1. The kind of attribute this is, one of these strings:\n               'class method'    created via classmethod()\n               'static method'   created via staticmethod()\n               'property'        created via property()\n               'method'          any other flavor of method or descriptor\n               'data'            not a method\n\n        2. The class which defined this attribute (a class).\n\n        3. The object as obtained by calling getattr; if this fails, or if the\n           resulting object does not live anywhere in the class' mro (including\n           metaclasses) then the object is looked up in the defining class's\n           dict (found by walking the mro).\n\n    If one of the items in dir(cls) is stored in the metaclass it will now\n    be discovered and not have None be listed as the class in which it was\n    defined.  Any items whose home class cannot be discovered are skipped.\n    \"\"\"\n\n    mro = getmro(cls)\n    metamro = getmro(type(cls)) # for attributes stored in the metaclass\n    metamro = tuple(cls for cls in metamro if cls not in (type, object))\n    class_bases = (cls,) + mro\n    all_bases = class_bases + metamro\n    names = dir(cls)\n    # :dd any DynamicClassAttributes to the list of names;\n    # this may result in duplicate entries if, for example, a virtual\n    # attribute with the same name as a DynamicClassAttribute exists.\n    for base in mro:\n        for k, v in base.__dict__.items():\n            if isinstance(v, types.DynamicClassAttribute) and v.fget is not None:\n                names.append(k)\n    result = []\n    processed = set()\n\n    for name in names:\n        # Get the object associated with the name, and where it was defined.\n        # Normal objects will be looked up with both getattr and directly in\n        # its class' dict (in case getattr fails [bug #1785], and also to look\n        # for a docstring).\n        # For DynamicClassAttributes on the second pass we only look in the\n        # class's dict.\n        #\n        # Getting an obj from the __dict__ sometimes reveals more than\n        # using getattr.  Static and class methods are dramatic examples.\n        homecls = None\n        get_obj = None\n        dict_obj = None\n        if name not in processed:\n            try:\n                if name == '__dict__':\n                    raise Exception(\"__dict__ is special, don't want the proxy\")\n                get_obj = getattr(cls, name)\n            except Exception as exc:\n                pass\n            else:\n                homecls = getattr(get_obj, \"__objclass__\", homecls)\n                if homecls not in class_bases:\n                    # if the resulting object does not live somewhere in the\n                    # mro, drop it and search the mro manually\n                    homecls = None\n                    last_cls = None\n                    # first look in the classes\n                    for srch_cls in class_bases:\n                        srch_obj = getattr(srch_cls, name, None)\n                        if srch_obj is get_obj:\n                            last_cls = srch_cls\n                    # then check the metaclasses\n                    for srch_cls in metamro:\n                        try:\n                            srch_obj = srch_cls.__getattr__(cls, name)\n                        except AttributeError:\n                            continue\n                        if srch_obj is get_obj:\n                            last_cls = srch_cls\n                    if last_cls is not None:\n                        homecls = last_cls\n        for base in all_bases:\n            if name in base.__dict__:\n                dict_obj = base.__dict__[name]\n                if homecls not in metamro:\n                    homecls = base\n                break\n        if homecls is None:\n            # unable to locate the attribute anywhere, most likely due to\n            # buggy custom __dir__; discard and move on\n            continue\n        obj = get_obj if get_obj is not None else dict_obj\n        # Classify the object or its descriptor.\n        if isinstance(dict_obj, (staticmethod, types.BuiltinMethodType)):\n            kind = \"static method\"\n            obj = dict_obj\n        elif isinstance(dict_obj, (classmethod, types.ClassMethodDescriptorType)):\n            kind = \"class method\"\n            obj = dict_obj\n        elif isinstance(dict_obj, property):\n            kind = \"property\"\n            obj = dict_obj\n        elif isroutine(obj):\n            kind = \"method\"\n        else:\n            kind = \"data\"\n        result.append(Attribute(name, kind, homecls, obj))\n        processed.add(name)\n    return result\n\n# ----------------------------------------------------------- class helpers\n\ndef getmro(cls):\n    \"Return tuple of base classes (including cls) in method resolution order.\"\n    return cls.__mro__\n\n# -------------------------------------------------------- function helpers\n\ndef unwrap(func, *, stop=None):\n    \"\"\"Get the object wrapped by *func*.\n\n   Follows the chain of :attr:`__wrapped__` attributes returning the last\n   object in the chain.\n\n   *stop* is an optional callback accepting an object in the wrapper chain\n   as its sole argument that allows the unwrapping to be terminated early if\n   the callback returns a true value. If the callback never returns a true\n   value, the last object in the chain is returned as usual. For example,\n   :func:`signature` uses this to stop unwrapping if any object in the\n   chain has a ``__signature__`` attribute defined.\n\n   :exc:`ValueError` is raised if a cycle is encountered.\n\n    \"\"\"\n    f = func  # remember the original func for error reporting\n    # Memoise by id to tolerate non-hashable objects, but store objects to\n    # ensure they aren't destroyed, which would allow their IDs to be reused.\n    memo = {id(f): f}\n    recursion_limit = sys.getrecursionlimit()\n    while not isinstance(func, type) and hasattr(func, '__wrapped__'):\n        if stop is not None and stop(func):\n            break\n        func = func.__wrapped__\n        id_func = id(func)\n        if (id_func in memo) or (len(memo) >= recursion_limit):\n            raise ValueError('wrapper loop when unwrapping {!r}'.format(f))\n        memo[id_func] = func\n    return func\n\n# -------------------------------------------------- source code extraction\ndef indentsize(line):\n    \"\"\"Return the indent size, in spaces, at the start of a line of text.\"\"\"\n    expline = line.expandtabs()\n    return len(expline) - len(expline.lstrip())\n\ndef _findclass(func):\n    cls = sys.modules.get(func.__module__)\n    if cls is None:\n        return None\n    for name in func.__qualname__.split('.')[:-1]:\n        cls = getattr(cls, name)\n    if not isclass(cls):\n        return None\n    return cls\n\ndef _finddoc(obj):\n    if isclass(obj):\n        for base in obj.__mro__:\n            if base is not object:\n                try:\n                    doc = base.__doc__\n                except AttributeError:\n                    continue\n                if doc is not None:\n                    return doc\n        return None\n\n    if ismethod(obj):\n        name = obj.__func__.__name__\n        self = obj.__self__\n        if (isclass(self) and\n            getattr(getattr(self, name, None), '__func__') is obj.__func__):\n            # classmethod\n            cls = self\n        else:\n            cls = self.__class__\n    elif isfunction(obj):\n        name = obj.__name__\n        cls = _findclass(obj)\n        if cls is None or getattr(cls, name) is not obj:\n            return None\n    elif isbuiltin(obj):\n        name = obj.__name__\n        self = obj.__self__\n        if (isclass(self) and\n            self.__qualname__ + '.' + name == obj.__qualname__):\n            # classmethod\n            cls = self\n        else:\n            cls = self.__class__\n    # Should be tested before isdatadescriptor().\n    elif isinstance(obj, property):\n        func = obj.fget\n        name = func.__name__\n        cls = _findclass(func)\n        if cls is None or getattr(cls, name) is not obj:\n            return None\n    elif ismethoddescriptor(obj) or isdatadescriptor(obj):\n        name = obj.__name__\n        cls = obj.__objclass__\n        if getattr(cls, name) is not obj:\n            return None\n        if ismemberdescriptor(obj):\n            slots = getattr(cls, '__slots__', None)\n            if isinstance(slots, dict) and name in slots:\n                return slots[name]\n    else:\n        return None\n    for base in cls.__mro__:\n        try:\n            doc = getattr(base, name).__doc__\n        except AttributeError:\n            continue\n        if doc is not None:\n            return doc\n    return None\n\ndef getdoc(object):\n    \"\"\"Get the documentation string for an object.\n\n    All tabs are expanded to spaces.  To clean up docstrings that are\n    indented to line up with blocks of code, any whitespace than can be\n    uniformly removed from the second line onwards is removed.\"\"\"\n    try:\n        doc = object.__doc__\n    except AttributeError:\n        return None\n    if doc is None:\n        try:\n            doc = _finddoc(object)\n        except (AttributeError, TypeError):\n            return None\n    if not isinstance(doc, str):\n        return None\n    return cleandoc(doc)\n\ndef cleandoc(doc):\n    \"\"\"Clean up indentation from docstrings.\n\n    Any whitespace that can be uniformly removed from the second line\n    onwards is removed.\"\"\"\n    try:\n        lines = doc.expandtabs().split('\\n')\n    except UnicodeError:\n        return None\n    else:\n        # Find minimum indentation of any non-blank lines after first line.\n        margin = sys.maxsize\n        for line in lines[1:]:\n            content = len(line.lstrip())\n            if content:\n                indent = len(line) - content\n                margin = min(margin, indent)\n        # Remove indentation.\n        if lines:\n            lines[0] = lines[0].lstrip()\n        if margin < sys.maxsize:\n            for i in range(1, len(lines)): lines[i] = lines[i][margin:]\n        # Remove any trailing or leading blank lines.\n        while lines and not lines[-1]:\n            lines.pop()\n        while lines and not lines[0]:\n            lines.pop(0)\n        return '\\n'.join(lines)\n\ndef getfile(object):\n    \"\"\"Work out which source or compiled file an object was defined in.\"\"\"\n    if ismodule(object):\n        if getattr(object, '__file__', None):\n            return object.__file__\n        raise TypeError('{!r} is a built-in module'.format(object))\n    if isclass(object):\n        if hasattr(object, '__module__'):\n            module = sys.modules.get(object.__module__)\n            if getattr(module, '__file__', None):\n                return module.__file__\n            if object.__module__ == '__main__':\n                raise OSError('source code not available')\n        raise TypeError('{!r} is a built-in class'.format(object))\n    if ismethod(object):\n        object = object.__func__\n    if isfunction(object):\n        object = object.__code__\n    if istraceback(object):\n        object = object.tb_frame\n    if isframe(object):\n        object = object.f_code\n    if iscode(object):\n        return object.co_filename\n    raise TypeError('module, class, method, function, traceback, frame, or '\n                    'code object was expected, got {}'.format(\n                    type(object).__name__))\n\ndef getmodulename(path):\n    \"\"\"Return the module name for a given file, or None.\"\"\"\n    fname = os.path.basename(path)\n    # Check for paths that look like an actual module file\n    suffixes = [(-len(suffix), suffix)\n                    for suffix in importlib.machinery.all_suffixes()]\n    suffixes.sort() # try longest suffixes first, in case they overlap\n    for neglen, suffix in suffixes:\n        if fname.endswith(suffix):\n            return fname[:neglen]\n    return None\n\ndef getsourcefile(object):\n    \"\"\"Return the filename that can be used to locate an object's source.\n    Return None if no way can be identified to get the source.\n    \"\"\"\n    filename = getfile(object)\n    all_bytecode_suffixes = importlib.machinery.DEBUG_BYTECODE_SUFFIXES[:]\n    all_bytecode_suffixes += importlib.machinery.OPTIMIZED_BYTECODE_SUFFIXES[:]\n    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n        filename = (os.path.splitext(filename)[0] +\n                    importlib.machinery.SOURCE_SUFFIXES[0])\n    elif any(filename.endswith(s) for s in\n                 importlib.machinery.EXTENSION_SUFFIXES):\n        return None\n    if os.path.exists(filename):\n        return filename\n    # only return a non-existent filename if the module has a PEP 302 loader\n    module = getmodule(object, filename)\n    if getattr(module, '__loader__', None) is not None:\n        return filename\n    elif getattr(getattr(module, \"__spec__\", None), \"loader\", None) is not None:\n        return filename\n    # or it is in the linecache\n    elif filename in linecache.cache:\n        return filename\n\ndef getabsfile(object, _filename=None):\n    \"\"\"Return an absolute path to the source or compiled file for an object.\n\n    The idea is for each object to have a unique origin, so this routine\n    normalizes the result as much as possible.\"\"\"\n    if _filename is None:\n        _filename = getsourcefile(object) or getfile(object)\n    return os.path.normcase(os.path.abspath(_filename))\n\nmodulesbyfile = {}\n_filesbymodname = {}\n\ndef getmodule(object, _filename=None):\n    \"\"\"Return the module an object was defined in, or None if not found.\"\"\"\n    if ismodule(object):\n        return object\n    if hasattr(object, '__module__'):\n        return sys.modules.get(object.__module__)\n    # Try the filename to modulename cache\n    if _filename is not None and _filename in modulesbyfile:\n        return sys.modules.get(modulesbyfile[_filename])\n    # Try the cache again with the absolute file name\n    try:\n        file = getabsfile(object, _filename)\n    except (TypeError, FileNotFoundError):\n        return None\n    if file in modulesbyfile:\n        return sys.modules.get(modulesbyfile[file])\n    # Update the filename to module name cache and check yet again\n    # Copy sys.modules in order to cope with changes while iterating\n    for modname, module in sys.modules.copy().items():\n        if ismodule(module) and hasattr(module, '__file__'):\n            f = module.__file__\n            if f == _filesbymodname.get(modname, None):\n                # Have already mapped this module, so skip it\n                continue\n            _filesbymodname[modname] = f\n            f = getabsfile(module)\n            # Always map to the name the module knows itself by\n            modulesbyfile[f] = modulesbyfile[\n                os.path.realpath(f)] = module.__name__\n    if file in modulesbyfile:\n        return sys.modules.get(modulesbyfile[file])\n    # Check the main module\n    main = sys.modules['__main__']\n    if not hasattr(object, '__name__'):\n        return None\n    if hasattr(main, object.__name__):\n        mainobject = getattr(main, object.__name__)\n        if mainobject is object:\n            return main\n    # Check builtins\n    builtin = sys.modules['builtins']\n    if hasattr(builtin, object.__name__):\n        builtinobject = getattr(builtin, object.__name__)\n        if builtinobject is object:\n            return builtin\n\n\nclass ClassFoundException(Exception):\n    pass\n\n\nclass _ClassFinder(ast.NodeVisitor):\n\n    def __init__(self, qualname):\n        self.stack = []\n        self.qualname = qualname\n\n    def visit_FunctionDef(self, node):\n        self.stack.append(node.name)\n        self.stack.append('<locals>')\n        self.generic_visit(node)\n        self.stack.pop()\n        self.stack.pop()\n\n    visit_AsyncFunctionDef = visit_FunctionDef\n\n    def visit_ClassDef(self, node):\n        self.stack.append(node.name)\n        if self.qualname == '.'.join(self.stack):\n            # Return the decorator for the class if present\n            if node.decorator_list:\n                line_number = node.decorator_list[0].lineno\n            else:\n                line_number = node.lineno\n\n            # decrement by one since lines starts with indexing by zero\n            line_number -= 1\n            raise ClassFoundException(line_number)\n        self.generic_visit(node)\n        self.stack.pop()\n\n\ndef findsource(object):\n    \"\"\"Return the entire source file and starting line number for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a list of all the lines\n    in the file and the line number indexes a line in that list.  An OSError\n    is raised if the source code cannot be retrieved.\"\"\"\n\n    file = getsourcefile(object)\n    if file:\n        # Invalidate cache if needed.\n        linecache.checkcache(file)\n    else:\n        file = getfile(object)\n        # Allow filenames in form of \"<something>\" to pass through.\n        # `doctest` monkeypatches `linecache` module to enable\n        # inspection, so let `linecache.getlines` to be called.\n        if not (file.startswith('<') and file.endswith('>')):\n            raise OSError('source code not available')\n\n    module = getmodule(object, file)\n    if module:\n        lines = linecache.getlines(file, module.__dict__)\n    else:\n        lines = linecache.getlines(file)\n    if not lines:\n        raise OSError('could not get source code')\n\n    if ismodule(object):\n        return lines, 0\n\n    if isclass(object):\n        qualname = object.__qualname__\n        source = ''.join(lines)\n        tree = ast.parse(source)\n        class_finder = _ClassFinder(qualname)\n        try:\n            class_finder.visit(tree)\n        except ClassFoundException as e:\n            line_number = e.args[0]\n            return lines, line_number\n        else:\n            raise OSError('could not find class definition')\n\n    if ismethod(object):\n        object = object.__func__\n    if isfunction(object):\n        object = object.__code__\n    if istraceback(object):\n        object = object.tb_frame\n    if isframe(object):\n        object = object.f_code\n    if iscode(object):\n        if not hasattr(object, 'co_firstlineno'):\n            raise OSError('could not find function definition')\n        lnum = object.co_firstlineno - 1\n        pat = re.compile(r'^(\\s*def\\s)|(\\s*async\\s+def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')\n        while lnum > 0:\n            try:\n                line = lines[lnum]\n            except IndexError:\n                raise OSError('lineno is out of bounds')\n            if pat.match(line):\n                break\n            lnum = lnum - 1\n        return lines, lnum\n    raise OSError('could not find code object')\n\ndef getcomments(object):\n    \"\"\"Get lines of comments immediately preceding an object's source code.\n\n    Returns None when source can't be found.\n    \"\"\"\n    try:\n        lines, lnum = findsource(object)\n    except (OSError, TypeError):\n        return None\n\n    if ismodule(object):\n        # Look for a comment block at the top of the file.\n        start = 0\n        if lines and lines[0][:2] == '#!': start = 1\n        while start < len(lines) and lines[start].strip() in ('', '#'):\n            start = start + 1\n        if start < len(lines) and lines[start][:1] == '#':\n            comments = []\n            end = start\n            while end < len(lines) and lines[end][:1] == '#':\n                comments.append(lines[end].expandtabs())\n                end = end + 1\n            return ''.join(comments)\n\n    # Look for a preceding block of comments at the same indentation.\n    elif lnum > 0:\n        indent = indentsize(lines[lnum])\n        end = lnum - 1\n        if end >= 0 and lines[end].lstrip()[:1] == '#' and \\\n            indentsize(lines[end]) == indent:\n            comments = [lines[end].expandtabs().lstrip()]\n            if end > 0:\n                end = end - 1\n                comment = lines[end].expandtabs().lstrip()\n                while comment[:1] == '#' and indentsize(lines[end]) == indent:\n                    comments[:0] = [comment]\n                    end = end - 1\n                    if end < 0: break\n                    comment = lines[end].expandtabs().lstrip()\n            while comments and comments[0].strip() == '#':\n                comments[:1] = []\n            while comments and comments[-1].strip() == '#':\n                comments[-1:] = []\n            return ''.join(comments)\n\nclass EndOfBlock(Exception): pass\n\nclass BlockFinder:\n    \"\"\"Provide a tokeneater() method to detect the end of a code block.\"\"\"\n    def __init__(self):\n        self.indent = 0\n        self.islambda = False\n        self.started = False\n        self.passline = False\n        self.indecorator = False\n        self.last = 1\n        self.body_col0 = None\n\n    def tokeneater(self, type, token, srowcol, erowcol, line):\n        if not self.started and not self.indecorator:\n            # skip any decorators\n            if token == \"@\":\n                self.indecorator = True\n            # look for the first \"def\", \"class\" or \"lambda\"\n            elif token in (\"def\", \"class\", \"lambda\"):\n                if token == \"lambda\":\n                    self.islambda = True\n                self.started = True\n            self.passline = True    # skip to the end of the line\n        elif type == tokenize.NEWLINE:\n            self.passline = False   # stop skipping when a NEWLINE is seen\n            self.last = srowcol[0]\n            if self.islambda:       # lambdas always end at the first NEWLINE\n                raise EndOfBlock\n            # hitting a NEWLINE when in a decorator without args\n            # ends the decorator\n            if self.indecorator:\n                self.indecorator = False\n        elif self.passline:\n            pass\n        elif type == tokenize.INDENT:\n            if self.body_col0 is None and self.started:\n                self.body_col0 = erowcol[1]\n            self.indent = self.indent + 1\n            self.passline = True\n        elif type == tokenize.DEDENT:\n            self.indent = self.indent - 1\n            # the end of matching indent/dedent pairs end a block\n            # (note that this only works for \"def\"/\"class\" blocks,\n            #  not e.g. for \"if: else:\" or \"try: finally:\" blocks)\n            if self.indent <= 0:\n                raise EndOfBlock\n        elif type == tokenize.COMMENT:\n            if self.body_col0 is not None and srowcol[1] >= self.body_col0:\n                # Include comments if indented at least as much as the block\n                self.last = srowcol[0]\n        elif self.indent == 0 and type not in (tokenize.COMMENT, tokenize.NL):\n            # any other token on the same indentation level end the previous\n            # block as well, except the pseudo-tokens COMMENT and NL.\n            raise EndOfBlock\n\ndef getblock(lines):\n    \"\"\"Extract the block of code at the top of the given list of lines.\"\"\"\n    blockfinder = BlockFinder()\n    try:\n        tokens = tokenize.generate_tokens(iter(lines).__next__)\n        for _token in tokens:\n            blockfinder.tokeneater(*_token)\n    except (EndOfBlock, IndentationError):\n        pass\n    return lines[:blockfinder.last]\n\ndef getsourcelines(object):\n    \"\"\"Return a list of source lines and starting line number for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a list of the lines\n    corresponding to the object and the line number indicates where in the\n    original source file the first line of code was found.  An OSError is\n    raised if the source code cannot be retrieved.\"\"\"\n    object = unwrap(object)\n    lines, lnum = findsource(object)\n\n    if istraceback(object):\n        object = object.tb_frame\n\n    # for module or frame that corresponds to module, return all source lines\n    if (ismodule(object) or\n        (isframe(object) and object.f_code.co_name == \"<module>\")):\n        return lines, 0\n    else:\n        return getblock(lines[lnum:]), lnum + 1\n\ndef getsource(object):\n    \"\"\"Return the text of the source code for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a single string.  An\n    OSError is raised if the source code cannot be retrieved.\"\"\"\n    lines, lnum = getsourcelines(object)\n    return ''.join(lines)\n\n# --------------------------------------------------- class tree extraction\ndef walktree(classes, children, parent):\n    \"\"\"Recursive helper function for getclasstree().\"\"\"\n    results = []\n    classes.sort(key=attrgetter('__module__', '__name__'))\n    for c in classes:\n        results.append((c, c.__bases__))\n        if c in children:\n            results.append(walktree(children[c], children, c))\n    return results\n\ndef getclasstree(classes, unique=False):\n    \"\"\"Arrange the given list of classes into a hierarchy of nested lists.\n\n    Where a nested list appears, it contains classes derived from the class\n    whose entry immediately precedes the list.  Each entry is a 2-tuple\n    containing a class and a tuple of its base classes.  If the 'unique'\n    argument is true, exactly one entry appears in the returned structure\n    for each class in the given list.  Otherwise, classes using multiple\n    inheritance and their descendants will appear multiple times.\"\"\"\n    children = {}\n    roots = []\n    for c in classes:\n        if c.__bases__:\n            for parent in c.__bases__:\n                if parent not in children:\n                    children[parent] = []\n                if c not in children[parent]:\n                    children[parent].append(c)\n                if unique and parent in classes: break\n        elif c not in roots:\n            roots.append(c)\n    for parent in children:\n        if parent not in classes:\n            roots.append(parent)\n    return walktree(roots, children, None)\n\n# ------------------------------------------------ argument list extraction\nArguments = namedtuple('Arguments', 'args, varargs, varkw')\n\ndef getargs(co):\n    \"\"\"Get information about the arguments accepted by a code object.\n\n    Three things are returned: (args, varargs, varkw), where\n    'args' is the list of argument names. Keyword-only arguments are\n    appended. 'varargs' and 'varkw' are the names of the * and **\n    arguments or None.\"\"\"\n    if not iscode(co):\n        raise TypeError('{!r} is not a code object'.format(co))\n\n    names = co.co_varnames\n    nargs = co.co_argcount\n    nkwargs = co.co_kwonlyargcount\n    args = list(names[:nargs])\n    kwonlyargs = list(names[nargs:nargs+nkwargs])\n    step = 0\n\n    nargs += nkwargs\n    varargs = None\n    if co.co_flags & CO_VARARGS:\n        varargs = co.co_varnames[nargs]\n        nargs = nargs + 1\n    varkw = None\n    if co.co_flags & CO_VARKEYWORDS:\n        varkw = co.co_varnames[nargs]\n    return Arguments(args + kwonlyargs, varargs, varkw)\n\n\nFullArgSpec = namedtuple('FullArgSpec',\n    'args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations')\n\ndef getfullargspec(func):\n    \"\"\"Get the names and default values of a callable object's parameters.\n\n    A tuple of seven things is returned:\n    (args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations).\n    'args' is a list of the parameter names.\n    'varargs' and 'varkw' are the names of the * and ** parameters or None.\n    'defaults' is an n-tuple of the default values of the last n parameters.\n    'kwonlyargs' is a list of keyword-only parameter names.\n    'kwonlydefaults' is a dictionary mapping names from kwonlyargs to defaults.\n    'annotations' is a dictionary mapping parameter names to annotations.\n\n    Notable differences from inspect.signature():\n      - the \"self\" parameter is always reported, even for bound methods\n      - wrapper chains defined by __wrapped__ *not* unwrapped automatically\n    \"\"\"\n    try:\n        # Re: `skip_bound_arg=False`\n        #\n        # There is a notable difference in behaviour between getfullargspec\n        # and Signature: the former always returns 'self' parameter for bound\n        # methods, whereas the Signature always shows the actual calling\n        # signature of the passed object.\n        #\n        # To simulate this behaviour, we \"unbind\" bound methods, to trick\n        # inspect.signature to always return their first parameter (\"self\",\n        # usually)\n\n        # Re: `follow_wrapper_chains=False`\n        #\n        # getfullargspec() historically ignored __wrapped__ attributes,\n        # so we ensure that remains the case in 3.3+\n\n        sig = _signature_from_callable(func,\n                                       follow_wrapper_chains=False,\n                                       skip_bound_arg=False,\n                                       sigcls=Signature,\n                                       eval_str=False)\n    except Exception as ex:\n        # Most of the times 'signature' will raise ValueError.\n        # But, it can also raise AttributeError, and, maybe something\n        # else. So to be fully backwards compatible, we catch all\n        # possible exceptions here, and reraise a TypeError.\n        raise TypeError('unsupported callable') from ex\n\n    args = []\n    varargs = None\n    varkw = None\n    posonlyargs = []\n    kwonlyargs = []\n    annotations = {}\n    defaults = ()\n    kwdefaults = {}\n\n    if sig.return_annotation is not sig.empty:\n        annotations['return'] = sig.return_annotation\n\n    for param in sig.parameters.values():\n        kind = param.kind\n        name = param.name\n\n        if kind is _POSITIONAL_ONLY:\n            posonlyargs.append(name)\n            if param.default is not param.empty:\n                defaults += (param.default,)\n        elif kind is _POSITIONAL_OR_KEYWORD:\n            args.append(name)\n            if param.default is not param.empty:\n                defaults += (param.default,)\n        elif kind is _VAR_POSITIONAL:\n            varargs = name\n        elif kind is _KEYWORD_ONLY:\n            kwonlyargs.append(name)\n            if param.default is not param.empty:\n                kwdefaults[name] = param.default\n        elif kind is _VAR_KEYWORD:\n            varkw = name\n\n        if param.annotation is not param.empty:\n            annotations[name] = param.annotation\n\n    if not kwdefaults:\n        # compatibility with 'func.__kwdefaults__'\n        kwdefaults = None\n\n    if not defaults:\n        # compatibility with 'func.__defaults__'\n        defaults = None\n\n    return FullArgSpec(posonlyargs + args, varargs, varkw, defaults,\n                       kwonlyargs, kwdefaults, annotations)\n\n\nArgInfo = namedtuple('ArgInfo', 'args varargs keywords locals')\n\ndef getargvalues(frame):\n    \"\"\"Get information about arguments passed into a particular frame.\n\n    A tuple of four things is returned: (args, varargs, varkw, locals).\n    'args' is a list of the argument names.\n    'varargs' and 'varkw' are the names of the * and ** arguments or None.\n    'locals' is the locals dictionary of the given frame.\"\"\"\n    args, varargs, varkw = getargs(frame.f_code)\n    return ArgInfo(args, varargs, varkw, frame.f_locals)\n\ndef formatannotation(annotation, base_module=None):\n    if getattr(annotation, '__module__', None) == 'typing':\n        def repl(match):\n            text = match.group()\n            return text.removeprefix('typing.')\n        return re.sub(r'[\\w\\.]+', repl, repr(annotation))\n    if isinstance(annotation, types.GenericAlias):\n        return str(annotation)\n    if isinstance(annotation, type):\n        if annotation.__module__ in ('builtins', base_module):\n            return annotation.__qualname__\n        return annotation.__module__+'.'+annotation.__qualname__\n    return repr(annotation)\n\ndef formatannotationrelativeto(object):\n    module = getattr(object, '__module__', None)\n    def _formatannotation(annotation):\n        return formatannotation(annotation, module)\n    return _formatannotation\n\n\ndef formatargvalues(args, varargs, varkw, locals,\n                    formatarg=str,\n                    formatvarargs=lambda name: '*' + name,\n                    formatvarkw=lambda name: '**' + name,\n                    formatvalue=lambda value: '=' + repr(value)):\n    \"\"\"Format an argument spec from the 4 values returned by getargvalues.\n\n    The first four arguments are (args, varargs, varkw, locals).  The\n    next four arguments are the corresponding optional formatting functions\n    that are called to turn names and values into strings.  The ninth\n    argument is an optional function to format the sequence of arguments.\"\"\"\n    def convert(name, locals=locals,\n                formatarg=formatarg, formatvalue=formatvalue):\n        return formatarg(name) + formatvalue(locals[name])\n    specs = []\n    for i in range(len(args)):\n        specs.append(convert(args[i]))\n    if varargs:\n        specs.append(formatvarargs(varargs) + formatvalue(locals[varargs]))\n    if varkw:\n        specs.append(formatvarkw(varkw) + formatvalue(locals[varkw]))\n    return '(' + ', '.join(specs) + ')'\n\ndef _missing_arguments(f_name, argnames, pos, values):\n    names = [repr(name) for name in argnames if name not in values]\n    missing = len(names)\n    if missing == 1:\n        s = names[0]\n    elif missing == 2:\n        s = \"{} and {}\".format(*names)\n    else:\n        tail = \", {} and {}\".format(*names[-2:])\n        del names[-2:]\n        s = \", \".join(names) + tail\n    raise TypeError(\"%s() missing %i required %s argument%s: %s\" %\n                    (f_name, missing,\n                      \"positional\" if pos else \"keyword-only\",\n                      \"\" if missing == 1 else \"s\", s))\n\ndef _too_many(f_name, args, kwonly, varargs, defcount, given, values):\n    atleast = len(args) - defcount\n    kwonly_given = len([arg for arg in kwonly if arg in values])\n    if varargs:\n        plural = atleast != 1\n        sig = \"at least %d\" % (atleast,)\n    elif defcount:\n        plural = True\n        sig = \"from %d to %d\" % (atleast, len(args))\n    else:\n        plural = len(args) != 1\n        sig = str(len(args))\n    kwonly_sig = \"\"\n    if kwonly_given:\n        msg = \" positional argument%s (and %d keyword-only argument%s)\"\n        kwonly_sig = (msg % (\"s\" if given != 1 else \"\", kwonly_given,\n                             \"s\" if kwonly_given != 1 else \"\"))\n    raise TypeError(\"%s() takes %s positional argument%s but %d%s %s given\" %\n            (f_name, sig, \"s\" if plural else \"\", given, kwonly_sig,\n             \"was\" if given == 1 and not kwonly_given else \"were\"))\n\ndef getcallargs(func, /, *positional, **named):\n    \"\"\"Get the mapping of arguments to values.\n\n    A dict is returned, with keys the function argument names (including the\n    names of the * and ** arguments, if any), and values the respective bound\n    values from 'positional' and 'named'.\"\"\"\n    spec = getfullargspec(func)\n    args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, ann = spec\n    f_name = func.__name__\n    arg2value = {}\n\n\n    if ismethod(func) and func.__self__ is not None:\n        # implicit 'self' (or 'cls' for classmethods) argument\n        positional = (func.__self__,) + positional\n    num_pos = len(positional)\n    num_args = len(args)\n    num_defaults = len(defaults) if defaults else 0\n\n    n = min(num_pos, num_args)\n    for i in range(n):\n        arg2value[args[i]] = positional[i]\n    if varargs:\n        arg2value[varargs] = tuple(positional[n:])\n    possible_kwargs = set(args + kwonlyargs)\n    if varkw:\n        arg2value[varkw] = {}\n    for kw, value in named.items():\n        if kw not in possible_kwargs:\n            if not varkw:\n                raise TypeError(\"%s() got an unexpected keyword argument %r\" %\n                                (f_name, kw))\n            arg2value[varkw][kw] = value\n            continue\n        if kw in arg2value:\n            raise TypeError(\"%s() got multiple values for argument %r\" %\n                            (f_name, kw))\n        arg2value[kw] = value\n    if num_pos > num_args and not varargs:\n        _too_many(f_name, args, kwonlyargs, varargs, num_defaults,\n                   num_pos, arg2value)\n    if num_pos < num_args:\n        req = args[:num_args - num_defaults]\n        for arg in req:\n            if arg not in arg2value:\n                _missing_arguments(f_name, req, True, arg2value)\n        for i, arg in enumerate(args[num_args - num_defaults:]):\n            if arg not in arg2value:\n                arg2value[arg] = defaults[i]\n    missing = 0\n    for kwarg in kwonlyargs:\n        if kwarg not in arg2value:\n            if kwonlydefaults and kwarg in kwonlydefaults:\n                arg2value[kwarg] = kwonlydefaults[kwarg]\n            else:\n                missing += 1\n    if missing:\n        _missing_arguments(f_name, kwonlyargs, False, arg2value)\n    return arg2value\n\nClosureVars = namedtuple('ClosureVars', 'nonlocals globals builtins unbound')\n\ndef getclosurevars(func):\n    \"\"\"\n    Get the mapping of free variables to their current values.\n\n    Returns a named tuple of dicts mapping the current nonlocal, global\n    and builtin references as seen by the body of the function. A final\n    set of unbound names that could not be resolved is also provided.\n    \"\"\"\n\n    if ismethod(func):\n        func = func.__func__\n\n    if not isfunction(func):\n        raise TypeError(\"{!r} is not a Python function\".format(func))\n\n    code = func.__code__\n    # Nonlocal references are named in co_freevars and resolved\n    # by looking them up in __closure__ by positional index\n    if func.__closure__ is None:\n        nonlocal_vars = {}\n    else:\n        nonlocal_vars = {\n            var : cell.cell_contents\n            for var, cell in zip(code.co_freevars, func.__closure__)\n       }\n\n    # Global and builtin references are named in co_names and resolved\n    # by looking them up in __globals__ or __builtins__\n    global_ns = func.__globals__\n    builtin_ns = global_ns.get(\"__builtins__\", builtins.__dict__)\n    if ismodule(builtin_ns):\n        builtin_ns = builtin_ns.__dict__\n    global_vars = {}\n    builtin_vars = {}\n    unbound_names = set()\n    for name in code.co_names:\n        if name in (\"None\", \"True\", \"False\"):\n            # Because these used to be builtins instead of keywords, they\n            # may still show up as name references. We ignore them.\n            continue\n        try:\n            global_vars[name] = global_ns[name]\n        except KeyError:\n            try:\n                builtin_vars[name] = builtin_ns[name]\n            except KeyError:\n                unbound_names.add(name)\n\n    return ClosureVars(nonlocal_vars, global_vars,\n                       builtin_vars, unbound_names)\n\n# -------------------------------------------------- stack frame extraction\n\n_Traceback = namedtuple('_Traceback', 'filename lineno function code_context index')\n\nclass Traceback(_Traceback):\n    def __new__(cls, filename, lineno, function, code_context, index, *, positions=None):\n        instance = super().__new__(cls, filename, lineno, function, code_context, index)\n        instance.positions = positions\n        return instance\n\n    def __repr__(self):\n        return ('Traceback(filename={!r}, lineno={!r}, function={!r}, '\n               'code_context={!r}, index={!r}, positions={!r})'.format(\n                self.filename, self.lineno, self.function, self.code_context,\n                self.index, self.positions))\n\ndef _get_code_position_from_tb(tb):\n    code, instruction_index = tb.tb_frame.f_code, tb.tb_lasti\n    return _get_code_position(code, instruction_index)\n\ndef _get_code_position(code, instruction_index):\n    if instruction_index < 0:\n        return (None, None, None, None)\n    positions_gen = code.co_positions()\n    # The nth entry in code.co_positions() corresponds to instruction (2*n)th since Python 3.10+\n    return next(itertools.islice(positions_gen, instruction_index // 2, None))\n\ndef getframeinfo(frame, context=1):\n    \"\"\"Get information about a frame or traceback object.\n\n    A tuple of five things is returned: the filename, the line number of\n    the current line, the function name, a list of lines of context from\n    the source code, and the index of the current line within that list.\n    The optional second argument specifies the number of lines of context\n    to return, which are centered around the current line.\"\"\"\n    if istraceback(frame):\n        positions = _get_code_position_from_tb(frame)\n        lineno = frame.tb_lineno\n        frame = frame.tb_frame\n    else:\n        lineno = frame.f_lineno\n        positions = _get_code_position(frame.f_code, frame.f_lasti)\n\n    if positions[0] is None:\n        frame, *positions = (frame, lineno, *positions[1:])\n    else:\n        frame, *positions = (frame, *positions)\n\n    lineno = positions[0]\n\n    if not isframe(frame):\n        raise TypeError('{!r} is not a frame or traceback object'.format(frame))\n\n    filename = getsourcefile(frame) or getfile(frame)\n    if context > 0:\n        start = lineno - 1 - context//2\n        try:\n            lines, lnum = findsource(frame)\n        except OSError:\n            lines = index = None\n        else:\n            start = max(0, min(start, len(lines) - context))\n            lines = lines[start:start+context]\n            index = lineno - 1 - start\n    else:\n        lines = index = None\n\n    return Traceback(filename, lineno, frame.f_code.co_name, lines,\n                     index, positions=dis.Positions(*positions))\n\ndef getlineno(frame):\n    \"\"\"Get the line number from a frame object, allowing for optimization.\"\"\"\n    # FrameType.f_lineno is now a descriptor that grovels co_lnotab\n    return frame.f_lineno\n\n_FrameInfo = namedtuple('_FrameInfo', ('frame',) + Traceback._fields)\nclass FrameInfo(_FrameInfo):\n    def __new__(cls, frame, filename, lineno, function, code_context, index, *, positions=None):\n        instance = super().__new__(cls, frame, filename, lineno, function, code_context, index)\n        instance.positions = positions\n        return instance\n\n    def __repr__(self):\n        return ('FrameInfo(frame={!r}, filename={!r}, lineno={!r}, function={!r}, '\n               'code_context={!r}, index={!r}, positions={!r})'.format(\n                self.frame, self.filename, self.lineno, self.function,\n                self.code_context, self.index, self.positions))\n\ndef getouterframes(frame, context=1):\n    \"\"\"Get a list of records for a frame and all higher (calling) frames.\n\n    Each record contains a frame object, filename, line number, function\n    name, a list of lines of context, and index within the context.\"\"\"\n    framelist = []\n    while frame:\n        traceback_info = getframeinfo(frame, context)\n        frameinfo = (frame,) + traceback_info\n        framelist.append(FrameInfo(*frameinfo, positions=traceback_info.positions))\n        frame = frame.f_back\n    return framelist\n\ndef getinnerframes(tb, context=1):\n    \"\"\"Get a list of records for a traceback's frame and all lower frames.\n\n    Each record contains a frame object, filename, line number, function\n    name, a list of lines of context, and index within the context.\"\"\"\n    framelist = []\n    while tb:\n        traceback_info = getframeinfo(tb, context)\n        frameinfo = (tb.tb_frame,) + traceback_info\n        framelist.append(FrameInfo(*frameinfo, positions=traceback_info.positions))\n        tb = tb.tb_next\n    return framelist\n\ndef currentframe():\n    \"\"\"Return the frame of the caller or None if this is not possible.\"\"\"\n    return sys._getframe(1) if hasattr(sys, \"_getframe\") else None\n\ndef stack(context=1):\n    \"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\n    return getouterframes(sys._getframe(1), context)\n\ndef trace(context=1):\n    \"\"\"Return a list of records for the stack below the current exception.\"\"\"\n    return getinnerframes(sys.exc_info()[2], context)\n\n\n# ------------------------------------------------ static version of getattr\n\n_sentinel = object()\n\ndef _static_getmro(klass):\n    return type.__dict__['__mro__'].__get__(klass)\n\ndef _check_instance(obj, attr):\n    instance_dict = {}\n    try:\n        instance_dict = object.__getattribute__(obj, \"__dict__\")\n    except AttributeError:\n        pass\n    return dict.get(instance_dict, attr, _sentinel)\n\n\ndef _check_class(klass, attr):\n    for entry in _static_getmro(klass):\n        if _shadowed_dict(type(entry)) is _sentinel:\n            try:\n                return entry.__dict__[attr]\n            except KeyError:\n                pass\n    return _sentinel\n\ndef _is_type(obj):\n    try:\n        _static_getmro(obj)\n    except TypeError:\n        return False\n    return True\n\ndef _shadowed_dict(klass):\n    dict_attr = type.__dict__[\"__dict__\"]\n    for entry in _static_getmro(klass):\n        try:\n            class_dict = dict_attr.__get__(entry)[\"__dict__\"]\n        except KeyError:\n            pass\n        else:\n            if not (type(class_dict) is types.GetSetDescriptorType and\n                    class_dict.__name__ == \"__dict__\" and\n                    class_dict.__objclass__ is entry):\n                return class_dict\n    return _sentinel\n\ndef getattr_static(obj, attr, default=_sentinel):\n    \"\"\"Retrieve attributes without triggering dynamic lookup via the\n       descriptor protocol,  __getattr__ or __getattribute__.\n\n       Note: this function may not be able to retrieve all attributes\n       that getattr can fetch (like dynamically created attributes)\n       and may find attributes that getattr can't (like descriptors\n       that raise AttributeError). It can also return descriptor objects\n       instead of instance members in some cases. See the\n       documentation for details.\n    \"\"\"\n    instance_result = _sentinel\n    if not _is_type(obj):\n        klass = type(obj)\n        dict_attr = _shadowed_dict(klass)\n        if (dict_attr is _sentinel or\n            type(dict_attr) is types.MemberDescriptorType):\n            instance_result = _check_instance(obj, attr)\n    else:\n        klass = obj\n\n    klass_result = _check_class(klass, attr)\n\n    if instance_result is not _sentinel and klass_result is not _sentinel:\n        if _check_class(type(klass_result), \"__get__\") is not _sentinel and (\n            _check_class(type(klass_result), \"__set__\") is not _sentinel\n            or _check_class(type(klass_result), \"__delete__\") is not _sentinel\n        ):\n            return klass_result\n\n    if instance_result is not _sentinel:\n        return instance_result\n    if klass_result is not _sentinel:\n        return klass_result\n\n    if obj is klass:\n        # for types we check the metaclass too\n        for entry in _static_getmro(type(klass)):\n            if _shadowed_dict(type(entry)) is _sentinel:\n                try:\n                    return entry.__dict__[attr]\n                except KeyError:\n                    pass\n    if default is not _sentinel:\n        return default\n    raise AttributeError(attr)\n\n\n# ------------------------------------------------ generator introspection\n\nGEN_CREATED = 'GEN_CREATED'\nGEN_RUNNING = 'GEN_RUNNING'\nGEN_SUSPENDED = 'GEN_SUSPENDED'\nGEN_CLOSED = 'GEN_CLOSED'\n\ndef getgeneratorstate(generator):\n    \"\"\"Get current state of a generator-iterator.\n\n    Possible states are:\n      GEN_CREATED: Waiting to start execution.\n      GEN_RUNNING: Currently being executed by the interpreter.\n      GEN_SUSPENDED: Currently suspended at a yield expression.\n      GEN_CLOSED: Execution has completed.\n    \"\"\"\n    if generator.gi_running:\n        return GEN_RUNNING\n    if generator.gi_suspended:\n        return GEN_SUSPENDED\n    if generator.gi_frame is None:\n        return GEN_CLOSED\n    return GEN_CREATED\n\n\ndef getgeneratorlocals(generator):\n    \"\"\"\n    Get the mapping of generator local variables to their current values.\n\n    A dict is returned, with the keys the local variable names and values the\n    bound values.\"\"\"\n\n    if not isgenerator(generator):\n        raise TypeError(\"{!r} is not a Python generator\".format(generator))\n\n    frame = getattr(generator, \"gi_frame\", None)\n    if frame is not None:\n        return generator.gi_frame.f_locals\n    else:\n        return {}\n\n\n# ------------------------------------------------ coroutine introspection\n\nCORO_CREATED = 'CORO_CREATED'\nCORO_RUNNING = 'CORO_RUNNING'\nCORO_SUSPENDED = 'CORO_SUSPENDED'\nCORO_CLOSED = 'CORO_CLOSED'\n\ndef getcoroutinestate(coroutine):\n    \"\"\"Get current state of a coroutine object.\n\n    Possible states are:\n      CORO_CREATED: Waiting to start execution.\n      CORO_RUNNING: Currently being executed by the interpreter.\n      CORO_SUSPENDED: Currently suspended at an await expression.\n      CORO_CLOSED: Execution has completed.\n    \"\"\"\n    if coroutine.cr_running:\n        return CORO_RUNNING\n    if coroutine.cr_suspended:\n        return CORO_SUSPENDED\n    if coroutine.cr_frame is None:\n        return CORO_CLOSED\n    return CORO_CREATED\n\n\ndef getcoroutinelocals(coroutine):\n    \"\"\"\n    Get the mapping of coroutine local variables to their current values.\n\n    A dict is returned, with the keys the local variable names and values the\n    bound values.\"\"\"\n    frame = getattr(coroutine, \"cr_frame\", None)\n    if frame is not None:\n        return frame.f_locals\n    else:\n        return {}\n\n\n###############################################################################\n### Function Signature Object (PEP 362)\n###############################################################################\n\n\n_NonUserDefinedCallables = (types.WrapperDescriptorType,\n                            types.MethodWrapperType,\n                            types.ClassMethodDescriptorType,\n                            types.BuiltinFunctionType)\n\n\ndef _signature_get_user_defined_method(cls, method_name):\n    \"\"\"Private helper. Checks if ``cls`` has an attribute\n    named ``method_name`` and returns it only if it is a\n    pure python function.\n    \"\"\"\n    if method_name == '__new__':\n        meth = getattr(cls, method_name, None)\n    else:\n        meth = getattr_static(cls, method_name, None)\n    if meth is None or isinstance(meth, _NonUserDefinedCallables):\n        # Once '__signature__' will be added to 'C'-level\n        # callables, this check won't be necessary\n        return None\n    if method_name != '__new__':\n        meth = _descriptor_get(meth, cls)\n    return meth\n\n\ndef _signature_get_partial(wrapped_sig, partial, extra_args=()):\n    \"\"\"Private helper to calculate how 'wrapped_sig' signature will\n    look like after applying a 'functools.partial' object (or alike)\n    on it.\n    \"\"\"\n\n    old_params = wrapped_sig.parameters\n    new_params = OrderedDict(old_params.items())\n\n    partial_args = partial.args or ()\n    partial_keywords = partial.keywords or {}\n\n    if extra_args:\n        partial_args = extra_args + partial_args\n\n    try:\n        ba = wrapped_sig.bind_partial(*partial_args, **partial_keywords)\n    except TypeError as ex:\n        msg = 'partial object {!r} has incorrect arguments'.format(partial)\n        raise ValueError(msg) from ex\n\n\n    transform_to_kwonly = False\n    for param_name, param in old_params.items():\n        try:\n            arg_value = ba.arguments[param_name]\n        except KeyError:\n            pass\n        else:\n            if param.kind is _POSITIONAL_ONLY:\n                # If positional-only parameter is bound by partial,\n                # it effectively disappears from the signature\n                new_params.pop(param_name)\n                continue\n\n            if param.kind is _POSITIONAL_OR_KEYWORD:\n                if param_name in partial_keywords:\n                    # This means that this parameter, and all parameters\n                    # after it should be keyword-only (and var-positional\n                    # should be removed). Here's why. Consider the following\n                    # function:\n                    #     foo(a, b, *args, c):\n                    #         pass\n                    #\n                    # \"partial(foo, a='spam')\" will have the following\n                    # signature: \"(*, a='spam', b, c)\". Because attempting\n                    # to call that partial with \"(10, 20)\" arguments will\n                    # raise a TypeError, saying that \"a\" argument received\n                    # multiple values.\n                    transform_to_kwonly = True\n                    # Set the new default value\n                    new_params[param_name] = param.replace(default=arg_value)\n                else:\n                    # was passed as a positional argument\n                    new_params.pop(param.name)\n                    continue\n\n            if param.kind is _KEYWORD_ONLY:\n                # Set the new default value\n                new_params[param_name] = param.replace(default=arg_value)\n\n        if transform_to_kwonly:\n            assert param.kind is not _POSITIONAL_ONLY\n\n            if param.kind is _POSITIONAL_OR_KEYWORD:\n                new_param = new_params[param_name].replace(kind=_KEYWORD_ONLY)\n                new_params[param_name] = new_param\n                new_params.move_to_end(param_name)\n            elif param.kind in (_KEYWORD_ONLY, _VAR_KEYWORD):\n                new_params.move_to_end(param_name)\n            elif param.kind is _VAR_POSITIONAL:\n                new_params.pop(param.name)\n\n    return wrapped_sig.replace(parameters=new_params.values())\n\n\ndef _signature_bound_method(sig):\n    \"\"\"Private helper to transform signatures for unbound\n    functions to bound methods.\n    \"\"\"\n\n    params = tuple(sig.parameters.values())\n\n    if not params or params[0].kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n        raise ValueError('invalid method signature')\n\n    kind = params[0].kind\n    if kind in (_POSITIONAL_OR_KEYWORD, _POSITIONAL_ONLY):\n        # Drop first parameter:\n        # '(p1, p2[, ...])' -> '(p2[, ...])'\n        params = params[1:]\n    else:\n        if kind is not _VAR_POSITIONAL:\n            # Unless we add a new parameter type we never\n            # get here\n            raise ValueError('invalid argument type')\n        # It's a var-positional parameter.\n        # Do nothing. '(*args[, ...])' -> '(*args[, ...])'\n\n    return sig.replace(parameters=params)\n\n\ndef _signature_is_builtin(obj):\n    \"\"\"Private helper to test if `obj` is a callable that might\n    support Argument Clinic's __text_signature__ protocol.\n    \"\"\"\n    return (isbuiltin(obj) or\n            ismethoddescriptor(obj) or\n            isinstance(obj, _NonUserDefinedCallables) or\n            # Can't test 'isinstance(type)' here, as it would\n            # also be True for regular python classes\n            obj in (type, object))\n\n\ndef _signature_is_functionlike(obj):\n    \"\"\"Private helper to test if `obj` is a duck type of FunctionType.\n    A good example of such objects are functions compiled with\n    Cython, which have all attributes that a pure Python function\n    would have, but have their code statically compiled.\n    \"\"\"\n\n    if not callable(obj) or isclass(obj):\n        # All function-like objects are obviously callables,\n        # and not classes.\n        return False\n\n    name = getattr(obj, '__name__', None)\n    code = getattr(obj, '__code__', None)\n    defaults = getattr(obj, '__defaults__', _void) # Important to use _void ...\n    kwdefaults = getattr(obj, '__kwdefaults__', _void) # ... and not None here\n    annotations = getattr(obj, '__annotations__', None)\n\n    return (isinstance(code, types.CodeType) and\n            isinstance(name, str) and\n            (defaults is None or isinstance(defaults, tuple)) and\n            (kwdefaults is None or isinstance(kwdefaults, dict)) and\n            (isinstance(annotations, (dict)) or annotations is None) )\n\n\ndef _signature_strip_non_python_syntax(signature):\n    \"\"\"\n    Private helper function. Takes a signature in Argument Clinic's\n    extended signature format.\n\n    Returns a tuple of three things:\n      * that signature re-rendered in standard Python syntax,\n      * the index of the \"self\" parameter (generally 0), or None if\n        the function does not have a \"self\" parameter, and\n      * the index of the last \"positional only\" parameter,\n        or None if the signature has no positional-only parameters.\n    \"\"\"\n\n    if not signature:\n        return signature, None, None\n\n    self_parameter = None\n    last_positional_only = None\n\n    lines = [l.encode('ascii') for l in signature.split('\\n') if l]\n    generator = iter(lines).__next__\n    token_stream = tokenize.tokenize(generator)\n\n    delayed_comma = False\n    skip_next_comma = False\n    text = []\n    add = text.append\n\n    current_parameter = 0\n    OP = token.OP\n    ERRORTOKEN = token.ERRORTOKEN\n\n    # token stream always starts with ENCODING token, skip it\n    t = next(token_stream)\n    assert t.type == tokenize.ENCODING\n\n    for t in token_stream:\n        type, string = t.type, t.string\n\n        if type == OP:\n            if string == ',':\n                if skip_next_comma:\n                    skip_next_comma = False\n                else:\n                    assert not delayed_comma\n                    delayed_comma = True\n                    current_parameter += 1\n                continue\n\n            if string == '/':\n                assert not skip_next_comma\n                assert last_positional_only is None\n                skip_next_comma = True\n                last_positional_only = current_parameter - 1\n                continue\n\n        if (type == ERRORTOKEN) and (string == '$'):\n            assert self_parameter is None\n            self_parameter = current_parameter\n            continue\n\n        if delayed_comma:\n            delayed_comma = False\n            if not ((type == OP) and (string == ')')):\n                add(', ')\n        add(string)\n        if (string == ','):\n            add(' ')\n    clean_signature = ''.join(text)\n    return clean_signature, self_parameter, last_positional_only\n\n\ndef _signature_fromstr(cls, obj, s, skip_bound_arg=True):\n    \"\"\"Private helper to parse content of '__text_signature__'\n    and return a Signature based on it.\n    \"\"\"\n    Parameter = cls._parameter_cls\n\n    clean_signature, self_parameter, last_positional_only = \\\n        _signature_strip_non_python_syntax(s)\n\n    program = \"def foo\" + clean_signature + \": pass\"\n\n    try:\n        module = ast.parse(program)\n    except SyntaxError:\n        module = None\n\n    if not isinstance(module, ast.Module):\n        raise ValueError(\"{!r} builtin has invalid signature\".format(obj))\n\n    f = module.body[0]\n\n    parameters = []\n    empty = Parameter.empty\n\n    module = None\n    module_dict = {}\n    module_name = getattr(obj, '__module__', None)\n    if module_name:\n        module = sys.modules.get(module_name, None)\n        if module:\n            module_dict = module.__dict__\n    sys_module_dict = sys.modules.copy()\n\n    def parse_name(node):\n        assert isinstance(node, ast.arg)\n        if node.annotation is not None:\n            raise ValueError(\"Annotations are not currently supported\")\n        return node.arg\n\n    def wrap_value(s):\n        try:\n            value = eval(s, module_dict)\n        except NameError:\n            try:\n                value = eval(s, sys_module_dict)\n            except NameError:\n                raise ValueError\n\n        if isinstance(value, (str, int, float, bytes, bool, type(None))):\n            return ast.Constant(value)\n        raise ValueError\n\n    class RewriteSymbolics(ast.NodeTransformer):\n        def visit_Attribute(self, node):\n            a = []\n            n = node\n            while isinstance(n, ast.Attribute):\n                a.append(n.attr)\n                n = n.value\n            if not isinstance(n, ast.Name):\n                raise ValueError\n            a.append(n.id)\n            value = \".\".join(reversed(a))\n            return wrap_value(value)\n\n        def visit_Name(self, node):\n            if not isinstance(node.ctx, ast.Load):\n                raise ValueError()\n            return wrap_value(node.id)\n\n        def visit_BinOp(self, node):\n            # Support constant folding of a couple simple binary operations\n            # commonly used to define default values in text signatures\n            left = self.visit(node.left)\n            right = self.visit(node.right)\n            if not isinstance(left, ast.Constant) or not isinstance(right, ast.Constant):\n                raise ValueError\n            if isinstance(node.op, ast.Add):\n                return ast.Constant(left.value + right.value)\n            elif isinstance(node.op, ast.Sub):\n                return ast.Constant(left.value - right.value)\n            elif isinstance(node.op, ast.BitOr):\n                return ast.Constant(left.value | right.value)\n            raise ValueError\n\n    def p(name_node, default_node, default=empty):\n        name = parse_name(name_node)\n        if default_node and default_node is not _empty:\n            try:\n                default_node = RewriteSymbolics().visit(default_node)\n                default = ast.literal_eval(default_node)\n            except ValueError:\n                raise ValueError(\"{!r} builtin has invalid signature\".format(obj)) from None\n        parameters.append(Parameter(name, kind, default=default, annotation=empty))\n\n    # non-keyword-only parameters\n    args = reversed(f.args.args)\n    defaults = reversed(f.args.defaults)\n    iter = itertools.zip_longest(args, defaults, fillvalue=None)\n    if last_positional_only is not None:\n        kind = Parameter.POSITIONAL_ONLY\n    else:\n        kind = Parameter.POSITIONAL_OR_KEYWORD\n    for i, (name, default) in enumerate(reversed(list(iter))):\n        p(name, default)\n        if i == last_positional_only:\n            kind = Parameter.POSITIONAL_OR_KEYWORD\n\n    # *args\n    if f.args.vararg:\n        kind = Parameter.VAR_POSITIONAL\n        p(f.args.vararg, empty)\n\n    # keyword-only arguments\n    kind = Parameter.KEYWORD_ONLY\n    for name, default in zip(f.args.kwonlyargs, f.args.kw_defaults):\n        p(name, default)\n\n    # **kwargs\n    if f.args.kwarg:\n        kind = Parameter.VAR_KEYWORD\n        p(f.args.kwarg, empty)\n\n    if self_parameter is not None:\n        # Possibly strip the bound argument:\n        #    - We *always* strip first bound argument if\n        #      it is a module.\n        #    - We don't strip first bound argument if\n        #      skip_bound_arg is False.\n        assert parameters\n        _self = getattr(obj, '__self__', None)\n        self_isbound = _self is not None\n        self_ismodule = ismodule(_self)\n        if self_isbound and (self_ismodule or skip_bound_arg):\n            parameters.pop(0)\n        else:\n            # for builtins, self parameter is always positional-only!\n            p = parameters[0].replace(kind=Parameter.POSITIONAL_ONLY)\n            parameters[0] = p\n\n    return cls(parameters, return_annotation=cls.empty)\n\n\ndef _signature_from_builtin(cls, func, skip_bound_arg=True):\n    \"\"\"Private helper function to get signature for\n    builtin callables.\n    \"\"\"\n\n    if not _signature_is_builtin(func):\n        raise TypeError(\"{!r} is not a Python builtin \"\n                        \"function\".format(func))\n\n    s = getattr(func, \"__text_signature__\", None)\n    if not s:\n        raise ValueError(\"no signature found for builtin {!r}\".format(func))\n\n    return _signature_fromstr(cls, func, s, skip_bound_arg)\n\n\ndef _signature_from_function(cls, func, skip_bound_arg=True,\n                             globals=None, locals=None, eval_str=False):\n    \"\"\"Private helper: constructs Signature for the given python function.\"\"\"\n\n    is_duck_function = False\n    if not isfunction(func):\n        if _signature_is_functionlike(func):\n            is_duck_function = True\n        else:\n            # If it's not a pure Python function, and not a duck type\n            # of pure function:\n            raise TypeError('{!r} is not a Python function'.format(func))\n\n    s = getattr(func, \"__text_signature__\", None)\n    if s:\n        return _signature_fromstr(cls, func, s, skip_bound_arg)\n\n    Parameter = cls._parameter_cls\n\n    # Parameter information.\n    func_code = func.__code__\n    pos_count = func_code.co_argcount\n    arg_names = func_code.co_varnames\n    posonly_count = func_code.co_posonlyargcount\n    positional = arg_names[:pos_count]\n    keyword_only_count = func_code.co_kwonlyargcount\n    keyword_only = arg_names[pos_count:pos_count + keyword_only_count]\n    annotations = get_annotations(func, globals=globals, locals=locals, eval_str=eval_str)\n    defaults = func.__defaults__\n    kwdefaults = func.__kwdefaults__\n\n    if defaults:\n        pos_default_count = len(defaults)\n    else:\n        pos_default_count = 0\n\n    parameters = []\n\n    non_default_count = pos_count - pos_default_count\n    posonly_left = posonly_count\n\n    # Non-keyword-only parameters w/o defaults.\n    for name in positional[:non_default_count]:\n        kind = _POSITIONAL_ONLY if posonly_left else _POSITIONAL_OR_KEYWORD\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=kind))\n        if posonly_left:\n            posonly_left -= 1\n\n    # ... w/ defaults.\n    for offset, name in enumerate(positional[non_default_count:]):\n        kind = _POSITIONAL_ONLY if posonly_left else _POSITIONAL_OR_KEYWORD\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=kind,\n                                    default=defaults[offset]))\n        if posonly_left:\n            posonly_left -= 1\n\n    # *args\n    if func_code.co_flags & CO_VARARGS:\n        name = arg_names[pos_count + keyword_only_count]\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=_VAR_POSITIONAL))\n\n    # Keyword-only parameters.\n    for name in keyword_only:\n        default = _empty\n        if kwdefaults is not None:\n            default = kwdefaults.get(name, _empty)\n\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=_KEYWORD_ONLY,\n                                    default=default))\n    # **kwargs\n    if func_code.co_flags & CO_VARKEYWORDS:\n        index = pos_count + keyword_only_count\n        if func_code.co_flags & CO_VARARGS:\n            index += 1\n\n        name = arg_names[index]\n        annotation = annotations.get(name, _empty)\n        parameters.append(Parameter(name, annotation=annotation,\n                                    kind=_VAR_KEYWORD))\n\n    # Is 'func' is a pure Python function - don't validate the\n    # parameters list (for correct order and defaults), it should be OK.\n    return cls(parameters,\n               return_annotation=annotations.get('return', _empty),\n               __validate_parameters__=is_duck_function)\n\n\ndef _descriptor_get(descriptor, obj):\n    if isclass(descriptor):\n        return descriptor\n    get = getattr(type(descriptor), '__get__', _sentinel)\n    if get is _sentinel:\n        return descriptor\n    return get(descriptor, obj, type(obj))\n\n\ndef _signature_from_callable(obj, *,\n                             follow_wrapper_chains=True,\n                             skip_bound_arg=True,\n                             globals=None,\n                             locals=None,\n                             eval_str=False,\n                             sigcls):\n\n    \"\"\"Private helper function to get signature for arbitrary\n    callable objects.\n    \"\"\"\n\n    _get_signature_of = functools.partial(_signature_from_callable,\n                                follow_wrapper_chains=follow_wrapper_chains,\n                                skip_bound_arg=skip_bound_arg,\n                                globals=globals,\n                                locals=locals,\n                                sigcls=sigcls,\n                                eval_str=eval_str)\n\n    if not callable(obj):\n        raise TypeError('{!r} is not a callable object'.format(obj))\n\n    if isinstance(obj, types.MethodType):\n        # In this case we skip the first parameter of the underlying\n        # function (usually `self` or `cls`).\n        sig = _get_signature_of(obj.__func__)\n\n        if skip_bound_arg:\n            return _signature_bound_method(sig)\n        else:\n            return sig\n\n    # Was this function wrapped by a decorator?\n    if follow_wrapper_chains:\n        # Unwrap until we find an explicit signature or a MethodType (which will be\n        # handled explicitly below).\n        obj = unwrap(obj, stop=(lambda f: hasattr(f, \"__signature__\")\n                                or isinstance(f, types.MethodType)))\n        if isinstance(obj, types.MethodType):\n            # If the unwrapped object is a *method*, we might want to\n            # skip its first parameter (self).\n            # See test_signature_wrapped_bound_method for details.\n            return _get_signature_of(obj)\n\n    try:\n        sig = obj.__signature__\n    except AttributeError:\n        pass\n    else:\n        if sig is not None:\n            if not isinstance(sig, Signature):\n                raise TypeError(\n                    'unexpected object {!r} in __signature__ '\n                    'attribute'.format(sig))\n            return sig\n\n    try:\n        partialmethod = obj._partialmethod\n    except AttributeError:\n        pass\n    else:\n        if isinstance(partialmethod, functools.partialmethod):\n            # Unbound partialmethod (see functools.partialmethod)\n            # This means, that we need to calculate the signature\n            # as if it's a regular partial object, but taking into\n            # account that the first positional argument\n            # (usually `self`, or `cls`) will not be passed\n            # automatically (as for boundmethods)\n\n            wrapped_sig = _get_signature_of(partialmethod.func)\n\n            sig = _signature_get_partial(wrapped_sig, partialmethod, (None,))\n            first_wrapped_param = tuple(wrapped_sig.parameters.values())[0]\n            if first_wrapped_param.kind is Parameter.VAR_POSITIONAL:\n                # First argument of the wrapped callable is `*args`, as in\n                # `partialmethod(lambda *args)`.\n                return sig\n            else:\n                sig_params = tuple(sig.parameters.values())\n                assert (not sig_params or\n                        first_wrapped_param is not sig_params[0])\n                new_params = (first_wrapped_param,) + sig_params\n                return sig.replace(parameters=new_params)\n\n    if isfunction(obj) or _signature_is_functionlike(obj):\n        # If it's a pure Python function, or an object that is duck type\n        # of a Python function (Cython functions, for instance), then:\n        return _signature_from_function(sigcls, obj,\n                                        skip_bound_arg=skip_bound_arg,\n                                        globals=globals, locals=locals, eval_str=eval_str)\n\n    if _signature_is_builtin(obj):\n        return _signature_from_builtin(sigcls, obj,\n                                       skip_bound_arg=skip_bound_arg)\n\n    if isinstance(obj, functools.partial):\n        wrapped_sig = _get_signature_of(obj.func)\n        return _signature_get_partial(wrapped_sig, obj)\n\n    if isinstance(obj, type):\n        # obj is a class or a metaclass\n\n        # First, let's see if it has an overloaded __call__ defined\n        # in its metaclass\n        call = _signature_get_user_defined_method(type(obj), '__call__')\n        if call is not None:\n            return _get_signature_of(call)\n\n        new = _signature_get_user_defined_method(obj, '__new__')\n        init = _signature_get_user_defined_method(obj, '__init__')\n\n        # Go through the MRO and see if any class has user-defined\n        # pure Python __new__ or __init__ method\n        for base in obj.__mro__:\n            # Now we check if the 'obj' class has an own '__new__' method\n            if new is not None and '__new__' in base.__dict__:\n                sig = _get_signature_of(new)\n                if skip_bound_arg:\n                    sig = _signature_bound_method(sig)\n                return sig\n            # or an own '__init__' method\n            elif init is not None and '__init__' in base.__dict__:\n                return _get_signature_of(init)\n\n        # At this point we know, that `obj` is a class, with no user-\n        # defined '__init__', '__new__', or class-level '__call__'\n\n        for base in obj.__mro__[:-1]:\n            # Since '__text_signature__' is implemented as a\n            # descriptor that extracts text signature from the\n            # class docstring, if 'obj' is derived from a builtin\n            # class, its own '__text_signature__' may be 'None'.\n            # Therefore, we go through the MRO (except the last\n            # class in there, which is 'object') to find the first\n            # class with non-empty text signature.\n            try:\n                text_sig = base.__text_signature__\n            except AttributeError:\n                pass\n            else:\n                if text_sig:\n                    # If 'base' class has a __text_signature__ attribute:\n                    # return a signature based on it\n                    return _signature_fromstr(sigcls, base, text_sig)\n\n        # No '__text_signature__' was found for the 'obj' class.\n        # Last option is to check if its '__init__' is\n        # object.__init__ or type.__init__.\n        if type not in obj.__mro__:\n            # We have a class (not metaclass), but no user-defined\n            # __init__ or __new__ for it\n            if (obj.__init__ is object.__init__ and\n                obj.__new__ is object.__new__):\n                # Return a signature of 'object' builtin.\n                return sigcls.from_callable(object)\n            else:\n                raise ValueError(\n                    'no signature found for builtin type {!r}'.format(obj))\n\n    else:\n        # An object with __call__\n        call = getattr_static(type(obj), '__call__', None)\n        if call is not None:\n            call = _descriptor_get(call, obj)\n            return _get_signature_of(call)\n\n    raise ValueError('callable {!r} is not supported by signature'.format(obj))\n\n\nclass _void:\n    \"\"\"A private marker - used in Parameter & Signature.\"\"\"\n\n\nclass _empty:\n    \"\"\"Marker object for Signature.empty and Parameter.empty.\"\"\"\n\n\nclass _ParameterKind(enum.IntEnum):\n    POSITIONAL_ONLY = 'positional-only'\n    POSITIONAL_OR_KEYWORD = 'positional or keyword'\n    VAR_POSITIONAL = 'variadic positional'\n    KEYWORD_ONLY = 'keyword-only'\n    VAR_KEYWORD = 'variadic keyword'\n\n    def __new__(cls, description):\n        value = len(cls.__members__)\n        member = int.__new__(cls, value)\n        member._value_ = value\n        member.description = description\n        return member\n\n    def __str__(self):\n        return self.name\n\n_POSITIONAL_ONLY         = _ParameterKind.POSITIONAL_ONLY\n_POSITIONAL_OR_KEYWORD   = _ParameterKind.POSITIONAL_OR_KEYWORD\n_VAR_POSITIONAL          = _ParameterKind.VAR_POSITIONAL\n_KEYWORD_ONLY            = _ParameterKind.KEYWORD_ONLY\n_VAR_KEYWORD             = _ParameterKind.VAR_KEYWORD\n\n\nclass Parameter:\n    \"\"\"Represents a parameter in a function signature.\n\n    Has the following public attributes:\n\n    * name : str\n        The name of the parameter as a string.\n    * default : object\n        The default value for the parameter if specified.  If the\n        parameter has no default value, this attribute is set to\n        `Parameter.empty`.\n    * annotation\n        The annotation for the parameter if specified.  If the\n        parameter has no annotation, this attribute is set to\n        `Parameter.empty`.\n    * kind : str\n        Describes how argument values are bound to the parameter.\n        Possible values: `Parameter.POSITIONAL_ONLY`,\n        `Parameter.POSITIONAL_OR_KEYWORD`, `Parameter.VAR_POSITIONAL`,\n        `Parameter.KEYWORD_ONLY`, `Parameter.VAR_KEYWORD`.\n    \"\"\"\n\n    __slots__ = ('_name', '_kind', '_default', '_annotation')\n\n    POSITIONAL_ONLY         = _POSITIONAL_ONLY\n    POSITIONAL_OR_KEYWORD   = _POSITIONAL_OR_KEYWORD\n    VAR_POSITIONAL          = _VAR_POSITIONAL\n    KEYWORD_ONLY            = _KEYWORD_ONLY\n    VAR_KEYWORD             = _VAR_KEYWORD\n\n    empty = _empty\n\n    def __init__(self, name, kind, *, default=_empty, annotation=_empty):\n        try:\n            self._kind = _ParameterKind(kind)\n        except ValueError:\n            raise ValueError(f'value {kind!r} is not a valid Parameter.kind')\n        if default is not _empty:\n            if self._kind in (_VAR_POSITIONAL, _VAR_KEYWORD):\n                msg = '{} parameters cannot have default values'\n                msg = msg.format(self._kind.description)\n                raise ValueError(msg)\n        self._default = default\n        self._annotation = annotation\n\n        if name is _empty:\n            raise ValueError('name is a required attribute for Parameter')\n\n        if not isinstance(name, str):\n            msg = 'name must be a str, not a {}'.format(type(name).__name__)\n            raise TypeError(msg)\n\n        if name[0] == '.' and name[1:].isdigit():\n            # These are implicit arguments generated by comprehensions. In\n            # order to provide a friendlier interface to users, we recast\n            # their name as \"implicitN\" and treat them as positional-only.\n            # See issue 19611.\n            if self._kind != _POSITIONAL_OR_KEYWORD:\n                msg = (\n                    'implicit arguments must be passed as '\n                    'positional or keyword arguments, not {}'\n                )\n                msg = msg.format(self._kind.description)\n                raise ValueError(msg)\n            self._kind = _POSITIONAL_ONLY\n            name = 'implicit{}'.format(name[1:])\n\n        # It's possible for C functions to have a positional-only parameter\n        # where the name is a keyword, so for compatibility we'll allow it.\n        is_keyword = iskeyword(name) and self._kind is not _POSITIONAL_ONLY\n        if is_keyword or not name.isidentifier():\n            raise ValueError('{!r} is not a valid parameter name'.format(name))\n\n        self._name = name\n\n    def __reduce__(self):\n        return (type(self),\n                (self._name, self._kind),\n                {'_default': self._default,\n                 '_annotation': self._annotation})\n\n    def __setstate__(self, state):\n        self._default = state['_default']\n        self._annotation = state['_annotation']\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def default(self):\n        return self._default\n\n    @property\n    def annotation(self):\n        return self._annotation\n\n    @property\n    def kind(self):\n        return self._kind\n\n    def replace(self, *, name=_void, kind=_void,\n                annotation=_void, default=_void):\n        \"\"\"Creates a customized copy of the Parameter.\"\"\"\n\n        if name is _void:\n            name = self._name\n\n        if kind is _void:\n            kind = self._kind\n\n        if annotation is _void:\n            annotation = self._annotation\n\n        if default is _void:\n            default = self._default\n\n        return type(self)(name, kind, default=default, annotation=annotation)\n\n    def __str__(self):\n        kind = self.kind\n        formatted = self._name\n\n        # Add annotation and default value\n        if self._annotation is not _empty:\n            formatted = '{}: {}'.format(formatted,\n                                       formatannotation(self._annotation))\n\n        if self._default is not _empty:\n            if self._annotation is not _empty:\n                formatted = '{} = {}'.format(formatted, repr(self._default))\n            else:\n                formatted = '{}={}'.format(formatted, repr(self._default))\n\n        if kind == _VAR_POSITIONAL:\n            formatted = '*' + formatted\n        elif kind == _VAR_KEYWORD:\n            formatted = '**' + formatted\n\n        return formatted\n\n    def __repr__(self):\n        return '<{} \"{}\">'.format(self.__class__.__name__, self)\n\n    def __hash__(self):\n        return hash((self.name, self.kind, self.annotation, self.default))\n\n    def __eq__(self, other):\n        if self is other:\n            return True\n        if not isinstance(other, Parameter):\n            return NotImplemented\n        return (self._name == other._name and\n                self._kind == other._kind and\n                self._default == other._default and\n                self._annotation == other._annotation)\n\n\nclass BoundArguments:\n    \"\"\"Result of `Signature.bind` call.  Holds the mapping of arguments\n    to the function's parameters.\n\n    Has the following public attributes:\n\n    * arguments : dict\n        An ordered mutable mapping of parameters' names to arguments' values.\n        Does not contain arguments' default values.\n    * signature : Signature\n        The Signature object that created this instance.\n    * args : tuple\n        Tuple of positional arguments values.\n    * kwargs : dict\n        Dict of keyword arguments values.\n    \"\"\"\n\n    __slots__ = ('arguments', '_signature', '__weakref__')\n\n    def __init__(self, signature, arguments):\n        self.arguments = arguments\n        self._signature = signature\n\n    @property\n    def signature(self):\n        return self._signature\n\n    @property\n    def args(self):\n        args = []\n        for param_name, param in self._signature.parameters.items():\n            if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n                break\n\n            try:\n                arg = self.arguments[param_name]\n            except KeyError:\n                # We're done here. Other arguments\n                # will be mapped in 'BoundArguments.kwargs'\n                break\n            else:\n                if param.kind == _VAR_POSITIONAL:\n                    # *args\n                    args.extend(arg)\n                else:\n                    # plain argument\n                    args.append(arg)\n\n        return tuple(args)\n\n    @property\n    def kwargs(self):\n        kwargs = {}\n        kwargs_started = False\n        for param_name, param in self._signature.parameters.items():\n            if not kwargs_started:\n                if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n                    kwargs_started = True\n                else:\n                    if param_name not in self.arguments:\n                        kwargs_started = True\n                        continue\n\n            if not kwargs_started:\n                continue\n\n            try:\n                arg = self.arguments[param_name]\n            except KeyError:\n                pass\n            else:\n                if param.kind == _VAR_KEYWORD:\n                    # **kwargs\n                    kwargs.update(arg)\n                else:\n                    # plain keyword argument\n                    kwargs[param_name] = arg\n\n        return kwargs\n\n    def apply_defaults(self):\n        \"\"\"Set default values for missing arguments.\n\n        For variable-positional arguments (*args) the default is an\n        empty tuple.\n\n        For variable-keyword arguments (**kwargs) the default is an\n        empty dict.\n        \"\"\"\n        arguments = self.arguments\n        new_arguments = []\n        for name, param in self._signature.parameters.items():\n            try:\n                new_arguments.append((name, arguments[name]))\n            except KeyError:\n                if param.default is not _empty:\n                    val = param.default\n                elif param.kind is _VAR_POSITIONAL:\n                    val = ()\n                elif param.kind is _VAR_KEYWORD:\n                    val = {}\n                else:\n                    # This BoundArguments was likely produced by\n                    # Signature.bind_partial().\n                    continue\n                new_arguments.append((name, val))\n        self.arguments = dict(new_arguments)\n\n    def __eq__(self, other):\n        if self is other:\n            return True\n        if not isinstance(other, BoundArguments):\n            return NotImplemented\n        return (self.signature == other.signature and\n                self.arguments == other.arguments)\n\n    def __setstate__(self, state):\n        self._signature = state['_signature']\n        self.arguments = state['arguments']\n\n    def __getstate__(self):\n        return {'_signature': self._signature, 'arguments': self.arguments}\n\n    def __repr__(self):\n        args = []\n        for arg, value in self.arguments.items():\n            args.append('{}={!r}'.format(arg, value))\n        return '<{} ({})>'.format(self.__class__.__name__, ', '.join(args))\n\n\nclass Signature:\n    \"\"\"A Signature object represents the overall signature of a function.\n    It stores a Parameter object for each parameter accepted by the\n    function, as well as information specific to the function itself.\n\n    A Signature object has the following public attributes and methods:\n\n    * parameters : OrderedDict\n        An ordered mapping of parameters' names to the corresponding\n        Parameter objects (keyword-only arguments are in the same order\n        as listed in `code.co_varnames`).\n    * return_annotation : object\n        The annotation for the return type of the function if specified.\n        If the function has no annotation for its return type, this\n        attribute is set to `Signature.empty`.\n    * bind(*args, **kwargs) -> BoundArguments\n        Creates a mapping from positional and keyword arguments to\n        parameters.\n    * bind_partial(*args, **kwargs) -> BoundArguments\n        Creates a partial mapping from positional and keyword arguments\n        to parameters (simulating 'functools.partial' behavior.)\n    \"\"\"\n\n    __slots__ = ('_return_annotation', '_parameters')\n\n    _parameter_cls = Parameter\n    _bound_arguments_cls = BoundArguments\n\n    empty = _empty\n\n    def __init__(self, parameters=None, *, return_annotation=_empty,\n                 __validate_parameters__=True):\n        \"\"\"Constructs Signature from the given list of Parameter\n        objects and 'return_annotation'.  All arguments are optional.\n        \"\"\"\n\n        if parameters is None:\n            params = OrderedDict()\n        else:\n            if __validate_parameters__:\n                params = OrderedDict()\n                top_kind = _POSITIONAL_ONLY\n                seen_default = False\n\n                for param in parameters:\n                    kind = param.kind\n                    name = param.name\n\n                    if kind < top_kind:\n                        msg = (\n                            'wrong parameter order: {} parameter before {} '\n                            'parameter'\n                        )\n                        msg = msg.format(top_kind.description,\n                                         kind.description)\n                        raise ValueError(msg)\n                    elif kind > top_kind:\n                        top_kind = kind\n\n                    if kind in (_POSITIONAL_ONLY, _POSITIONAL_OR_KEYWORD):\n                        if param.default is _empty:\n                            if seen_default:\n                                # No default for this parameter, but the\n                                # previous parameter of had a default\n                                msg = 'non-default argument follows default ' \\\n                                      'argument'\n                                raise ValueError(msg)\n                        else:\n                            # There is a default for this parameter.\n                            seen_default = True\n\n                    if name in params:\n                        msg = 'duplicate parameter name: {!r}'.format(name)\n                        raise ValueError(msg)\n\n                    params[name] = param\n            else:\n                params = OrderedDict((param.name, param) for param in parameters)\n\n        self._parameters = types.MappingProxyType(params)\n        self._return_annotation = return_annotation\n\n    @classmethod\n    def from_callable(cls, obj, *,\n                      follow_wrapped=True, globals=None, locals=None, eval_str=False):\n        \"\"\"Constructs Signature for the given callable object.\"\"\"\n        return _signature_from_callable(obj, sigcls=cls,\n                                        follow_wrapper_chains=follow_wrapped,\n                                        globals=globals, locals=locals, eval_str=eval_str)\n\n    @property\n    def parameters(self):\n        return self._parameters\n\n    @property\n    def return_annotation(self):\n        return self._return_annotation\n\n    def replace(self, *, parameters=_void, return_annotation=_void):\n        \"\"\"Creates a customized copy of the Signature.\n        Pass 'parameters' and/or 'return_annotation' arguments\n        to override them in the new copy.\n        \"\"\"\n\n        if parameters is _void:\n            parameters = self.parameters.values()\n\n        if return_annotation is _void:\n            return_annotation = self._return_annotation\n\n        return type(self)(parameters,\n                          return_annotation=return_annotation)\n\n    def _hash_basis(self):\n        params = tuple(param for param in self.parameters.values()\n                             if param.kind != _KEYWORD_ONLY)\n\n        kwo_params = {param.name: param for param in self.parameters.values()\n                                        if param.kind == _KEYWORD_ONLY}\n\n        return params, kwo_params, self.return_annotation\n\n    def __hash__(self):\n        params, kwo_params, return_annotation = self._hash_basis()\n        kwo_params = frozenset(kwo_params.values())\n        return hash((params, kwo_params, return_annotation))\n\n    def __eq__(self, other):\n        if self is other:\n            return True\n        if not isinstance(other, Signature):\n            return NotImplemented\n        return self._hash_basis() == other._hash_basis()\n\n    def _bind(self, args, kwargs, *, partial=False):\n        \"\"\"Private method. Don't use directly.\"\"\"\n\n        arguments = {}\n\n        parameters = iter(self.parameters.values())\n        parameters_ex = ()\n        arg_vals = iter(args)\n\n        while True:\n            # Let's iterate through the positional arguments and corresponding\n            # parameters\n            try:\n                arg_val = next(arg_vals)\n            except StopIteration:\n                # No more positional arguments\n                try:\n                    param = next(parameters)\n                except StopIteration:\n                    # No more parameters. That's it. Just need to check that\n                    # we have no `kwargs` after this while loop\n                    break\n                else:\n                    if param.kind == _VAR_POSITIONAL:\n                        # That's OK, just empty *args.  Let's start parsing\n                        # kwargs\n                        break\n                    elif param.name in kwargs:\n                        if param.kind == _POSITIONAL_ONLY:\n                            msg = '{arg!r} parameter is positional only, ' \\\n                                  'but was passed as a keyword'\n                            msg = msg.format(arg=param.name)\n                            raise TypeError(msg) from None\n                        parameters_ex = (param,)\n                        break\n                    elif (param.kind == _VAR_KEYWORD or\n                                                param.default is not _empty):\n                        # That's fine too - we have a default value for this\n                        # parameter.  So, lets start parsing `kwargs`, starting\n                        # with the current parameter\n                        parameters_ex = (param,)\n                        break\n                    else:\n                        # No default, not VAR_KEYWORD, not VAR_POSITIONAL,\n                        # not in `kwargs`\n                        if partial:\n                            parameters_ex = (param,)\n                            break\n                        else:\n                            msg = 'missing a required argument: {arg!r}'\n                            msg = msg.format(arg=param.name)\n                            raise TypeError(msg) from None\n            else:\n                # We have a positional argument to process\n                try:\n                    param = next(parameters)\n                except StopIteration:\n                    raise TypeError('too many positional arguments') from None\n                else:\n                    if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):\n                        # Looks like we have no parameter for this positional\n                        # argument\n                        raise TypeError(\n                            'too many positional arguments') from None\n\n                    if param.kind == _VAR_POSITIONAL:\n                        # We have an '*args'-like argument, let's fill it with\n                        # all positional arguments we have left and move on to\n                        # the next phase\n                        values = [arg_val]\n                        values.extend(arg_vals)\n                        arguments[param.name] = tuple(values)\n                        break\n\n                    if param.name in kwargs and param.kind != _POSITIONAL_ONLY:\n                        raise TypeError(\n                            'multiple values for argument {arg!r}'.format(\n                                arg=param.name)) from None\n\n                    arguments[param.name] = arg_val\n\n        # Now, we iterate through the remaining parameters to process\n        # keyword arguments\n        kwargs_param = None\n        for param in itertools.chain(parameters_ex, parameters):\n            if param.kind == _VAR_KEYWORD:\n                # Memorize that we have a '**kwargs'-like parameter\n                kwargs_param = param\n                continue\n\n            if param.kind == _VAR_POSITIONAL:\n                # Named arguments don't refer to '*args'-like parameters.\n                # We only arrive here if the positional arguments ended\n                # before reaching the last parameter before *args.\n                continue\n\n            param_name = param.name\n            try:\n                arg_val = kwargs.pop(param_name)\n            except KeyError:\n                # We have no value for this parameter.  It's fine though,\n                # if it has a default value, or it is an '*args'-like\n                # parameter, left alone by the processing of positional\n                # arguments.\n                if (not partial and param.kind != _VAR_POSITIONAL and\n                                                    param.default is _empty):\n                    raise TypeError('missing a required argument: {arg!r}'. \\\n                                    format(arg=param_name)) from None\n\n            else:\n                if param.kind == _POSITIONAL_ONLY:\n                    # This should never happen in case of a properly built\n                    # Signature object (but let's have this check here\n                    # to ensure correct behaviour just in case)\n                    raise TypeError('{arg!r} parameter is positional only, '\n                                    'but was passed as a keyword'. \\\n                                    format(arg=param.name))\n\n                arguments[param_name] = arg_val\n\n        if kwargs:\n            if kwargs_param is not None:\n                # Process our '**kwargs'-like parameter\n                arguments[kwargs_param.name] = kwargs\n            else:\n                raise TypeError(\n                    'got an unexpected keyword argument {arg!r}'.format(\n                        arg=next(iter(kwargs))))\n\n        return self._bound_arguments_cls(self, arguments)\n\n    def bind(self, /, *args, **kwargs):\n        \"\"\"Get a BoundArguments object, that maps the passed `args`\n        and `kwargs` to the function's signature.  Raises `TypeError`\n        if the passed arguments can not be bound.\n        \"\"\"\n        return self._bind(args, kwargs)\n\n    def bind_partial(self, /, *args, **kwargs):\n        \"\"\"Get a BoundArguments object, that partially maps the\n        passed `args` and `kwargs` to the function's signature.\n        Raises `TypeError` if the passed arguments can not be bound.\n        \"\"\"\n        return self._bind(args, kwargs, partial=True)\n\n    def __reduce__(self):\n        return (type(self),\n                (tuple(self._parameters.values()),),\n                {'_return_annotation': self._return_annotation})\n\n    def __setstate__(self, state):\n        self._return_annotation = state['_return_annotation']\n\n    def __repr__(self):\n        return '<{} {}>'.format(self.__class__.__name__, self)\n\n    def __str__(self):\n        result = []\n        render_pos_only_separator = False\n        render_kw_only_separator = True\n        for param in self.parameters.values():\n            formatted = str(param)\n\n            kind = param.kind\n\n            if kind == _POSITIONAL_ONLY:\n                render_pos_only_separator = True\n            elif render_pos_only_separator:\n                # It's not a positional-only parameter, and the flag\n                # is set to 'True' (there were pos-only params before.)\n                result.append('/')\n                render_pos_only_separator = False\n\n            if kind == _VAR_POSITIONAL:\n                # OK, we have an '*args'-like parameter, so we won't need\n                # a '*' to separate keyword-only arguments\n                render_kw_only_separator = False\n            elif kind == _KEYWORD_ONLY and render_kw_only_separator:\n                # We have a keyword-only parameter to render and we haven't\n                # rendered an '*args'-like parameter before, so add a '*'\n                # separator to the parameters list (\"foo(arg1, *, arg2)\" case)\n                result.append('*')\n                # This condition should be only triggered once, so\n                # reset the flag\n                render_kw_only_separator = False\n\n            result.append(formatted)\n\n        if render_pos_only_separator:\n            # There were only positional-only parameters, hence the\n            # flag was not reset to 'False'\n            result.append('/')\n\n        rendered = '({})'.format(', '.join(result))\n\n        if self.return_annotation is not _empty:\n            anno = formatannotation(self.return_annotation)\n            rendered += ' -> {}'.format(anno)\n\n        return rendered\n\n\ndef signature(obj, *, follow_wrapped=True, globals=None, locals=None, eval_str=False):\n    \"\"\"Get a signature object for the passed callable.\"\"\"\n    return Signature.from_callable(obj, follow_wrapped=follow_wrapped,\n                                   globals=globals, locals=locals, eval_str=eval_str)\n\n\ndef _main():\n    \"\"\" Logic for inspecting an object given at command line \"\"\"\n    import argparse\n    import importlib\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        'object',\n         help=\"The object to be analysed. \"\n              \"It supports the 'module:qualname' syntax\")\n    parser.add_argument(\n        '-d', '--details', action='store_true',\n        help='Display info about the module rather than its source code')\n\n    args = parser.parse_args()\n\n    target = args.object\n    mod_name, has_attrs, attrs = target.partition(\":\")\n    try:\n        obj = module = importlib.import_module(mod_name)\n    except Exception as exc:\n        msg = \"Failed to import {} ({}: {})\".format(mod_name,\n                                                    type(exc).__name__,\n                                                    exc)\n        print(msg, file=sys.stderr)\n        sys.exit(2)\n\n    if has_attrs:\n        parts = attrs.split(\".\")\n        obj = module\n        for part in parts:\n            obj = getattr(obj, part)\n\n    if module.__name__ in sys.builtin_module_names:\n        print(\"Can't get info for builtin modules.\", file=sys.stderr)\n        sys.exit(1)\n\n    if args.details:\n        print('Target: {}'.format(target))\n        print('Origin: {}'.format(getsourcefile(module)))\n        print('Cached: {}'.format(module.__cached__))\n        if obj is module:\n            print('Loader: {}'.format(repr(module.__loader__)))\n            if hasattr(module, '__path__'):\n                print('Submodule search path: {}'.format(module.__path__))\n        else:\n            try:\n                __, lineno = findsource(obj)\n            except Exception:\n                pass\n            else:\n                print('Line: {}'.format(lineno))\n\n        print('\\n')\n    else:\n        print(getsource(obj))\n\n\nif __name__ == \"__main__\":\n    _main()\n", 3326], "/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py": ["import asyncio\nimport copy\nimport inspect\nimport re\nimport warnings\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterator,\n    Awaitable,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Mapping,\n    MutableMapping,\n    Optional,\n    Protocol,\n    Set,\n    Tuple,\n    Type,\n    TypedDict,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom redis._parsers.helpers import (\n    _RedisCallbacks,\n    _RedisCallbacksRESP2,\n    _RedisCallbacksRESP3,\n    bool_ok,\n)\nfrom redis.asyncio.connection import (\n    Connection,\n    ConnectionPool,\n    SSLConnection,\n    UnixDomainSocketConnection,\n)\nfrom redis.asyncio.lock import Lock\nfrom redis.asyncio.retry import Retry\nfrom redis.backoff import ExponentialWithJitterBackoff\nfrom redis.client import (\n    EMPTY_RESPONSE,\n    NEVER_DECODE,\n    AbstractRedis,\n    CaseInsensitiveDict,\n)\nfrom redis.commands import (\n    AsyncCoreCommands,\n    AsyncRedisModuleCommands,\n    AsyncSentinelCommands,\n    list_or_args,\n)\nfrom redis.credentials import CredentialProvider\nfrom redis.event import (\n    AfterPooledConnectionsInstantiationEvent,\n    AfterPubSubConnectionInstantiationEvent,\n    AfterSingleConnectionInstantiationEvent,\n    ClientType,\n    EventDispatcher,\n)\nfrom redis.exceptions import (\n    ConnectionError,\n    ExecAbortError,\n    PubSubError,\n    RedisError,\n    ResponseError,\n    WatchError,\n)\nfrom redis.typing import ChannelT, EncodableT, KeyT\nfrom redis.utils import (\n    SSL_AVAILABLE,\n    _set_info_logger,\n    deprecated_args,\n    deprecated_function,\n    get_lib_version,\n    safe_str,\n    str_if_bytes,\n    truncate_text,\n)\n\nif TYPE_CHECKING and SSL_AVAILABLE:\n    from ssl import TLSVersion, VerifyMode\nelse:\n    TLSVersion = None\n    VerifyMode = None\n\nPubSubHandler = Callable[[Dict[str, str]], Awaitable[None]]\n_KeyT = TypeVar(\"_KeyT\", bound=KeyT)\n_ArgT = TypeVar(\"_ArgT\", KeyT, EncodableT)\n_RedisT = TypeVar(\"_RedisT\", bound=\"Redis\")\n_NormalizeKeysT = TypeVar(\"_NormalizeKeysT\", bound=Mapping[ChannelT, object])\nif TYPE_CHECKING:\n    from redis.commands.core import Script\n\n\nclass ResponseCallbackProtocol(Protocol):\n    def __call__(self, response: Any, **kwargs): ...\n\n\nclass AsyncResponseCallbackProtocol(Protocol):\n    async def __call__(self, response: Any, **kwargs): ...\n\n\nResponseCallbackT = Union[ResponseCallbackProtocol, AsyncResponseCallbackProtocol]\n\n\nclass Redis(\n    AbstractRedis, AsyncRedisModuleCommands, AsyncCoreCommands, AsyncSentinelCommands\n):\n    \"\"\"\n    Implementation of the Redis protocol.\n\n    This abstract class provides a Python interface to all Redis commands\n    and an implementation of the Redis protocol.\n\n    Pipelines derive from this, implementing how\n    the commands are sent and received to the Redis server. Based on\n    configuration, an instance will either use a ConnectionPool, or\n    Connection object to talk to redis.\n    \"\"\"\n\n    response_callbacks: MutableMapping[Union[str, bytes], ResponseCallbackT]\n\n    @classmethod\n    def from_url(\n        cls,\n        url: str,\n        single_connection_client: bool = False,\n        auto_close_connection_pool: Optional[bool] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Return a Redis client object configured from the given URL\n\n        For example::\n\n            redis://[[username]:[password]]@localhost:6379/0\n            rediss://[[username]:[password]]@localhost:6379/0\n            unix://[username@]/path/to/socket.sock?db=0[&password=password]\n\n        Three URL schemes are supported:\n\n        - `redis://` creates a TCP socket connection. See more at:\n          <https://www.iana.org/assignments/uri-schemes/prov/redis>\n        - `rediss://` creates a SSL wrapped TCP socket connection. See more at:\n          <https://www.iana.org/assignments/uri-schemes/prov/rediss>\n        - ``unix://``: creates a Unix Domain Socket connection.\n\n        The username, password, hostname, path and all querystring values\n        are passed through urllib.parse.unquote in order to replace any\n        percent-encoded values with their corresponding characters.\n\n        There are several ways to specify a database number. The first value\n        found will be used:\n\n        1. A ``db`` querystring option, e.g. redis://localhost?db=0\n\n        2. If using the redis:// or rediss:// schemes, the path argument\n               of the url, e.g. redis://localhost/0\n\n        3. A ``db`` keyword argument to this function.\n\n        If none of these options are specified, the default db=0 is used.\n\n        All querystring options are cast to their appropriate Python types.\n        Boolean arguments can be specified with string values \"True\"/\"False\"\n        or \"Yes\"/\"No\". Values that cannot be properly cast cause a\n        ``ValueError`` to be raised. Once parsed, the querystring arguments\n        and keyword arguments are passed to the ``ConnectionPool``'s\n        class initializer. In the case of conflicting arguments, querystring\n        arguments always win.\n\n        \"\"\"\n        connection_pool = ConnectionPool.from_url(url, **kwargs)\n        client = cls(\n            connection_pool=connection_pool,\n            single_connection_client=single_connection_client,\n        )\n        if auto_close_connection_pool is not None:\n            warnings.warn(\n                DeprecationWarning(\n                    '\"auto_close_connection_pool\" is deprecated '\n                    \"since version 5.0.1. \"\n                    \"Please create a ConnectionPool explicitly and \"\n                    \"provide to the Redis() constructor instead.\"\n                )\n            )\n        else:\n            auto_close_connection_pool = True\n        client.auto_close_connection_pool = auto_close_connection_pool\n        return client\n\n    @classmethod\n    def from_pool(\n        cls: Type[\"Redis\"],\n        connection_pool: ConnectionPool,\n    ) -> \"Redis\":\n        \"\"\"\n        Return a Redis client from the given connection pool.\n        The Redis client will take ownership of the connection pool and\n        close it when the Redis client is closed.\n        \"\"\"\n        client = cls(\n            connection_pool=connection_pool,\n        )\n        client.auto_close_connection_pool = True\n        return client\n\n    @deprecated_args(\n        args_to_warn=[\"retry_on_timeout\"],\n        reason=\"TimeoutError is included by default.\",\n        version=\"6.0.0\",\n    )\n    def __init__(\n        self,\n        *,\n        host: str = \"localhost\",\n        port: int = 6379,\n        db: Union[str, int] = 0,\n        password: Optional[str] = None,\n        socket_timeout: Optional[float] = None,\n        socket_connect_timeout: Optional[float] = None,\n        socket_keepalive: Optional[bool] = None,\n        socket_keepalive_options: Optional[Mapping[int, Union[int, bytes]]] = None,\n        connection_pool: Optional[ConnectionPool] = None,\n        unix_socket_path: Optional[str] = None,\n        encoding: str = \"utf-8\",\n        encoding_errors: str = \"strict\",\n        decode_responses: bool = False,\n        retry_on_timeout: bool = False,\n        retry: Retry = Retry(\n            backoff=ExponentialWithJitterBackoff(base=1, cap=10), retries=3\n        ),\n        retry_on_error: Optional[list] = None,\n        ssl: bool = False,\n        ssl_keyfile: Optional[str] = None,\n        ssl_certfile: Optional[str] = None,\n        ssl_cert_reqs: Union[str, VerifyMode] = \"required\",\n        ssl_ca_certs: Optional[str] = None,\n        ssl_ca_data: Optional[str] = None,\n        ssl_check_hostname: bool = True,\n        ssl_min_version: Optional[TLSVersion] = None,\n        ssl_ciphers: Optional[str] = None,\n        max_connections: Optional[int] = None,\n        single_connection_client: bool = False,\n        health_check_interval: int = 0,\n        client_name: Optional[str] = None,\n        lib_name: Optional[str] = \"redis-py\",\n        lib_version: Optional[str] = get_lib_version(),\n        username: Optional[str] = None,\n        auto_close_connection_pool: Optional[bool] = None,\n        redis_connect_func=None,\n        credential_provider: Optional[CredentialProvider] = None,\n        protocol: Optional[int] = 2,\n        event_dispatcher: Optional[EventDispatcher] = None,\n    ):\n        \"\"\"\n        Initialize a new Redis client.\n\n        To specify a retry policy for specific errors, you have two options:\n\n        1. Set the `retry_on_error` to a list of the error/s to retry on, and\n        you can also set `retry` to a valid `Retry` object(in case the default\n        one is not appropriate) - with this approach the retries will be triggered\n        on the default errors specified in the Retry object enriched with the\n        errors specified in `retry_on_error`.\n\n        2. Define a `Retry` object with configured 'supported_errors' and set\n        it to the `retry` parameter - with this approach you completely redefine\n        the errors on which retries will happen.\n\n        `retry_on_timeout` is deprecated - please include the TimeoutError\n        either in the Retry object or in the `retry_on_error` list.\n\n        When 'connection_pool' is provided - the retry configuration of the\n        provided pool will be used.\n        \"\"\"\n        kwargs: Dict[str, Any]\n        if event_dispatcher is None:\n            self._event_dispatcher = EventDispatcher()\n        else:\n            self._event_dispatcher = event_dispatcher\n        # auto_close_connection_pool only has an effect if connection_pool is\n        # None. It is assumed that if connection_pool is not None, the user\n        # wants to manage the connection pool themselves.\n        if auto_close_connection_pool is not None:\n            warnings.warn(\n                DeprecationWarning(\n                    '\"auto_close_connection_pool\" is deprecated '\n                    \"since version 5.0.1. \"\n                    \"Please create a ConnectionPool explicitly and \"\n                    \"provide to the Redis() constructor instead.\"\n                )\n            )\n        else:\n            auto_close_connection_pool = True\n\n        if not connection_pool:\n            # Create internal connection pool, expected to be closed by Redis instance\n            if not retry_on_error:\n                retry_on_error = []\n            kwargs = {\n                \"db\": db,\n                \"username\": username,\n                \"password\": password,\n                \"credential_provider\": credential_provider,\n                \"socket_timeout\": socket_timeout,\n                \"encoding\": encoding,\n                \"encoding_errors\": encoding_errors,\n                \"decode_responses\": decode_responses,\n                \"retry_on_error\": retry_on_error,\n                \"retry\": copy.deepcopy(retry),\n                \"max_connections\": max_connections,\n                \"health_check_interval\": health_check_interval,\n                \"client_name\": client_name,\n                \"lib_name\": lib_name,\n                \"lib_version\": lib_version,\n                \"redis_connect_func\": redis_connect_func,\n                \"protocol\": protocol,\n            }\n            # based on input, setup appropriate connection args\n            if unix_socket_path is not None:\n                kwargs.update(\n                    {\n                        \"path\": unix_socket_path,\n                        \"connection_class\": UnixDomainSocketConnection,\n                    }\n                )\n            else:\n                # TCP specific options\n                kwargs.update(\n                    {\n                        \"host\": host,\n                        \"port\": port,\n                        \"socket_connect_timeout\": socket_connect_timeout,\n                        \"socket_keepalive\": socket_keepalive,\n                        \"socket_keepalive_options\": socket_keepalive_options,\n                    }\n                )\n\n                if ssl:\n                    kwargs.update(\n                        {\n                            \"connection_class\": SSLConnection,\n                            \"ssl_keyfile\": ssl_keyfile,\n                            \"ssl_certfile\": ssl_certfile,\n                            \"ssl_cert_reqs\": ssl_cert_reqs,\n                            \"ssl_ca_certs\": ssl_ca_certs,\n                            \"ssl_ca_data\": ssl_ca_data,\n                            \"ssl_check_hostname\": ssl_check_hostname,\n                            \"ssl_min_version\": ssl_min_version,\n                            \"ssl_ciphers\": ssl_ciphers,\n                        }\n                    )\n            # This arg only used if no pool is passed in\n            self.auto_close_connection_pool = auto_close_connection_pool\n            connection_pool = ConnectionPool(**kwargs)\n            self._event_dispatcher.dispatch(\n                AfterPooledConnectionsInstantiationEvent(\n                    [connection_pool], ClientType.ASYNC, credential_provider\n                )\n            )\n        else:\n            # If a pool is passed in, do not close it\n            self.auto_close_connection_pool = False\n            self._event_dispatcher.dispatch(\n                AfterPooledConnectionsInstantiationEvent(\n                    [connection_pool], ClientType.ASYNC, credential_provider\n                )\n            )\n\n        self.connection_pool = connection_pool\n        self.single_connection_client = single_connection_client\n        self.connection: Optional[Connection] = None\n\n        self.response_callbacks = CaseInsensitiveDict(_RedisCallbacks)\n\n        if self.connection_pool.connection_kwargs.get(\"protocol\") in [\"3\", 3]:\n            self.response_callbacks.update(_RedisCallbacksRESP3)\n        else:\n            self.response_callbacks.update(_RedisCallbacksRESP2)\n\n        # If using a single connection client, we need to lock creation-of and use-of\n        # the client in order to avoid race conditions such as using asyncio.gather\n        # on a set of redis commands\n        self._single_conn_lock = asyncio.Lock()\n\n    def __repr__(self):\n        return (\n            f\"<{self.__class__.__module__}.{self.__class__.__name__}\"\n            f\"({self.connection_pool!r})>\"\n        )\n\n    def __await__(self):\n        return self.initialize().__await__()\n\n    async def initialize(self: _RedisT) -> _RedisT:\n        if self.single_connection_client:\n            async with self._single_conn_lock:\n                if self.connection is None:\n                    self.connection = await self.connection_pool.get_connection()\n\n            self._event_dispatcher.dispatch(\n                AfterSingleConnectionInstantiationEvent(\n                    self.connection, ClientType.ASYNC, self._single_conn_lock\n                )\n            )\n        return self\n\n    def set_response_callback(self, command: str, callback: ResponseCallbackT):\n        \"\"\"Set a custom Response Callback\"\"\"\n        self.response_callbacks[command] = callback\n\n    def get_encoder(self):\n        \"\"\"Get the connection pool's encoder\"\"\"\n        return self.connection_pool.get_encoder()\n\n    def get_connection_kwargs(self):\n        \"\"\"Get the connection's key-word arguments\"\"\"\n        return self.connection_pool.connection_kwargs\n\n    def get_retry(self) -> Optional[Retry]:\n        return self.get_connection_kwargs().get(\"retry\")\n\n    def set_retry(self, retry: Retry) -> None:\n        self.get_connection_kwargs().update({\"retry\": retry})\n        self.connection_pool.set_retry(retry)\n\n    def load_external_module(self, funcname, func):\n        \"\"\"\n        This function can be used to add externally defined redis modules,\n        and their namespaces to the redis client.\n\n        funcname - A string containing the name of the function to create\n        func - The function, being added to this class.\n\n        ex: Assume that one has a custom redis module named foomod that\n        creates command named 'foo.dothing' and 'foo.anotherthing' in redis.\n        To load function functions into this namespace:\n\n        from redis import Redis\n        from foomodule import F\n        r = Redis()\n        r.load_external_module(\"foo\", F)\n        r.foo().dothing('your', 'arguments')\n\n        For a concrete example see the reimport of the redisjson module in\n        tests/test_connection.py::test_loading_external_modules\n        \"\"\"\n        setattr(self, funcname, func)\n\n    def pipeline(\n        self, transaction: bool = True, shard_hint: Optional[str] = None\n    ) -> \"Pipeline\":\n        \"\"\"\n        Return a new pipeline object that can queue multiple commands for\n        later execution. ``transaction`` indicates whether all commands\n        should be executed atomically. Apart from making a group of operations\n        atomic, pipelines are useful for reducing the back-and-forth overhead\n        between the client and server.\n        \"\"\"\n        return Pipeline(\n            self.connection_pool, self.response_callbacks, transaction, shard_hint\n        )\n\n    async def transaction(\n        self,\n        func: Callable[[\"Pipeline\"], Union[Any, Awaitable[Any]]],\n        *watches: KeyT,\n        shard_hint: Optional[str] = None,\n        value_from_callable: bool = False,\n        watch_delay: Optional[float] = None,\n    ):\n        \"\"\"\n        Convenience method for executing the callable `func` as a transaction\n        while watching all keys specified in `watches`. The 'func' callable\n        should expect a single argument which is a Pipeline object.\n        \"\"\"\n        pipe: Pipeline\n        async with self.pipeline(True, shard_hint) as pipe:\n            while True:\n                try:\n                    if watches:\n                        await pipe.watch(*watches)\n                    func_value = func(pipe)\n                    if inspect.isawaitable(func_value):\n                        func_value = await func_value\n                    exec_value = await pipe.execute()\n                    return func_value if value_from_callable else exec_value\n                except WatchError:\n                    if watch_delay is not None and watch_delay > 0:\n                        await asyncio.sleep(watch_delay)\n                    continue\n\n    def lock(\n        self,\n        name: KeyT,\n        timeout: Optional[float] = None,\n        sleep: float = 0.1,\n        blocking: bool = True,\n        blocking_timeout: Optional[float] = None,\n        lock_class: Optional[Type[Lock]] = None,\n        thread_local: bool = True,\n        raise_on_release_error: bool = True,\n    ) -> Lock:\n        \"\"\"\n        Return a new Lock object using key ``name`` that mimics\n        the behavior of threading.Lock.\n\n        If specified, ``timeout`` indicates a maximum life for the lock.\n        By default, it will remain locked until release() is called.\n\n        ``sleep`` indicates the amount of time to sleep per loop iteration\n        when the lock is in blocking mode and another client is currently\n        holding the lock.\n\n        ``blocking`` indicates whether calling ``acquire`` should block until\n        the lock has been acquired or to fail immediately, causing ``acquire``\n        to return False and the lock not being acquired. Defaults to True.\n        Note this value can be overridden by passing a ``blocking``\n        argument to ``acquire``.\n\n        ``blocking_timeout`` indicates the maximum amount of time in seconds to\n        spend trying to acquire the lock. A value of ``None`` indicates\n        continue trying forever. ``blocking_timeout`` can be specified as a\n        float or integer, both representing the number of seconds to wait.\n\n        ``lock_class`` forces the specified lock implementation. Note that as\n        of redis-py 3.0, the only lock class we implement is ``Lock`` (which is\n        a Lua-based lock). So, it's unlikely you'll need this parameter, unless\n        you have created your own custom lock class.\n\n        ``thread_local`` indicates whether the lock token is placed in\n        thread-local storage. By default, the token is placed in thread local\n        storage so that a thread only sees its token, not a token set by\n        another thread. Consider the following timeline:\n\n            time: 0, thread-1 acquires `my-lock`, with a timeout of 5 seconds.\n                     thread-1 sets the token to \"abc\"\n            time: 1, thread-2 blocks trying to acquire `my-lock` using the\n                     Lock instance.\n            time: 5, thread-1 has not yet completed. redis expires the lock\n                     key.\n            time: 5, thread-2 acquired `my-lock` now that it's available.\n                     thread-2 sets the token to \"xyz\"\n            time: 6, thread-1 finishes its work and calls release(). if the\n                     token is *not* stored in thread local storage, then\n                     thread-1 would see the token value as \"xyz\" and would be\n                     able to successfully release the thread-2's lock.\n\n        ``raise_on_release_error`` indicates whether to raise an exception when\n        the lock is no longer owned when exiting the context manager. By default,\n        this is True, meaning an exception will be raised. If False, the warning\n        will be logged and the exception will be suppressed.\n\n        In some use cases it's necessary to disable thread local storage. For\n        example, if you have code where one thread acquires a lock and passes\n        that lock instance to a worker thread to release later. If thread\n        local storage isn't disabled in this case, the worker thread won't see\n        the token set by the thread that acquired the lock. Our assumption\n        is that these cases aren't common and as such default to using\n        thread local storage.\"\"\"\n        if lock_class is None:\n            lock_class = Lock\n        return lock_class(\n            self,\n            name,\n            timeout=timeout,\n            sleep=sleep,\n            blocking=blocking,\n            blocking_timeout=blocking_timeout,\n            thread_local=thread_local,\n            raise_on_release_error=raise_on_release_error,\n        )\n\n    def pubsub(self, **kwargs) -> \"PubSub\":\n        \"\"\"\n        Return a Publish/Subscribe object. With this object, you can\n        subscribe to channels and listen for messages that get published to\n        them.\n        \"\"\"\n        return PubSub(\n            self.connection_pool, event_dispatcher=self._event_dispatcher, **kwargs\n        )\n\n    def monitor(self) -> \"Monitor\":\n        return Monitor(self.connection_pool)\n\n    def client(self) -> \"Redis\":\n        return self.__class__(\n            connection_pool=self.connection_pool, single_connection_client=True\n        )\n\n    async def __aenter__(self: _RedisT) -> _RedisT:\n        return await self.initialize()\n\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        await self.aclose()\n\n    _DEL_MESSAGE = \"Unclosed Redis client\"\n\n    # passing _warnings and _grl as argument default since they may be gone\n    # by the time __del__ is called at shutdown\n    def __del__(\n        self,\n        _warn: Any = warnings.warn,\n        _grl: Any = asyncio.get_running_loop,\n    ) -> None:\n        if hasattr(self, \"connection\") and (self.connection is not None):\n            _warn(f\"Unclosed client session {self!r}\", ResourceWarning, source=self)\n            try:\n                context = {\"client\": self, \"message\": self._DEL_MESSAGE}\n                _grl().call_exception_handler(context)\n            except RuntimeError:\n                pass\n            self.connection._close()\n\n    async def aclose(self, close_connection_pool: Optional[bool] = None) -> None:\n        \"\"\"\n        Closes Redis client connection\n\n        Args:\n            close_connection_pool:\n                decides whether to close the connection pool used by this Redis client,\n                overriding Redis.auto_close_connection_pool.\n                By default, let Redis.auto_close_connection_pool decide\n                whether to close the connection pool.\n        \"\"\"\n        conn = self.connection\n        if conn:\n            self.connection = None\n            await self.connection_pool.release(conn)\n        if close_connection_pool or (\n            close_connection_pool is None and self.auto_close_connection_pool\n        ):\n            await self.connection_pool.disconnect()\n\n    @deprecated_function(version=\"5.0.1\", reason=\"Use aclose() instead\", name=\"close\")\n    async def close(self, close_connection_pool: Optional[bool] = None) -> None:\n        \"\"\"\n        Alias for aclose(), for backwards compatibility\n        \"\"\"\n        await self.aclose(close_connection_pool)\n\n    async def _send_command_parse_response(self, conn, command_name, *args, **options):\n        \"\"\"\n        Send a command and parse the response\n        \"\"\"\n        await conn.send_command(*args)\n        return await self.parse_response(conn, command_name, **options)\n\n    async def _close_connection(self, conn: Connection):\n        \"\"\"\n        Close the connection before retrying.\n\n        The supported exceptions are already checked in the\n        retry object so we don't need to do it here.\n\n        After we disconnect the connection, it will try to reconnect and\n        do a health check as part of the send_command logic(on connection level).\n        \"\"\"\n        await conn.disconnect()\n\n    # COMMAND EXECUTION AND PROTOCOL PARSING\n    async def execute_command(self, *args, **options):\n        \"\"\"Execute a command and return a parsed response\"\"\"\n        await self.initialize()\n        pool = self.connection_pool\n        command_name = args[0]\n        conn = self.connection or await pool.get_connection()\n\n        if self.single_connection_client:\n            await self._single_conn_lock.acquire()\n        try:\n            return await conn.retry.call_with_retry(\n                lambda: self._send_command_parse_response(\n                    conn, command_name, *args, **options\n                ),\n                lambda _: self._close_connection(conn),\n            )\n        finally:\n            if self.single_connection_client:\n                self._single_conn_lock.release()\n            if not self.connection:\n                await pool.release(conn)\n\n    async def parse_response(\n        self, connection: Connection, command_name: Union[str, bytes], **options\n    ):\n        \"\"\"Parses a response from the Redis server\"\"\"\n        try:\n            if NEVER_DECODE in options:\n                response = await connection.read_response(disable_decoding=True)\n                options.pop(NEVER_DECODE)\n            else:\n                response = await connection.read_response()\n        except ResponseError:\n            if EMPTY_RESPONSE in options:\n                return options[EMPTY_RESPONSE]\n            raise\n\n        if EMPTY_RESPONSE in options:\n            options.pop(EMPTY_RESPONSE)\n\n        # Remove keys entry, it needs only for cache.\n        options.pop(\"keys\", None)\n\n        if command_name in self.response_callbacks:\n            # Mypy bug: https://github.com/python/mypy/issues/10977\n            command_name = cast(str, command_name)\n            retval = self.response_callbacks[command_name](response, **options)\n            return await retval if inspect.isawaitable(retval) else retval\n        return response\n\n\nStrictRedis = Redis\n\n\nclass MonitorCommandInfo(TypedDict):\n    time: float\n    db: int\n    client_address: str\n    client_port: str\n    client_type: str\n    command: str\n\n\nclass Monitor:\n    \"\"\"\n    Monitor is useful for handling the MONITOR command to the redis server.\n    next_command() method returns one command from monitor\n    listen() method yields commands from monitor.\n    \"\"\"\n\n    monitor_re = re.compile(r\"\\[(\\d+) (.*?)\\] (.*)\")\n    command_re = re.compile(r'\"(.*?)(?<!\\\\)\"')\n\n    def __init__(self, connection_pool: ConnectionPool):\n        self.connection_pool = connection_pool\n        self.connection: Optional[Connection] = None\n\n    async def connect(self):\n        if self.connection is None:\n            self.connection = await self.connection_pool.get_connection()\n\n    async def __aenter__(self):\n        await self.connect()\n        await self.connection.send_command(\"MONITOR\")\n        # check that monitor returns 'OK', but don't return it to user\n        response = await self.connection.read_response()\n        if not bool_ok(response):\n            raise RedisError(f\"MONITOR failed: {response}\")\n        return self\n\n    async def __aexit__(self, *args):\n        await self.connection.disconnect()\n        await self.connection_pool.release(self.connection)\n\n    async def next_command(self) -> MonitorCommandInfo:\n        \"\"\"Parse the response from a monitor command\"\"\"\n        await self.connect()\n        response = await self.connection.read_response()\n        if isinstance(response, bytes):\n            response = self.connection.encoder.decode(response, force=True)\n        command_time, command_data = response.split(\" \", 1)\n        m = self.monitor_re.match(command_data)\n        db_id, client_info, command = m.groups()\n        command = \" \".join(self.command_re.findall(command))\n        # Redis escapes double quotes because each piece of the command\n        # string is surrounded by double quotes. We don't have that\n        # requirement so remove the escaping and leave the quote.\n        command = command.replace('\\\\\"', '\"')\n\n        if client_info == \"lua\":\n            client_address = \"lua\"\n            client_port = \"\"\n            client_type = \"lua\"\n        elif client_info.startswith(\"unix\"):\n            client_address = \"unix\"\n            client_port = client_info[5:]\n            client_type = \"unix\"\n        else:\n            # use rsplit as ipv6 addresses contain colons\n            client_address, client_port = client_info.rsplit(\":\", 1)\n            client_type = \"tcp\"\n        return {\n            \"time\": float(command_time),\n            \"db\": int(db_id),\n            \"client_address\": client_address,\n            \"client_port\": client_port,\n            \"client_type\": client_type,\n            \"command\": command,\n        }\n\n    async def listen(self) -> AsyncIterator[MonitorCommandInfo]:\n        \"\"\"Listen for commands coming to the server.\"\"\"\n        while True:\n            yield await self.next_command()\n\n\nclass PubSub:\n    \"\"\"\n    PubSub provides publish, subscribe and listen support to Redis channels.\n\n    After subscribing to one or more channels, the listen() method will block\n    until a message arrives on one of the subscribed channels. That message\n    will be returned and it's safe to start listening again.\n    \"\"\"\n\n    PUBLISH_MESSAGE_TYPES = (\"message\", \"pmessage\")\n    UNSUBSCRIBE_MESSAGE_TYPES = (\"unsubscribe\", \"punsubscribe\")\n    HEALTH_CHECK_MESSAGE = \"redis-py-health-check\"\n\n    def __init__(\n        self,\n        connection_pool: ConnectionPool,\n        shard_hint: Optional[str] = None,\n        ignore_subscribe_messages: bool = False,\n        encoder=None,\n        push_handler_func: Optional[Callable] = None,\n        event_dispatcher: Optional[\"EventDispatcher\"] = None,\n    ):\n        if event_dispatcher is None:\n            self._event_dispatcher = EventDispatcher()\n        else:\n            self._event_dispatcher = event_dispatcher\n        self.connection_pool = connection_pool\n        self.shard_hint = shard_hint\n        self.ignore_subscribe_messages = ignore_subscribe_messages\n        self.connection = None\n        # we need to know the encoding options for this connection in order\n        # to lookup channel and pattern names for callback handlers.\n        self.encoder = encoder\n        self.push_handler_func = push_handler_func\n        if self.encoder is None:\n            self.encoder = self.connection_pool.get_encoder()\n        if self.encoder.decode_responses:\n            self.health_check_response = [\n                [\"pong\", self.HEALTH_CHECK_MESSAGE],\n                self.HEALTH_CHECK_MESSAGE,\n            ]\n        else:\n            self.health_check_response = [\n                [b\"pong\", self.encoder.encode(self.HEALTH_CHECK_MESSAGE)],\n                self.encoder.encode(self.HEALTH_CHECK_MESSAGE),\n            ]\n        if self.push_handler_func is None:\n            _set_info_logger()\n        self.channels = {}\n        self.pending_unsubscribe_channels = set()\n        self.patterns = {}\n        self.pending_unsubscribe_patterns = set()\n        self._lock = asyncio.Lock()\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        await self.aclose()\n\n    def __del__(self):\n        if self.connection:\n            self.connection.deregister_connect_callback(self.on_connect)\n\n    async def aclose(self):\n        # In case a connection property does not yet exist\n        # (due to a crash earlier in the Redis() constructor), return\n        # immediately as there is nothing to clean-up.\n        if not hasattr(self, \"connection\"):\n            return\n        async with self._lock:\n            if self.connection:\n                await self.connection.disconnect()\n                self.connection.deregister_connect_callback(self.on_connect)\n                await self.connection_pool.release(self.connection)\n                self.connection = None\n            self.channels = {}\n            self.pending_unsubscribe_channels = set()\n            self.patterns = {}\n            self.pending_unsubscribe_patterns = set()\n\n    @deprecated_function(version=\"5.0.1\", reason=\"Use aclose() instead\", name=\"close\")\n    async def close(self) -> None:\n        \"\"\"Alias for aclose(), for backwards compatibility\"\"\"\n        await self.aclose()\n\n    @deprecated_function(version=\"5.0.1\", reason=\"Use aclose() instead\", name=\"reset\")\n    async def reset(self) -> None:\n        \"\"\"Alias for aclose(), for backwards compatibility\"\"\"\n        await self.aclose()\n\n    async def on_connect(self, connection: Connection):\n        \"\"\"Re-subscribe to any channels and patterns previously subscribed to\"\"\"\n        # NOTE: for python3, we can't pass bytestrings as keyword arguments\n        # so we need to decode channel/pattern names back to unicode strings\n        # before passing them to [p]subscribe.\n        self.pending_unsubscribe_channels.clear()\n        self.pending_unsubscribe_patterns.clear()\n        if self.channels:\n            channels = {}\n            for k, v in self.channels.items():\n                channels[self.encoder.decode(k, force=True)] = v\n            await self.subscribe(**channels)\n        if self.patterns:\n            patterns = {}\n            for k, v in self.patterns.items():\n                patterns[self.encoder.decode(k, force=True)] = v\n            await self.psubscribe(**patterns)\n\n    @property\n    def subscribed(self):\n        \"\"\"Indicates if there are subscriptions to any channels or patterns\"\"\"\n        return bool(self.channels or self.patterns)\n\n    async def execute_command(self, *args: EncodableT):\n        \"\"\"Execute a publish/subscribe command\"\"\"\n\n        # NOTE: don't parse the response in this function -- it could pull a\n        # legitimate message off the stack if the connection is already\n        # subscribed to one or more channels\n\n        await self.connect()\n        connection = self.connection\n        kwargs = {\"check_health\": not self.subscribed}\n        await self._execute(connection, connection.send_command, *args, **kwargs)\n\n    async def connect(self):\n        \"\"\"\n        Ensure that the PubSub is connected\n        \"\"\"\n        if self.connection is None:\n            self.connection = await self.connection_pool.get_connection()\n            # register a callback that re-subscribes to any channels we\n            # were listening to when we were disconnected\n            self.connection.register_connect_callback(self.on_connect)\n        else:\n            await self.connection.connect()\n        if self.push_handler_func is not None:\n            self.connection._parser.set_pubsub_push_handler(self.push_handler_func)\n\n        self._event_dispatcher.dispatch(\n            AfterPubSubConnectionInstantiationEvent(\n                self.connection, self.connection_pool, ClientType.ASYNC, self._lock\n            )\n        )\n\n    async def _reconnect(self, conn):\n        \"\"\"\n        Try to reconnect\n        \"\"\"\n        await conn.disconnect()\n        await conn.connect()\n\n    async def _execute(self, conn, command, *args, **kwargs):\n        \"\"\"\n        Connect manually upon disconnection. If the Redis server is down,\n        this will fail and raise a ConnectionError as desired.\n        After reconnection, the ``on_connect`` callback should have been\n        called by the # connection to resubscribe us to any channels and\n        patterns we were previously listening to\n        \"\"\"\n        return await conn.retry.call_with_retry(\n            lambda: command(*args, **kwargs),\n            lambda _: self._reconnect(conn),\n        )\n\n    async def parse_response(self, block: bool = True, timeout: float = 0):\n        \"\"\"Parse the response from a publish/subscribe command\"\"\"\n        conn = self.connection\n        if conn is None:\n            raise RuntimeError(\n                \"pubsub connection not set: \"\n                \"did you forget to call subscribe() or psubscribe()?\"\n            )\n\n        await self.check_health()\n\n        if not conn.is_connected:\n            await conn.connect()\n\n        read_timeout = None if block else timeout\n        response = await self._execute(\n            conn,\n            conn.read_response,\n            timeout=read_timeout,\n            disconnect_on_error=False,\n            push_request=True,\n        )\n\n        if conn.health_check_interval and response in self.health_check_response:\n            # ignore the health check message as user might not expect it\n            return None\n        return response\n\n    async def check_health(self):\n        conn = self.connection\n        if conn is None:\n            raise RuntimeError(\n                \"pubsub connection not set: \"\n                \"did you forget to call subscribe() or psubscribe()?\"\n            )\n\n        if (\n            conn.health_check_interval\n            and asyncio.get_running_loop().time() > conn.next_health_check\n        ):\n            await conn.send_command(\n                \"PING\", self.HEALTH_CHECK_MESSAGE, check_health=False\n            )\n\n    def _normalize_keys(self, data: _NormalizeKeysT) -> _NormalizeKeysT:\n        \"\"\"\n        normalize channel/pattern names to be either bytes or strings\n        based on whether responses are automatically decoded. this saves us\n        from coercing the value for each message coming in.\n        \"\"\"\n        encode = self.encoder.encode\n        decode = self.encoder.decode\n        return {decode(encode(k)): v for k, v in data.items()}  # type: ignore[return-value]  # noqa: E501\n\n    async def psubscribe(self, *args: ChannelT, **kwargs: PubSubHandler):\n        \"\"\"\n        Subscribe to channel patterns. Patterns supplied as keyword arguments\n        expect a pattern name as the key and a callable as the value. A\n        pattern's callable will be invoked automatically when a message is\n        received on that pattern rather than producing a message via\n        ``listen()``.\n        \"\"\"\n        parsed_args = list_or_args((args[0],), args[1:]) if args else args\n        new_patterns: Dict[ChannelT, PubSubHandler] = dict.fromkeys(parsed_args)\n        # Mypy bug: https://github.com/python/mypy/issues/10970\n        new_patterns.update(kwargs)  # type: ignore[arg-type]\n        ret_val = await self.execute_command(\"PSUBSCRIBE\", *new_patterns.keys())\n        # update the patterns dict AFTER we send the command. we don't want to\n        # subscribe twice to these patterns, once for the command and again\n        # for the reconnection.\n        new_patterns = self._normalize_keys(new_patterns)\n        self.patterns.update(new_patterns)\n        self.pending_unsubscribe_patterns.difference_update(new_patterns)\n        return ret_val\n\n    def punsubscribe(self, *args: ChannelT) -> Awaitable:\n        \"\"\"\n        Unsubscribe from the supplied patterns. If empty, unsubscribe from\n        all patterns.\n        \"\"\"\n        patterns: Iterable[ChannelT]\n        if args:\n            parsed_args = list_or_args((args[0],), args[1:])\n            patterns = self._normalize_keys(dict.fromkeys(parsed_args)).keys()\n        else:\n            parsed_args = []\n            patterns = self.patterns\n        self.pending_unsubscribe_patterns.update(patterns)\n        return self.execute_command(\"PUNSUBSCRIBE\", *parsed_args)\n\n    async def subscribe(self, *args: ChannelT, **kwargs: Callable):\n        \"\"\"\n        Subscribe to channels. Channels supplied as keyword arguments expect\n        a channel name as the key and a callable as the value. A channel's\n        callable will be invoked automatically when a message is received on\n        that channel rather than producing a message via ``listen()`` or\n        ``get_message()``.\n        \"\"\"\n        parsed_args = list_or_args((args[0],), args[1:]) if args else ()\n        new_channels = dict.fromkeys(parsed_args)\n        # Mypy bug: https://github.com/python/mypy/issues/10970\n        new_channels.update(kwargs)  # type: ignore[arg-type]\n        ret_val = await self.execute_command(\"SUBSCRIBE\", *new_channels.keys())\n        # update the channels dict AFTER we send the command. we don't want to\n        # subscribe twice to these channels, once for the command and again\n        # for the reconnection.\n        new_channels = self._normalize_keys(new_channels)\n        self.channels.update(new_channels)\n        self.pending_unsubscribe_channels.difference_update(new_channels)\n        return ret_val\n\n    def unsubscribe(self, *args) -> Awaitable:\n        \"\"\"\n        Unsubscribe from the supplied channels. If empty, unsubscribe from\n        all channels\n        \"\"\"\n        if args:\n            parsed_args = list_or_args(args[0], args[1:])\n            channels = self._normalize_keys(dict.fromkeys(parsed_args))\n        else:\n            parsed_args = []\n            channels = self.channels\n        self.pending_unsubscribe_channels.update(channels)\n        return self.execute_command(\"UNSUBSCRIBE\", *parsed_args)\n\n    async def listen(self) -> AsyncIterator:\n        \"\"\"Listen for messages on channels this client has been subscribed to\"\"\"\n        while self.subscribed:\n            response = await self.handle_message(await self.parse_response(block=True))\n            if response is not None:\n                yield response\n\n    async def get_message(\n        self, ignore_subscribe_messages: bool = False, timeout: Optional[float] = 0.0\n    ):\n        \"\"\"\n        Get the next message if one is available, otherwise None.\n\n        If timeout is specified, the system will wait for `timeout` seconds\n        before returning. Timeout should be specified as a floating point\n        number or None to wait indefinitely.\n        \"\"\"\n        response = await self.parse_response(block=(timeout is None), timeout=timeout)\n        if response:\n            return await self.handle_message(response, ignore_subscribe_messages)\n        return None\n\n    def ping(self, message=None) -> Awaitable:\n        \"\"\"\n        Ping the Redis server\n        \"\"\"\n        args = [\"PING\", message] if message is not None else [\"PING\"]\n        return self.execute_command(*args)\n\n    async def handle_message(self, response, ignore_subscribe_messages=False):\n        \"\"\"\n        Parses a pub/sub message. If the channel or pattern was subscribed to\n        with a message handler, the handler is invoked instead of a parsed\n        message being returned.\n        \"\"\"\n        if response is None:\n            return None\n        if isinstance(response, bytes):\n            response = [b\"pong\", response] if response != b\"PONG\" else [b\"pong\", b\"\"]\n        message_type = str_if_bytes(response[0])\n        if message_type == \"pmessage\":\n            message = {\n                \"type\": message_type,\n                \"pattern\": response[1],\n                \"channel\": response[2],\n                \"data\": response[3],\n            }\n        elif message_type == \"pong\":\n            message = {\n                \"type\": message_type,\n                \"pattern\": None,\n                \"channel\": None,\n                \"data\": response[1],\n            }\n        else:\n            message = {\n                \"type\": message_type,\n                \"pattern\": None,\n                \"channel\": response[1],\n                \"data\": response[2],\n            }\n\n        # if this is an unsubscribe message, remove it from memory\n        if message_type in self.UNSUBSCRIBE_MESSAGE_TYPES:\n            if message_type == \"punsubscribe\":\n                pattern = response[1]\n                if pattern in self.pending_unsubscribe_patterns:\n                    self.pending_unsubscribe_patterns.remove(pattern)\n                    self.patterns.pop(pattern, None)\n            else:\n                channel = response[1]\n                if channel in self.pending_unsubscribe_channels:\n                    self.pending_unsubscribe_channels.remove(channel)\n                    self.channels.pop(channel, None)\n\n        if message_type in self.PUBLISH_MESSAGE_TYPES:\n            # if there's a message handler, invoke it\n            if message_type == \"pmessage\":\n                handler = self.patterns.get(message[\"pattern\"], None)\n            else:\n                handler = self.channels.get(message[\"channel\"], None)\n            if handler:\n                if inspect.iscoroutinefunction(handler):\n                    await handler(message)\n                else:\n                    handler(message)\n                return None\n        elif message_type != \"pong\":\n            # this is a subscribe/unsubscribe message. ignore if we don't\n            # want them\n            if ignore_subscribe_messages or self.ignore_subscribe_messages:\n                return None\n\n        return message\n\n    async def run(\n        self,\n        *,\n        exception_handler: Optional[\"PSWorkerThreadExcHandlerT\"] = None,\n        poll_timeout: float = 1.0,\n    ) -> None:\n        \"\"\"Process pub/sub messages using registered callbacks.\n\n        This is the equivalent of :py:meth:`redis.PubSub.run_in_thread` in\n        redis-py, but it is a coroutine. To launch it as a separate task, use\n        ``asyncio.create_task``:\n\n            >>> task = asyncio.create_task(pubsub.run())\n\n        To shut it down, use asyncio cancellation:\n\n            >>> task.cancel()\n            >>> await task\n        \"\"\"\n        for channel, handler in self.channels.items():\n            if handler is None:\n                raise PubSubError(f\"Channel: '{channel}' has no handler registered\")\n        for pattern, handler in self.patterns.items():\n            if handler is None:\n                raise PubSubError(f\"Pattern: '{pattern}' has no handler registered\")\n\n        await self.connect()\n        while True:\n            try:\n                await self.get_message(\n                    ignore_subscribe_messages=True, timeout=poll_timeout\n                )\n            except asyncio.CancelledError:\n                raise\n            except BaseException as e:\n                if exception_handler is None:\n                    raise\n                res = exception_handler(e, self)\n                if inspect.isawaitable(res):\n                    await res\n            # Ensure that other tasks on the event loop get a chance to run\n            # if we didn't have to block for I/O anywhere.\n            await asyncio.sleep(0)\n\n\nclass PubsubWorkerExceptionHandler(Protocol):\n    def __call__(self, e: BaseException, pubsub: PubSub): ...\n\n\nclass AsyncPubsubWorkerExceptionHandler(Protocol):\n    async def __call__(self, e: BaseException, pubsub: PubSub): ...\n\n\nPSWorkerThreadExcHandlerT = Union[\n    PubsubWorkerExceptionHandler, AsyncPubsubWorkerExceptionHandler\n]\n\n\nCommandT = Tuple[Tuple[Union[str, bytes], ...], Mapping[str, Any]]\nCommandStackT = List[CommandT]\n\n\nclass Pipeline(Redis):  # lgtm [py/init-calls-subclass]\n    \"\"\"\n    Pipelines provide a way to transmit multiple commands to the Redis server\n    in one transmission.  This is convenient for batch processing, such as\n    saving all the values in a list to Redis.\n\n    All commands executed within a pipeline(when running in transactional mode,\n    which is the default behavior) are wrapped with MULTI and EXEC\n    calls. This guarantees all commands executed in the pipeline will be\n    executed atomically.\n\n    Any command raising an exception does *not* halt the execution of\n    subsequent commands in the pipeline. Instead, the exception is caught\n    and its instance is placed into the response list returned by execute().\n    Code iterating over the response list should be able to deal with an\n    instance of an exception as a potential value. In general, these will be\n    ResponseError exceptions, such as those raised when issuing a command\n    on a key of a different datatype.\n    \"\"\"\n\n    UNWATCH_COMMANDS = {\"DISCARD\", \"EXEC\", \"UNWATCH\"}\n\n    def __init__(\n        self,\n        connection_pool: ConnectionPool,\n        response_callbacks: MutableMapping[Union[str, bytes], ResponseCallbackT],\n        transaction: bool,\n        shard_hint: Optional[str],\n    ):\n        self.connection_pool = connection_pool\n        self.connection = None\n        self.response_callbacks = response_callbacks\n        self.is_transaction = transaction\n        self.shard_hint = shard_hint\n        self.watching = False\n        self.command_stack: CommandStackT = []\n        self.scripts: Set[Script] = set()\n        self.explicit_transaction = False\n\n    async def __aenter__(self: _RedisT) -> _RedisT:\n        return self\n\n    async def __aexit__(self, exc_type, exc_value, traceback):\n        await self.reset()\n\n    def __await__(self):\n        return self._async_self().__await__()\n\n    _DEL_MESSAGE = \"Unclosed Pipeline client\"\n\n    def __len__(self):\n        return len(self.command_stack)\n\n    def __bool__(self):\n        \"\"\"Pipeline instances should always evaluate to True\"\"\"\n        return True\n\n    async def _async_self(self):\n        return self\n\n    async def reset(self):\n        self.command_stack = []\n        self.scripts = set()\n        # make sure to reset the connection state in the event that we were\n        # watching something\n        if self.watching and self.connection:\n            try:\n                # call this manually since our unwatch or\n                # immediate_execute_command methods can call reset()\n                await self.connection.send_command(\"UNWATCH\")\n                await self.connection.read_response()\n            except ConnectionError:\n                # disconnect will also remove any previous WATCHes\n                if self.connection:\n                    await self.connection.disconnect()\n        # clean up the other instance attributes\n        self.watching = False\n        self.explicit_transaction = False\n        # we can safely return the connection to the pool here since we're\n        # sure we're no longer WATCHing anything\n        if self.connection:\n            await self.connection_pool.release(self.connection)\n            self.connection = None\n\n    async def aclose(self) -> None:\n        \"\"\"Alias for reset(), a standard method name for cleanup\"\"\"\n        await self.reset()\n\n    def multi(self):\n        \"\"\"\n        Start a transactional block of the pipeline after WATCH commands\n        are issued. End the transactional block with `execute`.\n        \"\"\"\n        if self.explicit_transaction:\n            raise RedisError(\"Cannot issue nested calls to MULTI\")\n        if self.command_stack:\n            raise RedisError(\n                \"Commands without an initial WATCH have already been issued\"\n            )\n        self.explicit_transaction = True\n\n    def execute_command(\n        self, *args, **kwargs\n    ) -> Union[\"Pipeline\", Awaitable[\"Pipeline\"]]:\n        if (self.watching or args[0] == \"WATCH\") and not self.explicit_transaction:\n            return self.immediate_execute_command(*args, **kwargs)\n        return self.pipeline_execute_command(*args, **kwargs)\n\n    async def _disconnect_reset_raise_on_watching(\n        self,\n        conn: Connection,\n        error: Exception,\n    ):\n        \"\"\"\n        Close the connection reset watching state and\n        raise an exception if we were watching.\n\n        The supported exceptions are already checked in the\n        retry object so we don't need to do it here.\n\n        After we disconnect the connection, it will try to reconnect and\n        do a health check as part of the send_command logic(on connection level).\n        \"\"\"\n        await conn.disconnect()\n        # if we were already watching a variable, the watch is no longer\n        # valid since this connection has died. raise a WatchError, which\n        # indicates the user should retry this transaction.\n        if self.watching:\n            await self.reset()\n            raise WatchError(\n                f\"A {type(error).__name__} occurred while watching one or more keys\"\n            )\n\n    async def immediate_execute_command(self, *args, **options):\n        \"\"\"\n        Execute a command immediately, but don't auto-retry on the supported\n        errors for retry if we're already WATCHing a variable.\n        Used when issuing WATCH or subsequent commands retrieving their values but before\n        MULTI is called.\n        \"\"\"\n        command_name = args[0]\n        conn = self.connection\n        # if this is the first call, we need a connection\n        if not conn:\n            conn = await self.connection_pool.get_connection()\n            self.connection = conn\n\n        return await conn.retry.call_with_retry(\n            lambda: self._send_command_parse_response(\n                conn, command_name, *args, **options\n            ),\n            lambda error: self._disconnect_reset_raise_on_watching(conn, error),\n        )\n\n    def pipeline_execute_command(self, *args, **options):\n        \"\"\"\n        Stage a command to be executed when execute() is next called\n\n        Returns the current Pipeline object back so commands can be\n        chained together, such as:\n\n        pipe = pipe.set('foo', 'bar').incr('baz').decr('bang')\n\n        At some other point, you can then run: pipe.execute(),\n        which will execute all commands queued in the pipe.\n        \"\"\"\n        self.command_stack.append((args, options))\n        return self\n\n    async def _execute_transaction(  # noqa: C901\n        self, connection: Connection, commands: CommandStackT, raise_on_error\n    ):\n        pre: CommandT = ((\"MULTI\",), {})\n        post: CommandT = ((\"EXEC\",), {})\n        cmds = (pre, *commands, post)\n        all_cmds = connection.pack_commands(\n            args for args, options in cmds if EMPTY_RESPONSE not in options\n        )\n        await connection.send_packed_command(all_cmds)\n        errors = []\n\n        # parse off the response for MULTI\n        # NOTE: we need to handle ResponseErrors here and continue\n        # so that we read all the additional command messages from\n        # the socket\n        try:\n            await self.parse_response(connection, \"_\")\n        except ResponseError as err:\n            errors.append((0, err))\n\n        # and all the other commands\n        for i, command in enumerate(commands):\n            if EMPTY_RESPONSE in command[1]:\n                errors.append((i, command[1][EMPTY_RESPONSE]))\n            else:\n                try:\n                    await self.parse_response(connection, \"_\")\n                except ResponseError as err:\n                    self.annotate_exception(err, i + 1, command[0])\n                    errors.append((i, err))\n\n        # parse the EXEC.\n        try:\n            response = await self.parse_response(connection, \"_\")\n        except ExecAbortError as err:\n            if errors:\n                raise errors[0][1] from err\n            raise\n\n        # EXEC clears any watched keys\n        self.watching = False\n\n        if response is None:\n            raise WatchError(\"Watched variable changed.\") from None\n\n        # put any parse errors into the response\n        for i, e in errors:\n            response.insert(i, e)\n\n        if len(response) != len(commands):\n            if self.connection:\n                await self.connection.disconnect()\n            raise ResponseError(\n                \"Wrong number of response items from pipeline execution\"\n            ) from None\n\n        # find any errors in the response and raise if necessary\n        if raise_on_error:\n            self.raise_first_error(commands, response)\n\n        # We have to run response callbacks manually\n        data = []\n        for r, cmd in zip(response, commands):\n            if not isinstance(r, Exception):\n                args, options = cmd\n                command_name = args[0]\n\n                # Remove keys entry, it needs only for cache.\n                options.pop(\"keys\", None)\n\n                if command_name in self.response_callbacks:\n                    r = self.response_callbacks[command_name](r, **options)\n                    if inspect.isawaitable(r):\n                        r = await r\n            data.append(r)\n        return data\n\n    async def _execute_pipeline(\n        self, connection: Connection, commands: CommandStackT, raise_on_error: bool\n    ):\n        # build up all commands into a single request to increase network perf\n        all_cmds = connection.pack_commands([args for args, _ in commands])\n        await connection.send_packed_command(all_cmds)\n\n        response = []\n        for args, options in commands:\n            try:\n                response.append(\n                    await self.parse_response(connection, args[0], **options)\n                )\n            except ResponseError as e:\n                response.append(e)\n\n        if raise_on_error:\n            self.raise_first_error(commands, response)\n        return response\n\n    def raise_first_error(self, commands: CommandStackT, response: Iterable[Any]):\n        for i, r in enumerate(response):\n            if isinstance(r, ResponseError):\n                self.annotate_exception(r, i + 1, commands[i][0])\n                raise r\n\n    def annotate_exception(\n        self, exception: Exception, number: int, command: Iterable[object]\n    ) -> None:\n        cmd = \" \".join(map(safe_str, command))\n        msg = (\n            f\"Command # {number} ({truncate_text(cmd)}) \"\n            \"of pipeline caused error: {exception.args}\"\n        )\n        exception.args = (msg,) + exception.args[1:]\n\n    async def parse_response(\n        self, connection: Connection, command_name: Union[str, bytes], **options\n    ):\n        result = await super().parse_response(connection, command_name, **options)\n        if command_name in self.UNWATCH_COMMANDS:\n            self.watching = False\n        elif command_name == \"WATCH\":\n            self.watching = True\n        return result\n\n    async def load_scripts(self):\n        # make sure all scripts that are about to be run on this pipeline exist\n        scripts = list(self.scripts)\n        immediate = self.immediate_execute_command\n        shas = [s.sha for s in scripts]\n        # we can't use the normal script_* methods because they would just\n        # get buffered in the pipeline.\n        exists = await immediate(\"SCRIPT EXISTS\", *shas)\n        if not all(exists):\n            for s, exist in zip(scripts, exists):\n                if not exist:\n                    s.sha = await immediate(\"SCRIPT LOAD\", s.script)\n\n    async def _disconnect_raise_on_watching(self, conn: Connection, error: Exception):\n        \"\"\"\n        Close the connection, raise an exception if we were watching.\n\n        The supported exceptions are already checked in the\n        retry object so we don't need to do it here.\n\n        After we disconnect the connection, it will try to reconnect and\n        do a health check as part of the send_command logic(on connection level).\n        \"\"\"\n        await conn.disconnect()\n        # if we were watching a variable, the watch is no longer valid\n        # since this connection has died. raise a WatchError, which\n        # indicates the user should retry this transaction.\n        if self.watching:\n            raise WatchError(\n                f\"A {type(error).__name__} occurred while watching one or more keys\"\n            )\n\n    async def execute(self, raise_on_error: bool = True) -> List[Any]:\n        \"\"\"Execute all the commands in the current pipeline\"\"\"\n        stack = self.command_stack\n        if not stack and not self.watching:\n            return []\n        if self.scripts:\n            await self.load_scripts()\n        if self.is_transaction or self.explicit_transaction:\n            execute = self._execute_transaction\n        else:\n            execute = self._execute_pipeline\n\n        conn = self.connection\n        if not conn:\n            conn = await self.connection_pool.get_connection()\n            # assign to self.connection so reset() releases the connection\n            # back to the pool after we're done\n            self.connection = conn\n        conn = cast(Connection, conn)\n\n        try:\n            return await conn.retry.call_with_retry(\n                lambda: execute(conn, stack, raise_on_error),\n                lambda error: self._disconnect_raise_on_watching(conn, error),\n            )\n        finally:\n            await self.reset()\n\n    async def discard(self):\n        \"\"\"Flushes all previously queued commands\n        See: https://redis.io/commands/DISCARD\n        \"\"\"\n        await self.execute_command(\"DISCARD\")\n\n    async def watch(self, *names: KeyT):\n        \"\"\"Watches the values at keys ``names``\"\"\"\n        if self.explicit_transaction:\n            raise RedisError(\"Cannot issue a WATCH after a MULTI\")\n        return await self.execute_command(\"WATCH\", *names)\n\n    async def unwatch(self):\n        \"\"\"Unwatches all previously specified keys\"\"\"\n        return self.watching and await self.execute_command(\"UNWATCH\") or True\n", 1618], "/usr/local/lib/python3.11/site-packages/redis/asyncio/retry.py": ["from asyncio import sleep\nfrom typing import TYPE_CHECKING, Any, Awaitable, Callable, Tuple, Type, TypeVar\n\nfrom redis.exceptions import ConnectionError, RedisError, TimeoutError\nfrom redis.retry import AbstractRetry\n\nT = TypeVar(\"T\")\n\nif TYPE_CHECKING:\n    from redis.backoff import AbstractBackoff\n\n\nclass Retry(AbstractRetry[RedisError]):\n    __hash__ = AbstractRetry.__hash__\n\n    def __init__(\n        self,\n        backoff: \"AbstractBackoff\",\n        retries: int,\n        supported_errors: Tuple[Type[RedisError], ...] = (\n            ConnectionError,\n            TimeoutError,\n        ),\n    ):\n        super().__init__(backoff, retries, supported_errors)\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, Retry):\n            return NotImplemented\n\n        return (\n            self._backoff == other._backoff\n            and self._retries == other._retries\n            and set(self._supported_errors) == set(other._supported_errors)\n        )\n\n    async def call_with_retry(\n        self, do: Callable[[], Awaitable[T]], fail: Callable[[RedisError], Any]\n    ) -> T:\n        \"\"\"\n        Execute an operation that might fail and returns its result, or\n        raise the exception that was thrown depending on the `Backoff` object.\n        `do`: the operation to call. Expects no argument.\n        `fail`: the failure handler, expects the last error that was thrown\n        \"\"\"\n        self._backoff.reset()\n        failures = 0\n        while True:\n            try:\n                return await do()\n            except self._supported_errors as error:\n                failures += 1\n                await fail(error)\n                if self._retries >= 0 and failures > self._retries:\n                    raise error\n                backoff = self._backoff.compute(failures)\n                if backoff > 0:\n                    await sleep(backoff)\n", 58], "/usr/local/lib/python3.11/site-packages/redis/event.py": ["import asyncio\nimport threading\nfrom abc import ABC, abstractmethod\nfrom enum import Enum\nfrom typing import List, Optional, Union\n\nfrom redis.auth.token import TokenInterface\nfrom redis.credentials import CredentialProvider, StreamingCredentialProvider\n\n\nclass EventListenerInterface(ABC):\n    \"\"\"\n    Represents a listener for given event object.\n    \"\"\"\n\n    @abstractmethod\n    def listen(self, event: object):\n        pass\n\n\nclass AsyncEventListenerInterface(ABC):\n    \"\"\"\n    Represents an async listener for given event object.\n    \"\"\"\n\n    @abstractmethod\n    async def listen(self, event: object):\n        pass\n\n\nclass EventDispatcherInterface(ABC):\n    \"\"\"\n    Represents a dispatcher that dispatches events to listeners\n    associated with given event.\n    \"\"\"\n\n    @abstractmethod\n    def dispatch(self, event: object):\n        pass\n\n    @abstractmethod\n    async def dispatch_async(self, event: object):\n        pass\n\n\nclass EventException(Exception):\n    \"\"\"\n    Exception wrapper that adds an event object into exception context.\n    \"\"\"\n\n    def __init__(self, exception: Exception, event: object):\n        self.exception = exception\n        self.event = event\n        super().__init__(exception)\n\n\nclass EventDispatcher(EventDispatcherInterface):\n    # TODO: Make dispatcher to accept external mappings.\n    def __init__(self):\n        \"\"\"\n        Mapping should be extended for any new events or listeners to be added.\n        \"\"\"\n        self._event_listeners_mapping = {\n            AfterConnectionReleasedEvent: [\n                ReAuthConnectionListener(),\n            ],\n            AfterPooledConnectionsInstantiationEvent: [\n                RegisterReAuthForPooledConnections()\n            ],\n            AfterSingleConnectionInstantiationEvent: [\n                RegisterReAuthForSingleConnection()\n            ],\n            AfterPubSubConnectionInstantiationEvent: [RegisterReAuthForPubSub()],\n            AfterAsyncClusterInstantiationEvent: [RegisterReAuthForAsyncClusterNodes()],\n            AsyncAfterConnectionReleasedEvent: [\n                AsyncReAuthConnectionListener(),\n            ],\n        }\n\n    def dispatch(self, event: object):\n        listeners = self._event_listeners_mapping.get(type(event))\n\n        for listener in listeners:\n            listener.listen(event)\n\n    async def dispatch_async(self, event: object):\n        listeners = self._event_listeners_mapping.get(type(event))\n\n        for listener in listeners:\n            await listener.listen(event)\n\n\nclass AfterConnectionReleasedEvent:\n    \"\"\"\n    Event that will be fired before each command execution.\n    \"\"\"\n\n    def __init__(self, connection):\n        self._connection = connection\n\n    @property\n    def connection(self):\n        return self._connection\n\n\nclass AsyncAfterConnectionReleasedEvent(AfterConnectionReleasedEvent):\n    pass\n\n\nclass ClientType(Enum):\n    SYNC = (\"sync\",)\n    ASYNC = (\"async\",)\n\n\nclass AfterPooledConnectionsInstantiationEvent:\n    \"\"\"\n    Event that will be fired after pooled connection instances was created.\n    \"\"\"\n\n    def __init__(\n        self,\n        connection_pools: List,\n        client_type: ClientType,\n        credential_provider: Optional[CredentialProvider] = None,\n    ):\n        self._connection_pools = connection_pools\n        self._client_type = client_type\n        self._credential_provider = credential_provider\n\n    @property\n    def connection_pools(self):\n        return self._connection_pools\n\n    @property\n    def client_type(self) -> ClientType:\n        return self._client_type\n\n    @property\n    def credential_provider(self) -> Union[CredentialProvider, None]:\n        return self._credential_provider\n\n\nclass AfterSingleConnectionInstantiationEvent:\n    \"\"\"\n    Event that will be fired after single connection instances was created.\n\n    :param connection_lock: For sync client thread-lock should be provided,\n    for async asyncio.Lock\n    \"\"\"\n\n    def __init__(\n        self,\n        connection,\n        client_type: ClientType,\n        connection_lock: Union[threading.RLock, asyncio.Lock],\n    ):\n        self._connection = connection\n        self._client_type = client_type\n        self._connection_lock = connection_lock\n\n    @property\n    def connection(self):\n        return self._connection\n\n    @property\n    def client_type(self) -> ClientType:\n        return self._client_type\n\n    @property\n    def connection_lock(self) -> Union[threading.RLock, asyncio.Lock]:\n        return self._connection_lock\n\n\nclass AfterPubSubConnectionInstantiationEvent:\n    def __init__(\n        self,\n        pubsub_connection,\n        connection_pool,\n        client_type: ClientType,\n        connection_lock: Union[threading.RLock, asyncio.Lock],\n    ):\n        self._pubsub_connection = pubsub_connection\n        self._connection_pool = connection_pool\n        self._client_type = client_type\n        self._connection_lock = connection_lock\n\n    @property\n    def pubsub_connection(self):\n        return self._pubsub_connection\n\n    @property\n    def connection_pool(self):\n        return self._connection_pool\n\n    @property\n    def client_type(self) -> ClientType:\n        return self._client_type\n\n    @property\n    def connection_lock(self) -> Union[threading.RLock, asyncio.Lock]:\n        return self._connection_lock\n\n\nclass AfterAsyncClusterInstantiationEvent:\n    \"\"\"\n    Event that will be fired after async cluster instance was created.\n\n    Async cluster doesn't use connection pools,\n    instead ClusterNode object manages connections.\n    \"\"\"\n\n    def __init__(\n        self,\n        nodes: dict,\n        credential_provider: Optional[CredentialProvider] = None,\n    ):\n        self._nodes = nodes\n        self._credential_provider = credential_provider\n\n    @property\n    def nodes(self) -> dict:\n        return self._nodes\n\n    @property\n    def credential_provider(self) -> Union[CredentialProvider, None]:\n        return self._credential_provider\n\n\nclass ReAuthConnectionListener(EventListenerInterface):\n    \"\"\"\n    Listener that performs re-authentication of given connection.\n    \"\"\"\n\n    def listen(self, event: AfterConnectionReleasedEvent):\n        event.connection.re_auth()\n\n\nclass AsyncReAuthConnectionListener(AsyncEventListenerInterface):\n    \"\"\"\n    Async listener that performs re-authentication of given connection.\n    \"\"\"\n\n    async def listen(self, event: AsyncAfterConnectionReleasedEvent):\n        await event.connection.re_auth()\n\n\nclass RegisterReAuthForPooledConnections(EventListenerInterface):\n    \"\"\"\n    Listener that registers a re-authentication callback for pooled connections.\n    Required by :class:`StreamingCredentialProvider`.\n    \"\"\"\n\n    def __init__(self):\n        self._event = None\n\n    def listen(self, event: AfterPooledConnectionsInstantiationEvent):\n        if isinstance(event.credential_provider, StreamingCredentialProvider):\n            self._event = event\n\n            if event.client_type == ClientType.SYNC:\n                event.credential_provider.on_next(self._re_auth)\n                event.credential_provider.on_error(self._raise_on_error)\n            else:\n                event.credential_provider.on_next(self._re_auth_async)\n                event.credential_provider.on_error(self._raise_on_error_async)\n\n    def _re_auth(self, token):\n        for pool in self._event.connection_pools:\n            pool.re_auth_callback(token)\n\n    async def _re_auth_async(self, token):\n        for pool in self._event.connection_pools:\n            await pool.re_auth_callback(token)\n\n    def _raise_on_error(self, error: Exception):\n        raise EventException(error, self._event)\n\n    async def _raise_on_error_async(self, error: Exception):\n        raise EventException(error, self._event)\n\n\nclass RegisterReAuthForSingleConnection(EventListenerInterface):\n    \"\"\"\n    Listener that registers a re-authentication callback for single connection.\n    Required by :class:`StreamingCredentialProvider`.\n    \"\"\"\n\n    def __init__(self):\n        self._event = None\n\n    def listen(self, event: AfterSingleConnectionInstantiationEvent):\n        if isinstance(\n            event.connection.credential_provider, StreamingCredentialProvider\n        ):\n            self._event = event\n\n            if event.client_type == ClientType.SYNC:\n                event.connection.credential_provider.on_next(self._re_auth)\n                event.connection.credential_provider.on_error(self._raise_on_error)\n            else:\n                event.connection.credential_provider.on_next(self._re_auth_async)\n                event.connection.credential_provider.on_error(\n                    self._raise_on_error_async\n                )\n\n    def _re_auth(self, token):\n        with self._event.connection_lock:\n            self._event.connection.send_command(\n                \"AUTH\", token.try_get(\"oid\"), token.get_value()\n            )\n            self._event.connection.read_response()\n\n    async def _re_auth_async(self, token):\n        async with self._event.connection_lock:\n            await self._event.connection.send_command(\n                \"AUTH\", token.try_get(\"oid\"), token.get_value()\n            )\n            await self._event.connection.read_response()\n\n    def _raise_on_error(self, error: Exception):\n        raise EventException(error, self._event)\n\n    async def _raise_on_error_async(self, error: Exception):\n        raise EventException(error, self._event)\n\n\nclass RegisterReAuthForAsyncClusterNodes(EventListenerInterface):\n    def __init__(self):\n        self._event = None\n\n    def listen(self, event: AfterAsyncClusterInstantiationEvent):\n        if isinstance(event.credential_provider, StreamingCredentialProvider):\n            self._event = event\n            event.credential_provider.on_next(self._re_auth)\n            event.credential_provider.on_error(self._raise_on_error)\n\n    async def _re_auth(self, token: TokenInterface):\n        for key in self._event.nodes:\n            await self._event.nodes[key].re_auth_callback(token)\n\n    async def _raise_on_error(self, error: Exception):\n        raise EventException(error, self._event)\n\n\nclass RegisterReAuthForPubSub(EventListenerInterface):\n    def __init__(self):\n        self._connection = None\n        self._connection_pool = None\n        self._client_type = None\n        self._connection_lock = None\n        self._event = None\n\n    def listen(self, event: AfterPubSubConnectionInstantiationEvent):\n        if isinstance(\n            event.pubsub_connection.credential_provider, StreamingCredentialProvider\n        ) and event.pubsub_connection.get_protocol() in [3, \"3\"]:\n            self._event = event\n            self._connection = event.pubsub_connection\n            self._connection_pool = event.connection_pool\n            self._client_type = event.client_type\n            self._connection_lock = event.connection_lock\n\n            if self._client_type == ClientType.SYNC:\n                self._connection.credential_provider.on_next(self._re_auth)\n                self._connection.credential_provider.on_error(self._raise_on_error)\n            else:\n                self._connection.credential_provider.on_next(self._re_auth_async)\n                self._connection.credential_provider.on_error(\n                    self._raise_on_error_async\n                )\n\n    def _re_auth(self, token: TokenInterface):\n        with self._connection_lock:\n            self._connection.send_command(\n                \"AUTH\", token.try_get(\"oid\"), token.get_value()\n            )\n            self._connection.read_response()\n\n        self._connection_pool.re_auth_callback(token)\n\n    async def _re_auth_async(self, token: TokenInterface):\n        async with self._connection_lock:\n            await self._connection.send_command(\n                \"AUTH\", token.try_get(\"oid\"), token.get_value()\n            )\n            await self._connection.read_response()\n\n        await self._connection_pool.re_auth_callback(token)\n\n    def _raise_on_error(self, error: Exception):\n        raise EventException(error, self._event)\n\n    async def _raise_on_error_async(self, error: Exception):\n        raise EventException(error, self._event)\n", 394], "/app/app/services/data_collector.py": ["\"\"\"\nData Collector Service - M2/M3 Implementation\nHandles RLC job consumption, gap detection, and backfill orchestration.\n\"\"\"\nfrom __future__ import annotations\n\nimport asyncio\nimport json\nimport logging\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional\n\nfrom app.core.config import get_settings\nfrom app.observability.metrics import (\n    BACKFILL_COMPLETED,\n    BACKFILL_ENQUEUED,\n    BACKFILL_OLDEST_AGE,\n    BACKFILL_QUEUE_DEPTH,\n    GAPS_FOUND,\n    JOBS_PROCESSED,\n)\nfrom app.services.database import DatabaseService\n\nlogger = logging.getLogger(__name__)\n\n\ndef _delta_from_interval(interval: str) -> timedelta:\n    \"\"\"Convert interval string to timedelta.\"\"\"\n    if interval == \"1m\":\n        return timedelta(minutes=1)\n    elif interval == \"5m\":\n        return timedelta(minutes=5)\n    elif interval == \"15m\":\n        return timedelta(minutes=15)\n    elif interval == \"1h\":\n        return timedelta(hours=1)\n    elif interval == \"1d\":\n        return timedelta(days=1)\n    else:\n        return timedelta(minutes=1)  # default\n\n\nclass CollectorJobs:\n    \"\"\"Simple Redis-backed job queue for RLC integration and backfills.\"\"\"\n\n    def __init__(self, redis_url: str, jobs_key: str, backfill_keys: Dict[str, str]):\n        try:\n            import redis.asyncio as aioredis\n            self.r = aioredis.from_url(redis_url, encoding=\"utf-8\", decode_responses=True)\n        except ImportError:\n            logger.warning(\"redis not installed; RLC mode disabled\")\n            self.r = None\n        self.jobs_key = jobs_key\n        self.backfill_keys = backfill_keys  # {\"T0\": \"market:backfills:T0\", ...}\n\n    async def push_job(self, job: Dict) -> None:\n        if not self.r:\n            return\n        await self.r.lpush(self.jobs_key, json.dumps(job))\n\n    async def pop_job(self, timeout: int = 1) -> Optional[Dict]:\n        if not self.r:\n            return None\n        try:\n            v = await self.r.brpop(self.jobs_key, timeout=timeout)\n            if not v:\n                return None\n            _, payload = v\n            return json.loads(payload)\n        except Exception as exc:\n            logger.debug(\"pop_job error: %s\", exc)\n            return None\n\n    async def push_backfill(self, tier: str, job: Dict) -> None:\n        if not self.r:\n            return\n        key = self.backfill_keys.get(tier, self.backfill_keys[\"T2\"])\n        await self.r.lpush(key, json.dumps(job))\n\n    async def pop_backfill(self, tiers: List[str], timeout: int = 1) -> Optional[Dict]:\n        if not self.r:\n            return None\n        # Try tiers in priority order\n        for tier in sorted(tiers, key=lambda t: {\"T0\": 0, \"T1\": 1, \"T2\": 2}.get(t, 9)):\n            key = self.backfill_keys.get(tier)\n            if not key:\n                continue\n            try:\n                v = await self.r.brpop(key, timeout=0)  # non-blocking check\n                if v:\n                    _, payload = v\n                    return json.loads(payload)\n            except Exception:\n                continue\n        return None\n\n    async def backfill_depth(self, tier: str) -> int:\n        if not self.r:\n            return 0\n        key = self.backfill_keys.get(tier)\n        if not key:\n            return 0\n        try:\n            return await self.r.llen(key)\n        except Exception:\n            return 0\n\n    async def backfill_oldest(self, tier: str) -> Optional[Dict]:\n        if not self.r:\n            return None\n        key = self.backfill_keys.get(tier)\n        if not key:\n            return None\n        try:\n            oldest_raw = await self.r.lindex(key, -1)  # oldest at tail\n            if oldest_raw:\n                return json.loads(oldest_raw)\n        except Exception:\n            pass\n        return None\n\n\nclass DataCollectorService:\n    \"\"\"\n    Background service for collecting and storing market data.\n    Implements M2 (RLC jobs + local sweep) and M3 (gap detection + backfill).\n    \"\"\"\n\n    def __init__(self, db: DatabaseService, registry=None):\n        self.db = db\n        self.registry = registry  # provider registry from main\n        self.running = False\n        self.task: Optional[asyncio.Task] = None\n\n        s = get_settings()\n        self.jobs = CollectorJobs(\n            s.RLC_REDIS_URL,\n            s.RLC_REDIS_JOBS_KEY,\n            s.RLC_REDIS_BACKFILL_KEYS,\n        )\n\n        # Backfill concurrency semaphores (per tier)\n        self.sem = {\n            \"T0\": asyncio.Semaphore(s.BACKFILL_MAX_CONCURRENCY_T0),\n            \"T1\": asyncio.Semaphore(s.BACKFILL_MAX_CONCURRENCY_T1),\n            \"T2\": asyncio.Semaphore(s.BACKFILL_MAX_CONCURRENCY_T2),\n        }\n        self.dispatch_tokens = s.BACKFILL_DISPATCH_RATE_PER_SEC\n        self._last_dispatch = datetime.utcnow()\n\n    # --------- PUBLIC TASKS ----------\n\n    async def run(self) -> None:\n        \"\"\"Run job consumer + optional local sweeper + backfill worker.\"\"\"\n        if self.running:\n            logger.warning(\"Data collector already running\")\n            return\n\n        s = get_settings()\n        self.running = True\n\n        tasks = [asyncio.create_task(self.consume_jobs())]\n        if s.LOCAL_SWEEP_ENABLED and not s.USE_RLC:\n            tasks.append(asyncio.create_task(self.local_sweeper()))\n        tasks.append(asyncio.create_task(self.backfill_worker()))\n        tasks.append(asyncio.create_task(self.metrics_reporter()))\n\n        self.task = asyncio.gather(*tasks, return_exceptions=True)\n        logger.info(\"Data collector started (USE_RLC=%s, LOCAL_SWEEP=%s)\", s.USE_RLC, s.LOCAL_SWEEP_ENABLED)\n\n    async def stop(self) -> None:\n        \"\"\"Stop the data collection background task.\"\"\"\n        self.running = False\n        if self.task:\n            self.task.cancel()\n            try:\n                await self.task\n            except asyncio.CancelledError:\n                pass\n        logger.info(\"Data collector stopped\")\n\n    # --------- JOB CONSUMER ----------\n\n    async def consume_jobs(self) -> None:\n        \"\"\"Consume RLC jobs; fallback: no-op if queue is empty.\"\"\"\n        while self.running:\n            job = await self.jobs.pop_job(timeout=2)\n            if not job:\n                await asyncio.sleep(0.1)\n                continue\n            try:\n                t = job.get(\"type\")\n                if t == \"bars_fetch\":\n                    await self._handle_bars_job(job)\n                elif t == \"quotes_fetch\":\n                    await self._handle_quotes_job(job)\n                elif t == \"backfill\":\n                    # push into backfill stream (throttled)\n                    tier = job.get(\"priority\", \"T2\")\n                    await self.jobs.push_backfill(tier, job)\n                else:\n                    logger.debug(\"Unknown job type: %s\", t)\n            except Exception as exc:\n                logger.error(\"Error handling job: %s\", exc, exc_info=True)\n                # TODO: emit metric + optional DLQ\n                continue\n\n    # --------- LOCAL SWEEPER (DEV) ----------\n\n    async def local_sweeper(self) -> None:\n        \"\"\"Emit simple bars_fetch jobs by tier and cadence when RLC is off.\"\"\"\n        s = get_settings()\n        offset = {\"T0\": 0, \"T1\": 0, \"T2\": 0}\n        while self.running:\n            now = datetime.utcnow()\n            for tier, cadence in [(\"T0\", s.CADENCE_T0), (\"T1\", s.CADENCE_T1), (\"T2\", s.CADENCE_T2)]:\n                if \"bars_1m\" not in cadence and \"eod\" not in cadence:\n                    continue\n                try:\n                    batch = await self.db.get_symbols_by_tier(tier, s.LOCAL_SWEEP_BATCH, offset[tier])\n                except Exception as exc:\n                    logger.warning(\"Error getting symbols for tier %s: %s\", tier, exc)\n                    offset[tier] = 0\n                    continue\n\n                if not batch:\n                    offset[tier] = 0\n                    continue\n                offset[tier] += len(batch)\n\n                end = now.replace(second=0, microsecond=0)\n                start = end - timedelta(minutes=5) if tier != \"T2\" else end - timedelta(days=2)\n\n                job = {\n                    \"type\": \"bars_fetch\",\n                    \"symbols\": batch,\n                    \"interval\": \"1m\" if tier != \"T2\" else \"1d\",\n                    \"time_window\": {\"start\": start.isoformat() + \"Z\", \"end\": end.isoformat() + \"Z\"},\n                    \"priority\": tier,\n                    \"provider_hint\": None,\n                }\n                await self.jobs.push_job(job)\n\n            await asyncio.sleep(s.LOCAL_SWEEP_TICK_SEC)\n\n    # --------- JOB HANDLERS ----------\n\n    async def _handle_bars_job(self, job: Dict) -> None:\n        \"\"\"Handle bars_fetch job: rank providers, fetch, write, detect gaps.\"\"\"\n        if not self.registry:\n            logger.warning(\"Registry not available; skipping bars job\")\n            return\n\n        sym_list: List[str] = job.get(\"symbols\", [])\n        interval: str = job.get(\"interval\", \"1m\")\n        start = datetime.fromisoformat(job[\"time_window\"][\"start\"].replace(\"Z\", \"\"))\n        end = datetime.fromisoformat(job[\"time_window\"][\"end\"].replace(\"Z\", \"\"))\n        tier = job.get(\"priority\", \"T1\")\n        hint = job.get(\"provider_hint\")\n\n        # Rank providers once for this capability\n        capability = \"bars_1m\" if interval == \"1m\" else \"eod\"\n        providers = self.registry.rank(capability, provider_hint=hint)\n        if not providers:\n            logger.warning(\"No providers available for capability %s\", capability)\n            return\n\n        for symbol in sym_list:\n            provider_used = None\n            bars_all: List[Dict] = []\n            for pname in providers:\n                adapter = self.registry.providers[pname].adapter\n                try:\n                    # Fetch bars from provider\n                    data = await adapter.fetch_bars(symbol=symbol, start=start, end=end, interval=interval)\n                    if not data:\n                        continue\n                    provider_used = pname\n                    bars_all = data\n                    JOBS_PROCESSED.labels(type=\"bars_fetch\", provider=pname).inc()\n                    break\n                except Exception as exc:\n                    logger.debug(\"Provider %s failed for %s: %s\", pname, symbol, exc)\n                    continue\n\n            if not bars_all:\n                # Could emit a small gap/backfill from start\u2192end for this symbol\n                await self._enqueue_gap(symbol, interval, start, end, tier)\n                continue\n\n            # Gap detection & upserts\n            await self._write_with_gaps(symbol, interval, provider_used, bars_all, tier)\n\n    async def _handle_quotes_job(self, job: Dict) -> None:\n        \"\"\"Handle quotes_fetch job: similar pattern to bars.\"\"\"\n        # Placeholder: same routing pattern, write to quotes table, etc.\n        logger.debug(\"quotes_fetch job received but not implemented: %s\", job)\n        return\n\n    # --------- GAP & BACKFILL CORE ----------\n\n    async def _write_with_gaps(\n        self,\n        symbol: str,\n        interval: str,\n        provider_used: str,\n        bars: List[Dict],\n        tier: str,\n    ) -> None:\n        \"\"\"Write bars and detect gaps.\"\"\"\n        # Sort bars and load cursor\n        bars.sort(key=lambda b: b.get(\"ts\", datetime.min))\n        last_ts = await self.db.get_cursor(symbol, interval, provider_used)\n        dt = _delta_from_interval(interval)\n\n        # If no cursor, set it to first-interval-before first bar\n        if last_ts is None and bars:\n            last_ts = bars[0][\"ts\"] - dt\n\n        expected = last_ts + dt if last_ts else (bars[0][\"ts\"] if bars else datetime.utcnow())\n\n        # Detect gap before first bar\n        if bars and bars[0][\"ts\"] > expected:\n            await self._enqueue_gap(symbol, interval, expected, bars[0][\"ts\"] - dt, tier)\n\n        # Bulk write all bars (idempotent)\n        s = get_settings()\n        await self.db.upsert_bars_bulk(bars, batch_size=s.LIVE_BATCH_SIZE, provider_used=provider_used)\n\n        # Detect gaps between bars\n        if len(bars) > 1:\n            prev = bars[0][\"ts\"]\n            for b in bars[1:]:\n                want = prev + dt\n                if b[\"ts\"] > want:\n                    await self._enqueue_gap(symbol, interval, want, b[\"ts\"] - dt, tier)\n                prev = b[\"ts\"]\n\n        # Advance cursor\n        if bars:\n            await self.db.update_cursor(symbol, interval, provider_used, bars[-1][\"ts\"], status=\"ok\")\n\n    async def _enqueue_gap(\n        self,\n        symbol: str,\n        interval: str,\n        start_ts: datetime,\n        end_ts: datetime,\n        tier: str,\n    ) -> None:\n        \"\"\"Split large gaps into chunks and enqueue backfill jobs.\"\"\"\n        s = get_settings()\n        chunk_duration = timedelta(minutes=s.BACKFILL_CHUNK_MINUTES) if interval == \"1m\" else timedelta(days=30)\n        cur = start_ts\n        count = 0\n        while cur <= end_ts:\n            nxt = min(end_ts, cur + chunk_duration)\n            job = {\n                \"type\": \"backfill\",\n                \"symbol\": symbol,\n                \"interval\": interval,\n                \"time_window\": {\"start\": cur.isoformat() + \"Z\", \"end\": nxt.isoformat() + \"Z\"},\n                \"priority\": tier,\n            }\n            await self.jobs.push_backfill(tier, job)\n            GAPS_FOUND.labels(interval=interval).inc()\n            BACKFILL_ENQUEUED.labels(tier=tier).inc()\n            count += 1\n            cur = nxt + _delta_from_interval(interval)\n\n        if count > 0:\n            logger.info(\"Enqueued %d backfill chunks for %s [%s] %s\u2192%s\", count, symbol, interval, start_ts, end_ts)\n\n    # --------- BACKFILL WORKER ----------\n\n    async def backfill_worker(self) -> None:\n        \"\"\"Backfill worker with rate-limiting and concurrency control.\"\"\"\n        s = get_settings()\n        bucket = 0.0\n        last = datetime.utcnow()\n        while self.running:\n            # refill tokens\n            now = datetime.utcnow()\n            bucket += (now - last).total_seconds() * s.BACKFILL_DISPATCH_RATE_PER_SEC\n            bucket = min(bucket, 10.0)\n            last = now\n\n            if bucket < 1.0:\n                await asyncio.sleep(0.1)\n                continue\n\n            job = await self.jobs.pop_backfill([\"T0\", \"T1\", \"T2\"], timeout=1)\n            if not job:\n                await asyncio.sleep(0.1)\n                continue\n\n            bucket -= 1.0\n            tier = job.get(\"priority\", \"T2\")\n            async with self.sem.get(tier, self.sem[\"T2\"]):\n                await self._run_backfill(job)\n\n    async def _run_backfill(self, job: Dict) -> None:\n        \"\"\"Execute a single backfill job.\"\"\"\n        if not self.registry:\n            logger.warning(\"Registry not available; skipping backfill\")\n            BACKFILL_COMPLETED.labels(tier=job.get(\"priority\", \"T2\"), status=\"failed\").inc()\n            return\n\n        symbol = job[\"symbol\"]\n        interval = job.get(\"interval\", \"1m\")\n        start = datetime.fromisoformat(job[\"time_window\"][\"start\"].replace(\"Z\", \"\"))\n        end = datetime.fromisoformat(job[\"time_window\"][\"end\"].replace(\"Z\", \"\"))\n        tier = job.get(\"priority\", \"T2\")\n\n        providers = self.registry.rank(\"bars_1m\" if interval == \"1m\" else \"eod\", provider_hint=None)\n        s = get_settings()\n\n        for pname in providers:\n            adapter = self.registry.providers[pname].adapter\n            try:\n                data = await adapter.fetch_bars(symbol=symbol, start=start, end=end, interval=interval)\n                if not data:\n                    continue\n                await self.db.upsert_bars_bulk(\n                    data,\n                    batch_size=s.BACKFILL_BATCH_SIZE,\n                    provider_used=pname,\n                )\n                # advance cursor conservatively if needed\n                if data:\n                    max_ts = max(b[\"ts\"] for b in data)\n                    await self.db.update_cursor(symbol, interval, pname, max_ts, status=\"backfilled\")\n                BACKFILL_COMPLETED.labels(tier=tier, status=\"done\").inc()\n                JOBS_PROCESSED.labels(type=\"backfill\", provider=pname).inc()\n                logger.debug(\"Backfill completed for %s [%s] %s\u2192%s via %s\", symbol, interval, start, end, pname)\n                return\n            except Exception as exc:\n                logger.debug(\"Backfill provider %s failed for %s: %s\", pname, symbol, exc)\n                continue\n\n        # If we reach here, all providers failed\n        BACKFILL_COMPLETED.labels(tier=tier, status=\"failed\").inc()\n        logger.warning(\"Backfill failed for %s [%s] %s\u2192%s (all providers failed)\", symbol, interval, start, end)\n\n    # --------- METRICS REPORTER ----------\n\n    async def metrics_reporter(self) -> None:\n        \"\"\"Periodically update backfill queue depth and oldest age metrics.\"\"\"\n        while self.running:\n            try:\n                for tier in [\"T0\", \"T1\", \"T2\"]:\n                    depth = await self.jobs.backfill_depth(tier)\n                    BACKFILL_QUEUE_DEPTH.set(depth)\n\n                    if depth > 0:\n                        oldest = await self.jobs.backfill_oldest(tier)\n                        if oldest:\n                            start_str = oldest.get(\"time_window\", {}).get(\"start\")\n                            if start_str:\n                                start_dt = datetime.fromisoformat(start_str.replace(\"Z\", \"\"))\n                                age = (datetime.utcnow() - start_dt).total_seconds()\n                                BACKFILL_OLDEST_AGE.set(max(age, 0.0))\n            except Exception as exc:\n                logger.debug(\"Error updating backfill metrics: %s\", exc)\n\n            await asyncio.sleep(10)\n\n    # --------- LEGACY API (for backwards compatibility) ----------\n\n    async def start(self) -> None:\n        \"\"\"Legacy start method.\"\"\"\n        await self.run()\n\n    async def collect_symbol_data(\n        self,\n        symbol: str,\n        days_back: int = 365,\n        force_update: bool = False,\n    ) -> bool:\n        \"\"\"\n        Legacy method for on-demand symbol collection.\n        Now delegates to job queue.\n        \"\"\"\n        logger.info(\"Legacy collect_symbol_data called for %s\", symbol)\n        now = datetime.utcnow()\n        start = now - timedelta(days=days_back)\n        job = {\n            \"type\": \"bars_fetch\",\n            \"symbols\": [symbol.upper()],\n            \"interval\": \"1d\",\n            \"time_window\": {\"start\": start.isoformat() + \"Z\", \"end\": now.isoformat() + \"Z\"},\n            \"priority\": \"T2\",\n            \"provider_hint\": None,\n        }\n        await self.jobs.push_job(job)\n        return True\n\n    async def get_collection_status(self) -> dict:\n        \"\"\"Get status information about the data collector.\"\"\"\n        s = get_settings()\n        depths = {}\n        for tier in [\"T0\", \"T1\", \"T2\"]:\n            depths[tier] = await self.jobs.backfill_depth(tier)\n\n        return {\n            \"running\": self.running,\n            \"use_rlc\": s.USE_RLC,\n            \"local_sweep_enabled\": s.LOCAL_SWEEP_ENABLED,\n            \"backfill_queue_depths\": depths,\n            \"policy_version\": s.policy_version,\n        }\n", 511], "/usr/local/lib/python3.11/site-packages/redis/commands/helpers.py": ["import copy\nimport random\nimport string\nfrom typing import List, Tuple\n\nimport redis\nfrom redis.typing import KeysT, KeyT\n\n\ndef list_or_args(keys: KeysT, args: Tuple[KeyT, ...]) -> List[KeyT]:\n    # returns a single new list combining keys and args\n    try:\n        iter(keys)\n        # a string or bytes instance can be iterated, but indicates\n        # keys wasn't passed as a list\n        if isinstance(keys, (bytes, str)):\n            keys = [keys]\n        else:\n            keys = list(keys)\n    except TypeError:\n        keys = [keys]\n    if args:\n        keys.extend(args)\n    return keys\n\n\ndef nativestr(x):\n    \"\"\"Return the decoded binary string, or a string, depending on type.\"\"\"\n    r = x.decode(\"utf-8\", \"replace\") if isinstance(x, bytes) else x\n    if r == \"null\":\n        return\n    return r\n\n\ndef delist(x):\n    \"\"\"Given a list of binaries, return the stringified version.\"\"\"\n    if x is None:\n        return x\n    return [nativestr(obj) for obj in x]\n\n\ndef parse_to_list(response):\n    \"\"\"Optimistically parse the response to a list.\"\"\"\n    res = []\n\n    special_values = {\"infinity\", \"nan\", \"-infinity\"}\n\n    if response is None:\n        return res\n\n    for item in response:\n        if item is None:\n            res.append(None)\n            continue\n        try:\n            item_str = nativestr(item)\n        except TypeError:\n            res.append(None)\n            continue\n\n        if isinstance(item_str, str) and item_str.lower() in special_values:\n            res.append(item_str)  # Keep as string\n        else:\n            try:\n                res.append(int(item))\n            except ValueError:\n                try:\n                    res.append(float(item))\n                except ValueError:\n                    res.append(item_str)\n\n    return res\n\n\ndef parse_list_to_dict(response):\n    res = {}\n    for i in range(0, len(response), 2):\n        if isinstance(response[i], list):\n            res[\"Child iterators\"].append(parse_list_to_dict(response[i]))\n            try:\n                if isinstance(response[i + 1], list):\n                    res[\"Child iterators\"].append(parse_list_to_dict(response[i + 1]))\n            except IndexError:\n                pass\n        elif isinstance(response[i + 1], list):\n            res[\"Child iterators\"] = [parse_list_to_dict(response[i + 1])]\n        else:\n            try:\n                res[response[i]] = float(response[i + 1])\n            except (TypeError, ValueError):\n                res[response[i]] = response[i + 1]\n    return res\n\n\ndef random_string(length=10):\n    \"\"\"\n    Returns a random N character long string.\n    \"\"\"\n    return \"\".join(  # nosec\n        random.choice(string.ascii_lowercase) for x in range(length)\n    )\n\n\ndef decode_dict_keys(obj):\n    \"\"\"Decode the keys of the given dictionary with utf-8.\"\"\"\n    newobj = copy.copy(obj)\n    for k in obj.keys():\n        if isinstance(k, bytes):\n            newobj[k.decode(\"utf-8\")] = newobj[k]\n            newobj.pop(k)\n    return newobj\n\n\ndef get_protocol_version(client):\n    if isinstance(client, redis.Redis) or isinstance(client, redis.asyncio.Redis):\n        return client.connection_pool.connection_kwargs.get(\"protocol\")\n    elif isinstance(client, redis.cluster.AbstractRedisCluster):\n        return client.nodes_manager.connection_kwargs.get(\"protocol\")\n", 118], "/usr/local/lib/python3.11/site-packages/redis/commands/core.py": ["# from __future__ import annotations\n\nimport datetime\nimport hashlib\nimport warnings\nfrom enum import Enum\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    AsyncIterator,\n    Awaitable,\n    Callable,\n    Dict,\n    Iterable,\n    Iterator,\n    List,\n    Literal,\n    Mapping,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Union,\n)\n\nfrom redis.exceptions import ConnectionError, DataError, NoScriptError, RedisError\nfrom redis.typing import (\n    AbsExpiryT,\n    AnyKeyT,\n    BitfieldOffsetT,\n    ChannelT,\n    CommandsProtocol,\n    ConsumerT,\n    EncodableT,\n    ExpiryT,\n    FieldT,\n    GroupT,\n    KeysT,\n    KeyT,\n    Number,\n    PatternT,\n    ResponseT,\n    ScriptTextT,\n    StreamIdT,\n    TimeoutSecT,\n    ZScoreBoundT,\n)\nfrom redis.utils import (\n    deprecated_function,\n    extract_expire_flags,\n)\n\nfrom .helpers import list_or_args\n\nif TYPE_CHECKING:\n    import redis.asyncio.client\n    import redis.client\n\n\nclass ACLCommands(CommandsProtocol):\n    \"\"\"\n    Redis Access Control List (ACL) commands.\n    see: https://redis.io/topics/acl\n    \"\"\"\n\n    def acl_cat(self, category: Optional[str] = None, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns a list of categories or commands within a category.\n\n        If ``category`` is not supplied, returns a list of all categories.\n        If ``category`` is supplied, returns a list of all commands within\n        that category.\n\n        For more information see https://redis.io/commands/acl-cat\n        \"\"\"\n        pieces: list[EncodableT] = [category] if category else []\n        return self.execute_command(\"ACL CAT\", *pieces, **kwargs)\n\n    def acl_dryrun(self, username, *args, **kwargs):\n        \"\"\"\n        Simulate the execution of a given command by a given ``username``.\n\n        For more information see https://redis.io/commands/acl-dryrun\n        \"\"\"\n        return self.execute_command(\"ACL DRYRUN\", username, *args, **kwargs)\n\n    def acl_deluser(self, *username: str, **kwargs) -> ResponseT:\n        \"\"\"\n        Delete the ACL for the specified ``username``\\\\s\n\n        For more information see https://redis.io/commands/acl-deluser\n        \"\"\"\n        return self.execute_command(\"ACL DELUSER\", *username, **kwargs)\n\n    def acl_genpass(self, bits: Optional[int] = None, **kwargs) -> ResponseT:\n        \"\"\"Generate a random password value.\n        If ``bits`` is supplied then use this number of bits, rounded to\n        the next multiple of 4.\n        See: https://redis.io/commands/acl-genpass\n        \"\"\"\n        pieces = []\n        if bits is not None:\n            try:\n                b = int(bits)\n                if b < 0 or b > 4096:\n                    raise ValueError\n                pieces.append(b)\n            except ValueError:\n                raise DataError(\n                    \"genpass optionally accepts a bits argument, between 0 and 4096.\"\n                )\n        return self.execute_command(\"ACL GENPASS\", *pieces, **kwargs)\n\n    def acl_getuser(self, username: str, **kwargs) -> ResponseT:\n        \"\"\"\n        Get the ACL details for the specified ``username``.\n\n        If ``username`` does not exist, return None\n\n        For more information see https://redis.io/commands/acl-getuser\n        \"\"\"\n        return self.execute_command(\"ACL GETUSER\", username, **kwargs)\n\n    def acl_help(self, **kwargs) -> ResponseT:\n        \"\"\"The ACL HELP command returns helpful text describing\n        the different subcommands.\n\n        For more information see https://redis.io/commands/acl-help\n        \"\"\"\n        return self.execute_command(\"ACL HELP\", **kwargs)\n\n    def acl_list(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Return a list of all ACLs on the server\n\n        For more information see https://redis.io/commands/acl-list\n        \"\"\"\n        return self.execute_command(\"ACL LIST\", **kwargs)\n\n    def acl_log(self, count: Optional[int] = None, **kwargs) -> ResponseT:\n        \"\"\"\n        Get ACL logs as a list.\n        :param int count: Get logs[0:count].\n        :rtype: List.\n\n        For more information see https://redis.io/commands/acl-log\n        \"\"\"\n        args = []\n        if count is not None:\n            if not isinstance(count, int):\n                raise DataError(\"ACL LOG count must be an integer\")\n            args.append(count)\n\n        return self.execute_command(\"ACL LOG\", *args, **kwargs)\n\n    def acl_log_reset(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Reset ACL logs.\n        :rtype: Boolean.\n\n        For more information see https://redis.io/commands/acl-log\n        \"\"\"\n        args = [b\"RESET\"]\n        return self.execute_command(\"ACL LOG\", *args, **kwargs)\n\n    def acl_load(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Load ACL rules from the configured ``aclfile``.\n\n        Note that the server must be configured with the ``aclfile``\n        directive to be able to load ACL rules from an aclfile.\n\n        For more information see https://redis.io/commands/acl-load\n        \"\"\"\n        return self.execute_command(\"ACL LOAD\", **kwargs)\n\n    def acl_save(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Save ACL rules to the configured ``aclfile``.\n\n        Note that the server must be configured with the ``aclfile``\n        directive to be able to save ACL rules to an aclfile.\n\n        For more information see https://redis.io/commands/acl-save\n        \"\"\"\n        return self.execute_command(\"ACL SAVE\", **kwargs)\n\n    def acl_setuser(\n        self,\n        username: str,\n        enabled: bool = False,\n        nopass: bool = False,\n        passwords: Optional[Union[str, Iterable[str]]] = None,\n        hashed_passwords: Optional[Union[str, Iterable[str]]] = None,\n        categories: Optional[Iterable[str]] = None,\n        commands: Optional[Iterable[str]] = None,\n        keys: Optional[Iterable[KeyT]] = None,\n        channels: Optional[Iterable[ChannelT]] = None,\n        selectors: Optional[Iterable[Tuple[str, KeyT]]] = None,\n        reset: bool = False,\n        reset_keys: bool = False,\n        reset_channels: bool = False,\n        reset_passwords: bool = False,\n        **kwargs,\n    ) -> ResponseT:\n        \"\"\"\n        Create or update an ACL user.\n\n        Create or update the ACL for `username`. If the user already exists,\n        the existing ACL is completely overwritten and replaced with the\n        specified values.\n\n        For more information, see https://redis.io/commands/acl-setuser\n\n        Args:\n            username: The name of the user whose ACL is to be created or updated.\n            enabled: Indicates whether the user should be allowed to authenticate.\n                     Defaults to `False`.\n            nopass: Indicates whether the user can authenticate without a password.\n                    This cannot be `True` if `passwords` are also specified.\n            passwords: A list of plain text passwords to add to or remove from the user.\n                       Each password must be prefixed with a '+' to add or a '-' to\n                       remove. For convenience, a single prefixed string can be used\n                       when adding or removing a single password.\n            hashed_passwords: A list of SHA-256 hashed passwords to add to or remove\n                              from the user. Each hashed password must be prefixed with\n                              a '+' to add or a '-' to remove. For convenience, a single\n                              prefixed string can be used when adding or removing a\n                              single password.\n            categories: A list of strings representing category permissions. Each string\n                        must be prefixed with either a '+' to add the category\n                        permission or a '-' to remove the category permission.\n            commands: A list of strings representing command permissions. Each string\n                      must be prefixed with either a '+' to add the command permission\n                      or a '-' to remove the command permission.\n            keys: A list of key patterns to grant the user access to. Key patterns allow\n                  ``'*'`` to support wildcard matching. For example, ``'*'`` grants\n                  access to all keys while ``'cache:*'`` grants access to all keys that\n                  are prefixed with ``cache:``.\n                  `keys` should not be prefixed with a ``'~'``.\n            reset: Indicates whether the user should be fully reset prior to applying\n                   the new ACL. Setting this to `True` will remove all existing\n                   passwords, flags, and privileges from the user and then apply the\n                   specified rules. If `False`, the user's existing passwords, flags,\n                   and privileges will be kept and any new specified rules will be\n                   applied on top.\n            reset_keys: Indicates whether the user's key permissions should be reset\n                        prior to applying any new key permissions specified in `keys`.\n                        If `False`, the user's existing key permissions will be kept and\n                        any new specified key permissions will be applied on top.\n            reset_channels: Indicates whether the user's channel permissions should be\n                            reset prior to applying any new channel permissions\n                            specified in `channels`. If `False`, the user's existing\n                            channel permissions will be kept and any new specified\n                            channel permissions will be applied on top.\n            reset_passwords: Indicates whether to remove all existing passwords and the\n                             `nopass` flag from the user prior to applying any new\n                             passwords specified in `passwords` or `hashed_passwords`.\n                             If `False`, the user's existing passwords and `nopass`\n                             status will be kept and any new specified passwords or\n                             hashed passwords will be applied on top.\n        \"\"\"\n        encoder = self.get_encoder()\n        pieces: List[EncodableT] = [username]\n\n        if reset:\n            pieces.append(b\"reset\")\n\n        if reset_keys:\n            pieces.append(b\"resetkeys\")\n\n        if reset_channels:\n            pieces.append(b\"resetchannels\")\n\n        if reset_passwords:\n            pieces.append(b\"resetpass\")\n\n        if enabled:\n            pieces.append(b\"on\")\n        else:\n            pieces.append(b\"off\")\n\n        if (passwords or hashed_passwords) and nopass:\n            raise DataError(\n                \"Cannot set 'nopass' and supply 'passwords' or 'hashed_passwords'\"\n            )\n\n        if passwords:\n            # as most users will have only one password, allow remove_passwords\n            # to be specified as a simple string or a list\n            passwords = list_or_args(passwords, [])\n            for i, password in enumerate(passwords):\n                password = encoder.encode(password)\n                if password.startswith(b\"+\"):\n                    pieces.append(b\">%s\" % password[1:])\n                elif password.startswith(b\"-\"):\n                    pieces.append(b\"<%s\" % password[1:])\n                else:\n                    raise DataError(\n                        f\"Password {i} must be prefixed with a \"\n                        f'\"+\" to add or a \"-\" to remove'\n                    )\n\n        if hashed_passwords:\n            # as most users will have only one password, allow remove_passwords\n            # to be specified as a simple string or a list\n            hashed_passwords = list_or_args(hashed_passwords, [])\n            for i, hashed_password in enumerate(hashed_passwords):\n                hashed_password = encoder.encode(hashed_password)\n                if hashed_password.startswith(b\"+\"):\n                    pieces.append(b\"#%s\" % hashed_password[1:])\n                elif hashed_password.startswith(b\"-\"):\n                    pieces.append(b\"!%s\" % hashed_password[1:])\n                else:\n                    raise DataError(\n                        f\"Hashed password {i} must be prefixed with a \"\n                        f'\"+\" to add or a \"-\" to remove'\n                    )\n\n        if nopass:\n            pieces.append(b\"nopass\")\n\n        if categories:\n            for category in categories:\n                category = encoder.encode(category)\n                # categories can be prefixed with one of (+@, +, -@, -)\n                if category.startswith(b\"+@\"):\n                    pieces.append(category)\n                elif category.startswith(b\"+\"):\n                    pieces.append(b\"+@%s\" % category[1:])\n                elif category.startswith(b\"-@\"):\n                    pieces.append(category)\n                elif category.startswith(b\"-\"):\n                    pieces.append(b\"-@%s\" % category[1:])\n                else:\n                    raise DataError(\n                        f'Category \"{encoder.decode(category, force=True)}\" '\n                        'must be prefixed with \"+\" or \"-\"'\n                    )\n        if commands:\n            for cmd in commands:\n                cmd = encoder.encode(cmd)\n                if not cmd.startswith(b\"+\") and not cmd.startswith(b\"-\"):\n                    raise DataError(\n                        f'Command \"{encoder.decode(cmd, force=True)}\" '\n                        'must be prefixed with \"+\" or \"-\"'\n                    )\n                pieces.append(cmd)\n\n        if keys:\n            for key in keys:\n                key = encoder.encode(key)\n                if not key.startswith(b\"%\") and not key.startswith(b\"~\"):\n                    key = b\"~%s\" % key\n                pieces.append(key)\n\n        if channels:\n            for channel in channels:\n                channel = encoder.encode(channel)\n                pieces.append(b\"&%s\" % channel)\n\n        if selectors:\n            for cmd, key in selectors:\n                cmd = encoder.encode(cmd)\n                if not cmd.startswith(b\"+\") and not cmd.startswith(b\"-\"):\n                    raise DataError(\n                        f'Command \"{encoder.decode(cmd, force=True)}\" '\n                        'must be prefixed with \"+\" or \"-\"'\n                    )\n\n                key = encoder.encode(key)\n                if not key.startswith(b\"%\") and not key.startswith(b\"~\"):\n                    key = b\"~%s\" % key\n\n                pieces.append(b\"(%s %s)\" % (cmd, key))\n\n        return self.execute_command(\"ACL SETUSER\", *pieces, **kwargs)\n\n    def acl_users(self, **kwargs) -> ResponseT:\n        \"\"\"Returns a list of all registered users on the server.\n\n        For more information see https://redis.io/commands/acl-users\n        \"\"\"\n        return self.execute_command(\"ACL USERS\", **kwargs)\n\n    def acl_whoami(self, **kwargs) -> ResponseT:\n        \"\"\"Get the username for the current connection\n\n        For more information see https://redis.io/commands/acl-whoami\n        \"\"\"\n        return self.execute_command(\"ACL WHOAMI\", **kwargs)\n\n\nAsyncACLCommands = ACLCommands\n\n\nclass ManagementCommands(CommandsProtocol):\n    \"\"\"\n    Redis management commands\n    \"\"\"\n\n    def auth(self, password: str, username: Optional[str] = None, **kwargs):\n        \"\"\"\n        Authenticates the user. If you do not pass username, Redis will try to\n        authenticate for the \"default\" user. If you do pass username, it will\n        authenticate for the given user.\n        For more information see https://redis.io/commands/auth\n        \"\"\"\n        pieces = []\n        if username is not None:\n            pieces.append(username)\n        pieces.append(password)\n        return self.execute_command(\"AUTH\", *pieces, **kwargs)\n\n    def bgrewriteaof(self, **kwargs):\n        \"\"\"Tell the Redis server to rewrite the AOF file from data in memory.\n\n        For more information see https://redis.io/commands/bgrewriteaof\n        \"\"\"\n        return self.execute_command(\"BGREWRITEAOF\", **kwargs)\n\n    def bgsave(self, schedule: bool = True, **kwargs) -> ResponseT:\n        \"\"\"\n        Tell the Redis server to save its data to disk.  Unlike save(),\n        this method is asynchronous and returns immediately.\n\n        For more information see https://redis.io/commands/bgsave\n        \"\"\"\n        pieces = []\n        if schedule:\n            pieces.append(\"SCHEDULE\")\n        return self.execute_command(\"BGSAVE\", *pieces, **kwargs)\n\n    def role(self) -> ResponseT:\n        \"\"\"\n        Provide information on the role of a Redis instance in\n        the context of replication, by returning if the instance\n        is currently a master, slave, or sentinel.\n\n        For more information see https://redis.io/commands/role\n        \"\"\"\n        return self.execute_command(\"ROLE\")\n\n    def client_kill(self, address: str, **kwargs) -> ResponseT:\n        \"\"\"Disconnects the client at ``address`` (ip:port)\n\n        For more information see https://redis.io/commands/client-kill\n        \"\"\"\n        return self.execute_command(\"CLIENT KILL\", address, **kwargs)\n\n    def client_kill_filter(\n        self,\n        _id: Optional[str] = None,\n        _type: Optional[str] = None,\n        addr: Optional[str] = None,\n        skipme: Optional[bool] = None,\n        laddr: Optional[bool] = None,\n        user: Optional[str] = None,\n        maxage: Optional[int] = None,\n        **kwargs,\n    ) -> ResponseT:\n        \"\"\"\n        Disconnects client(s) using a variety of filter options\n        :param _id: Kills a client by its unique ID field\n        :param _type: Kills a client by type where type is one of 'normal',\n        'master', 'slave' or 'pubsub'\n        :param addr: Kills a client by its 'address:port'\n        :param skipme: If True, then the client calling the command\n        will not get killed even if it is identified by one of the filter\n        options. If skipme is not provided, the server defaults to skipme=True\n        :param laddr: Kills a client by its 'local (bind) address:port'\n        :param user: Kills a client for a specific user name\n        :param maxage: Kills clients that are older than the specified age in seconds\n        \"\"\"\n        args = []\n        if _type is not None:\n            client_types = (\"normal\", \"master\", \"slave\", \"pubsub\")\n            if str(_type).lower() not in client_types:\n                raise DataError(f\"CLIENT KILL type must be one of {client_types!r}\")\n            args.extend((b\"TYPE\", _type))\n        if skipme is not None:\n            if not isinstance(skipme, bool):\n                raise DataError(\"CLIENT KILL skipme must be a bool\")\n            if skipme:\n                args.extend((b\"SKIPME\", b\"YES\"))\n            else:\n                args.extend((b\"SKIPME\", b\"NO\"))\n        if _id is not None:\n            args.extend((b\"ID\", _id))\n        if addr is not None:\n            args.extend((b\"ADDR\", addr))\n        if laddr is not None:\n            args.extend((b\"LADDR\", laddr))\n        if user is not None:\n            args.extend((b\"USER\", user))\n        if maxage is not None:\n            args.extend((b\"MAXAGE\", maxage))\n        if not args:\n            raise DataError(\n                \"CLIENT KILL <filter> <value> ... ... <filter> \"\n                \"<value> must specify at least one filter\"\n            )\n        return self.execute_command(\"CLIENT KILL\", *args, **kwargs)\n\n    def client_info(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns information and statistics about the current\n        client connection.\n\n        For more information see https://redis.io/commands/client-info\n        \"\"\"\n        return self.execute_command(\"CLIENT INFO\", **kwargs)\n\n    def client_list(\n        self, _type: Optional[str] = None, client_id: List[EncodableT] = [], **kwargs\n    ) -> ResponseT:\n        \"\"\"\n        Returns a list of currently connected clients.\n        If type of client specified, only that type will be returned.\n\n        :param _type: optional. one of the client types (normal, master,\n         replica, pubsub)\n        :param client_id: optional. a list of client ids\n\n        For more information see https://redis.io/commands/client-list\n        \"\"\"\n        args = []\n        if _type is not None:\n            client_types = (\"normal\", \"master\", \"replica\", \"pubsub\")\n            if str(_type).lower() not in client_types:\n                raise DataError(f\"CLIENT LIST _type must be one of {client_types!r}\")\n            args.append(b\"TYPE\")\n            args.append(_type)\n        if not isinstance(client_id, list):\n            raise DataError(\"client_id must be a list\")\n        if client_id:\n            args.append(b\"ID\")\n            args += client_id\n        return self.execute_command(\"CLIENT LIST\", *args, **kwargs)\n\n    def client_getname(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns the current connection name\n\n        For more information see https://redis.io/commands/client-getname\n        \"\"\"\n        return self.execute_command(\"CLIENT GETNAME\", **kwargs)\n\n    def client_getredir(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns the ID (an integer) of the client to whom we are\n        redirecting tracking notifications.\n\n        see: https://redis.io/commands/client-getredir\n        \"\"\"\n        return self.execute_command(\"CLIENT GETREDIR\", **kwargs)\n\n    def client_reply(\n        self, reply: Union[Literal[\"ON\"], Literal[\"OFF\"], Literal[\"SKIP\"]], **kwargs\n    ) -> ResponseT:\n        \"\"\"\n        Enable and disable redis server replies.\n\n        ``reply`` Must be ON OFF or SKIP,\n        ON - The default most with server replies to commands\n        OFF - Disable server responses to commands\n        SKIP - Skip the response of the immediately following command.\n\n        Note: When setting OFF or SKIP replies, you will need a client object\n        with a timeout specified in seconds, and will need to catch the\n        TimeoutError.\n        The test_client_reply unit test illustrates this, and\n        conftest.py has a client with a timeout.\n\n        See https://redis.io/commands/client-reply\n        \"\"\"\n        replies = [\"ON\", \"OFF\", \"SKIP\"]\n        if reply not in replies:\n            raise DataError(f\"CLIENT REPLY must be one of {replies!r}\")\n        return self.execute_command(\"CLIENT REPLY\", reply, **kwargs)\n\n    def client_id(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns the current connection id\n\n        For more information see https://redis.io/commands/client-id\n        \"\"\"\n        return self.execute_command(\"CLIENT ID\", **kwargs)\n\n    def client_tracking_on(\n        self,\n        clientid: Optional[int] = None,\n        prefix: Sequence[KeyT] = [],\n        bcast: bool = False,\n        optin: bool = False,\n        optout: bool = False,\n        noloop: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Turn on the tracking mode.\n        For more information about the options look at client_tracking func.\n\n        See https://redis.io/commands/client-tracking\n        \"\"\"\n        return self.client_tracking(\n            True, clientid, prefix, bcast, optin, optout, noloop\n        )\n\n    def client_tracking_off(\n        self,\n        clientid: Optional[int] = None,\n        prefix: Sequence[KeyT] = [],\n        bcast: bool = False,\n        optin: bool = False,\n        optout: bool = False,\n        noloop: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Turn off the tracking mode.\n        For more information about the options look at client_tracking func.\n\n        See https://redis.io/commands/client-tracking\n        \"\"\"\n        return self.client_tracking(\n            False, clientid, prefix, bcast, optin, optout, noloop\n        )\n\n    def client_tracking(\n        self,\n        on: bool = True,\n        clientid: Optional[int] = None,\n        prefix: Sequence[KeyT] = [],\n        bcast: bool = False,\n        optin: bool = False,\n        optout: bool = False,\n        noloop: bool = False,\n        **kwargs,\n    ) -> ResponseT:\n        \"\"\"\n        Enables the tracking feature of the Redis server, that is used\n        for server assisted client side caching.\n\n        ``on`` indicate for tracking on or tracking off. The dafualt is on.\n\n        ``clientid`` send invalidation messages to the connection with\n        the specified ID.\n\n        ``bcast`` enable tracking in broadcasting mode. In this mode\n        invalidation messages are reported for all the prefixes\n        specified, regardless of the keys requested by the connection.\n\n        ``optin``  when broadcasting is NOT active, normally don't track\n        keys in read only commands, unless they are called immediately\n        after a CLIENT CACHING yes command.\n\n        ``optout`` when broadcasting is NOT active, normally track keys in\n        read only commands, unless they are called immediately after a\n        CLIENT CACHING no command.\n\n        ``noloop`` don't send notifications about keys modified by this\n        connection itself.\n\n        ``prefix``  for broadcasting, register a given key prefix, so that\n        notifications will be provided only for keys starting with this string.\n\n        See https://redis.io/commands/client-tracking\n        \"\"\"\n\n        if len(prefix) != 0 and bcast is False:\n            raise DataError(\"Prefix can only be used with bcast\")\n\n        pieces = [\"ON\"] if on else [\"OFF\"]\n        if clientid is not None:\n            pieces.extend([\"REDIRECT\", clientid])\n        for p in prefix:\n            pieces.extend([\"PREFIX\", p])\n        if bcast:\n            pieces.append(\"BCAST\")\n        if optin:\n            pieces.append(\"OPTIN\")\n        if optout:\n            pieces.append(\"OPTOUT\")\n        if noloop:\n            pieces.append(\"NOLOOP\")\n\n        return self.execute_command(\"CLIENT TRACKING\", *pieces)\n\n    def client_trackinginfo(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns the information about the current client connection's\n        use of the server assisted client side cache.\n\n        See https://redis.io/commands/client-trackinginfo\n        \"\"\"\n        return self.execute_command(\"CLIENT TRACKINGINFO\", **kwargs)\n\n    def client_setname(self, name: str, **kwargs) -> ResponseT:\n        \"\"\"\n        Sets the current connection name\n\n        For more information see https://redis.io/commands/client-setname\n\n        .. note::\n           This method sets client name only for **current** connection.\n\n           If you want to set a common name for all connections managed\n           by this client, use ``client_name`` constructor argument.\n        \"\"\"\n        return self.execute_command(\"CLIENT SETNAME\", name, **kwargs)\n\n    def client_setinfo(self, attr: str, value: str, **kwargs) -> ResponseT:\n        \"\"\"\n        Sets the current connection library name or version\n        For mor information see https://redis.io/commands/client-setinfo\n        \"\"\"\n        return self.execute_command(\"CLIENT SETINFO\", attr, value, **kwargs)\n\n    def client_unblock(\n        self, client_id: int, error: bool = False, **kwargs\n    ) -> ResponseT:\n        \"\"\"\n        Unblocks a connection by its client id.\n        If ``error`` is True, unblocks the client with a special error message.\n        If ``error`` is False (default), the client is unblocked using the\n        regular timeout mechanism.\n\n        For more information see https://redis.io/commands/client-unblock\n        \"\"\"\n        args = [\"CLIENT UNBLOCK\", int(client_id)]\n        if error:\n            args.append(b\"ERROR\")\n        return self.execute_command(*args, **kwargs)\n\n    def client_pause(self, timeout: int, all: bool = True, **kwargs) -> ResponseT:\n        \"\"\"\n        Suspend all the Redis clients for the specified amount of time.\n\n\n        For more information see https://redis.io/commands/client-pause\n\n        Args:\n            timeout: milliseconds to pause clients\n            all: If true (default) all client commands are blocked.\n                 otherwise, clients are only blocked if they attempt to execute\n                 a write command.\n\n        For the WRITE mode, some commands have special behavior:\n\n        * EVAL/EVALSHA: Will block client for all scripts.\n        * PUBLISH: Will block client.\n        * PFCOUNT: Will block client.\n        * WAIT: Acknowledgments will be delayed, so this command will\n            appear blocked.\n        \"\"\"\n        args = [\"CLIENT PAUSE\", str(timeout)]\n        if not isinstance(timeout, int):\n            raise DataError(\"CLIENT PAUSE timeout must be an integer\")\n        if not all:\n            args.append(\"WRITE\")\n        return self.execute_command(*args, **kwargs)\n\n    def client_unpause(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Unpause all redis clients\n\n        For more information see https://redis.io/commands/client-unpause\n        \"\"\"\n        return self.execute_command(\"CLIENT UNPAUSE\", **kwargs)\n\n    def client_no_evict(self, mode: str) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Sets the client eviction mode for the current connection.\n\n        For more information see https://redis.io/commands/client-no-evict\n        \"\"\"\n        return self.execute_command(\"CLIENT NO-EVICT\", mode)\n\n    def client_no_touch(self, mode: str) -> Union[Awaitable[str], str]:\n        \"\"\"\n        # The command controls whether commands sent by the client will alter\n        # the LRU/LFU of the keys they access.\n        # When turned on, the current client will not change LFU/LRU stats,\n        # unless it sends the TOUCH command.\n\n        For more information see https://redis.io/commands/client-no-touch\n        \"\"\"\n        return self.execute_command(\"CLIENT NO-TOUCH\", mode)\n\n    def command(self, **kwargs):\n        \"\"\"\n        Returns dict reply of details about all Redis commands.\n\n        For more information see https://redis.io/commands/command\n        \"\"\"\n        return self.execute_command(\"COMMAND\", **kwargs)\n\n    def command_info(self, **kwargs) -> None:\n        raise NotImplementedError(\n            \"COMMAND INFO is intentionally not implemented in the client.\"\n        )\n\n    def command_count(self, **kwargs) -> ResponseT:\n        return self.execute_command(\"COMMAND COUNT\", **kwargs)\n\n    def command_list(\n        self,\n        module: Optional[str] = None,\n        category: Optional[str] = None,\n        pattern: Optional[str] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Return an array of the server's command names.\n        You can use one of the following filters:\n        ``module``: get the commands that belong to the module\n        ``category``: get the commands in the ACL category\n        ``pattern``: get the commands that match the given pattern\n\n        For more information see https://redis.io/commands/command-list/\n        \"\"\"\n        pieces = []\n        if module is not None:\n            pieces.extend([\"MODULE\", module])\n        if category is not None:\n            pieces.extend([\"ACLCAT\", category])\n        if pattern is not None:\n            pieces.extend([\"PATTERN\", pattern])\n\n        if pieces:\n            pieces.insert(0, \"FILTERBY\")\n\n        return self.execute_command(\"COMMAND LIST\", *pieces)\n\n    def command_getkeysandflags(self, *args: List[str]) -> List[Union[str, List[str]]]:\n        \"\"\"\n        Returns array of keys from a full Redis command and their usage flags.\n\n        For more information see https://redis.io/commands/command-getkeysandflags\n        \"\"\"\n        return self.execute_command(\"COMMAND GETKEYSANDFLAGS\", *args)\n\n    def command_docs(self, *args):\n        \"\"\"\n        This function throws a NotImplementedError since it is intentionally\n        not supported.\n        \"\"\"\n        raise NotImplementedError(\n            \"COMMAND DOCS is intentionally not implemented in the client.\"\n        )\n\n    def config_get(\n        self, pattern: PatternT = \"*\", *args: List[PatternT], **kwargs\n    ) -> ResponseT:\n        \"\"\"\n        Return a dictionary of configuration based on the ``pattern``\n\n        For more information see https://redis.io/commands/config-get\n        \"\"\"\n        return self.execute_command(\"CONFIG GET\", pattern, *args, **kwargs)\n\n    def config_set(\n        self,\n        name: KeyT,\n        value: EncodableT,\n        *args: List[Union[KeyT, EncodableT]],\n        **kwargs,\n    ) -> ResponseT:\n        \"\"\"Set config item ``name`` with ``value``\n\n        For more information see https://redis.io/commands/config-set\n        \"\"\"\n        return self.execute_command(\"CONFIG SET\", name, value, *args, **kwargs)\n\n    def config_resetstat(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Reset runtime statistics\n\n        For more information see https://redis.io/commands/config-resetstat\n        \"\"\"\n        return self.execute_command(\"CONFIG RESETSTAT\", **kwargs)\n\n    def config_rewrite(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Rewrite config file with the minimal change to reflect running config.\n\n        For more information see https://redis.io/commands/config-rewrite\n        \"\"\"\n        return self.execute_command(\"CONFIG REWRITE\", **kwargs)\n\n    def dbsize(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns the number of keys in the current database\n\n        For more information see https://redis.io/commands/dbsize\n        \"\"\"\n        return self.execute_command(\"DBSIZE\", **kwargs)\n\n    def debug_object(self, key: KeyT, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns version specific meta information about a given key\n\n        For more information see https://redis.io/commands/debug-object\n        \"\"\"\n        return self.execute_command(\"DEBUG OBJECT\", key, **kwargs)\n\n    def debug_segfault(self, **kwargs) -> None:\n        raise NotImplementedError(\n            \"\"\"\n            DEBUG SEGFAULT is intentionally not implemented in the client.\n\n            For more information see https://redis.io/commands/debug-segfault\n            \"\"\"\n        )\n\n    def echo(self, value: EncodableT, **kwargs) -> ResponseT:\n        \"\"\"\n        Echo the string back from the server\n\n        For more information see https://redis.io/commands/echo\n        \"\"\"\n        return self.execute_command(\"ECHO\", value, **kwargs)\n\n    def flushall(self, asynchronous: bool = False, **kwargs) -> ResponseT:\n        \"\"\"\n        Delete all keys in all databases on the current host.\n\n        ``asynchronous`` indicates whether the operation is\n        executed asynchronously by the server.\n\n        For more information see https://redis.io/commands/flushall\n        \"\"\"\n        args = []\n        if asynchronous:\n            args.append(b\"ASYNC\")\n        return self.execute_command(\"FLUSHALL\", *args, **kwargs)\n\n    def flushdb(self, asynchronous: bool = False, **kwargs) -> ResponseT:\n        \"\"\"\n        Delete all keys in the current database.\n\n        ``asynchronous`` indicates whether the operation is\n        executed asynchronously by the server.\n\n        For more information see https://redis.io/commands/flushdb\n        \"\"\"\n        args = []\n        if asynchronous:\n            args.append(b\"ASYNC\")\n        return self.execute_command(\"FLUSHDB\", *args, **kwargs)\n\n    def sync(self) -> ResponseT:\n        \"\"\"\n        Initiates a replication stream from the master.\n\n        For more information see https://redis.io/commands/sync\n        \"\"\"\n        from redis.client import NEVER_DECODE\n\n        options = {}\n        options[NEVER_DECODE] = []\n        return self.execute_command(\"SYNC\", **options)\n\n    def psync(self, replicationid: str, offset: int):\n        \"\"\"\n        Initiates a replication stream from the master.\n        Newer version for `sync`.\n\n        For more information see https://redis.io/commands/sync\n        \"\"\"\n        from redis.client import NEVER_DECODE\n\n        options = {}\n        options[NEVER_DECODE] = []\n        return self.execute_command(\"PSYNC\", replicationid, offset, **options)\n\n    def swapdb(self, first: int, second: int, **kwargs) -> ResponseT:\n        \"\"\"\n        Swap two databases\n\n        For more information see https://redis.io/commands/swapdb\n        \"\"\"\n        return self.execute_command(\"SWAPDB\", first, second, **kwargs)\n\n    def select(self, index: int, **kwargs) -> ResponseT:\n        \"\"\"Select the Redis logical database at index.\n\n        See: https://redis.io/commands/select\n        \"\"\"\n        return self.execute_command(\"SELECT\", index, **kwargs)\n\n    def info(\n        self, section: Optional[str] = None, *args: List[str], **kwargs\n    ) -> ResponseT:\n        \"\"\"\n        Returns a dictionary containing information about the Redis server\n\n        The ``section`` option can be used to select a specific section\n        of information\n\n        The section option is not supported by older versions of Redis Server,\n        and will generate ResponseError\n\n        For more information see https://redis.io/commands/info\n        \"\"\"\n        if section is None:\n            return self.execute_command(\"INFO\", **kwargs)\n        else:\n            return self.execute_command(\"INFO\", section, *args, **kwargs)\n\n    def lastsave(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Return a Python datetime object representing the last time the\n        Redis database was saved to disk\n\n        For more information see https://redis.io/commands/lastsave\n        \"\"\"\n        return self.execute_command(\"LASTSAVE\", **kwargs)\n\n    def latency_doctor(self):\n        \"\"\"Raise a NotImplementedError, as the client will not support LATENCY DOCTOR.\n        This funcion is best used within the redis-cli.\n\n        For more information see https://redis.io/commands/latency-doctor\n        \"\"\"\n        raise NotImplementedError(\n            \"\"\"\n            LATENCY DOCTOR is intentionally not implemented in the client.\n\n            For more information see https://redis.io/commands/latency-doctor\n            \"\"\"\n        )\n\n    def latency_graph(self):\n        \"\"\"Raise a NotImplementedError, as the client will not support LATENCY GRAPH.\n        This funcion is best used within the redis-cli.\n\n        For more information see https://redis.io/commands/latency-graph.\n        \"\"\"\n        raise NotImplementedError(\n            \"\"\"\n            LATENCY GRAPH is intentionally not implemented in the client.\n\n            For more information see https://redis.io/commands/latency-graph\n            \"\"\"\n        )\n\n    def lolwut(self, *version_numbers: Union[str, float], **kwargs) -> ResponseT:\n        \"\"\"\n        Get the Redis version and a piece of generative computer art\n\n        See: https://redis.io/commands/lolwut\n        \"\"\"\n        if version_numbers:\n            return self.execute_command(\"LOLWUT VERSION\", *version_numbers, **kwargs)\n        else:\n            return self.execute_command(\"LOLWUT\", **kwargs)\n\n    def reset(self) -> ResponseT:\n        \"\"\"Perform a full reset on the connection's server side contenxt.\n\n        See: https://redis.io/commands/reset\n        \"\"\"\n        return self.execute_command(\"RESET\")\n\n    def migrate(\n        self,\n        host: str,\n        port: int,\n        keys: KeysT,\n        destination_db: int,\n        timeout: int,\n        copy: bool = False,\n        replace: bool = False,\n        auth: Optional[str] = None,\n        **kwargs,\n    ) -> ResponseT:\n        \"\"\"\n        Migrate 1 or more keys from the current Redis server to a different\n        server specified by the ``host``, ``port`` and ``destination_db``.\n\n        The ``timeout``, specified in milliseconds, indicates the maximum\n        time the connection between the two servers can be idle before the\n        command is interrupted.\n\n        If ``copy`` is True, the specified ``keys`` are NOT deleted from\n        the source server.\n\n        If ``replace`` is True, this operation will overwrite the keys\n        on the destination server if they exist.\n\n        If ``auth`` is specified, authenticate to the destination server with\n        the password provided.\n\n        For more information see https://redis.io/commands/migrate\n        \"\"\"\n        keys = list_or_args(keys, [])\n        if not keys:\n            raise DataError(\"MIGRATE requires at least one key\")\n        pieces = []\n        if copy:\n            pieces.append(b\"COPY\")\n        if replace:\n            pieces.append(b\"REPLACE\")\n        if auth:\n            pieces.append(b\"AUTH\")\n            pieces.append(auth)\n        pieces.append(b\"KEYS\")\n        pieces.extend(keys)\n        return self.execute_command(\n            \"MIGRATE\", host, port, \"\", destination_db, timeout, *pieces, **kwargs\n        )\n\n    def object(self, infotype: str, key: KeyT, **kwargs) -> ResponseT:\n        \"\"\"\n        Return the encoding, idletime, or refcount about the key\n        \"\"\"\n        return self.execute_command(\n            \"OBJECT\", infotype, key, infotype=infotype, **kwargs\n        )\n\n    def memory_doctor(self, **kwargs) -> None:\n        raise NotImplementedError(\n            \"\"\"\n            MEMORY DOCTOR is intentionally not implemented in the client.\n\n            For more information see https://redis.io/commands/memory-doctor\n            \"\"\"\n        )\n\n    def memory_help(self, **kwargs) -> None:\n        raise NotImplementedError(\n            \"\"\"\n            MEMORY HELP is intentionally not implemented in the client.\n\n            For more information see https://redis.io/commands/memory-help\n            \"\"\"\n        )\n\n    def memory_stats(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Return a dictionary of memory stats\n\n        For more information see https://redis.io/commands/memory-stats\n        \"\"\"\n        return self.execute_command(\"MEMORY STATS\", **kwargs)\n\n    def memory_malloc_stats(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Return an internal statistics report from the memory allocator.\n\n        See: https://redis.io/commands/memory-malloc-stats\n        \"\"\"\n        return self.execute_command(\"MEMORY MALLOC-STATS\", **kwargs)\n\n    def memory_usage(\n        self, key: KeyT, samples: Optional[int] = None, **kwargs\n    ) -> ResponseT:\n        \"\"\"\n        Return the total memory usage for key, its value and associated\n        administrative overheads.\n\n        For nested data structures, ``samples`` is the number of elements to\n        sample. If left unspecified, the server's default is 5. Use 0 to sample\n        all elements.\n\n        For more information see https://redis.io/commands/memory-usage\n        \"\"\"\n        args = []\n        if isinstance(samples, int):\n            args.extend([b\"SAMPLES\", samples])\n        return self.execute_command(\"MEMORY USAGE\", key, *args, **kwargs)\n\n    def memory_purge(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Attempts to purge dirty pages for reclamation by allocator\n\n        For more information see https://redis.io/commands/memory-purge\n        \"\"\"\n        return self.execute_command(\"MEMORY PURGE\", **kwargs)\n\n    def latency_histogram(self, *args):\n        \"\"\"\n        This function throws a NotImplementedError since it is intentionally\n        not supported.\n        \"\"\"\n        raise NotImplementedError(\n            \"LATENCY HISTOGRAM is intentionally not implemented in the client.\"\n        )\n\n    def latency_history(self, event: str) -> ResponseT:\n        \"\"\"\n        Returns the raw data of the ``event``'s latency spikes time series.\n\n        For more information see https://redis.io/commands/latency-history\n        \"\"\"\n        return self.execute_command(\"LATENCY HISTORY\", event)\n\n    def latency_latest(self) -> ResponseT:\n        \"\"\"\n        Reports the latest latency events logged.\n\n        For more information see https://redis.io/commands/latency-latest\n        \"\"\"\n        return self.execute_command(\"LATENCY LATEST\")\n\n    def latency_reset(self, *events: str) -> ResponseT:\n        \"\"\"\n        Resets the latency spikes time series of all, or only some, events.\n\n        For more information see https://redis.io/commands/latency-reset\n        \"\"\"\n        return self.execute_command(\"LATENCY RESET\", *events)\n\n    def ping(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Ping the Redis server\n\n        For more information see https://redis.io/commands/ping\n        \"\"\"\n        return self.execute_command(\"PING\", **kwargs)\n\n    def quit(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Ask the server to close the connection.\n\n        For more information see https://redis.io/commands/quit\n        \"\"\"\n        return self.execute_command(\"QUIT\", **kwargs)\n\n    def replicaof(self, *args, **kwargs) -> ResponseT:\n        \"\"\"\n        Update the replication settings of a redis replica, on the fly.\n\n        Examples of valid arguments include:\n\n        NO ONE (set no replication)\n        host port (set to the host and port of a redis server)\n\n        For more information see  https://redis.io/commands/replicaof\n        \"\"\"\n        return self.execute_command(\"REPLICAOF\", *args, **kwargs)\n\n    def save(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Tell the Redis server to save its data to disk,\n        blocking until the save is complete\n\n        For more information see https://redis.io/commands/save\n        \"\"\"\n        return self.execute_command(\"SAVE\", **kwargs)\n\n    def shutdown(\n        self,\n        save: bool = False,\n        nosave: bool = False,\n        now: bool = False,\n        force: bool = False,\n        abort: bool = False,\n        **kwargs,\n    ) -> None:\n        \"\"\"Shutdown the Redis server.  If Redis has persistence configured,\n        data will be flushed before shutdown.\n        It is possible to specify modifiers to alter the behavior of the command:\n        ``save`` will force a DB saving operation even if no save points are configured.\n        ``nosave`` will prevent a DB saving operation even if one or more save points\n        are configured.\n        ``now`` skips waiting for lagging replicas, i.e. it bypasses the first step in\n        the shutdown sequence.\n        ``force`` ignores any errors that would normally prevent the server from exiting\n        ``abort`` cancels an ongoing shutdown and cannot be combined with other flags.\n\n        For more information see https://redis.io/commands/shutdown\n        \"\"\"\n        if save and nosave:\n            raise DataError(\"SHUTDOWN save and nosave cannot both be set\")\n        args = [\"SHUTDOWN\"]\n        if save:\n            args.append(\"SAVE\")\n        if nosave:\n            args.append(\"NOSAVE\")\n        if now:\n            args.append(\"NOW\")\n        if force:\n            args.append(\"FORCE\")\n        if abort:\n            args.append(\"ABORT\")\n        try:\n            self.execute_command(*args, **kwargs)\n        except ConnectionError:\n            # a ConnectionError here is expected\n            return\n        raise RedisError(\"SHUTDOWN seems to have failed.\")\n\n    def slaveof(\n        self, host: Optional[str] = None, port: Optional[int] = None, **kwargs\n    ) -> ResponseT:\n        \"\"\"\n        Set the server to be a replicated slave of the instance identified\n        by the ``host`` and ``port``. If called without arguments, the\n        instance is promoted to a master instead.\n\n        For more information see https://redis.io/commands/slaveof\n        \"\"\"\n        if host is None and port is None:\n            return self.execute_command(\"SLAVEOF\", b\"NO\", b\"ONE\", **kwargs)\n        return self.execute_command(\"SLAVEOF\", host, port, **kwargs)\n\n    def slowlog_get(self, num: Optional[int] = None, **kwargs) -> ResponseT:\n        \"\"\"\n        Get the entries from the slowlog. If ``num`` is specified, get the\n        most recent ``num`` items.\n\n        For more information see https://redis.io/commands/slowlog-get\n        \"\"\"\n        from redis.client import NEVER_DECODE\n\n        args = [\"SLOWLOG GET\"]\n        if num is not None:\n            args.append(num)\n        decode_responses = self.get_connection_kwargs().get(\"decode_responses\", False)\n        if decode_responses is True:\n            kwargs[NEVER_DECODE] = []\n        return self.execute_command(*args, **kwargs)\n\n    def slowlog_len(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Get the number of items in the slowlog\n\n        For more information see https://redis.io/commands/slowlog-len\n        \"\"\"\n        return self.execute_command(\"SLOWLOG LEN\", **kwargs)\n\n    def slowlog_reset(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Remove all items in the slowlog\n\n        For more information see https://redis.io/commands/slowlog-reset\n        \"\"\"\n        return self.execute_command(\"SLOWLOG RESET\", **kwargs)\n\n    def time(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns the server time as a 2-item tuple of ints:\n        (seconds since epoch, microseconds into this second).\n\n        For more information see https://redis.io/commands/time\n        \"\"\"\n        return self.execute_command(\"TIME\", **kwargs)\n\n    def wait(self, num_replicas: int, timeout: int, **kwargs) -> ResponseT:\n        \"\"\"\n        Redis synchronous replication\n        That returns the number of replicas that processed the query when\n        we finally have at least ``num_replicas``, or when the ``timeout`` was\n        reached.\n\n        For more information see https://redis.io/commands/wait\n        \"\"\"\n        return self.execute_command(\"WAIT\", num_replicas, timeout, **kwargs)\n\n    def waitaof(\n        self, num_local: int, num_replicas: int, timeout: int, **kwargs\n    ) -> ResponseT:\n        \"\"\"\n        This command blocks the current client until all previous write\n        commands by that client are acknowledged as having been fsynced\n        to the AOF of the local Redis and/or at least the specified number\n        of replicas.\n\n        For more information see https://redis.io/commands/waitaof\n        \"\"\"\n        return self.execute_command(\n            \"WAITAOF\", num_local, num_replicas, timeout, **kwargs\n        )\n\n    def hello(self):\n        \"\"\"\n        This function throws a NotImplementedError since it is intentionally\n        not supported.\n        \"\"\"\n        raise NotImplementedError(\n            \"HELLO is intentionally not implemented in the client.\"\n        )\n\n    def failover(self):\n        \"\"\"\n        This function throws a NotImplementedError since it is intentionally\n        not supported.\n        \"\"\"\n        raise NotImplementedError(\n            \"FAILOVER is intentionally not implemented in the client.\"\n        )\n\n\nclass AsyncManagementCommands(ManagementCommands):\n    async def command_info(self, **kwargs) -> None:\n        return super().command_info(**kwargs)\n\n    async def debug_segfault(self, **kwargs) -> None:\n        return super().debug_segfault(**kwargs)\n\n    async def memory_doctor(self, **kwargs) -> None:\n        return super().memory_doctor(**kwargs)\n\n    async def memory_help(self, **kwargs) -> None:\n        return super().memory_help(**kwargs)\n\n    async def shutdown(\n        self,\n        save: bool = False,\n        nosave: bool = False,\n        now: bool = False,\n        force: bool = False,\n        abort: bool = False,\n        **kwargs,\n    ) -> None:\n        \"\"\"Shutdown the Redis server.  If Redis has persistence configured,\n        data will be flushed before shutdown.  If the \"save\" option is set,\n        a data flush will be attempted even if there is no persistence\n        configured.  If the \"nosave\" option is set, no data flush will be\n        attempted.  The \"save\" and \"nosave\" options cannot both be set.\n\n        For more information see https://redis.io/commands/shutdown\n        \"\"\"\n        if save and nosave:\n            raise DataError(\"SHUTDOWN save and nosave cannot both be set\")\n        args = [\"SHUTDOWN\"]\n        if save:\n            args.append(\"SAVE\")\n        if nosave:\n            args.append(\"NOSAVE\")\n        if now:\n            args.append(\"NOW\")\n        if force:\n            args.append(\"FORCE\")\n        if abort:\n            args.append(\"ABORT\")\n        try:\n            await self.execute_command(*args, **kwargs)\n        except ConnectionError:\n            # a ConnectionError here is expected\n            return\n        raise RedisError(\"SHUTDOWN seems to have failed.\")\n\n\nclass BitFieldOperation:\n    \"\"\"\n    Command builder for BITFIELD commands.\n    \"\"\"\n\n    def __init__(\n        self,\n        client: Union[\"redis.client.Redis\", \"redis.asyncio.client.Redis\"],\n        key: str,\n        default_overflow: Optional[str] = None,\n    ):\n        self.client = client\n        self.key = key\n        self._default_overflow = default_overflow\n        # for typing purposes, run the following in constructor and in reset()\n        self.operations: list[tuple[EncodableT, ...]] = []\n        self._last_overflow = \"WRAP\"\n        self.reset()\n\n    def reset(self):\n        \"\"\"\n        Reset the state of the instance to when it was constructed\n        \"\"\"\n        self.operations = []\n        self._last_overflow = \"WRAP\"\n        self.overflow(self._default_overflow or self._last_overflow)\n\n    def overflow(self, overflow: str):\n        \"\"\"\n        Update the overflow algorithm of successive INCRBY operations\n        :param overflow: Overflow algorithm, one of WRAP, SAT, FAIL. See the\n            Redis docs for descriptions of these algorithmsself.\n        :returns: a :py:class:`BitFieldOperation` instance.\n        \"\"\"\n        overflow = overflow.upper()\n        if overflow != self._last_overflow:\n            self._last_overflow = overflow\n            self.operations.append((\"OVERFLOW\", overflow))\n        return self\n\n    def incrby(\n        self,\n        fmt: str,\n        offset: BitfieldOffsetT,\n        increment: int,\n        overflow: Optional[str] = None,\n    ):\n        \"\"\"\n        Increment a bitfield by a given amount.\n        :param fmt: format-string for the bitfield being updated, e.g. 'u8'\n            for an unsigned 8-bit integer.\n        :param offset: offset (in number of bits). If prefixed with a\n            '#', this is an offset multiplier, e.g. given the arguments\n            fmt='u8', offset='#2', the offset will be 16.\n        :param int increment: value to increment the bitfield by.\n        :param str overflow: overflow algorithm. Defaults to WRAP, but other\n            acceptable values are SAT and FAIL. See the Redis docs for\n            descriptions of these algorithms.\n        :returns: a :py:class:`BitFieldOperation` instance.\n        \"\"\"\n        if overflow is not None:\n            self.overflow(overflow)\n\n        self.operations.append((\"INCRBY\", fmt, offset, increment))\n        return self\n\n    def get(self, fmt: str, offset: BitfieldOffsetT):\n        \"\"\"\n        Get the value of a given bitfield.\n        :param fmt: format-string for the bitfield being read, e.g. 'u8' for\n            an unsigned 8-bit integer.\n        :param offset: offset (in number of bits). If prefixed with a\n            '#', this is an offset multiplier, e.g. given the arguments\n            fmt='u8', offset='#2', the offset will be 16.\n        :returns: a :py:class:`BitFieldOperation` instance.\n        \"\"\"\n        self.operations.append((\"GET\", fmt, offset))\n        return self\n\n    def set(self, fmt: str, offset: BitfieldOffsetT, value: int):\n        \"\"\"\n        Set the value of a given bitfield.\n        :param fmt: format-string for the bitfield being read, e.g. 'u8' for\n            an unsigned 8-bit integer.\n        :param offset: offset (in number of bits). If prefixed with a\n            '#', this is an offset multiplier, e.g. given the arguments\n            fmt='u8', offset='#2', the offset will be 16.\n        :param int value: value to set at the given position.\n        :returns: a :py:class:`BitFieldOperation` instance.\n        \"\"\"\n        self.operations.append((\"SET\", fmt, offset, value))\n        return self\n\n    @property\n    def command(self):\n        cmd = [\"BITFIELD\", self.key]\n        for ops in self.operations:\n            cmd.extend(ops)\n        return cmd\n\n    def execute(self) -> ResponseT:\n        \"\"\"\n        Execute the operation(s) in a single BITFIELD command. The return value\n        is a list of values corresponding to each operation. If the client\n        used to create this instance was a pipeline, the list of values\n        will be present within the pipeline's execute.\n        \"\"\"\n        command = self.command\n        self.reset()\n        return self.client.execute_command(*command)\n\n\nclass BasicKeyCommands(CommandsProtocol):\n    \"\"\"\n    Redis basic key-based commands\n    \"\"\"\n\n    def append(self, key: KeyT, value: EncodableT) -> ResponseT:\n        \"\"\"\n        Appends the string ``value`` to the value at ``key``. If ``key``\n        doesn't already exist, create it with a value of ``value``.\n        Returns the new length of the value at ``key``.\n\n        For more information see https://redis.io/commands/append\n        \"\"\"\n        return self.execute_command(\"APPEND\", key, value)\n\n    def bitcount(\n        self,\n        key: KeyT,\n        start: Optional[int] = None,\n        end: Optional[int] = None,\n        mode: Optional[str] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Returns the count of set bits in the value of ``key``.  Optional\n        ``start`` and ``end`` parameters indicate which bytes to consider\n\n        For more information see https://redis.io/commands/bitcount\n        \"\"\"\n        params = [key]\n        if start is not None and end is not None:\n            params.append(start)\n            params.append(end)\n        elif (start is not None and end is None) or (end is not None and start is None):\n            raise DataError(\"Both start and end must be specified\")\n        if mode is not None:\n            params.append(mode)\n        return self.execute_command(\"BITCOUNT\", *params, keys=[key])\n\n    def bitfield(\n        self: Union[\"redis.client.Redis\", \"redis.asyncio.client.Redis\"],\n        key: KeyT,\n        default_overflow: Optional[str] = None,\n    ) -> BitFieldOperation:\n        \"\"\"\n        Return a BitFieldOperation instance to conveniently construct one or\n        more bitfield operations on ``key``.\n\n        For more information see https://redis.io/commands/bitfield\n        \"\"\"\n        return BitFieldOperation(self, key, default_overflow=default_overflow)\n\n    def bitfield_ro(\n        self: Union[\"redis.client.Redis\", \"redis.asyncio.client.Redis\"],\n        key: KeyT,\n        encoding: str,\n        offset: BitfieldOffsetT,\n        items: Optional[list] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Return an array of the specified bitfield values\n        where the first value is found using ``encoding`` and ``offset``\n        parameters and remaining values are result of corresponding\n        encoding/offset pairs in optional list ``items``\n        Read-only variant of the BITFIELD command.\n\n        For more information see https://redis.io/commands/bitfield_ro\n        \"\"\"\n        params = [key, \"GET\", encoding, offset]\n\n        items = items or []\n        for encoding, offset in items:\n            params.extend([\"GET\", encoding, offset])\n        return self.execute_command(\"BITFIELD_RO\", *params, keys=[key])\n\n    def bitop(self, operation: str, dest: KeyT, *keys: KeyT) -> ResponseT:\n        \"\"\"\n        Perform a bitwise operation using ``operation`` between ``keys`` and\n        store the result in ``dest``.\n\n        For more information see https://redis.io/commands/bitop\n        \"\"\"\n        return self.execute_command(\"BITOP\", operation, dest, *keys)\n\n    def bitpos(\n        self,\n        key: KeyT,\n        bit: int,\n        start: Optional[int] = None,\n        end: Optional[int] = None,\n        mode: Optional[str] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Return the position of the first bit set to 1 or 0 in a string.\n        ``start`` and ``end`` defines search range. The range is interpreted\n        as a range of bytes and not a range of bits, so start=0 and end=2\n        means to look at the first three bytes.\n\n        For more information see https://redis.io/commands/bitpos\n        \"\"\"\n        if bit not in (0, 1):\n            raise DataError(\"bit must be 0 or 1\")\n        params = [key, bit]\n\n        start is not None and params.append(start)\n\n        if start is not None and end is not None:\n            params.append(end)\n        elif start is None and end is not None:\n            raise DataError(\"start argument is not set, when end is specified\")\n\n        if mode is not None:\n            params.append(mode)\n        return self.execute_command(\"BITPOS\", *params, keys=[key])\n\n    def copy(\n        self,\n        source: str,\n        destination: str,\n        destination_db: Optional[str] = None,\n        replace: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Copy the value stored in the ``source`` key to the ``destination`` key.\n\n        ``destination_db`` an alternative destination database. By default,\n        the ``destination`` key is created in the source Redis database.\n\n        ``replace`` whether the ``destination`` key should be removed before\n        copying the value to it. By default, the value is not copied if\n        the ``destination`` key already exists.\n\n        For more information see https://redis.io/commands/copy\n        \"\"\"\n        params = [source, destination]\n        if destination_db is not None:\n            params.extend([\"DB\", destination_db])\n        if replace:\n            params.append(\"REPLACE\")\n        return self.execute_command(\"COPY\", *params)\n\n    def decrby(self, name: KeyT, amount: int = 1) -> ResponseT:\n        \"\"\"\n        Decrements the value of ``key`` by ``amount``.  If no key exists,\n        the value will be initialized as 0 - ``amount``\n\n        For more information see https://redis.io/commands/decrby\n        \"\"\"\n        return self.execute_command(\"DECRBY\", name, amount)\n\n    decr = decrby\n\n    def delete(self, *names: KeyT) -> ResponseT:\n        \"\"\"\n        Delete one or more keys specified by ``names``\n        \"\"\"\n        return self.execute_command(\"DEL\", *names)\n\n    def __delitem__(self, name: KeyT):\n        self.delete(name)\n\n    def dump(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Return a serialized version of the value stored at the specified key.\n        If key does not exist a nil bulk reply is returned.\n\n        For more information see https://redis.io/commands/dump\n        \"\"\"\n        from redis.client import NEVER_DECODE\n\n        options = {}\n        options[NEVER_DECODE] = []\n        return self.execute_command(\"DUMP\", name, **options)\n\n    def exists(self, *names: KeyT) -> ResponseT:\n        \"\"\"\n        Returns the number of ``names`` that exist\n\n        For more information see https://redis.io/commands/exists\n        \"\"\"\n        return self.execute_command(\"EXISTS\", *names, keys=names)\n\n    __contains__ = exists\n\n    def expire(\n        self,\n        name: KeyT,\n        time: ExpiryT,\n        nx: bool = False,\n        xx: bool = False,\n        gt: bool = False,\n        lt: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Set an expire flag on key ``name`` for ``time`` seconds with given\n        ``option``. ``time`` can be represented by an integer or a Python timedelta\n        object.\n\n        Valid options are:\n            NX -> Set expiry only when the key has no expiry\n            XX -> Set expiry only when the key has an existing expiry\n            GT -> Set expiry only when the new expiry is greater than current one\n            LT -> Set expiry only when the new expiry is less than current one\n\n        For more information see https://redis.io/commands/expire\n        \"\"\"\n        if isinstance(time, datetime.timedelta):\n            time = int(time.total_seconds())\n\n        exp_option = list()\n        if nx:\n            exp_option.append(\"NX\")\n        if xx:\n            exp_option.append(\"XX\")\n        if gt:\n            exp_option.append(\"GT\")\n        if lt:\n            exp_option.append(\"LT\")\n\n        return self.execute_command(\"EXPIRE\", name, time, *exp_option)\n\n    def expireat(\n        self,\n        name: KeyT,\n        when: AbsExpiryT,\n        nx: bool = False,\n        xx: bool = False,\n        gt: bool = False,\n        lt: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Set an expire flag on key ``name`` with given ``option``. ``when``\n        can be represented as an integer indicating unix time or a Python\n        datetime object.\n\n        Valid options are:\n            -> NX -- Set expiry only when the key has no expiry\n            -> XX -- Set expiry only when the key has an existing expiry\n            -> GT -- Set expiry only when the new expiry is greater than current one\n            -> LT -- Set expiry only when the new expiry is less than current one\n\n        For more information see https://redis.io/commands/expireat\n        \"\"\"\n        if isinstance(when, datetime.datetime):\n            when = int(when.timestamp())\n\n        exp_option = list()\n        if nx:\n            exp_option.append(\"NX\")\n        if xx:\n            exp_option.append(\"XX\")\n        if gt:\n            exp_option.append(\"GT\")\n        if lt:\n            exp_option.append(\"LT\")\n\n        return self.execute_command(\"EXPIREAT\", name, when, *exp_option)\n\n    def expiretime(self, key: str) -> int:\n        \"\"\"\n        Returns the absolute Unix timestamp (since January 1, 1970) in seconds\n        at which the given key will expire.\n\n        For more information see https://redis.io/commands/expiretime\n        \"\"\"\n        return self.execute_command(\"EXPIRETIME\", key)\n\n    def get(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Return the value at key ``name``, or None if the key doesn't exist\n\n        For more information see https://redis.io/commands/get\n        \"\"\"\n        return self.execute_command(\"GET\", name, keys=[name])\n\n    def getdel(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Get the value at key ``name`` and delete the key. This command\n        is similar to GET, except for the fact that it also deletes\n        the key on success (if and only if the key's value type\n        is a string).\n\n        For more information see https://redis.io/commands/getdel\n        \"\"\"\n        return self.execute_command(\"GETDEL\", name)\n\n    def getex(\n        self,\n        name: KeyT,\n        ex: Optional[ExpiryT] = None,\n        px: Optional[ExpiryT] = None,\n        exat: Optional[AbsExpiryT] = None,\n        pxat: Optional[AbsExpiryT] = None,\n        persist: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Get the value of key and optionally set its expiration.\n        GETEX is similar to GET, but is a write command with\n        additional options. All time parameters can be given as\n        datetime.timedelta or integers.\n\n        ``ex`` sets an expire flag on key ``name`` for ``ex`` seconds.\n\n        ``px`` sets an expire flag on key ``name`` for ``px`` milliseconds.\n\n        ``exat`` sets an expire flag on key ``name`` for ``ex`` seconds,\n        specified in unix time.\n\n        ``pxat`` sets an expire flag on key ``name`` for ``ex`` milliseconds,\n        specified in unix time.\n\n        ``persist`` remove the time to live associated with ``name``.\n\n        For more information see https://redis.io/commands/getex\n        \"\"\"\n        opset = {ex, px, exat, pxat}\n        if len(opset) > 2 or len(opset) > 1 and persist:\n            raise DataError(\n                \"``ex``, ``px``, ``exat``, ``pxat``, \"\n                \"and ``persist`` are mutually exclusive.\"\n            )\n\n        exp_options: list[EncodableT] = extract_expire_flags(ex, px, exat, pxat)\n\n        if persist:\n            exp_options.append(\"PERSIST\")\n\n        return self.execute_command(\"GETEX\", name, *exp_options)\n\n    def __getitem__(self, name: KeyT):\n        \"\"\"\n        Return the value at key ``name``, raises a KeyError if the key\n        doesn't exist.\n        \"\"\"\n        value = self.get(name)\n        if value is not None:\n            return value\n        raise KeyError(name)\n\n    def getbit(self, name: KeyT, offset: int) -> ResponseT:\n        \"\"\"\n        Returns an integer indicating the value of ``offset`` in ``name``\n\n        For more information see https://redis.io/commands/getbit\n        \"\"\"\n        return self.execute_command(\"GETBIT\", name, offset, keys=[name])\n\n    def getrange(self, key: KeyT, start: int, end: int) -> ResponseT:\n        \"\"\"\n        Returns the substring of the string value stored at ``key``,\n        determined by the offsets ``start`` and ``end`` (both are inclusive)\n\n        For more information see https://redis.io/commands/getrange\n        \"\"\"\n        return self.execute_command(\"GETRANGE\", key, start, end, keys=[key])\n\n    def getset(self, name: KeyT, value: EncodableT) -> ResponseT:\n        \"\"\"\n        Sets the value at key ``name`` to ``value``\n        and returns the old value at key ``name`` atomically.\n\n        As per Redis 6.2, GETSET is considered deprecated.\n        Please use SET with GET parameter in new code.\n\n        For more information see https://redis.io/commands/getset\n        \"\"\"\n        return self.execute_command(\"GETSET\", name, value)\n\n    def incrby(self, name: KeyT, amount: int = 1) -> ResponseT:\n        \"\"\"\n        Increments the value of ``key`` by ``amount``.  If no key exists,\n        the value will be initialized as ``amount``\n\n        For more information see https://redis.io/commands/incrby\n        \"\"\"\n        return self.execute_command(\"INCRBY\", name, amount)\n\n    incr = incrby\n\n    def incrbyfloat(self, name: KeyT, amount: float = 1.0) -> ResponseT:\n        \"\"\"\n        Increments the value at key ``name`` by floating ``amount``.\n        If no key exists, the value will be initialized as ``amount``\n\n        For more information see https://redis.io/commands/incrbyfloat\n        \"\"\"\n        return self.execute_command(\"INCRBYFLOAT\", name, amount)\n\n    def keys(self, pattern: PatternT = \"*\", **kwargs) -> ResponseT:\n        \"\"\"\n        Returns a list of keys matching ``pattern``\n\n        For more information see https://redis.io/commands/keys\n        \"\"\"\n        return self.execute_command(\"KEYS\", pattern, **kwargs)\n\n    def lmove(\n        self, first_list: str, second_list: str, src: str = \"LEFT\", dest: str = \"RIGHT\"\n    ) -> ResponseT:\n        \"\"\"\n        Atomically returns and removes the first/last element of a list,\n        pushing it as the first/last element on the destination list.\n        Returns the element being popped and pushed.\n\n        For more information see https://redis.io/commands/lmove\n        \"\"\"\n        params = [first_list, second_list, src, dest]\n        return self.execute_command(\"LMOVE\", *params)\n\n    def blmove(\n        self,\n        first_list: str,\n        second_list: str,\n        timeout: int,\n        src: str = \"LEFT\",\n        dest: str = \"RIGHT\",\n    ) -> ResponseT:\n        \"\"\"\n        Blocking version of lmove.\n\n        For more information see https://redis.io/commands/blmove\n        \"\"\"\n        params = [first_list, second_list, src, dest, timeout]\n        return self.execute_command(\"BLMOVE\", *params)\n\n    def mget(self, keys: KeysT, *args: EncodableT) -> ResponseT:\n        \"\"\"\n        Returns a list of values ordered identically to ``keys``\n\n        For more information see https://redis.io/commands/mget\n        \"\"\"\n        from redis.client import EMPTY_RESPONSE\n\n        args = list_or_args(keys, args)\n        options = {}\n        if not args:\n            options[EMPTY_RESPONSE] = []\n        options[\"keys\"] = args\n        return self.execute_command(\"MGET\", *args, **options)\n\n    def mset(self, mapping: Mapping[AnyKeyT, EncodableT]) -> ResponseT:\n        \"\"\"\n        Sets key/values based on a mapping. Mapping is a dictionary of\n        key/value pairs. Both keys and values should be strings or types that\n        can be cast to a string via str().\n\n        For more information see https://redis.io/commands/mset\n        \"\"\"\n        items = []\n        for pair in mapping.items():\n            items.extend(pair)\n        return self.execute_command(\"MSET\", *items)\n\n    def msetnx(self, mapping: Mapping[AnyKeyT, EncodableT]) -> ResponseT:\n        \"\"\"\n        Sets key/values based on a mapping if none of the keys are already set.\n        Mapping is a dictionary of key/value pairs. Both keys and values\n        should be strings or types that can be cast to a string via str().\n        Returns a boolean indicating if the operation was successful.\n\n        For more information see https://redis.io/commands/msetnx\n        \"\"\"\n        items = []\n        for pair in mapping.items():\n            items.extend(pair)\n        return self.execute_command(\"MSETNX\", *items)\n\n    def move(self, name: KeyT, db: int) -> ResponseT:\n        \"\"\"\n        Moves the key ``name`` to a different Redis database ``db``\n\n        For more information see https://redis.io/commands/move\n        \"\"\"\n        return self.execute_command(\"MOVE\", name, db)\n\n    def persist(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Removes an expiration on ``name``\n\n        For more information see https://redis.io/commands/persist\n        \"\"\"\n        return self.execute_command(\"PERSIST\", name)\n\n    def pexpire(\n        self,\n        name: KeyT,\n        time: ExpiryT,\n        nx: bool = False,\n        xx: bool = False,\n        gt: bool = False,\n        lt: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Set an expire flag on key ``name`` for ``time`` milliseconds\n        with given ``option``. ``time`` can be represented by an\n        integer or a Python timedelta object.\n\n        Valid options are:\n            NX -> Set expiry only when the key has no expiry\n            XX -> Set expiry only when the key has an existing expiry\n            GT -> Set expiry only when the new expiry is greater than current one\n            LT -> Set expiry only when the new expiry is less than current one\n\n        For more information see https://redis.io/commands/pexpire\n        \"\"\"\n        if isinstance(time, datetime.timedelta):\n            time = int(time.total_seconds() * 1000)\n\n        exp_option = list()\n        if nx:\n            exp_option.append(\"NX\")\n        if xx:\n            exp_option.append(\"XX\")\n        if gt:\n            exp_option.append(\"GT\")\n        if lt:\n            exp_option.append(\"LT\")\n        return self.execute_command(\"PEXPIRE\", name, time, *exp_option)\n\n    def pexpireat(\n        self,\n        name: KeyT,\n        when: AbsExpiryT,\n        nx: bool = False,\n        xx: bool = False,\n        gt: bool = False,\n        lt: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Set an expire flag on key ``name`` with given ``option``. ``when``\n        can be represented as an integer representing unix time in\n        milliseconds (unix time * 1000) or a Python datetime object.\n\n        Valid options are:\n            NX -> Set expiry only when the key has no expiry\n            XX -> Set expiry only when the key has an existing expiry\n            GT -> Set expiry only when the new expiry is greater than current one\n            LT -> Set expiry only when the new expiry is less than current one\n\n        For more information see https://redis.io/commands/pexpireat\n        \"\"\"\n        if isinstance(when, datetime.datetime):\n            when = int(when.timestamp() * 1000)\n        exp_option = list()\n        if nx:\n            exp_option.append(\"NX\")\n        if xx:\n            exp_option.append(\"XX\")\n        if gt:\n            exp_option.append(\"GT\")\n        if lt:\n            exp_option.append(\"LT\")\n        return self.execute_command(\"PEXPIREAT\", name, when, *exp_option)\n\n    def pexpiretime(self, key: str) -> int:\n        \"\"\"\n        Returns the absolute Unix timestamp (since January 1, 1970) in milliseconds\n        at which the given key will expire.\n\n        For more information see https://redis.io/commands/pexpiretime\n        \"\"\"\n        return self.execute_command(\"PEXPIRETIME\", key)\n\n    def psetex(self, name: KeyT, time_ms: ExpiryT, value: EncodableT):\n        \"\"\"\n        Set the value of key ``name`` to ``value`` that expires in ``time_ms``\n        milliseconds. ``time_ms`` can be represented by an integer or a Python\n        timedelta object\n\n        For more information see https://redis.io/commands/psetex\n        \"\"\"\n        if isinstance(time_ms, datetime.timedelta):\n            time_ms = int(time_ms.total_seconds() * 1000)\n        return self.execute_command(\"PSETEX\", name, time_ms, value)\n\n    def pttl(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Returns the number of milliseconds until the key ``name`` will expire\n\n        For more information see https://redis.io/commands/pttl\n        \"\"\"\n        return self.execute_command(\"PTTL\", name)\n\n    def hrandfield(\n        self, key: str, count: Optional[int] = None, withvalues: bool = False\n    ) -> ResponseT:\n        \"\"\"\n        Return a random field from the hash value stored at key.\n\n        count: if the argument is positive, return an array of distinct fields.\n        If called with a negative count, the behavior changes and the command\n        is allowed to return the same field multiple times. In this case,\n        the number of returned fields is the absolute value of the\n        specified count.\n        withvalues: The optional WITHVALUES modifier changes the reply so it\n        includes the respective values of the randomly selected hash fields.\n\n        For more information see https://redis.io/commands/hrandfield\n        \"\"\"\n        params = []\n        if count is not None:\n            params.append(count)\n        if withvalues:\n            params.append(\"WITHVALUES\")\n\n        return self.execute_command(\"HRANDFIELD\", key, *params)\n\n    def randomkey(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns the name of a random key\n\n        For more information see https://redis.io/commands/randomkey\n        \"\"\"\n        return self.execute_command(\"RANDOMKEY\", **kwargs)\n\n    def rename(self, src: KeyT, dst: KeyT) -> ResponseT:\n        \"\"\"\n        Rename key ``src`` to ``dst``\n\n        For more information see https://redis.io/commands/rename\n        \"\"\"\n        return self.execute_command(\"RENAME\", src, dst)\n\n    def renamenx(self, src: KeyT, dst: KeyT):\n        \"\"\"\n        Rename key ``src`` to ``dst`` if ``dst`` doesn't already exist\n\n        For more information see https://redis.io/commands/renamenx\n        \"\"\"\n        return self.execute_command(\"RENAMENX\", src, dst)\n\n    def restore(\n        self,\n        name: KeyT,\n        ttl: float,\n        value: EncodableT,\n        replace: bool = False,\n        absttl: bool = False,\n        idletime: Optional[int] = None,\n        frequency: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Create a key using the provided serialized value, previously obtained\n        using DUMP.\n\n        ``replace`` allows an existing key on ``name`` to be overridden. If\n        it's not specified an error is raised on collision.\n\n        ``absttl`` if True, specified ``ttl`` should represent an absolute Unix\n        timestamp in milliseconds in which the key will expire. (Redis 5.0 or\n        greater).\n\n        ``idletime`` Used for eviction, this is the number of seconds the\n        key must be idle, prior to execution.\n\n        ``frequency`` Used for eviction, this is the frequency counter of\n        the object stored at the key, prior to execution.\n\n        For more information see https://redis.io/commands/restore\n        \"\"\"\n        params = [name, ttl, value]\n        if replace:\n            params.append(\"REPLACE\")\n        if absttl:\n            params.append(\"ABSTTL\")\n        if idletime is not None:\n            params.append(\"IDLETIME\")\n            try:\n                params.append(int(idletime))\n            except ValueError:\n                raise DataError(\"idletimemust be an integer\")\n\n        if frequency is not None:\n            params.append(\"FREQ\")\n            try:\n                params.append(int(frequency))\n            except ValueError:\n                raise DataError(\"frequency must be an integer\")\n\n        return self.execute_command(\"RESTORE\", *params)\n\n    def set(\n        self,\n        name: KeyT,\n        value: EncodableT,\n        ex: Optional[ExpiryT] = None,\n        px: Optional[ExpiryT] = None,\n        nx: bool = False,\n        xx: bool = False,\n        keepttl: bool = False,\n        get: bool = False,\n        exat: Optional[AbsExpiryT] = None,\n        pxat: Optional[AbsExpiryT] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Set the value at key ``name`` to ``value``\n\n        ``ex`` sets an expire flag on key ``name`` for ``ex`` seconds.\n\n        ``px`` sets an expire flag on key ``name`` for ``px`` milliseconds.\n\n        ``nx`` if set to True, set the value at key ``name`` to ``value`` only\n            if it does not exist.\n\n        ``xx`` if set to True, set the value at key ``name`` to ``value`` only\n            if it already exists.\n\n        ``keepttl`` if True, retain the time to live associated with the key.\n            (Available since Redis 6.0)\n\n        ``get`` if True, set the value at key ``name`` to ``value`` and return\n            the old value stored at key, or None if the key did not exist.\n            (Available since Redis 6.2)\n\n        ``exat`` sets an expire flag on key ``name`` for ``ex`` seconds,\n            specified in unix time.\n\n        ``pxat`` sets an expire flag on key ``name`` for ``ex`` milliseconds,\n            specified in unix time.\n\n        For more information see https://redis.io/commands/set\n        \"\"\"\n        opset = {ex, px, exat, pxat}\n        if len(opset) > 2 or len(opset) > 1 and keepttl:\n            raise DataError(\n                \"``ex``, ``px``, ``exat``, ``pxat``, \"\n                \"and ``keepttl`` are mutually exclusive.\"\n            )\n\n        if nx and xx:\n            raise DataError(\"``nx`` and ``xx`` are mutually exclusive.\")\n\n        pieces: list[EncodableT] = [name, value]\n        options = {}\n\n        pieces.extend(extract_expire_flags(ex, px, exat, pxat))\n\n        if keepttl:\n            pieces.append(\"KEEPTTL\")\n\n        if nx:\n            pieces.append(\"NX\")\n        if xx:\n            pieces.append(\"XX\")\n\n        if get:\n            pieces.append(\"GET\")\n            options[\"get\"] = True\n\n        return self.execute_command(\"SET\", *pieces, **options)\n\n    def __setitem__(self, name: KeyT, value: EncodableT):\n        self.set(name, value)\n\n    def setbit(self, name: KeyT, offset: int, value: int) -> ResponseT:\n        \"\"\"\n        Flag the ``offset`` in ``name`` as ``value``. Returns an integer\n        indicating the previous value of ``offset``.\n\n        For more information see https://redis.io/commands/setbit\n        \"\"\"\n        value = value and 1 or 0\n        return self.execute_command(\"SETBIT\", name, offset, value)\n\n    def setex(self, name: KeyT, time: ExpiryT, value: EncodableT) -> ResponseT:\n        \"\"\"\n        Set the value of key ``name`` to ``value`` that expires in ``time``\n        seconds. ``time`` can be represented by an integer or a Python\n        timedelta object.\n\n        For more information see https://redis.io/commands/setex\n        \"\"\"\n        if isinstance(time, datetime.timedelta):\n            time = int(time.total_seconds())\n        return self.execute_command(\"SETEX\", name, time, value)\n\n    def setnx(self, name: KeyT, value: EncodableT) -> ResponseT:\n        \"\"\"\n        Set the value of key ``name`` to ``value`` if key doesn't exist\n\n        For more information see https://redis.io/commands/setnx\n        \"\"\"\n        return self.execute_command(\"SETNX\", name, value)\n\n    def setrange(self, name: KeyT, offset: int, value: EncodableT) -> ResponseT:\n        \"\"\"\n        Overwrite bytes in the value of ``name`` starting at ``offset`` with\n        ``value``. If ``offset`` plus the length of ``value`` exceeds the\n        length of the original value, the new value will be larger than before.\n        If ``offset`` exceeds the length of the original value, null bytes\n        will be used to pad between the end of the previous value and the start\n        of what's being injected.\n\n        Returns the length of the new string.\n\n        For more information see https://redis.io/commands/setrange\n        \"\"\"\n        return self.execute_command(\"SETRANGE\", name, offset, value)\n\n    def stralgo(\n        self,\n        algo: Literal[\"LCS\"],\n        value1: KeyT,\n        value2: KeyT,\n        specific_argument: Union[Literal[\"strings\"], Literal[\"keys\"]] = \"strings\",\n        len: bool = False,\n        idx: bool = False,\n        minmatchlen: Optional[int] = None,\n        withmatchlen: bool = False,\n        **kwargs,\n    ) -> ResponseT:\n        \"\"\"\n        Implements complex algorithms that operate on strings.\n        Right now the only algorithm implemented is the LCS algorithm\n        (longest common substring). However new algorithms could be\n        implemented in the future.\n\n        ``algo`` Right now must be LCS\n        ``value1`` and ``value2`` Can be two strings or two keys\n        ``specific_argument`` Specifying if the arguments to the algorithm\n        will be keys or strings. strings is the default.\n        ``len`` Returns just the len of the match.\n        ``idx`` Returns the match positions in each string.\n        ``minmatchlen`` Restrict the list of matches to the ones of a given\n        minimal length. Can be provided only when ``idx`` set to True.\n        ``withmatchlen`` Returns the matches with the len of the match.\n        Can be provided only when ``idx`` set to True.\n\n        For more information see https://redis.io/commands/stralgo\n        \"\"\"\n        # check validity\n        supported_algo = [\"LCS\"]\n        if algo not in supported_algo:\n            supported_algos_str = \", \".join(supported_algo)\n            raise DataError(f\"The supported algorithms are: {supported_algos_str}\")\n        if specific_argument not in [\"keys\", \"strings\"]:\n            raise DataError(\"specific_argument can be only keys or strings\")\n        if len and idx:\n            raise DataError(\"len and idx cannot be provided together.\")\n\n        pieces: list[EncodableT] = [algo, specific_argument.upper(), value1, value2]\n        if len:\n            pieces.append(b\"LEN\")\n        if idx:\n            pieces.append(b\"IDX\")\n        try:\n            int(minmatchlen)\n            pieces.extend([b\"MINMATCHLEN\", minmatchlen])\n        except TypeError:\n            pass\n        if withmatchlen:\n            pieces.append(b\"WITHMATCHLEN\")\n\n        return self.execute_command(\n            \"STRALGO\",\n            *pieces,\n            len=len,\n            idx=idx,\n            minmatchlen=minmatchlen,\n            withmatchlen=withmatchlen,\n            **kwargs,\n        )\n\n    def strlen(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Return the number of bytes stored in the value of ``name``\n\n        For more information see https://redis.io/commands/strlen\n        \"\"\"\n        return self.execute_command(\"STRLEN\", name, keys=[name])\n\n    def substr(self, name: KeyT, start: int, end: int = -1) -> ResponseT:\n        \"\"\"\n        Return a substring of the string at key ``name``. ``start`` and ``end``\n        are 0-based integers specifying the portion of the string to return.\n        \"\"\"\n        return self.execute_command(\"SUBSTR\", name, start, end, keys=[name])\n\n    def touch(self, *args: KeyT) -> ResponseT:\n        \"\"\"\n        Alters the last access time of a key(s) ``*args``. A key is ignored\n        if it does not exist.\n\n        For more information see https://redis.io/commands/touch\n        \"\"\"\n        return self.execute_command(\"TOUCH\", *args)\n\n    def ttl(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Returns the number of seconds until the key ``name`` will expire\n\n        For more information see https://redis.io/commands/ttl\n        \"\"\"\n        return self.execute_command(\"TTL\", name)\n\n    def type(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Returns the type of key ``name``\n\n        For more information see https://redis.io/commands/type\n        \"\"\"\n        return self.execute_command(\"TYPE\", name, keys=[name])\n\n    def watch(self, *names: KeyT) -> None:\n        \"\"\"\n        Watches the values at keys ``names``, or None if the key doesn't exist\n\n        For more information see https://redis.io/commands/watch\n        \"\"\"\n        warnings.warn(DeprecationWarning(\"Call WATCH from a Pipeline object\"))\n\n    def unwatch(self) -> None:\n        \"\"\"\n        Unwatches all previously watched keys for a transaction\n\n        For more information see https://redis.io/commands/unwatch\n        \"\"\"\n        warnings.warn(DeprecationWarning(\"Call UNWATCH from a Pipeline object\"))\n\n    def unlink(self, *names: KeyT) -> ResponseT:\n        \"\"\"\n        Unlink one or more keys specified by ``names``\n\n        For more information see https://redis.io/commands/unlink\n        \"\"\"\n        return self.execute_command(\"UNLINK\", *names)\n\n    def lcs(\n        self,\n        key1: str,\n        key2: str,\n        len: Optional[bool] = False,\n        idx: Optional[bool] = False,\n        minmatchlen: Optional[int] = 0,\n        withmatchlen: Optional[bool] = False,\n    ) -> Union[str, int, list]:\n        \"\"\"\n        Find the longest common subsequence between ``key1`` and ``key2``.\n        If ``len`` is true the length of the match will will be returned.\n        If ``idx`` is true the match position in each strings will be returned.\n        ``minmatchlen`` restrict the list of matches to the ones of\n        the given ``minmatchlen``.\n        If ``withmatchlen`` the length of the match also will be returned.\n        For more information see https://redis.io/commands/lcs\n        \"\"\"\n        pieces = [key1, key2]\n        if len:\n            pieces.append(\"LEN\")\n        if idx:\n            pieces.append(\"IDX\")\n        if minmatchlen != 0:\n            pieces.extend([\"MINMATCHLEN\", minmatchlen])\n        if withmatchlen:\n            pieces.append(\"WITHMATCHLEN\")\n        return self.execute_command(\"LCS\", *pieces, keys=[key1, key2])\n\n\nclass AsyncBasicKeyCommands(BasicKeyCommands):\n    def __delitem__(self, name: KeyT):\n        raise TypeError(\"Async Redis client does not support class deletion\")\n\n    def __contains__(self, name: KeyT):\n        raise TypeError(\"Async Redis client does not support class inclusion\")\n\n    def __getitem__(self, name: KeyT):\n        raise TypeError(\"Async Redis client does not support class retrieval\")\n\n    def __setitem__(self, name: KeyT, value: EncodableT):\n        raise TypeError(\"Async Redis client does not support class assignment\")\n\n    async def watch(self, *names: KeyT) -> None:\n        return super().watch(*names)\n\n    async def unwatch(self) -> None:\n        return super().unwatch()\n\n\nclass ListCommands(CommandsProtocol):\n    \"\"\"\n    Redis commands for List data type.\n    see: https://redis.io/topics/data-types#lists\n    \"\"\"\n\n    def blpop(\n        self, keys: List, timeout: Optional[Number] = 0\n    ) -> Union[Awaitable[list], list]:\n        \"\"\"\n        LPOP a value off of the first non-empty list\n        named in the ``keys`` list.\n\n        If none of the lists in ``keys`` has a value to LPOP, then block\n        for ``timeout`` seconds, or until a value gets pushed on to one\n        of the lists.\n\n        If timeout is 0, then block indefinitely.\n\n        For more information see https://redis.io/commands/blpop\n        \"\"\"\n        if timeout is None:\n            timeout = 0\n        keys = list_or_args(keys, None)\n        keys.append(timeout)\n        return self.execute_command(\"BLPOP\", *keys)\n\n    def brpop(\n        self, keys: List, timeout: Optional[Number] = 0\n    ) -> Union[Awaitable[list], list]:\n        \"\"\"\n        RPOP a value off of the first non-empty list\n        named in the ``keys`` list.\n\n        If none of the lists in ``keys`` has a value to RPOP, then block\n        for ``timeout`` seconds, or until a value gets pushed on to one\n        of the lists.\n\n        If timeout is 0, then block indefinitely.\n\n        For more information see https://redis.io/commands/brpop\n        \"\"\"\n        if timeout is None:\n            timeout = 0\n        keys = list_or_args(keys, None)\n        keys.append(timeout)\n        return self.execute_command(\"BRPOP\", *keys)\n\n    def brpoplpush(\n        self, src: str, dst: str, timeout: Optional[Number] = 0\n    ) -> Union[Awaitable[Optional[str]], Optional[str]]:\n        \"\"\"\n        Pop a value off the tail of ``src``, push it on the head of ``dst``\n        and then return it.\n\n        This command blocks until a value is in ``src`` or until ``timeout``\n        seconds elapse, whichever is first. A ``timeout`` value of 0 blocks\n        forever.\n\n        For more information see https://redis.io/commands/brpoplpush\n        \"\"\"\n        if timeout is None:\n            timeout = 0\n        return self.execute_command(\"BRPOPLPUSH\", src, dst, timeout)\n\n    def blmpop(\n        self,\n        timeout: float,\n        numkeys: int,\n        *args: List[str],\n        direction: str,\n        count: Optional[int] = 1,\n    ) -> Optional[list]:\n        \"\"\"\n        Pop ``count`` values (default 1) from first non-empty in the list\n        of provided key names.\n\n        When all lists are empty this command blocks the connection until another\n        client pushes to it or until the timeout, timeout of 0 blocks indefinitely\n\n        For more information see https://redis.io/commands/blmpop\n        \"\"\"\n        args = [timeout, numkeys, *args, direction, \"COUNT\", count]\n\n        return self.execute_command(\"BLMPOP\", *args)\n\n    def lmpop(\n        self,\n        num_keys: int,\n        *args: List[str],\n        direction: str,\n        count: Optional[int] = 1,\n    ) -> Union[Awaitable[list], list]:\n        \"\"\"\n        Pop ``count`` values (default 1) first non-empty list key from the list\n        of args provided key names.\n\n        For more information see https://redis.io/commands/lmpop\n        \"\"\"\n        args = [num_keys] + list(args) + [direction]\n        if count != 1:\n            args.extend([\"COUNT\", count])\n\n        return self.execute_command(\"LMPOP\", *args)\n\n    def lindex(\n        self, name: str, index: int\n    ) -> Union[Awaitable[Optional[str]], Optional[str]]:\n        \"\"\"\n        Return the item from list ``name`` at position ``index``\n\n        Negative indexes are supported and will return an item at the\n        end of the list\n\n        For more information see https://redis.io/commands/lindex\n        \"\"\"\n        return self.execute_command(\"LINDEX\", name, index, keys=[name])\n\n    def linsert(\n        self, name: str, where: str, refvalue: str, value: str\n    ) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Insert ``value`` in list ``name`` either immediately before or after\n        [``where``] ``refvalue``\n\n        Returns the new length of the list on success or -1 if ``refvalue``\n        is not in the list.\n\n        For more information see https://redis.io/commands/linsert\n        \"\"\"\n        return self.execute_command(\"LINSERT\", name, where, refvalue, value)\n\n    def llen(self, name: str) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Return the length of the list ``name``\n\n        For more information see https://redis.io/commands/llen\n        \"\"\"\n        return self.execute_command(\"LLEN\", name, keys=[name])\n\n    def lpop(\n        self,\n        name: str,\n        count: Optional[int] = None,\n    ) -> Union[Awaitable[Union[str, List, None]], Union[str, List, None]]:\n        \"\"\"\n        Removes and returns the first elements of the list ``name``.\n\n        By default, the command pops a single element from the beginning of\n        the list. When provided with the optional ``count`` argument, the reply\n        will consist of up to count elements, depending on the list's length.\n\n        For more information see https://redis.io/commands/lpop\n        \"\"\"\n        if count is not None:\n            return self.execute_command(\"LPOP\", name, count)\n        else:\n            return self.execute_command(\"LPOP\", name)\n\n    def lpush(self, name: str, *values: FieldT) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Push ``values`` onto the head of the list ``name``\n\n        For more information see https://redis.io/commands/lpush\n        \"\"\"\n        return self.execute_command(\"LPUSH\", name, *values)\n\n    def lpushx(self, name: str, *values: FieldT) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Push ``value`` onto the head of the list ``name`` if ``name`` exists\n\n        For more information see https://redis.io/commands/lpushx\n        \"\"\"\n        return self.execute_command(\"LPUSHX\", name, *values)\n\n    def lrange(self, name: str, start: int, end: int) -> Union[Awaitable[list], list]:\n        \"\"\"\n        Return a slice of the list ``name`` between\n        position ``start`` and ``end``\n\n        ``start`` and ``end`` can be negative numbers just like\n        Python slicing notation\n\n        For more information see https://redis.io/commands/lrange\n        \"\"\"\n        return self.execute_command(\"LRANGE\", name, start, end, keys=[name])\n\n    def lrem(self, name: str, count: int, value: str) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Remove the first ``count`` occurrences of elements equal to ``value``\n        from the list stored at ``name``.\n\n        The count argument influences the operation in the following ways:\n            count > 0: Remove elements equal to value moving from head to tail.\n            count < 0: Remove elements equal to value moving from tail to head.\n            count = 0: Remove all elements equal to value.\n\n            For more information see https://redis.io/commands/lrem\n        \"\"\"\n        return self.execute_command(\"LREM\", name, count, value)\n\n    def lset(self, name: str, index: int, value: str) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Set element at ``index`` of list ``name`` to ``value``\n\n        For more information see https://redis.io/commands/lset\n        \"\"\"\n        return self.execute_command(\"LSET\", name, index, value)\n\n    def ltrim(self, name: str, start: int, end: int) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Trim the list ``name``, removing all values not within the slice\n        between ``start`` and ``end``\n\n        ``start`` and ``end`` can be negative numbers just like\n        Python slicing notation\n\n        For more information see https://redis.io/commands/ltrim\n        \"\"\"\n        return self.execute_command(\"LTRIM\", name, start, end)\n\n    def rpop(\n        self,\n        name: str,\n        count: Optional[int] = None,\n    ) -> Union[Awaitable[Union[str, List, None]], Union[str, List, None]]:\n        \"\"\"\n        Removes and returns the last elements of the list ``name``.\n\n        By default, the command pops a single element from the end of the list.\n        When provided with the optional ``count`` argument, the reply will\n        consist of up to count elements, depending on the list's length.\n\n        For more information see https://redis.io/commands/rpop\n        \"\"\"\n        if count is not None:\n            return self.execute_command(\"RPOP\", name, count)\n        else:\n            return self.execute_command(\"RPOP\", name)\n\n    def rpoplpush(self, src: str, dst: str) -> Union[Awaitable[str], str]:\n        \"\"\"\n        RPOP a value off of the ``src`` list and atomically LPUSH it\n        on to the ``dst`` list.  Returns the value.\n\n        For more information see https://redis.io/commands/rpoplpush\n        \"\"\"\n        return self.execute_command(\"RPOPLPUSH\", src, dst)\n\n    def rpush(self, name: str, *values: FieldT) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Push ``values`` onto the tail of the list ``name``\n\n        For more information see https://redis.io/commands/rpush\n        \"\"\"\n        return self.execute_command(\"RPUSH\", name, *values)\n\n    def rpushx(self, name: str, *values: str) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Push ``value`` onto the tail of the list ``name`` if ``name`` exists\n\n        For more information see https://redis.io/commands/rpushx\n        \"\"\"\n        return self.execute_command(\"RPUSHX\", name, *values)\n\n    def lpos(\n        self,\n        name: str,\n        value: str,\n        rank: Optional[int] = None,\n        count: Optional[int] = None,\n        maxlen: Optional[int] = None,\n    ) -> Union[str, List, None]:\n        \"\"\"\n        Get position of ``value`` within the list ``name``\n\n         If specified, ``rank`` indicates the \"rank\" of the first element to\n         return in case there are multiple copies of ``value`` in the list.\n         By default, LPOS returns the position of the first occurrence of\n         ``value`` in the list. When ``rank`` 2, LPOS returns the position of\n         the second ``value`` in the list. If ``rank`` is negative, LPOS\n         searches the list in reverse. For example, -1 would return the\n         position of the last occurrence of ``value`` and -2 would return the\n         position of the next to last occurrence of ``value``.\n\n         If specified, ``count`` indicates that LPOS should return a list of\n         up to ``count`` positions. A ``count`` of 2 would return a list of\n         up to 2 positions. A ``count`` of 0 returns a list of all positions\n         matching ``value``. When ``count`` is specified and but ``value``\n         does not exist in the list, an empty list is returned.\n\n         If specified, ``maxlen`` indicates the maximum number of list\n         elements to scan. A ``maxlen`` of 1000 will only return the\n         position(s) of items within the first 1000 entries in the list.\n         A ``maxlen`` of 0 (the default) will scan the entire list.\n\n         For more information see https://redis.io/commands/lpos\n        \"\"\"\n        pieces: list[EncodableT] = [name, value]\n        if rank is not None:\n            pieces.extend([\"RANK\", rank])\n\n        if count is not None:\n            pieces.extend([\"COUNT\", count])\n\n        if maxlen is not None:\n            pieces.extend([\"MAXLEN\", maxlen])\n\n        return self.execute_command(\"LPOS\", *pieces, keys=[name])\n\n    def sort(\n        self,\n        name: str,\n        start: Optional[int] = None,\n        num: Optional[int] = None,\n        by: Optional[str] = None,\n        get: Optional[List[str]] = None,\n        desc: bool = False,\n        alpha: bool = False,\n        store: Optional[str] = None,\n        groups: Optional[bool] = False,\n    ) -> Union[List, int]:\n        \"\"\"\n        Sort and return the list, set or sorted set at ``name``.\n\n        ``start`` and ``num`` allow for paging through the sorted data\n\n        ``by`` allows using an external key to weight and sort the items.\n            Use an \"*\" to indicate where in the key the item value is located\n\n        ``get`` allows for returning items from external keys rather than the\n            sorted data itself.  Use an \"*\" to indicate where in the key\n            the item value is located\n\n        ``desc`` allows for reversing the sort\n\n        ``alpha`` allows for sorting lexicographically rather than numerically\n\n        ``store`` allows for storing the result of the sort into\n            the key ``store``\n\n        ``groups`` if set to True and if ``get`` contains at least two\n            elements, sort will return a list of tuples, each containing the\n            values fetched from the arguments to ``get``.\n\n        For more information see https://redis.io/commands/sort\n        \"\"\"\n        if (start is not None and num is None) or (num is not None and start is None):\n            raise DataError(\"``start`` and ``num`` must both be specified\")\n\n        pieces: list[EncodableT] = [name]\n        if by is not None:\n            pieces.extend([b\"BY\", by])\n        if start is not None and num is not None:\n            pieces.extend([b\"LIMIT\", start, num])\n        if get is not None:\n            # If get is a string assume we want to get a single value.\n            # Otherwise assume it's an interable and we want to get multiple\n            # values. We can't just iterate blindly because strings are\n            # iterable.\n            if isinstance(get, (bytes, str)):\n                pieces.extend([b\"GET\", get])\n            else:\n                for g in get:\n                    pieces.extend([b\"GET\", g])\n        if desc:\n            pieces.append(b\"DESC\")\n        if alpha:\n            pieces.append(b\"ALPHA\")\n        if store is not None:\n            pieces.extend([b\"STORE\", store])\n        if groups:\n            if not get or isinstance(get, (bytes, str)) or len(get) < 2:\n                raise DataError(\n                    'when using \"groups\" the \"get\" argument '\n                    \"must be specified and contain at least \"\n                    \"two keys\"\n                )\n\n        options = {\"groups\": len(get) if groups else None}\n        options[\"keys\"] = [name]\n        return self.execute_command(\"SORT\", *pieces, **options)\n\n    def sort_ro(\n        self,\n        key: str,\n        start: Optional[int] = None,\n        num: Optional[int] = None,\n        by: Optional[str] = None,\n        get: Optional[List[str]] = None,\n        desc: bool = False,\n        alpha: bool = False,\n    ) -> list:\n        \"\"\"\n        Returns the elements contained in the list, set or sorted set at key.\n        (read-only variant of the SORT command)\n\n        ``start`` and ``num`` allow for paging through the sorted data\n\n        ``by`` allows using an external key to weight and sort the items.\n            Use an \"*\" to indicate where in the key the item value is located\n\n        ``get`` allows for returning items from external keys rather than the\n            sorted data itself.  Use an \"*\" to indicate where in the key\n            the item value is located\n\n        ``desc`` allows for reversing the sort\n\n        ``alpha`` allows for sorting lexicographically rather than numerically\n\n        For more information see https://redis.io/commands/sort_ro\n        \"\"\"\n        return self.sort(\n            key, start=start, num=num, by=by, get=get, desc=desc, alpha=alpha\n        )\n\n\nAsyncListCommands = ListCommands\n\n\nclass ScanCommands(CommandsProtocol):\n    \"\"\"\n    Redis SCAN commands.\n    see: https://redis.io/commands/scan\n    \"\"\"\n\n    def scan(\n        self,\n        cursor: int = 0,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n        _type: Optional[str] = None,\n        **kwargs,\n    ) -> ResponseT:\n        \"\"\"\n        Incrementally return lists of key names. Also return a cursor\n        indicating the scan position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` provides a hint to Redis about the number of keys to\n            return per batch.\n\n        ``_type`` filters the returned values by a particular Redis type.\n            Stock Redis instances allow for the following types:\n            HASH, LIST, SET, STREAM, STRING, ZSET\n            Additionally, Redis modules can expose other types as well.\n\n        For more information see https://redis.io/commands/scan\n        \"\"\"\n        pieces: list[EncodableT] = [cursor]\n        if match is not None:\n            pieces.extend([b\"MATCH\", match])\n        if count is not None:\n            pieces.extend([b\"COUNT\", count])\n        if _type is not None:\n            pieces.extend([b\"TYPE\", _type])\n        return self.execute_command(\"SCAN\", *pieces, **kwargs)\n\n    def scan_iter(\n        self,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n        _type: Optional[str] = None,\n        **kwargs,\n    ) -> Iterator:\n        \"\"\"\n        Make an iterator using the SCAN command so that the client doesn't\n        need to remember the cursor position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` provides a hint to Redis about the number of keys to\n            return per batch.\n\n        ``_type`` filters the returned values by a particular Redis type.\n            Stock Redis instances allow for the following types:\n            HASH, LIST, SET, STREAM, STRING, ZSET\n            Additionally, Redis modules can expose other types as well.\n        \"\"\"\n        cursor = \"0\"\n        while cursor != 0:\n            cursor, data = self.scan(\n                cursor=cursor, match=match, count=count, _type=_type, **kwargs\n            )\n            yield from data\n\n    def sscan(\n        self,\n        name: KeyT,\n        cursor: int = 0,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Incrementally return lists of elements in a set. Also return a cursor\n        indicating the scan position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` allows for hint the minimum number of returns\n\n        For more information see https://redis.io/commands/sscan\n        \"\"\"\n        pieces: list[EncodableT] = [name, cursor]\n        if match is not None:\n            pieces.extend([b\"MATCH\", match])\n        if count is not None:\n            pieces.extend([b\"COUNT\", count])\n        return self.execute_command(\"SSCAN\", *pieces)\n\n    def sscan_iter(\n        self,\n        name: KeyT,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n    ) -> Iterator:\n        \"\"\"\n        Make an iterator using the SSCAN command so that the client doesn't\n        need to remember the cursor position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` allows for hint the minimum number of returns\n        \"\"\"\n        cursor = \"0\"\n        while cursor != 0:\n            cursor, data = self.sscan(name, cursor=cursor, match=match, count=count)\n            yield from data\n\n    def hscan(\n        self,\n        name: KeyT,\n        cursor: int = 0,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n        no_values: Union[bool, None] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Incrementally return key/value slices in a hash. Also return a cursor\n        indicating the scan position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` allows for hint the minimum number of returns\n\n        ``no_values`` indicates to return only the keys, without values.\n\n        For more information see https://redis.io/commands/hscan\n        \"\"\"\n        pieces: list[EncodableT] = [name, cursor]\n        if match is not None:\n            pieces.extend([b\"MATCH\", match])\n        if count is not None:\n            pieces.extend([b\"COUNT\", count])\n        if no_values is not None:\n            pieces.extend([b\"NOVALUES\"])\n        return self.execute_command(\"HSCAN\", *pieces, no_values=no_values)\n\n    def hscan_iter(\n        self,\n        name: str,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n        no_values: Union[bool, None] = None,\n    ) -> Iterator:\n        \"\"\"\n        Make an iterator using the HSCAN command so that the client doesn't\n        need to remember the cursor position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` allows for hint the minimum number of returns\n\n        ``no_values`` indicates to return only the keys, without values\n        \"\"\"\n        cursor = \"0\"\n        while cursor != 0:\n            cursor, data = self.hscan(\n                name, cursor=cursor, match=match, count=count, no_values=no_values\n            )\n            if no_values:\n                yield from data\n            else:\n                yield from data.items()\n\n    def zscan(\n        self,\n        name: KeyT,\n        cursor: int = 0,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n        score_cast_func: Union[type, Callable] = float,\n    ) -> ResponseT:\n        \"\"\"\n        Incrementally return lists of elements in a sorted set. Also return a\n        cursor indicating the scan position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` allows for hint the minimum number of returns\n\n        ``score_cast_func`` a callable used to cast the score return value\n\n        For more information see https://redis.io/commands/zscan\n        \"\"\"\n        pieces = [name, cursor]\n        if match is not None:\n            pieces.extend([b\"MATCH\", match])\n        if count is not None:\n            pieces.extend([b\"COUNT\", count])\n        options = {\"score_cast_func\": score_cast_func}\n        return self.execute_command(\"ZSCAN\", *pieces, **options)\n\n    def zscan_iter(\n        self,\n        name: KeyT,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n        score_cast_func: Union[type, Callable] = float,\n    ) -> Iterator:\n        \"\"\"\n        Make an iterator using the ZSCAN command so that the client doesn't\n        need to remember the cursor position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` allows for hint the minimum number of returns\n\n        ``score_cast_func`` a callable used to cast the score return value\n        \"\"\"\n        cursor = \"0\"\n        while cursor != 0:\n            cursor, data = self.zscan(\n                name,\n                cursor=cursor,\n                match=match,\n                count=count,\n                score_cast_func=score_cast_func,\n            )\n            yield from data\n\n\nclass AsyncScanCommands(ScanCommands):\n    async def scan_iter(\n        self,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n        _type: Optional[str] = None,\n        **kwargs,\n    ) -> AsyncIterator:\n        \"\"\"\n        Make an iterator using the SCAN command so that the client doesn't\n        need to remember the cursor position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` provides a hint to Redis about the number of keys to\n            return per batch.\n\n        ``_type`` filters the returned values by a particular Redis type.\n            Stock Redis instances allow for the following types:\n            HASH, LIST, SET, STREAM, STRING, ZSET\n            Additionally, Redis modules can expose other types as well.\n        \"\"\"\n        cursor = \"0\"\n        while cursor != 0:\n            cursor, data = await self.scan(\n                cursor=cursor, match=match, count=count, _type=_type, **kwargs\n            )\n            for d in data:\n                yield d\n\n    async def sscan_iter(\n        self,\n        name: KeyT,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n    ) -> AsyncIterator:\n        \"\"\"\n        Make an iterator using the SSCAN command so that the client doesn't\n        need to remember the cursor position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` allows for hint the minimum number of returns\n        \"\"\"\n        cursor = \"0\"\n        while cursor != 0:\n            cursor, data = await self.sscan(\n                name, cursor=cursor, match=match, count=count\n            )\n            for d in data:\n                yield d\n\n    async def hscan_iter(\n        self,\n        name: str,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n        no_values: Union[bool, None] = None,\n    ) -> AsyncIterator:\n        \"\"\"\n        Make an iterator using the HSCAN command so that the client doesn't\n        need to remember the cursor position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` allows for hint the minimum number of returns\n\n        ``no_values`` indicates to return only the keys, without values\n        \"\"\"\n        cursor = \"0\"\n        while cursor != 0:\n            cursor, data = await self.hscan(\n                name, cursor=cursor, match=match, count=count, no_values=no_values\n            )\n            if no_values:\n                for it in data:\n                    yield it\n            else:\n                for it in data.items():\n                    yield it\n\n    async def zscan_iter(\n        self,\n        name: KeyT,\n        match: Union[PatternT, None] = None,\n        count: Optional[int] = None,\n        score_cast_func: Union[type, Callable] = float,\n    ) -> AsyncIterator:\n        \"\"\"\n        Make an iterator using the ZSCAN command so that the client doesn't\n        need to remember the cursor position.\n\n        ``match`` allows for filtering the keys by pattern\n\n        ``count`` allows for hint the minimum number of returns\n\n        ``score_cast_func`` a callable used to cast the score return value\n        \"\"\"\n        cursor = \"0\"\n        while cursor != 0:\n            cursor, data = await self.zscan(\n                name,\n                cursor=cursor,\n                match=match,\n                count=count,\n                score_cast_func=score_cast_func,\n            )\n            for d in data:\n                yield d\n\n\nclass SetCommands(CommandsProtocol):\n    \"\"\"\n    Redis commands for Set data type.\n    see: https://redis.io/topics/data-types#sets\n    \"\"\"\n\n    def sadd(self, name: KeyT, *values: FieldT) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Add ``value(s)`` to set ``name``\n\n        For more information see https://redis.io/commands/sadd\n        \"\"\"\n        return self.execute_command(\"SADD\", name, *values)\n\n    def scard(self, name: KeyT) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Return the number of elements in set ``name``\n\n        For more information see https://redis.io/commands/scard\n        \"\"\"\n        return self.execute_command(\"SCARD\", name, keys=[name])\n\n    def sdiff(self, keys: List, *args: List) -> Union[Awaitable[list], list]:\n        \"\"\"\n        Return the difference of sets specified by ``keys``\n\n        For more information see https://redis.io/commands/sdiff\n        \"\"\"\n        args = list_or_args(keys, args)\n        return self.execute_command(\"SDIFF\", *args, keys=args)\n\n    def sdiffstore(\n        self, dest: str, keys: List, *args: List\n    ) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Store the difference of sets specified by ``keys`` into a new\n        set named ``dest``.  Returns the number of keys in the new set.\n\n        For more information see https://redis.io/commands/sdiffstore\n        \"\"\"\n        args = list_or_args(keys, args)\n        return self.execute_command(\"SDIFFSTORE\", dest, *args)\n\n    def sinter(self, keys: List, *args: List) -> Union[Awaitable[list], list]:\n        \"\"\"\n        Return the intersection of sets specified by ``keys``\n\n        For more information see https://redis.io/commands/sinter\n        \"\"\"\n        args = list_or_args(keys, args)\n        return self.execute_command(\"SINTER\", *args, keys=args)\n\n    def sintercard(\n        self, numkeys: int, keys: List[KeyT], limit: int = 0\n    ) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Return the cardinality of the intersect of multiple sets specified by ``keys``.\n\n        When LIMIT provided (defaults to 0 and means unlimited), if the intersection\n        cardinality reaches limit partway through the computation, the algorithm will\n        exit and yield limit as the cardinality\n\n        For more information see https://redis.io/commands/sintercard\n        \"\"\"\n        args = [numkeys, *keys, \"LIMIT\", limit]\n        return self.execute_command(\"SINTERCARD\", *args, keys=keys)\n\n    def sinterstore(\n        self, dest: KeyT, keys: List, *args: List\n    ) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Store the intersection of sets specified by ``keys`` into a new\n        set named ``dest``.  Returns the number of keys in the new set.\n\n        For more information see https://redis.io/commands/sinterstore\n        \"\"\"\n        args = list_or_args(keys, args)\n        return self.execute_command(\"SINTERSTORE\", dest, *args)\n\n    def sismember(\n        self, name: KeyT, value: str\n    ) -> Union[Awaitable[Union[Literal[0], Literal[1]]], Union[Literal[0], Literal[1]]]:\n        \"\"\"\n        Return whether ``value`` is a member of set ``name``:\n        - 1 if the value is a member of the set.\n        - 0 if the value is not a member of the set or if key does not exist.\n\n        For more information see https://redis.io/commands/sismember\n        \"\"\"\n        return self.execute_command(\"SISMEMBER\", name, value, keys=[name])\n\n    def smembers(self, name: KeyT) -> Union[Awaitable[Set], Set]:\n        \"\"\"\n        Return all members of the set ``name``\n\n        For more information see https://redis.io/commands/smembers\n        \"\"\"\n        return self.execute_command(\"SMEMBERS\", name, keys=[name])\n\n    def smismember(\n        self, name: KeyT, values: List, *args: List\n    ) -> Union[\n        Awaitable[List[Union[Literal[0], Literal[1]]]],\n        List[Union[Literal[0], Literal[1]]],\n    ]:\n        \"\"\"\n        Return whether each value in ``values`` is a member of the set ``name``\n        as a list of ``int`` in the order of ``values``:\n        - 1 if the value is a member of the set.\n        - 0 if the value is not a member of the set or if key does not exist.\n\n        For more information see https://redis.io/commands/smismember\n        \"\"\"\n        args = list_or_args(values, args)\n        return self.execute_command(\"SMISMEMBER\", name, *args, keys=[name])\n\n    def smove(self, src: KeyT, dst: KeyT, value: str) -> Union[Awaitable[bool], bool]:\n        \"\"\"\n        Move ``value`` from set ``src`` to set ``dst`` atomically\n\n        For more information see https://redis.io/commands/smove\n        \"\"\"\n        return self.execute_command(\"SMOVE\", src, dst, value)\n\n    def spop(self, name: KeyT, count: Optional[int] = None) -> Union[str, List, None]:\n        \"\"\"\n        Remove and return a random member of set ``name``\n\n        For more information see https://redis.io/commands/spop\n        \"\"\"\n        args = (count is not None) and [count] or []\n        return self.execute_command(\"SPOP\", name, *args)\n\n    def srandmember(\n        self, name: KeyT, number: Optional[int] = None\n    ) -> Union[str, List, None]:\n        \"\"\"\n        If ``number`` is None, returns a random member of set ``name``.\n\n        If ``number`` is supplied, returns a list of ``number`` random\n        members of set ``name``. Note this is only available when running\n        Redis 2.6+.\n\n        For more information see https://redis.io/commands/srandmember\n        \"\"\"\n        args = (number is not None) and [number] or []\n        return self.execute_command(\"SRANDMEMBER\", name, *args)\n\n    def srem(self, name: KeyT, *values: FieldT) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Remove ``values`` from set ``name``\n\n        For more information see https://redis.io/commands/srem\n        \"\"\"\n        return self.execute_command(\"SREM\", name, *values)\n\n    def sunion(self, keys: List, *args: List) -> Union[Awaitable[List], List]:\n        \"\"\"\n        Return the union of sets specified by ``keys``\n\n        For more information see https://redis.io/commands/sunion\n        \"\"\"\n        args = list_or_args(keys, args)\n        return self.execute_command(\"SUNION\", *args, keys=args)\n\n    def sunionstore(\n        self, dest: KeyT, keys: List, *args: List\n    ) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Store the union of sets specified by ``keys`` into a new\n        set named ``dest``.  Returns the number of keys in the new set.\n\n        For more information see https://redis.io/commands/sunionstore\n        \"\"\"\n        args = list_or_args(keys, args)\n        return self.execute_command(\"SUNIONSTORE\", dest, *args)\n\n\nAsyncSetCommands = SetCommands\n\n\nclass StreamCommands(CommandsProtocol):\n    \"\"\"\n    Redis commands for Stream data type.\n    see: https://redis.io/topics/streams-intro\n    \"\"\"\n\n    def xack(self, name: KeyT, groupname: GroupT, *ids: StreamIdT) -> ResponseT:\n        \"\"\"\n        Acknowledges the successful processing of one or more messages.\n\n        Args:\n            name: name of the stream.\n            groupname: name of the consumer group.\n            *ids: message ids to acknowledge.\n\n        For more information see https://redis.io/commands/xack\n        \"\"\"\n        return self.execute_command(\"XACK\", name, groupname, *ids)\n\n    def xackdel(\n        self,\n        name: KeyT,\n        groupname: GroupT,\n        *ids: StreamIdT,\n        ref_policy: Literal[\"KEEPREF\", \"DELREF\", \"ACKED\"] = \"KEEPREF\",\n    ) -> ResponseT:\n        \"\"\"\n        Combines the functionality of XACK and XDEL. Acknowledges the specified\n        message IDs in the given consumer group and simultaneously attempts to\n        delete the corresponding entries from the stream.\n        \"\"\"\n        if not ids:\n            raise DataError(\"XACKDEL requires at least one message ID\")\n\n        if ref_policy not in {\"KEEPREF\", \"DELREF\", \"ACKED\"}:\n            raise DataError(\"XACKDEL ref_policy must be one of: KEEPREF, DELREF, ACKED\")\n\n        pieces = [name, groupname, ref_policy, \"IDS\", len(ids)]\n        pieces.extend(ids)\n        return self.execute_command(\"XACKDEL\", *pieces)\n\n    def xadd(\n        self,\n        name: KeyT,\n        fields: Dict[FieldT, EncodableT],\n        id: StreamIdT = \"*\",\n        maxlen: Optional[int] = None,\n        approximate: bool = True,\n        nomkstream: bool = False,\n        minid: Union[StreamIdT, None] = None,\n        limit: Optional[int] = None,\n        ref_policy: Optional[Literal[\"KEEPREF\", \"DELREF\", \"ACKED\"]] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Add to a stream.\n        name: name of the stream\n        fields: dict of field/value pairs to insert into the stream\n        id: Location to insert this record. By default it is appended.\n        maxlen: truncate old stream members beyond this size.\n        Can't be specified with minid.\n        approximate: actual stream length may be slightly more than maxlen\n        nomkstream: When set to true, do not make a stream\n        minid: the minimum id in the stream to query.\n        Can't be specified with maxlen.\n        limit: specifies the maximum number of entries to retrieve\n        ref_policy: optional reference policy for consumer groups when trimming:\n            - KEEPREF (default): When trimming, preserves references in consumer groups' PEL\n            - DELREF: When trimming, removes all references from consumer groups' PEL\n            - ACKED: When trimming, only removes entries acknowledged by all consumer groups\n\n        For more information see https://redis.io/commands/xadd\n        \"\"\"\n        pieces: list[EncodableT] = []\n        if maxlen is not None and minid is not None:\n            raise DataError(\"Only one of ```maxlen``` or ```minid``` may be specified\")\n\n        if ref_policy is not None and ref_policy not in {\"KEEPREF\", \"DELREF\", \"ACKED\"}:\n            raise DataError(\"XADD ref_policy must be one of: KEEPREF, DELREF, ACKED\")\n\n        if maxlen is not None:\n            if not isinstance(maxlen, int) or maxlen < 0:\n                raise DataError(\"XADD maxlen must be non-negative integer\")\n            pieces.append(b\"MAXLEN\")\n            if approximate:\n                pieces.append(b\"~\")\n            pieces.append(str(maxlen))\n        if minid is not None:\n            pieces.append(b\"MINID\")\n            if approximate:\n                pieces.append(b\"~\")\n            pieces.append(minid)\n        if limit is not None:\n            pieces.extend([b\"LIMIT\", limit])\n        if nomkstream:\n            pieces.append(b\"NOMKSTREAM\")\n        if ref_policy is not None:\n            pieces.append(ref_policy)\n        pieces.append(id)\n        if not isinstance(fields, dict) or len(fields) == 0:\n            raise DataError(\"XADD fields must be a non-empty dict\")\n        for pair in fields.items():\n            pieces.extend(pair)\n        return self.execute_command(\"XADD\", name, *pieces)\n\n    def xautoclaim(\n        self,\n        name: KeyT,\n        groupname: GroupT,\n        consumername: ConsumerT,\n        min_idle_time: int,\n        start_id: StreamIdT = \"0-0\",\n        count: Optional[int] = None,\n        justid: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Transfers ownership of pending stream entries that match the specified\n        criteria. Conceptually, equivalent to calling XPENDING and then XCLAIM,\n        but provides a more straightforward way to deal with message delivery\n        failures via SCAN-like semantics.\n        name: name of the stream.\n        groupname: name of the consumer group.\n        consumername: name of a consumer that claims the message.\n        min_idle_time: filter messages that were idle less than this amount of\n        milliseconds.\n        start_id: filter messages with equal or greater ID.\n        count: optional integer, upper limit of the number of entries that the\n        command attempts to claim. Set to 100 by default.\n        justid: optional boolean, false by default. Return just an array of IDs\n        of messages successfully claimed, without returning the actual message\n\n        For more information see https://redis.io/commands/xautoclaim\n        \"\"\"\n        try:\n            if int(min_idle_time) < 0:\n                raise DataError(\n                    \"XAUTOCLAIM min_idle_time must be a nonnegative integer\"\n                )\n        except TypeError:\n            pass\n\n        kwargs = {}\n        pieces = [name, groupname, consumername, min_idle_time, start_id]\n\n        try:\n            if int(count) < 0:\n                raise DataError(\"XPENDING count must be a integer >= 0\")\n            pieces.extend([b\"COUNT\", count])\n        except TypeError:\n            pass\n        if justid:\n            pieces.append(b\"JUSTID\")\n            kwargs[\"parse_justid\"] = True\n\n        return self.execute_command(\"XAUTOCLAIM\", *pieces, **kwargs)\n\n    def xclaim(\n        self,\n        name: KeyT,\n        groupname: GroupT,\n        consumername: ConsumerT,\n        min_idle_time: int,\n        message_ids: Union[List[StreamIdT], Tuple[StreamIdT]],\n        idle: Optional[int] = None,\n        time: Optional[int] = None,\n        retrycount: Optional[int] = None,\n        force: bool = False,\n        justid: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Changes the ownership of a pending message.\n\n        name: name of the stream.\n\n        groupname: name of the consumer group.\n\n        consumername: name of a consumer that claims the message.\n\n        min_idle_time: filter messages that were idle less than this amount of\n        milliseconds\n\n        message_ids: non-empty list or tuple of message IDs to claim\n\n        idle: optional. Set the idle time (last time it was delivered) of the\n        message in ms\n\n        time: optional integer. This is the same as idle but instead of a\n        relative amount of milliseconds, it sets the idle time to a specific\n        Unix time (in milliseconds).\n\n        retrycount: optional integer. set the retry counter to the specified\n        value. This counter is incremented every time a message is delivered\n        again.\n\n        force: optional boolean, false by default. Creates the pending message\n        entry in the PEL even if certain specified IDs are not already in the\n        PEL assigned to a different client.\n\n        justid: optional boolean, false by default. Return just an array of IDs\n        of messages successfully claimed, without returning the actual message\n\n        For more information see https://redis.io/commands/xclaim\n        \"\"\"\n        if not isinstance(min_idle_time, int) or min_idle_time < 0:\n            raise DataError(\"XCLAIM min_idle_time must be a non negative integer\")\n        if not isinstance(message_ids, (list, tuple)) or not message_ids:\n            raise DataError(\n                \"XCLAIM message_ids must be a non empty list or \"\n                \"tuple of message IDs to claim\"\n            )\n\n        kwargs = {}\n        pieces: list[EncodableT] = [name, groupname, consumername, str(min_idle_time)]\n        pieces.extend(list(message_ids))\n\n        if idle is not None:\n            if not isinstance(idle, int):\n                raise DataError(\"XCLAIM idle must be an integer\")\n            pieces.extend((b\"IDLE\", str(idle)))\n        if time is not None:\n            if not isinstance(time, int):\n                raise DataError(\"XCLAIM time must be an integer\")\n            pieces.extend((b\"TIME\", str(time)))\n        if retrycount is not None:\n            if not isinstance(retrycount, int):\n                raise DataError(\"XCLAIM retrycount must be an integer\")\n            pieces.extend((b\"RETRYCOUNT\", str(retrycount)))\n\n        if force:\n            if not isinstance(force, bool):\n                raise DataError(\"XCLAIM force must be a boolean\")\n            pieces.append(b\"FORCE\")\n        if justid:\n            if not isinstance(justid, bool):\n                raise DataError(\"XCLAIM justid must be a boolean\")\n            pieces.append(b\"JUSTID\")\n            kwargs[\"parse_justid\"] = True\n        return self.execute_command(\"XCLAIM\", *pieces, **kwargs)\n\n    def xdel(self, name: KeyT, *ids: StreamIdT) -> ResponseT:\n        \"\"\"\n        Deletes one or more messages from a stream.\n\n        Args:\n            name: name of the stream.\n            *ids: message ids to delete.\n\n        For more information see https://redis.io/commands/xdel\n        \"\"\"\n        return self.execute_command(\"XDEL\", name, *ids)\n\n    def xdelex(\n        self,\n        name: KeyT,\n        *ids: StreamIdT,\n        ref_policy: Literal[\"KEEPREF\", \"DELREF\", \"ACKED\"] = \"KEEPREF\",\n    ) -> ResponseT:\n        \"\"\"\n        Extended version of XDEL that provides more control over how message entries\n        are deleted concerning consumer groups.\n        \"\"\"\n        if not ids:\n            raise DataError(\"XDELEX requires at least one message ID\")\n\n        if ref_policy not in {\"KEEPREF\", \"DELREF\", \"ACKED\"}:\n            raise DataError(\"XDELEX ref_policy must be one of: KEEPREF, DELREF, ACKED\")\n\n        pieces = [name, ref_policy, \"IDS\", len(ids)]\n        pieces.extend(ids)\n        return self.execute_command(\"XDELEX\", *pieces)\n\n    def xgroup_create(\n        self,\n        name: KeyT,\n        groupname: GroupT,\n        id: StreamIdT = \"$\",\n        mkstream: bool = False,\n        entries_read: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Create a new consumer group associated with a stream.\n        name: name of the stream.\n        groupname: name of the consumer group.\n        id: ID of the last item in the stream to consider already delivered.\n\n        For more information see https://redis.io/commands/xgroup-create\n        \"\"\"\n        pieces: list[EncodableT] = [\"XGROUP CREATE\", name, groupname, id]\n        if mkstream:\n            pieces.append(b\"MKSTREAM\")\n        if entries_read is not None:\n            pieces.extend([\"ENTRIESREAD\", entries_read])\n\n        return self.execute_command(*pieces)\n\n    def xgroup_delconsumer(\n        self, name: KeyT, groupname: GroupT, consumername: ConsumerT\n    ) -> ResponseT:\n        \"\"\"\n        Remove a specific consumer from a consumer group.\n        Returns the number of pending messages that the consumer had before it\n        was deleted.\n        name: name of the stream.\n        groupname: name of the consumer group.\n        consumername: name of consumer to delete\n\n        For more information see https://redis.io/commands/xgroup-delconsumer\n        \"\"\"\n        return self.execute_command(\"XGROUP DELCONSUMER\", name, groupname, consumername)\n\n    def xgroup_destroy(self, name: KeyT, groupname: GroupT) -> ResponseT:\n        \"\"\"\n        Destroy a consumer group.\n        name: name of the stream.\n        groupname: name of the consumer group.\n\n        For more information see https://redis.io/commands/xgroup-destroy\n        \"\"\"\n        return self.execute_command(\"XGROUP DESTROY\", name, groupname)\n\n    def xgroup_createconsumer(\n        self, name: KeyT, groupname: GroupT, consumername: ConsumerT\n    ) -> ResponseT:\n        \"\"\"\n        Consumers in a consumer group are auto-created every time a new\n        consumer name is mentioned by some command.\n        They can be explicitly created by using this command.\n        name: name of the stream.\n        groupname: name of the consumer group.\n        consumername: name of consumer to create.\n\n        See: https://redis.io/commands/xgroup-createconsumer\n        \"\"\"\n        return self.execute_command(\n            \"XGROUP CREATECONSUMER\", name, groupname, consumername\n        )\n\n    def xgroup_setid(\n        self,\n        name: KeyT,\n        groupname: GroupT,\n        id: StreamIdT,\n        entries_read: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Set the consumer group last delivered ID to something else.\n        name: name of the stream.\n        groupname: name of the consumer group.\n        id: ID of the last item in the stream to consider already delivered.\n\n        For more information see https://redis.io/commands/xgroup-setid\n        \"\"\"\n        pieces = [name, groupname, id]\n        if entries_read is not None:\n            pieces.extend([\"ENTRIESREAD\", entries_read])\n        return self.execute_command(\"XGROUP SETID\", *pieces)\n\n    def xinfo_consumers(self, name: KeyT, groupname: GroupT) -> ResponseT:\n        \"\"\"\n        Returns general information about the consumers in the group.\n        name: name of the stream.\n        groupname: name of the consumer group.\n\n        For more information see https://redis.io/commands/xinfo-consumers\n        \"\"\"\n        return self.execute_command(\"XINFO CONSUMERS\", name, groupname)\n\n    def xinfo_groups(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Returns general information about the consumer groups of the stream.\n        name: name of the stream.\n\n        For more information see https://redis.io/commands/xinfo-groups\n        \"\"\"\n        return self.execute_command(\"XINFO GROUPS\", name)\n\n    def xinfo_stream(self, name: KeyT, full: bool = False) -> ResponseT:\n        \"\"\"\n        Returns general information about the stream.\n        name: name of the stream.\n        full: optional boolean, false by default. Return full summary\n\n        For more information see https://redis.io/commands/xinfo-stream\n        \"\"\"\n        pieces = [name]\n        options = {}\n        if full:\n            pieces.append(b\"FULL\")\n            options = {\"full\": full}\n        return self.execute_command(\"XINFO STREAM\", *pieces, **options)\n\n    def xlen(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Returns the number of elements in a given stream.\n\n        For more information see https://redis.io/commands/xlen\n        \"\"\"\n        return self.execute_command(\"XLEN\", name, keys=[name])\n\n    def xpending(self, name: KeyT, groupname: GroupT) -> ResponseT:\n        \"\"\"\n        Returns information about pending messages of a group.\n        name: name of the stream.\n        groupname: name of the consumer group.\n\n        For more information see https://redis.io/commands/xpending\n        \"\"\"\n        return self.execute_command(\"XPENDING\", name, groupname, keys=[name])\n\n    def xpending_range(\n        self,\n        name: KeyT,\n        groupname: GroupT,\n        min: StreamIdT,\n        max: StreamIdT,\n        count: int,\n        consumername: Union[ConsumerT, None] = None,\n        idle: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Returns information about pending messages, in a range.\n\n        name: name of the stream.\n        groupname: name of the consumer group.\n        idle: available from  version 6.2. filter entries by their\n        idle-time, given in milliseconds (optional).\n        min: minimum stream ID.\n        max: maximum stream ID.\n        count: number of messages to return\n        consumername: name of a consumer to filter by (optional).\n        \"\"\"\n        if {min, max, count} == {None}:\n            if idle is not None or consumername is not None:\n                raise DataError(\n                    \"if XPENDING is provided with idle time\"\n                    \" or consumername, it must be provided\"\n                    \" with min, max and count parameters\"\n                )\n            return self.xpending(name, groupname)\n\n        pieces = [name, groupname]\n        if min is None or max is None or count is None:\n            raise DataError(\n                \"XPENDING must be provided with min, max \"\n                \"and count parameters, or none of them.\"\n            )\n        # idle\n        try:\n            if int(idle) < 0:\n                raise DataError(\"XPENDING idle must be a integer >= 0\")\n            pieces.extend([\"IDLE\", idle])\n        except TypeError:\n            pass\n        # count\n        try:\n            if int(count) < 0:\n                raise DataError(\"XPENDING count must be a integer >= 0\")\n            pieces.extend([min, max, count])\n        except TypeError:\n            pass\n        # consumername\n        if consumername:\n            pieces.append(consumername)\n\n        return self.execute_command(\"XPENDING\", *pieces, parse_detail=True)\n\n    def xrange(\n        self,\n        name: KeyT,\n        min: StreamIdT = \"-\",\n        max: StreamIdT = \"+\",\n        count: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Read stream values within an interval.\n\n        name: name of the stream.\n\n        start: first stream ID. defaults to '-',\n               meaning the earliest available.\n\n        finish: last stream ID. defaults to '+',\n                meaning the latest available.\n\n        count: if set, only return this many items, beginning with the\n               earliest available.\n\n        For more information see https://redis.io/commands/xrange\n        \"\"\"\n        pieces = [min, max]\n        if count is not None:\n            if not isinstance(count, int) or count < 1:\n                raise DataError(\"XRANGE count must be a positive integer\")\n            pieces.append(b\"COUNT\")\n            pieces.append(str(count))\n\n        return self.execute_command(\"XRANGE\", name, *pieces, keys=[name])\n\n    def xread(\n        self,\n        streams: Dict[KeyT, StreamIdT],\n        count: Optional[int] = None,\n        block: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Block and monitor multiple streams for new data.\n\n        streams: a dict of stream names to stream IDs, where\n                   IDs indicate the last ID already seen.\n\n        count: if set, only return this many items, beginning with the\n               earliest available.\n\n        block: number of milliseconds to wait, if nothing already present.\n\n        For more information see https://redis.io/commands/xread\n        \"\"\"\n        pieces = []\n        if block is not None:\n            if not isinstance(block, int) or block < 0:\n                raise DataError(\"XREAD block must be a non-negative integer\")\n            pieces.append(b\"BLOCK\")\n            pieces.append(str(block))\n        if count is not None:\n            if not isinstance(count, int) or count < 1:\n                raise DataError(\"XREAD count must be a positive integer\")\n            pieces.append(b\"COUNT\")\n            pieces.append(str(count))\n        if not isinstance(streams, dict) or len(streams) == 0:\n            raise DataError(\"XREAD streams must be a non empty dict\")\n        pieces.append(b\"STREAMS\")\n        keys, values = zip(*streams.items())\n        pieces.extend(keys)\n        pieces.extend(values)\n        return self.execute_command(\"XREAD\", *pieces, keys=keys)\n\n    def xreadgroup(\n        self,\n        groupname: str,\n        consumername: str,\n        streams: Dict[KeyT, StreamIdT],\n        count: Optional[int] = None,\n        block: Optional[int] = None,\n        noack: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Read from a stream via a consumer group.\n\n        groupname: name of the consumer group.\n\n        consumername: name of the requesting consumer.\n\n        streams: a dict of stream names to stream IDs, where\n               IDs indicate the last ID already seen.\n\n        count: if set, only return this many items, beginning with the\n               earliest available.\n\n        block: number of milliseconds to wait, if nothing already present.\n        noack: do not add messages to the PEL\n\n        For more information see https://redis.io/commands/xreadgroup\n        \"\"\"\n        pieces: list[EncodableT] = [b\"GROUP\", groupname, consumername]\n        if count is not None:\n            if not isinstance(count, int) or count < 1:\n                raise DataError(\"XREADGROUP count must be a positive integer\")\n            pieces.append(b\"COUNT\")\n            pieces.append(str(count))\n        if block is not None:\n            if not isinstance(block, int) or block < 0:\n                raise DataError(\"XREADGROUP block must be a non-negative integer\")\n            pieces.append(b\"BLOCK\")\n            pieces.append(str(block))\n        if noack:\n            pieces.append(b\"NOACK\")\n        if not isinstance(streams, dict) or len(streams) == 0:\n            raise DataError(\"XREADGROUP streams must be a non empty dict\")\n        pieces.append(b\"STREAMS\")\n        pieces.extend(streams.keys())\n        pieces.extend(streams.values())\n        return self.execute_command(\"XREADGROUP\", *pieces)\n\n    def xrevrange(\n        self,\n        name: KeyT,\n        max: StreamIdT = \"+\",\n        min: StreamIdT = \"-\",\n        count: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Read stream values within an interval, in reverse order.\n\n        name: name of the stream\n\n        start: first stream ID. defaults to '+',\n               meaning the latest available.\n\n        finish: last stream ID. defaults to '-',\n                meaning the earliest available.\n\n        count: if set, only return this many items, beginning with the\n               latest available.\n\n        For more information see https://redis.io/commands/xrevrange\n        \"\"\"\n        pieces: list[EncodableT] = [max, min]\n        if count is not None:\n            if not isinstance(count, int) or count < 1:\n                raise DataError(\"XREVRANGE count must be a positive integer\")\n            pieces.append(b\"COUNT\")\n            pieces.append(str(count))\n\n        return self.execute_command(\"XREVRANGE\", name, *pieces, keys=[name])\n\n    def xtrim(\n        self,\n        name: KeyT,\n        maxlen: Optional[int] = None,\n        approximate: bool = True,\n        minid: Union[StreamIdT, None] = None,\n        limit: Optional[int] = None,\n        ref_policy: Optional[Literal[\"KEEPREF\", \"DELREF\", \"ACKED\"]] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Trims old messages from a stream.\n        name: name of the stream.\n        maxlen: truncate old stream messages beyond this size\n        Can't be specified with minid.\n        approximate: actual stream length may be slightly more than maxlen\n        minid: the minimum id in the stream to query\n        Can't be specified with maxlen.\n        limit: specifies the maximum number of entries to retrieve\n        ref_policy: optional reference policy for consumer groups:\n            - KEEPREF (default): Trims entries but preserves references in consumer groups' PEL\n            - DELREF: Trims entries and removes all references from consumer groups' PEL\n            - ACKED: Only trims entries that were read and acknowledged by all consumer groups\n\n        For more information see https://redis.io/commands/xtrim\n        \"\"\"\n        pieces: list[EncodableT] = []\n        if maxlen is not None and minid is not None:\n            raise DataError(\"Only one of ``maxlen`` or ``minid`` may be specified\")\n\n        if maxlen is None and minid is None:\n            raise DataError(\"One of ``maxlen`` or ``minid`` must be specified\")\n\n        if ref_policy is not None and ref_policy not in {\"KEEPREF\", \"DELREF\", \"ACKED\"}:\n            raise DataError(\"XTRIM ref_policy must be one of: KEEPREF, DELREF, ACKED\")\n\n        if maxlen is not None:\n            pieces.append(b\"MAXLEN\")\n        if minid is not None:\n            pieces.append(b\"MINID\")\n        if approximate:\n            pieces.append(b\"~\")\n        if maxlen is not None:\n            pieces.append(maxlen)\n        if minid is not None:\n            pieces.append(minid)\n        if limit is not None:\n            pieces.append(b\"LIMIT\")\n            pieces.append(limit)\n        if ref_policy is not None:\n            pieces.append(ref_policy)\n\n        return self.execute_command(\"XTRIM\", name, *pieces)\n\n\nAsyncStreamCommands = StreamCommands\n\n\nclass SortedSetCommands(CommandsProtocol):\n    \"\"\"\n    Redis commands for Sorted Sets data type.\n    see: https://redis.io/topics/data-types-intro#redis-sorted-sets\n    \"\"\"\n\n    def zadd(\n        self,\n        name: KeyT,\n        mapping: Mapping[AnyKeyT, EncodableT],\n        nx: bool = False,\n        xx: bool = False,\n        ch: bool = False,\n        incr: bool = False,\n        gt: bool = False,\n        lt: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Set any number of element-name, score pairs to the key ``name``. Pairs\n        are specified as a dict of element-names keys to score values.\n\n        ``nx`` forces ZADD to only create new elements and not to update\n        scores for elements that already exist.\n\n        ``xx`` forces ZADD to only update scores of elements that already\n        exist. New elements will not be added.\n\n        ``ch`` modifies the return value to be the numbers of elements changed.\n        Changed elements include new elements that were added and elements\n        whose scores changed.\n\n        ``incr`` modifies ZADD to behave like ZINCRBY. In this mode only a\n        single element/score pair can be specified and the score is the amount\n        the existing score will be incremented by. When using this mode the\n        return value of ZADD will be the new score of the element.\n\n        ``LT`` Only update existing elements if the new score is less than\n        the current score. This flag doesn't prevent adding new elements.\n\n        ``GT`` Only update existing elements if the new score is greater than\n        the current score. This flag doesn't prevent adding new elements.\n\n        The return value of ZADD varies based on the mode specified. With no\n        options, ZADD returns the number of new elements added to the sorted\n        set.\n\n        ``NX``, ``LT``, and ``GT`` are mutually exclusive options.\n\n        See: https://redis.io/commands/ZADD\n        \"\"\"\n        if not mapping:\n            raise DataError(\"ZADD requires at least one element/score pair\")\n        if nx and xx:\n            raise DataError(\"ZADD allows either 'nx' or 'xx', not both\")\n        if gt and lt:\n            raise DataError(\"ZADD allows either 'gt' or 'lt', not both\")\n        if incr and len(mapping) != 1:\n            raise DataError(\n                \"ZADD option 'incr' only works when passing a single element/score pair\"\n            )\n        if nx and (gt or lt):\n            raise DataError(\"Only one of 'nx', 'lt', or 'gr' may be defined.\")\n\n        pieces: list[EncodableT] = []\n        options = {}\n        if nx:\n            pieces.append(b\"NX\")\n        if xx:\n            pieces.append(b\"XX\")\n        if ch:\n            pieces.append(b\"CH\")\n        if incr:\n            pieces.append(b\"INCR\")\n            options[\"as_score\"] = True\n        if gt:\n            pieces.append(b\"GT\")\n        if lt:\n            pieces.append(b\"LT\")\n        for pair in mapping.items():\n            pieces.append(pair[1])\n            pieces.append(pair[0])\n        return self.execute_command(\"ZADD\", name, *pieces, **options)\n\n    def zcard(self, name: KeyT) -> ResponseT:\n        \"\"\"\n        Return the number of elements in the sorted set ``name``\n\n        For more information see https://redis.io/commands/zcard\n        \"\"\"\n        return self.execute_command(\"ZCARD\", name, keys=[name])\n\n    def zcount(self, name: KeyT, min: ZScoreBoundT, max: ZScoreBoundT) -> ResponseT:\n        \"\"\"\n        Returns the number of elements in the sorted set at key ``name`` with\n        a score between ``min`` and ``max``.\n\n        For more information see https://redis.io/commands/zcount\n        \"\"\"\n        return self.execute_command(\"ZCOUNT\", name, min, max, keys=[name])\n\n    def zdiff(self, keys: KeysT, withscores: bool = False) -> ResponseT:\n        \"\"\"\n        Returns the difference between the first and all successive input\n        sorted sets provided in ``keys``.\n\n        For more information see https://redis.io/commands/zdiff\n        \"\"\"\n        pieces = [len(keys), *keys]\n        if withscores:\n            pieces.append(\"WITHSCORES\")\n        return self.execute_command(\"ZDIFF\", *pieces, keys=keys)\n\n    def zdiffstore(self, dest: KeyT, keys: KeysT) -> ResponseT:\n        \"\"\"\n        Computes the difference between the first and all successive input\n        sorted sets provided in ``keys`` and stores the result in ``dest``.\n\n        For more information see https://redis.io/commands/zdiffstore\n        \"\"\"\n        pieces = [len(keys), *keys]\n        return self.execute_command(\"ZDIFFSTORE\", dest, *pieces)\n\n    def zincrby(self, name: KeyT, amount: float, value: EncodableT) -> ResponseT:\n        \"\"\"\n        Increment the score of ``value`` in sorted set ``name`` by ``amount``\n\n        For more information see https://redis.io/commands/zincrby\n        \"\"\"\n        return self.execute_command(\"ZINCRBY\", name, amount, value)\n\n    def zinter(\n        self, keys: KeysT, aggregate: Optional[str] = None, withscores: bool = False\n    ) -> ResponseT:\n        \"\"\"\n        Return the intersect of multiple sorted sets specified by ``keys``.\n        With the ``aggregate`` option, it is possible to specify how the\n        results of the union are aggregated. This option defaults to SUM,\n        where the score of an element is summed across the inputs where it\n        exists. When this option is set to either MIN or MAX, the resulting\n        set will contain the minimum or maximum score of an element across\n        the inputs where it exists.\n\n        For more information see https://redis.io/commands/zinter\n        \"\"\"\n        return self._zaggregate(\"ZINTER\", None, keys, aggregate, withscores=withscores)\n\n    def zinterstore(\n        self,\n        dest: KeyT,\n        keys: Union[Sequence[KeyT], Mapping[AnyKeyT, float]],\n        aggregate: Optional[str] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Intersect multiple sorted sets specified by ``keys`` into a new\n        sorted set, ``dest``. Scores in the destination will be aggregated\n        based on the ``aggregate``. This option defaults to SUM, where the\n        score of an element is summed across the inputs where it exists.\n        When this option is set to either MIN or MAX, the resulting set will\n        contain the minimum or maximum score of an element across the inputs\n        where it exists.\n\n        For more information see https://redis.io/commands/zinterstore\n        \"\"\"\n        return self._zaggregate(\"ZINTERSTORE\", dest, keys, aggregate)\n\n    def zintercard(\n        self, numkeys: int, keys: List[str], limit: int = 0\n    ) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Return the cardinality of the intersect of multiple sorted sets\n        specified by ``keys``.\n        When LIMIT provided (defaults to 0 and means unlimited), if the intersection\n        cardinality reaches limit partway through the computation, the algorithm will\n        exit and yield limit as the cardinality\n\n        For more information see https://redis.io/commands/zintercard\n        \"\"\"\n        args = [numkeys, *keys, \"LIMIT\", limit]\n        return self.execute_command(\"ZINTERCARD\", *args, keys=keys)\n\n    def zlexcount(self, name, min, max):\n        \"\"\"\n        Return the number of items in the sorted set ``name`` between the\n        lexicographical range ``min`` and ``max``.\n\n        For more information see https://redis.io/commands/zlexcount\n        \"\"\"\n        return self.execute_command(\"ZLEXCOUNT\", name, min, max, keys=[name])\n\n    def zpopmax(self, name: KeyT, count: Optional[int] = None) -> ResponseT:\n        \"\"\"\n        Remove and return up to ``count`` members with the highest scores\n        from the sorted set ``name``.\n\n        For more information see https://redis.io/commands/zpopmax\n        \"\"\"\n        args = (count is not None) and [count] or []\n        options = {\"withscores\": True}\n        return self.execute_command(\"ZPOPMAX\", name, *args, **options)\n\n    def zpopmin(self, name: KeyT, count: Optional[int] = None) -> ResponseT:\n        \"\"\"\n        Remove and return up to ``count`` members with the lowest scores\n        from the sorted set ``name``.\n\n        For more information see https://redis.io/commands/zpopmin\n        \"\"\"\n        args = (count is not None) and [count] or []\n        options = {\"withscores\": True}\n        return self.execute_command(\"ZPOPMIN\", name, *args, **options)\n\n    def zrandmember(\n        self, key: KeyT, count: Optional[int] = None, withscores: bool = False\n    ) -> ResponseT:\n        \"\"\"\n        Return a random element from the sorted set value stored at key.\n\n        ``count`` if the argument is positive, return an array of distinct\n        fields. If called with a negative count, the behavior changes and\n        the command is allowed to return the same field multiple times.\n        In this case, the number of returned fields is the absolute value\n        of the specified count.\n\n        ``withscores`` The optional WITHSCORES modifier changes the reply so it\n        includes the respective scores of the randomly selected elements from\n        the sorted set.\n\n        For more information see https://redis.io/commands/zrandmember\n        \"\"\"\n        params = []\n        if count is not None:\n            params.append(count)\n        if withscores:\n            params.append(\"WITHSCORES\")\n\n        return self.execute_command(\"ZRANDMEMBER\", key, *params)\n\n    def bzpopmax(self, keys: KeysT, timeout: TimeoutSecT = 0) -> ResponseT:\n        \"\"\"\n        ZPOPMAX a value off of the first non-empty sorted set\n        named in the ``keys`` list.\n\n        If none of the sorted sets in ``keys`` has a value to ZPOPMAX,\n        then block for ``timeout`` seconds, or until a member gets added\n        to one of the sorted sets.\n\n        If timeout is 0, then block indefinitely.\n\n        For more information see https://redis.io/commands/bzpopmax\n        \"\"\"\n        if timeout is None:\n            timeout = 0\n        keys = list_or_args(keys, None)\n        keys.append(timeout)\n        return self.execute_command(\"BZPOPMAX\", *keys)\n\n    def bzpopmin(self, keys: KeysT, timeout: TimeoutSecT = 0) -> ResponseT:\n        \"\"\"\n        ZPOPMIN a value off of the first non-empty sorted set\n        named in the ``keys`` list.\n\n        If none of the sorted sets in ``keys`` has a value to ZPOPMIN,\n        then block for ``timeout`` seconds, or until a member gets added\n        to one of the sorted sets.\n\n        If timeout is 0, then block indefinitely.\n\n        For more information see https://redis.io/commands/bzpopmin\n        \"\"\"\n        if timeout is None:\n            timeout = 0\n        keys: list[EncodableT] = list_or_args(keys, None)\n        keys.append(timeout)\n        return self.execute_command(\"BZPOPMIN\", *keys)\n\n    def zmpop(\n        self,\n        num_keys: int,\n        keys: List[str],\n        min: Optional[bool] = False,\n        max: Optional[bool] = False,\n        count: Optional[int] = 1,\n    ) -> Union[Awaitable[list], list]:\n        \"\"\"\n        Pop ``count`` values (default 1) off of the first non-empty sorted set\n        named in the ``keys`` list.\n        For more information see https://redis.io/commands/zmpop\n        \"\"\"\n        args = [num_keys] + keys\n        if (min and max) or (not min and not max):\n            raise DataError\n        elif min:\n            args.append(\"MIN\")\n        else:\n            args.append(\"MAX\")\n        if count != 1:\n            args.extend([\"COUNT\", count])\n\n        return self.execute_command(\"ZMPOP\", *args)\n\n    def bzmpop(\n        self,\n        timeout: float,\n        numkeys: int,\n        keys: List[str],\n        min: Optional[bool] = False,\n        max: Optional[bool] = False,\n        count: Optional[int] = 1,\n    ) -> Optional[list]:\n        \"\"\"\n        Pop ``count`` values (default 1) off of the first non-empty sorted set\n        named in the ``keys`` list.\n\n        If none of the sorted sets in ``keys`` has a value to pop,\n        then block for ``timeout`` seconds, or until a member gets added\n        to one of the sorted sets.\n\n        If timeout is 0, then block indefinitely.\n\n        For more information see https://redis.io/commands/bzmpop\n        \"\"\"\n        args = [timeout, numkeys, *keys]\n        if (min and max) or (not min and not max):\n            raise DataError(\"Either min or max, but not both must be set\")\n        elif min:\n            args.append(\"MIN\")\n        else:\n            args.append(\"MAX\")\n        args.extend([\"COUNT\", count])\n\n        return self.execute_command(\"BZMPOP\", *args)\n\n    def _zrange(\n        self,\n        command,\n        dest: Union[KeyT, None],\n        name: KeyT,\n        start: int,\n        end: int,\n        desc: bool = False,\n        byscore: bool = False,\n        bylex: bool = False,\n        withscores: bool = False,\n        score_cast_func: Union[type, Callable, None] = float,\n        offset: Optional[int] = None,\n        num: Optional[int] = None,\n    ) -> ResponseT:\n        if byscore and bylex:\n            raise DataError(\"``byscore`` and ``bylex`` can not be specified together.\")\n        if (offset is not None and num is None) or (num is not None and offset is None):\n            raise DataError(\"``offset`` and ``num`` must both be specified.\")\n        if bylex and withscores:\n            raise DataError(\n                \"``withscores`` not supported in combination with ``bylex``.\"\n            )\n        pieces = [command]\n        if dest:\n            pieces.append(dest)\n        pieces.extend([name, start, end])\n        if byscore:\n            pieces.append(\"BYSCORE\")\n        if bylex:\n            pieces.append(\"BYLEX\")\n        if desc:\n            pieces.append(\"REV\")\n        if offset is not None and num is not None:\n            pieces.extend([\"LIMIT\", offset, num])\n        if withscores:\n            pieces.append(\"WITHSCORES\")\n        options = {\"withscores\": withscores, \"score_cast_func\": score_cast_func}\n        options[\"keys\"] = [name]\n        return self.execute_command(*pieces, **options)\n\n    def zrange(\n        self,\n        name: KeyT,\n        start: int,\n        end: int,\n        desc: bool = False,\n        withscores: bool = False,\n        score_cast_func: Union[type, Callable] = float,\n        byscore: bool = False,\n        bylex: bool = False,\n        offset: Optional[int] = None,\n        num: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Return a range of values from sorted set ``name`` between\n        ``start`` and ``end`` sorted in ascending order.\n\n        ``start`` and ``end`` can be negative, indicating the end of the range.\n\n        ``desc`` a boolean indicating whether to sort the results in reversed\n        order.\n\n        ``withscores`` indicates to return the scores along with the values.\n        The return type is a list of (value, score) pairs.\n\n        ``score_cast_func`` a callable used to cast the score return value.\n\n        ``byscore`` when set to True, returns the range of elements from the\n        sorted set having scores equal or between ``start`` and ``end``.\n\n        ``bylex`` when set to True, returns the range of elements from the\n        sorted set between the ``start`` and ``end`` lexicographical closed\n        range intervals.\n        Valid ``start`` and ``end`` must start with ( or [, in order to specify\n        whether the range interval is exclusive or inclusive, respectively.\n\n        ``offset`` and ``num`` are specified, then return a slice of the range.\n        Can't be provided when using ``bylex``.\n\n        For more information see https://redis.io/commands/zrange\n        \"\"\"\n        # Need to support ``desc`` also when using old redis version\n        # because it was supported in 3.5.3 (of redis-py)\n        if not byscore and not bylex and (offset is None and num is None) and desc:\n            return self.zrevrange(name, start, end, withscores, score_cast_func)\n\n        return self._zrange(\n            \"ZRANGE\",\n            None,\n            name,\n            start,\n            end,\n            desc,\n            byscore,\n            bylex,\n            withscores,\n            score_cast_func,\n            offset,\n            num,\n        )\n\n    def zrevrange(\n        self,\n        name: KeyT,\n        start: int,\n        end: int,\n        withscores: bool = False,\n        score_cast_func: Union[type, Callable] = float,\n    ) -> ResponseT:\n        \"\"\"\n        Return a range of values from sorted set ``name`` between\n        ``start`` and ``end`` sorted in descending order.\n\n        ``start`` and ``end`` can be negative, indicating the end of the range.\n\n        ``withscores`` indicates to return the scores along with the values\n        The return type is a list of (value, score) pairs\n\n        ``score_cast_func`` a callable used to cast the score return value\n\n        For more information see https://redis.io/commands/zrevrange\n        \"\"\"\n        pieces = [\"ZREVRANGE\", name, start, end]\n        if withscores:\n            pieces.append(b\"WITHSCORES\")\n        options = {\"withscores\": withscores, \"score_cast_func\": score_cast_func}\n        options[\"keys\"] = name\n        return self.execute_command(*pieces, **options)\n\n    def zrangestore(\n        self,\n        dest: KeyT,\n        name: KeyT,\n        start: int,\n        end: int,\n        byscore: bool = False,\n        bylex: bool = False,\n        desc: bool = False,\n        offset: Optional[int] = None,\n        num: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Stores in ``dest`` the result of a range of values from sorted set\n        ``name`` between ``start`` and ``end`` sorted in ascending order.\n\n        ``start`` and ``end`` can be negative, indicating the end of the range.\n\n        ``byscore`` when set to True, returns the range of elements from the\n        sorted set having scores equal or between ``start`` and ``end``.\n\n        ``bylex`` when set to True, returns the range of elements from the\n        sorted set between the ``start`` and ``end`` lexicographical closed\n        range intervals.\n        Valid ``start`` and ``end`` must start with ( or [, in order to specify\n        whether the range interval is exclusive or inclusive, respectively.\n\n        ``desc`` a boolean indicating whether to sort the results in reversed\n        order.\n\n        ``offset`` and ``num`` are specified, then return a slice of the range.\n        Can't be provided when using ``bylex``.\n\n        For more information see https://redis.io/commands/zrangestore\n        \"\"\"\n        return self._zrange(\n            \"ZRANGESTORE\",\n            dest,\n            name,\n            start,\n            end,\n            desc,\n            byscore,\n            bylex,\n            False,\n            None,\n            offset,\n            num,\n        )\n\n    def zrangebylex(\n        self,\n        name: KeyT,\n        min: EncodableT,\n        max: EncodableT,\n        start: Optional[int] = None,\n        num: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Return the lexicographical range of values from sorted set ``name``\n        between ``min`` and ``max``.\n\n        If ``start`` and ``num`` are specified, then return a slice of the\n        range.\n\n        For more information see https://redis.io/commands/zrangebylex\n        \"\"\"\n        if (start is not None and num is None) or (num is not None and start is None):\n            raise DataError(\"``start`` and ``num`` must both be specified\")\n        pieces = [\"ZRANGEBYLEX\", name, min, max]\n        if start is not None and num is not None:\n            pieces.extend([b\"LIMIT\", start, num])\n        return self.execute_command(*pieces, keys=[name])\n\n    def zrevrangebylex(\n        self,\n        name: KeyT,\n        max: EncodableT,\n        min: EncodableT,\n        start: Optional[int] = None,\n        num: Optional[int] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Return the reversed lexicographical range of values from sorted set\n        ``name`` between ``max`` and ``min``.\n\n        If ``start`` and ``num`` are specified, then return a slice of the\n        range.\n\n        For more information see https://redis.io/commands/zrevrangebylex\n        \"\"\"\n        if (start is not None and num is None) or (num is not None and start is None):\n            raise DataError(\"``start`` and ``num`` must both be specified\")\n        pieces = [\"ZREVRANGEBYLEX\", name, max, min]\n        if start is not None and num is not None:\n            pieces.extend([\"LIMIT\", start, num])\n        return self.execute_command(*pieces, keys=[name])\n\n    def zrangebyscore(\n        self,\n        name: KeyT,\n        min: ZScoreBoundT,\n        max: ZScoreBoundT,\n        start: Optional[int] = None,\n        num: Optional[int] = None,\n        withscores: bool = False,\n        score_cast_func: Union[type, Callable] = float,\n    ) -> ResponseT:\n        \"\"\"\n        Return a range of values from the sorted set ``name`` with scores\n        between ``min`` and ``max``.\n\n        If ``start`` and ``num`` are specified, then return a slice\n        of the range.\n\n        ``withscores`` indicates to return the scores along with the values.\n        The return type is a list of (value, score) pairs\n\n        `score_cast_func`` a callable used to cast the score return value\n\n        For more information see https://redis.io/commands/zrangebyscore\n        \"\"\"\n        if (start is not None and num is None) or (num is not None and start is None):\n            raise DataError(\"``start`` and ``num`` must both be specified\")\n        pieces = [\"ZRANGEBYSCORE\", name, min, max]\n        if start is not None and num is not None:\n            pieces.extend([\"LIMIT\", start, num])\n        if withscores:\n            pieces.append(\"WITHSCORES\")\n        options = {\"withscores\": withscores, \"score_cast_func\": score_cast_func}\n        options[\"keys\"] = [name]\n        return self.execute_command(*pieces, **options)\n\n    def zrevrangebyscore(\n        self,\n        name: KeyT,\n        max: ZScoreBoundT,\n        min: ZScoreBoundT,\n        start: Optional[int] = None,\n        num: Optional[int] = None,\n        withscores: bool = False,\n        score_cast_func: Union[type, Callable] = float,\n    ):\n        \"\"\"\n        Return a range of values from the sorted set ``name`` with scores\n        between ``min`` and ``max`` in descending order.\n\n        If ``start`` and ``num`` are specified, then return a slice\n        of the range.\n\n        ``withscores`` indicates to return the scores along with the values.\n        The return type is a list of (value, score) pairs\n\n        ``score_cast_func`` a callable used to cast the score return value\n\n        For more information see https://redis.io/commands/zrevrangebyscore\n        \"\"\"\n        if (start is not None and num is None) or (num is not None and start is None):\n            raise DataError(\"``start`` and ``num`` must both be specified\")\n        pieces = [\"ZREVRANGEBYSCORE\", name, max, min]\n        if start is not None and num is not None:\n            pieces.extend([\"LIMIT\", start, num])\n        if withscores:\n            pieces.append(\"WITHSCORES\")\n        options = {\"withscores\": withscores, \"score_cast_func\": score_cast_func}\n        options[\"keys\"] = [name]\n        return self.execute_command(*pieces, **options)\n\n    def zrank(\n        self,\n        name: KeyT,\n        value: EncodableT,\n        withscore: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Returns a 0-based value indicating the rank of ``value`` in sorted set\n        ``name``.\n        The optional WITHSCORE argument supplements the command's\n        reply with the score of the element returned.\n\n        For more information see https://redis.io/commands/zrank\n        \"\"\"\n        if withscore:\n            return self.execute_command(\"ZRANK\", name, value, \"WITHSCORE\", keys=[name])\n        return self.execute_command(\"ZRANK\", name, value, keys=[name])\n\n    def zrem(self, name: KeyT, *values: FieldT) -> ResponseT:\n        \"\"\"\n        Remove member ``values`` from sorted set ``name``\n\n        For more information see https://redis.io/commands/zrem\n        \"\"\"\n        return self.execute_command(\"ZREM\", name, *values)\n\n    def zremrangebylex(self, name: KeyT, min: EncodableT, max: EncodableT) -> ResponseT:\n        \"\"\"\n        Remove all elements in the sorted set ``name`` between the\n        lexicographical range specified by ``min`` and ``max``.\n\n        Returns the number of elements removed.\n\n        For more information see https://redis.io/commands/zremrangebylex\n        \"\"\"\n        return self.execute_command(\"ZREMRANGEBYLEX\", name, min, max)\n\n    def zremrangebyrank(self, name: KeyT, min: int, max: int) -> ResponseT:\n        \"\"\"\n        Remove all elements in the sorted set ``name`` with ranks between\n        ``min`` and ``max``. Values are 0-based, ordered from smallest score\n        to largest. Values can be negative indicating the highest scores.\n        Returns the number of elements removed\n\n        For more information see https://redis.io/commands/zremrangebyrank\n        \"\"\"\n        return self.execute_command(\"ZREMRANGEBYRANK\", name, min, max)\n\n    def zremrangebyscore(\n        self, name: KeyT, min: ZScoreBoundT, max: ZScoreBoundT\n    ) -> ResponseT:\n        \"\"\"\n        Remove all elements in the sorted set ``name`` with scores\n        between ``min`` and ``max``. Returns the number of elements removed.\n\n        For more information see https://redis.io/commands/zremrangebyscore\n        \"\"\"\n        return self.execute_command(\"ZREMRANGEBYSCORE\", name, min, max)\n\n    def zrevrank(\n        self,\n        name: KeyT,\n        value: EncodableT,\n        withscore: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Returns a 0-based value indicating the descending rank of\n        ``value`` in sorted set ``name``.\n        The optional ``withscore`` argument supplements the command's\n        reply with the score of the element returned.\n\n        For more information see https://redis.io/commands/zrevrank\n        \"\"\"\n        if withscore:\n            return self.execute_command(\n                \"ZREVRANK\", name, value, \"WITHSCORE\", keys=[name]\n            )\n        return self.execute_command(\"ZREVRANK\", name, value, keys=[name])\n\n    def zscore(self, name: KeyT, value: EncodableT) -> ResponseT:\n        \"\"\"\n        Return the score of element ``value`` in sorted set ``name``\n\n        For more information see https://redis.io/commands/zscore\n        \"\"\"\n        return self.execute_command(\"ZSCORE\", name, value, keys=[name])\n\n    def zunion(\n        self,\n        keys: Union[Sequence[KeyT], Mapping[AnyKeyT, float]],\n        aggregate: Optional[str] = None,\n        withscores: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Return the union of multiple sorted sets specified by ``keys``.\n        ``keys`` can be provided as dictionary of keys and their weights.\n        Scores will be aggregated based on the ``aggregate``, or SUM if\n        none is provided.\n\n        For more information see https://redis.io/commands/zunion\n        \"\"\"\n        return self._zaggregate(\"ZUNION\", None, keys, aggregate, withscores=withscores)\n\n    def zunionstore(\n        self,\n        dest: KeyT,\n        keys: Union[Sequence[KeyT], Mapping[AnyKeyT, float]],\n        aggregate: Optional[str] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Union multiple sorted sets specified by ``keys`` into\n        a new sorted set, ``dest``. Scores in the destination will be\n        aggregated based on the ``aggregate``, or SUM if none is provided.\n\n        For more information see https://redis.io/commands/zunionstore\n        \"\"\"\n        return self._zaggregate(\"ZUNIONSTORE\", dest, keys, aggregate)\n\n    def zmscore(self, key: KeyT, members: List[str]) -> ResponseT:\n        \"\"\"\n        Returns the scores associated with the specified members\n        in the sorted set stored at key.\n        ``members`` should be a list of the member name.\n        Return type is a list of score.\n        If the member does not exist, a None will be returned\n        in corresponding position.\n\n        For more information see https://redis.io/commands/zmscore\n        \"\"\"\n        if not members:\n            raise DataError(\"ZMSCORE members must be a non-empty list\")\n        pieces = [key] + members\n        return self.execute_command(\"ZMSCORE\", *pieces, keys=[key])\n\n    def _zaggregate(\n        self,\n        command: str,\n        dest: Union[KeyT, None],\n        keys: Union[Sequence[KeyT], Mapping[AnyKeyT, float]],\n        aggregate: Optional[str] = None,\n        **options,\n    ) -> ResponseT:\n        pieces: list[EncodableT] = [command]\n        if dest is not None:\n            pieces.append(dest)\n        pieces.append(len(keys))\n        if isinstance(keys, dict):\n            keys, weights = keys.keys(), keys.values()\n        else:\n            weights = None\n        pieces.extend(keys)\n        if weights:\n            pieces.append(b\"WEIGHTS\")\n            pieces.extend(weights)\n        if aggregate:\n            if aggregate.upper() in [\"SUM\", \"MIN\", \"MAX\"]:\n                pieces.append(b\"AGGREGATE\")\n                pieces.append(aggregate)\n            else:\n                raise DataError(\"aggregate can be sum, min or max.\")\n        if options.get(\"withscores\", False):\n            pieces.append(b\"WITHSCORES\")\n        options[\"keys\"] = keys\n        return self.execute_command(*pieces, **options)\n\n\nAsyncSortedSetCommands = SortedSetCommands\n\n\nclass HyperlogCommands(CommandsProtocol):\n    \"\"\"\n    Redis commands of HyperLogLogs data type.\n    see: https://redis.io/topics/data-types-intro#hyperloglogs\n    \"\"\"\n\n    def pfadd(self, name: KeyT, *values: FieldT) -> ResponseT:\n        \"\"\"\n        Adds the specified elements to the specified HyperLogLog.\n\n        For more information see https://redis.io/commands/pfadd\n        \"\"\"\n        return self.execute_command(\"PFADD\", name, *values)\n\n    def pfcount(self, *sources: KeyT) -> ResponseT:\n        \"\"\"\n        Return the approximated cardinality of\n        the set observed by the HyperLogLog at key(s).\n\n        For more information see https://redis.io/commands/pfcount\n        \"\"\"\n        return self.execute_command(\"PFCOUNT\", *sources)\n\n    def pfmerge(self, dest: KeyT, *sources: KeyT) -> ResponseT:\n        \"\"\"\n        Merge N different HyperLogLogs into a single one.\n\n        For more information see https://redis.io/commands/pfmerge\n        \"\"\"\n        return self.execute_command(\"PFMERGE\", dest, *sources)\n\n\nAsyncHyperlogCommands = HyperlogCommands\n\n\nclass HashDataPersistOptions(Enum):\n    # set the value for each provided key to each\n    # provided value only if all do not already exist.\n    FNX = \"FNX\"\n\n    # set the value for each provided key to each\n    # provided value only if all already exist.\n    FXX = \"FXX\"\n\n\nclass HashCommands(CommandsProtocol):\n    \"\"\"\n    Redis commands for Hash data type.\n    see: https://redis.io/topics/data-types-intro#redis-hashes\n    \"\"\"\n\n    def hdel(self, name: str, *keys: str) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Delete ``keys`` from hash ``name``\n\n        For more information see https://redis.io/commands/hdel\n        \"\"\"\n        return self.execute_command(\"HDEL\", name, *keys)\n\n    def hexists(self, name: str, key: str) -> Union[Awaitable[bool], bool]:\n        \"\"\"\n        Returns a boolean indicating if ``key`` exists within hash ``name``\n\n        For more information see https://redis.io/commands/hexists\n        \"\"\"\n        return self.execute_command(\"HEXISTS\", name, key, keys=[name])\n\n    def hget(\n        self, name: str, key: str\n    ) -> Union[Awaitable[Optional[str]], Optional[str]]:\n        \"\"\"\n        Return the value of ``key`` within the hash ``name``\n\n        For more information see https://redis.io/commands/hget\n        \"\"\"\n        return self.execute_command(\"HGET\", name, key, keys=[name])\n\n    def hgetall(self, name: str) -> Union[Awaitable[dict], dict]:\n        \"\"\"\n        Return a Python dict of the hash's name/value pairs\n\n        For more information see https://redis.io/commands/hgetall\n        \"\"\"\n        return self.execute_command(\"HGETALL\", name, keys=[name])\n\n    def hgetdel(\n        self, name: str, *keys: str\n    ) -> Union[\n        Awaitable[Optional[List[Union[str, bytes]]]], Optional[List[Union[str, bytes]]]\n    ]:\n        \"\"\"\n        Return the value of ``key`` within the hash ``name`` and\n        delete the field in the hash.\n        This command is similar to HGET, except for the fact that it also deletes\n        the key on success from the hash with the provided ```name```.\n\n        Available since Redis 8.0\n        For more information see https://redis.io/commands/hgetdel\n        \"\"\"\n        if len(keys) == 0:\n            raise DataError(\"'hgetdel' should have at least one key provided\")\n\n        return self.execute_command(\"HGETDEL\", name, \"FIELDS\", len(keys), *keys)\n\n    def hgetex(\n        self,\n        name: KeyT,\n        *keys: str,\n        ex: Optional[ExpiryT] = None,\n        px: Optional[ExpiryT] = None,\n        exat: Optional[AbsExpiryT] = None,\n        pxat: Optional[AbsExpiryT] = None,\n        persist: bool = False,\n    ) -> Union[\n        Awaitable[Optional[List[Union[str, bytes]]]], Optional[List[Union[str, bytes]]]\n    ]:\n        \"\"\"\n        Return the values of ``key`` and ``keys`` within the hash ``name``\n        and optionally set their expiration.\n\n        ``ex`` sets an expire flag on ``kyes`` for ``ex`` seconds.\n\n        ``px`` sets an expire flag on ``keys`` for ``px`` milliseconds.\n\n        ``exat`` sets an expire flag on ``keys`` for ``ex`` seconds,\n        specified in unix time.\n\n        ``pxat`` sets an expire flag on ``keys`` for ``ex`` milliseconds,\n        specified in unix time.\n\n        ``persist`` remove the time to live associated with the ``keys``.\n\n        Available since Redis 8.0\n        For more information see https://redis.io/commands/hgetex\n        \"\"\"\n        if not keys:\n            raise DataError(\"'hgetex' should have at least one key provided\")\n\n        opset = {ex, px, exat, pxat}\n        if len(opset) > 2 or len(opset) > 1 and persist:\n            raise DataError(\n                \"``ex``, ``px``, ``exat``, ``pxat``, \"\n                \"and ``persist`` are mutually exclusive.\"\n            )\n\n        exp_options: list[EncodableT] = extract_expire_flags(ex, px, exat, pxat)\n\n        if persist:\n            exp_options.append(\"PERSIST\")\n\n        return self.execute_command(\n            \"HGETEX\",\n            name,\n            *exp_options,\n            \"FIELDS\",\n            len(keys),\n            *keys,\n        )\n\n    def hincrby(\n        self, name: str, key: str, amount: int = 1\n    ) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Increment the value of ``key`` in hash ``name`` by ``amount``\n\n        For more information see https://redis.io/commands/hincrby\n        \"\"\"\n        return self.execute_command(\"HINCRBY\", name, key, amount)\n\n    def hincrbyfloat(\n        self, name: str, key: str, amount: float = 1.0\n    ) -> Union[Awaitable[float], float]:\n        \"\"\"\n        Increment the value of ``key`` in hash ``name`` by floating ``amount``\n\n        For more information see https://redis.io/commands/hincrbyfloat\n        \"\"\"\n        return self.execute_command(\"HINCRBYFLOAT\", name, key, amount)\n\n    def hkeys(self, name: str) -> Union[Awaitable[List], List]:\n        \"\"\"\n        Return the list of keys within hash ``name``\n\n        For more information see https://redis.io/commands/hkeys\n        \"\"\"\n        return self.execute_command(\"HKEYS\", name, keys=[name])\n\n    def hlen(self, name: str) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Return the number of elements in hash ``name``\n\n        For more information see https://redis.io/commands/hlen\n        \"\"\"\n        return self.execute_command(\"HLEN\", name, keys=[name])\n\n    def hset(\n        self,\n        name: str,\n        key: Optional[str] = None,\n        value: Optional[str] = None,\n        mapping: Optional[dict] = None,\n        items: Optional[list] = None,\n    ) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Set ``key`` to ``value`` within hash ``name``,\n        ``mapping`` accepts a dict of key/value pairs that will be\n        added to hash ``name``.\n        ``items`` accepts a list of key/value pairs that will be\n        added to hash ``name``.\n        Returns the number of fields that were added.\n\n        For more information see https://redis.io/commands/hset\n        \"\"\"\n\n        if key is None and not mapping and not items:\n            raise DataError(\"'hset' with no key value pairs\")\n\n        pieces = []\n        if items:\n            pieces.extend(items)\n        if key is not None:\n            pieces.extend((key, value))\n        if mapping:\n            for pair in mapping.items():\n                pieces.extend(pair)\n\n        return self.execute_command(\"HSET\", name, *pieces)\n\n    def hsetex(\n        self,\n        name: str,\n        key: Optional[str] = None,\n        value: Optional[str] = None,\n        mapping: Optional[dict] = None,\n        items: Optional[list] = None,\n        ex: Optional[ExpiryT] = None,\n        px: Optional[ExpiryT] = None,\n        exat: Optional[AbsExpiryT] = None,\n        pxat: Optional[AbsExpiryT] = None,\n        data_persist_option: Optional[HashDataPersistOptions] = None,\n        keepttl: bool = False,\n    ) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Set ``key`` to ``value`` within hash ``name``\n\n        ``mapping`` accepts a dict of key/value pairs that will be\n        added to hash ``name``.\n\n        ``items`` accepts a list of key/value pairs that will be\n        added to hash ``name``.\n\n        ``ex`` sets an expire flag on ``keys`` for ``ex`` seconds.\n\n        ``px`` sets an expire flag on ``keys`` for ``px`` milliseconds.\n\n        ``exat`` sets an expire flag on ``keys`` for ``ex`` seconds,\n            specified in unix time.\n\n        ``pxat`` sets an expire flag on ``keys`` for ``ex`` milliseconds,\n            specified in unix time.\n\n        ``data_persist_option`` can be set to ``FNX`` or ``FXX`` to control the\n            behavior of the command.\n            ``FNX`` will set the value for each provided key to each\n                provided value only if all do not already exist.\n            ``FXX`` will set the value for each provided key to each\n                provided value only if all already exist.\n\n        ``keepttl`` if True, retain the time to live associated with the keys.\n\n        Returns the number of fields that were added.\n\n        Available since Redis 8.0\n        For more information see https://redis.io/commands/hsetex\n        \"\"\"\n        if key is None and not mapping and not items:\n            raise DataError(\"'hsetex' with no key value pairs\")\n\n        if items and len(items) % 2 != 0:\n            raise DataError(\n                \"'hsetex' with odd number of items. \"\n                \"'items' must contain a list of key/value pairs.\"\n            )\n\n        opset = {ex, px, exat, pxat}\n        if len(opset) > 2 or len(opset) > 1 and keepttl:\n            raise DataError(\n                \"``ex``, ``px``, ``exat``, ``pxat``, \"\n                \"and ``keepttl`` are mutually exclusive.\"\n            )\n\n        exp_options: list[EncodableT] = extract_expire_flags(ex, px, exat, pxat)\n        if data_persist_option:\n            exp_options.append(data_persist_option.value)\n\n        if keepttl:\n            exp_options.append(\"KEEPTTL\")\n\n        pieces = []\n        if items:\n            pieces.extend(items)\n        if key is not None:\n            pieces.extend((key, value))\n        if mapping:\n            for pair in mapping.items():\n                pieces.extend(pair)\n\n        return self.execute_command(\n            \"HSETEX\", name, *exp_options, \"FIELDS\", int(len(pieces) / 2), *pieces\n        )\n\n    def hsetnx(self, name: str, key: str, value: str) -> Union[Awaitable[bool], bool]:\n        \"\"\"\n        Set ``key`` to ``value`` within hash ``name`` if ``key`` does not\n        exist.  Returns 1 if HSETNX created a field, otherwise 0.\n\n        For more information see https://redis.io/commands/hsetnx\n        \"\"\"\n        return self.execute_command(\"HSETNX\", name, key, value)\n\n    @deprecated_function(\n        version=\"4.0.0\",\n        reason=\"Use 'hset' instead.\",\n        name=\"hmset\",\n    )\n    def hmset(self, name: str, mapping: dict) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Set key to value within hash ``name`` for each corresponding\n        key and value from the ``mapping`` dict.\n\n        For more information see https://redis.io/commands/hmset\n        \"\"\"\n        if not mapping:\n            raise DataError(\"'hmset' with 'mapping' of length 0\")\n        items = []\n        for pair in mapping.items():\n            items.extend(pair)\n        return self.execute_command(\"HMSET\", name, *items)\n\n    def hmget(self, name: str, keys: List, *args: List) -> Union[Awaitable[List], List]:\n        \"\"\"\n        Returns a list of values ordered identically to ``keys``\n\n        For more information see https://redis.io/commands/hmget\n        \"\"\"\n        args = list_or_args(keys, args)\n        return self.execute_command(\"HMGET\", name, *args, keys=[name])\n\n    def hvals(self, name: str) -> Union[Awaitable[List], List]:\n        \"\"\"\n        Return the list of values within hash ``name``\n\n        For more information see https://redis.io/commands/hvals\n        \"\"\"\n        return self.execute_command(\"HVALS\", name, keys=[name])\n\n    def hstrlen(self, name: str, key: str) -> Union[Awaitable[int], int]:\n        \"\"\"\n        Return the number of bytes stored in the value of ``key``\n        within hash ``name``\n\n        For more information see https://redis.io/commands/hstrlen\n        \"\"\"\n        return self.execute_command(\"HSTRLEN\", name, key, keys=[name])\n\n    def hexpire(\n        self,\n        name: KeyT,\n        seconds: ExpiryT,\n        *fields: str,\n        nx: bool = False,\n        xx: bool = False,\n        gt: bool = False,\n        lt: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Sets or updates the expiration time for fields within a hash key, using relative\n        time in seconds.\n\n        If a field already has an expiration time, the behavior of the update can be\n        controlled using the `nx`, `xx`, `gt`, and `lt` parameters.\n\n        The return value provides detailed information about the outcome for each field.\n\n        For more information, see https://redis.io/commands/hexpire\n\n        Args:\n            name: The name of the hash key.\n            seconds: Expiration time in seconds, relative. Can be an integer, or a\n                     Python `timedelta` object.\n            fields: List of fields within the hash to apply the expiration time to.\n            nx: Set expiry only when the field has no expiry.\n            xx: Set expiry only when the field has an existing expiry.\n            gt: Set expiry only when the new expiry is greater than the current one.\n            lt: Set expiry only when the new expiry is less than the current one.\n\n        Returns:\n            Returns a list which contains for each field in the request:\n                - `-2` if the field does not exist, or if the key does not exist.\n                - `0` if the specified NX | XX | GT | LT condition was not met.\n                - `1` if the expiration time was set or updated.\n                - `2` if the field was deleted because the specified expiration time is\n                  in the past.\n        \"\"\"\n        conditions = [nx, xx, gt, lt]\n        if sum(conditions) > 1:\n            raise ValueError(\"Only one of 'nx', 'xx', 'gt', 'lt' can be specified.\")\n\n        if isinstance(seconds, datetime.timedelta):\n            seconds = int(seconds.total_seconds())\n\n        options = []\n        if nx:\n            options.append(\"NX\")\n        if xx:\n            options.append(\"XX\")\n        if gt:\n            options.append(\"GT\")\n        if lt:\n            options.append(\"LT\")\n\n        return self.execute_command(\n            \"HEXPIRE\", name, seconds, *options, \"FIELDS\", len(fields), *fields\n        )\n\n    def hpexpire(\n        self,\n        name: KeyT,\n        milliseconds: ExpiryT,\n        *fields: str,\n        nx: bool = False,\n        xx: bool = False,\n        gt: bool = False,\n        lt: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Sets or updates the expiration time for fields within a hash key, using relative\n        time in milliseconds.\n\n        If a field already has an expiration time, the behavior of the update can be\n        controlled using the `nx`, `xx`, `gt`, and `lt` parameters.\n\n        The return value provides detailed information about the outcome for each field.\n\n        For more information, see https://redis.io/commands/hpexpire\n\n        Args:\n            name: The name of the hash key.\n            milliseconds: Expiration time in milliseconds, relative. Can be an integer,\n                          or a Python `timedelta` object.\n            fields: List of fields within the hash to apply the expiration time to.\n            nx: Set expiry only when the field has no expiry.\n            xx: Set expiry only when the field has an existing expiry.\n            gt: Set expiry only when the new expiry is greater than the current one.\n            lt: Set expiry only when the new expiry is less than the current one.\n\n        Returns:\n            Returns a list which contains for each field in the request:\n                - `-2` if the field does not exist, or if the key does not exist.\n                - `0` if the specified NX | XX | GT | LT condition was not met.\n                - `1` if the expiration time was set or updated.\n                - `2` if the field was deleted because the specified expiration time is\n                  in the past.\n        \"\"\"\n        conditions = [nx, xx, gt, lt]\n        if sum(conditions) > 1:\n            raise ValueError(\"Only one of 'nx', 'xx', 'gt', 'lt' can be specified.\")\n\n        if isinstance(milliseconds, datetime.timedelta):\n            milliseconds = int(milliseconds.total_seconds() * 1000)\n\n        options = []\n        if nx:\n            options.append(\"NX\")\n        if xx:\n            options.append(\"XX\")\n        if gt:\n            options.append(\"GT\")\n        if lt:\n            options.append(\"LT\")\n\n        return self.execute_command(\n            \"HPEXPIRE\", name, milliseconds, *options, \"FIELDS\", len(fields), *fields\n        )\n\n    def hexpireat(\n        self,\n        name: KeyT,\n        unix_time_seconds: AbsExpiryT,\n        *fields: str,\n        nx: bool = False,\n        xx: bool = False,\n        gt: bool = False,\n        lt: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Sets or updates the expiration time for fields within a hash key, using an\n        absolute Unix timestamp in seconds.\n\n        If a field already has an expiration time, the behavior of the update can be\n        controlled using the `nx`, `xx`, `gt`, and `lt` parameters.\n\n        The return value provides detailed information about the outcome for each field.\n\n        For more information, see https://redis.io/commands/hexpireat\n\n        Args:\n            name: The name of the hash key.\n            unix_time_seconds: Expiration time as Unix timestamp in seconds. Can be an\n                               integer or a Python `datetime` object.\n            fields: List of fields within the hash to apply the expiration time to.\n            nx: Set expiry only when the field has no expiry.\n            xx: Set expiry only when the field has an existing expiration time.\n            gt: Set expiry only when the new expiry is greater than the current one.\n            lt: Set expiry only when the new expiry is less than the current one.\n\n        Returns:\n            Returns a list which contains for each field in the request:\n                - `-2` if the field does not exist, or if the key does not exist.\n                - `0` if the specified NX | XX | GT | LT condition was not met.\n                - `1` if the expiration time was set or updated.\n                - `2` if the field was deleted because the specified expiration time is\n                  in the past.\n        \"\"\"\n        conditions = [nx, xx, gt, lt]\n        if sum(conditions) > 1:\n            raise ValueError(\"Only one of 'nx', 'xx', 'gt', 'lt' can be specified.\")\n\n        if isinstance(unix_time_seconds, datetime.datetime):\n            unix_time_seconds = int(unix_time_seconds.timestamp())\n\n        options = []\n        if nx:\n            options.append(\"NX\")\n        if xx:\n            options.append(\"XX\")\n        if gt:\n            options.append(\"GT\")\n        if lt:\n            options.append(\"LT\")\n\n        return self.execute_command(\n            \"HEXPIREAT\",\n            name,\n            unix_time_seconds,\n            *options,\n            \"FIELDS\",\n            len(fields),\n            *fields,\n        )\n\n    def hpexpireat(\n        self,\n        name: KeyT,\n        unix_time_milliseconds: AbsExpiryT,\n        *fields: str,\n        nx: bool = False,\n        xx: bool = False,\n        gt: bool = False,\n        lt: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Sets or updates the expiration time for fields within a hash key, using an\n        absolute Unix timestamp in milliseconds.\n\n        If a field already has an expiration time, the behavior of the update can be\n        controlled using the `nx`, `xx`, `gt`, and `lt` parameters.\n\n        The return value provides detailed information about the outcome for each field.\n\n        For more information, see https://redis.io/commands/hpexpireat\n\n        Args:\n            name: The name of the hash key.\n            unix_time_milliseconds: Expiration time as Unix timestamp in milliseconds.\n                                    Can be an integer or a Python `datetime` object.\n            fields: List of fields within the hash to apply the expiry.\n            nx: Set expiry only when the field has no expiry.\n            xx: Set expiry only when the field has an existing expiry.\n            gt: Set expiry only when the new expiry is greater than the current one.\n            lt: Set expiry only when the new expiry is less than the current one.\n\n        Returns:\n            Returns a list which contains for each field in the request:\n                - `-2` if the field does not exist, or if the key does not exist.\n                - `0` if the specified NX | XX | GT | LT condition was not met.\n                - `1` if the expiration time was set or updated.\n                - `2` if the field was deleted because the specified expiration time is\n                  in the past.\n        \"\"\"\n        conditions = [nx, xx, gt, lt]\n        if sum(conditions) > 1:\n            raise ValueError(\"Only one of 'nx', 'xx', 'gt', 'lt' can be specified.\")\n\n        if isinstance(unix_time_milliseconds, datetime.datetime):\n            unix_time_milliseconds = int(unix_time_milliseconds.timestamp() * 1000)\n\n        options = []\n        if nx:\n            options.append(\"NX\")\n        if xx:\n            options.append(\"XX\")\n        if gt:\n            options.append(\"GT\")\n        if lt:\n            options.append(\"LT\")\n\n        return self.execute_command(\n            \"HPEXPIREAT\",\n            name,\n            unix_time_milliseconds,\n            *options,\n            \"FIELDS\",\n            len(fields),\n            *fields,\n        )\n\n    def hpersist(self, name: KeyT, *fields: str) -> ResponseT:\n        \"\"\"\n        Removes the expiration time for each specified field in a hash.\n\n        For more information, see https://redis.io/commands/hpersist\n\n        Args:\n            name: The name of the hash key.\n            fields: A list of fields within the hash from which to remove the\n                    expiration time.\n\n        Returns:\n            Returns a list which contains for each field in the request:\n                - `-2` if the field does not exist, or if the key does not exist.\n                - `-1` if the field exists but has no associated expiration time.\n                - `1` if the expiration time was successfully removed from the field.\n        \"\"\"\n        return self.execute_command(\"HPERSIST\", name, \"FIELDS\", len(fields), *fields)\n\n    def hexpiretime(self, key: KeyT, *fields: str) -> ResponseT:\n        \"\"\"\n        Returns the expiration times of hash fields as Unix timestamps in seconds.\n\n        For more information, see https://redis.io/commands/hexpiretime\n\n        Args:\n            key: The hash key.\n            fields: A list of fields within the hash for which to get the expiration\n                    time.\n\n        Returns:\n            Returns a list which contains for each field in the request:\n                - `-2` if the field does not exist, or if the key does not exist.\n                - `-1` if the field exists but has no associated expire time.\n                - A positive integer representing the expiration Unix timestamp in\n                  seconds, if the field has an associated expiration time.\n        \"\"\"\n        return self.execute_command(\n            \"HEXPIRETIME\", key, \"FIELDS\", len(fields), *fields, keys=[key]\n        )\n\n    def hpexpiretime(self, key: KeyT, *fields: str) -> ResponseT:\n        \"\"\"\n        Returns the expiration times of hash fields as Unix timestamps in milliseconds.\n\n        For more information, see https://redis.io/commands/hpexpiretime\n\n        Args:\n            key: The hash key.\n            fields: A list of fields within the hash for which to get the expiration\n                    time.\n\n        Returns:\n            Returns a list which contains for each field in the request:\n                - `-2` if the field does not exist, or if the key does not exist.\n                - `-1` if the field exists but has no associated expire time.\n                - A positive integer representing the expiration Unix timestamp in\n                  milliseconds, if the field has an associated expiration time.\n        \"\"\"\n        return self.execute_command(\n            \"HPEXPIRETIME\", key, \"FIELDS\", len(fields), *fields, keys=[key]\n        )\n\n    def httl(self, key: KeyT, *fields: str) -> ResponseT:\n        \"\"\"\n        Returns the TTL (Time To Live) in seconds for each specified field within a hash\n        key.\n\n        For more information, see https://redis.io/commands/httl\n\n        Args:\n            key: The hash key.\n            fields: A list of fields within the hash for which to get the TTL.\n\n        Returns:\n            Returns a list which contains for each field in the request:\n                - `-2` if the field does not exist, or if the key does not exist.\n                - `-1` if the field exists but has no associated expire time.\n                - A positive integer representing the TTL in seconds if the field has\n                  an associated expiration time.\n        \"\"\"\n        return self.execute_command(\n            \"HTTL\", key, \"FIELDS\", len(fields), *fields, keys=[key]\n        )\n\n    def hpttl(self, key: KeyT, *fields: str) -> ResponseT:\n        \"\"\"\n        Returns the TTL (Time To Live) in milliseconds for each specified field within a\n        hash key.\n\n        For more information, see https://redis.io/commands/hpttl\n\n        Args:\n            key: The hash key.\n            fields: A list of fields within the hash for which to get the TTL.\n\n        Returns:\n            Returns a list which contains for each field in the request:\n                - `-2` if the field does not exist, or if the key does not exist.\n                - `-1` if the field exists but has no associated expire time.\n                - A positive integer representing the TTL in milliseconds if the field\n                  has an associated expiration time.\n        \"\"\"\n        return self.execute_command(\n            \"HPTTL\", key, \"FIELDS\", len(fields), *fields, keys=[key]\n        )\n\n\nAsyncHashCommands = HashCommands\n\n\nclass Script:\n    \"\"\"\n    An executable Lua script object returned by ``register_script``\n    \"\"\"\n\n    def __init__(self, registered_client: \"redis.client.Redis\", script: ScriptTextT):\n        self.registered_client = registered_client\n        self.script = script\n        # Precalculate and store the SHA1 hex digest of the script.\n\n        if isinstance(script, str):\n            # We need the encoding from the client in order to generate an\n            # accurate byte representation of the script\n            encoder = self.get_encoder()\n            script = encoder.encode(script)\n        self.sha = hashlib.sha1(script).hexdigest()\n\n    def __call__(\n        self,\n        keys: Union[Sequence[KeyT], None] = None,\n        args: Union[Iterable[EncodableT], None] = None,\n        client: Union[\"redis.client.Redis\", None] = None,\n    ):\n        \"\"\"Execute the script, passing any required ``args``\"\"\"\n        keys = keys or []\n        args = args or []\n        if client is None:\n            client = self.registered_client\n        args = tuple(keys) + tuple(args)\n        # make sure the Redis server knows about the script\n        from redis.client import Pipeline\n\n        if isinstance(client, Pipeline):\n            # Make sure the pipeline can register the script before executing.\n            client.scripts.add(self)\n        try:\n            return client.evalsha(self.sha, len(keys), *args)\n        except NoScriptError:\n            # Maybe the client is pointed to a different server than the client\n            # that created this instance?\n            # Overwrite the sha just in case there was a discrepancy.\n            self.sha = client.script_load(self.script)\n            return client.evalsha(self.sha, len(keys), *args)\n\n    def get_encoder(self):\n        \"\"\"Get the encoder to encode string scripts into bytes.\"\"\"\n        try:\n            return self.registered_client.get_encoder()\n        except AttributeError:\n            # DEPRECATED\n            # In version <=4.1.2, this was the code we used to get the encoder.\n            # However, after 4.1.2 we added support for scripting in clustered\n            # redis. ClusteredRedis doesn't have a `.connection_pool` attribute\n            # so we changed the Script class to use\n            # `self.registered_client.get_encoder` (see above).\n            # However, that is technically a breaking change, as consumers who\n            # use Scripts directly might inject a `registered_client` that\n            # doesn't have a `.get_encoder` field. This try/except prevents us\n            # from breaking backward-compatibility. Ideally, it would be\n            # removed in the next major release.\n            return self.registered_client.connection_pool.get_encoder()\n\n\nclass AsyncScript:\n    \"\"\"\n    An executable Lua script object returned by ``register_script``\n    \"\"\"\n\n    def __init__(\n        self,\n        registered_client: \"redis.asyncio.client.Redis\",\n        script: ScriptTextT,\n    ):\n        self.registered_client = registered_client\n        self.script = script\n        # Precalculate and store the SHA1 hex digest of the script.\n\n        if isinstance(script, str):\n            # We need the encoding from the client in order to generate an\n            # accurate byte representation of the script\n            try:\n                encoder = registered_client.connection_pool.get_encoder()\n            except AttributeError:\n                # Cluster\n                encoder = registered_client.get_encoder()\n            script = encoder.encode(script)\n        self.sha = hashlib.sha1(script).hexdigest()\n\n    async def __call__(\n        self,\n        keys: Union[Sequence[KeyT], None] = None,\n        args: Union[Iterable[EncodableT], None] = None,\n        client: Union[\"redis.asyncio.client.Redis\", None] = None,\n    ):\n        \"\"\"Execute the script, passing any required ``args``\"\"\"\n        keys = keys or []\n        args = args or []\n        if client is None:\n            client = self.registered_client\n        args = tuple(keys) + tuple(args)\n        # make sure the Redis server knows about the script\n        from redis.asyncio.client import Pipeline\n\n        if isinstance(client, Pipeline):\n            # Make sure the pipeline can register the script before executing.\n            client.scripts.add(self)\n        try:\n            return await client.evalsha(self.sha, len(keys), *args)\n        except NoScriptError:\n            # Maybe the client is pointed to a different server than the client\n            # that created this instance?\n            # Overwrite the sha just in case there was a discrepancy.\n            self.sha = await client.script_load(self.script)\n            return await client.evalsha(self.sha, len(keys), *args)\n\n\nclass PubSubCommands(CommandsProtocol):\n    \"\"\"\n    Redis PubSub commands.\n    see https://redis.io/topics/pubsub\n    \"\"\"\n\n    def publish(self, channel: ChannelT, message: EncodableT, **kwargs) -> ResponseT:\n        \"\"\"\n        Publish ``message`` on ``channel``.\n        Returns the number of subscribers the message was delivered to.\n\n        For more information see https://redis.io/commands/publish\n        \"\"\"\n        return self.execute_command(\"PUBLISH\", channel, message, **kwargs)\n\n    def spublish(self, shard_channel: ChannelT, message: EncodableT) -> ResponseT:\n        \"\"\"\n        Posts a message to the given shard channel.\n        Returns the number of clients that received the message\n\n        For more information see https://redis.io/commands/spublish\n        \"\"\"\n        return self.execute_command(\"SPUBLISH\", shard_channel, message)\n\n    def pubsub_channels(self, pattern: PatternT = \"*\", **kwargs) -> ResponseT:\n        \"\"\"\n        Return a list of channels that have at least one subscriber\n\n        For more information see https://redis.io/commands/pubsub-channels\n        \"\"\"\n        return self.execute_command(\"PUBSUB CHANNELS\", pattern, **kwargs)\n\n    def pubsub_shardchannels(self, pattern: PatternT = \"*\", **kwargs) -> ResponseT:\n        \"\"\"\n        Return a list of shard_channels that have at least one subscriber\n\n        For more information see https://redis.io/commands/pubsub-shardchannels\n        \"\"\"\n        return self.execute_command(\"PUBSUB SHARDCHANNELS\", pattern, **kwargs)\n\n    def pubsub_numpat(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Returns the number of subscriptions to patterns\n\n        For more information see https://redis.io/commands/pubsub-numpat\n        \"\"\"\n        return self.execute_command(\"PUBSUB NUMPAT\", **kwargs)\n\n    def pubsub_numsub(self, *args: ChannelT, **kwargs) -> ResponseT:\n        \"\"\"\n        Return a list of (channel, number of subscribers) tuples\n        for each channel given in ``*args``\n\n        For more information see https://redis.io/commands/pubsub-numsub\n        \"\"\"\n        return self.execute_command(\"PUBSUB NUMSUB\", *args, **kwargs)\n\n    def pubsub_shardnumsub(self, *args: ChannelT, **kwargs) -> ResponseT:\n        \"\"\"\n        Return a list of (shard_channel, number of subscribers) tuples\n        for each channel given in ``*args``\n\n        For more information see https://redis.io/commands/pubsub-shardnumsub\n        \"\"\"\n        return self.execute_command(\"PUBSUB SHARDNUMSUB\", *args, **kwargs)\n\n\nAsyncPubSubCommands = PubSubCommands\n\n\nclass ScriptCommands(CommandsProtocol):\n    \"\"\"\n    Redis Lua script commands. see:\n    https://redis.io/ebook/part-3-next-steps/chapter-11-scripting-redis-with-lua/\n    \"\"\"\n\n    def _eval(\n        self, command: str, script: str, numkeys: int, *keys_and_args: str\n    ) -> Union[Awaitable[str], str]:\n        return self.execute_command(command, script, numkeys, *keys_and_args)\n\n    def eval(\n        self, script: str, numkeys: int, *keys_and_args: str\n    ) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Execute the Lua ``script``, specifying the ``numkeys`` the script\n        will touch and the key names and argument values in ``keys_and_args``.\n        Returns the result of the script.\n\n        In practice, use the object returned by ``register_script``. This\n        function exists purely for Redis API completion.\n\n        For more information see  https://redis.io/commands/eval\n        \"\"\"\n        return self._eval(\"EVAL\", script, numkeys, *keys_and_args)\n\n    def eval_ro(\n        self, script: str, numkeys: int, *keys_and_args: str\n    ) -> Union[Awaitable[str], str]:\n        \"\"\"\n        The read-only variant of the EVAL command\n\n        Execute the read-only Lua ``script`` specifying the ``numkeys`` the script\n        will touch and the key names and argument values in ``keys_and_args``.\n        Returns the result of the script.\n\n        For more information see  https://redis.io/commands/eval_ro\n        \"\"\"\n        return self._eval(\"EVAL_RO\", script, numkeys, *keys_and_args)\n\n    def _evalsha(\n        self, command: str, sha: str, numkeys: int, *keys_and_args: list\n    ) -> Union[Awaitable[str], str]:\n        return self.execute_command(command, sha, numkeys, *keys_and_args)\n\n    def evalsha(\n        self, sha: str, numkeys: int, *keys_and_args: str\n    ) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Use the ``sha`` to execute a Lua script already registered via EVAL\n        or SCRIPT LOAD. Specify the ``numkeys`` the script will touch and the\n        key names and argument values in ``keys_and_args``. Returns the result\n        of the script.\n\n        In practice, use the object returned by ``register_script``. This\n        function exists purely for Redis API completion.\n\n        For more information see  https://redis.io/commands/evalsha\n        \"\"\"\n        return self._evalsha(\"EVALSHA\", sha, numkeys, *keys_and_args)\n\n    def evalsha_ro(\n        self, sha: str, numkeys: int, *keys_and_args: str\n    ) -> Union[Awaitable[str], str]:\n        \"\"\"\n        The read-only variant of the EVALSHA command\n\n        Use the ``sha`` to execute a read-only Lua script already registered via EVAL\n        or SCRIPT LOAD. Specify the ``numkeys`` the script will touch and the\n        key names and argument values in ``keys_and_args``. Returns the result\n        of the script.\n\n        For more information see  https://redis.io/commands/evalsha_ro\n        \"\"\"\n        return self._evalsha(\"EVALSHA_RO\", sha, numkeys, *keys_and_args)\n\n    def script_exists(self, *args: str) -> ResponseT:\n        \"\"\"\n        Check if a script exists in the script cache by specifying the SHAs of\n        each script as ``args``. Returns a list of boolean values indicating if\n        if each already script exists in the cache_data.\n\n        For more information see  https://redis.io/commands/script-exists\n        \"\"\"\n        return self.execute_command(\"SCRIPT EXISTS\", *args)\n\n    def script_debug(self, *args) -> None:\n        raise NotImplementedError(\n            \"SCRIPT DEBUG is intentionally not implemented in the client.\"\n        )\n\n    def script_flush(\n        self, sync_type: Union[Literal[\"SYNC\"], Literal[\"ASYNC\"]] = None\n    ) -> ResponseT:\n        \"\"\"Flush all scripts from the script cache_data.\n\n        ``sync_type`` is by default SYNC (synchronous) but it can also be\n                      ASYNC.\n\n        For more information see  https://redis.io/commands/script-flush\n        \"\"\"\n\n        # Redis pre 6 had no sync_type.\n        if sync_type not in [\"SYNC\", \"ASYNC\", None]:\n            raise DataError(\n                \"SCRIPT FLUSH defaults to SYNC in redis > 6.2, or \"\n                \"accepts SYNC/ASYNC. For older versions, \"\n                \"of redis leave as None.\"\n            )\n        if sync_type is None:\n            pieces = []\n        else:\n            pieces = [sync_type]\n        return self.execute_command(\"SCRIPT FLUSH\", *pieces)\n\n    def script_kill(self) -> ResponseT:\n        \"\"\"\n        Kill the currently executing Lua script\n\n        For more information see https://redis.io/commands/script-kill\n        \"\"\"\n        return self.execute_command(\"SCRIPT KILL\")\n\n    def script_load(self, script: ScriptTextT) -> ResponseT:\n        \"\"\"\n        Load a Lua ``script`` into the script cache_data. Returns the SHA.\n\n        For more information see https://redis.io/commands/script-load\n        \"\"\"\n        return self.execute_command(\"SCRIPT LOAD\", script)\n\n    def register_script(self: \"redis.client.Redis\", script: ScriptTextT) -> Script:\n        \"\"\"\n        Register a Lua ``script`` specifying the ``keys`` it will touch.\n        Returns a Script object that is callable and hides the complexity of\n        deal with scripts, keys, and shas. This is the preferred way to work\n        with Lua scripts.\n        \"\"\"\n        return Script(self, script)\n\n\nclass AsyncScriptCommands(ScriptCommands):\n    async def script_debug(self, *args) -> None:\n        return super().script_debug()\n\n    def register_script(\n        self: \"redis.asyncio.client.Redis\",\n        script: ScriptTextT,\n    ) -> AsyncScript:\n        \"\"\"\n        Register a Lua ``script`` specifying the ``keys`` it will touch.\n        Returns a Script object that is callable and hides the complexity of\n        deal with scripts, keys, and shas. This is the preferred way to work\n        with Lua scripts.\n        \"\"\"\n        return AsyncScript(self, script)\n\n\nclass GeoCommands(CommandsProtocol):\n    \"\"\"\n    Redis Geospatial commands.\n    see: https://redis.com/redis-best-practices/indexing-patterns/geospatial/\n    \"\"\"\n\n    def geoadd(\n        self,\n        name: KeyT,\n        values: Sequence[EncodableT],\n        nx: bool = False,\n        xx: bool = False,\n        ch: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Add the specified geospatial items to the specified key identified\n        by the ``name`` argument. The Geospatial items are given as ordered\n        members of the ``values`` argument, each item or place is formed by\n        the triad longitude, latitude and name.\n\n        Note: You can use ZREM to remove elements.\n\n        ``nx`` forces ZADD to only create new elements and not to update\n        scores for elements that already exist.\n\n        ``xx`` forces ZADD to only update scores of elements that already\n        exist. New elements will not be added.\n\n        ``ch`` modifies the return value to be the numbers of elements changed.\n        Changed elements include new elements that were added and elements\n        whose scores changed.\n\n        For more information see https://redis.io/commands/geoadd\n        \"\"\"\n        if nx and xx:\n            raise DataError(\"GEOADD allows either 'nx' or 'xx', not both\")\n        if len(values) % 3 != 0:\n            raise DataError(\"GEOADD requires places with lon, lat and name values\")\n        pieces = [name]\n        if nx:\n            pieces.append(\"NX\")\n        if xx:\n            pieces.append(\"XX\")\n        if ch:\n            pieces.append(\"CH\")\n        pieces.extend(values)\n        return self.execute_command(\"GEOADD\", *pieces)\n\n    def geodist(\n        self, name: KeyT, place1: FieldT, place2: FieldT, unit: Optional[str] = None\n    ) -> ResponseT:\n        \"\"\"\n        Return the distance between ``place1`` and ``place2`` members of the\n        ``name`` key.\n        The units must be one of the following : m, km mi, ft. By default\n        meters are used.\n\n        For more information see https://redis.io/commands/geodist\n        \"\"\"\n        pieces: list[EncodableT] = [name, place1, place2]\n        if unit and unit not in (\"m\", \"km\", \"mi\", \"ft\"):\n            raise DataError(\"GEODIST invalid unit\")\n        elif unit:\n            pieces.append(unit)\n        return self.execute_command(\"GEODIST\", *pieces, keys=[name])\n\n    def geohash(self, name: KeyT, *values: FieldT) -> ResponseT:\n        \"\"\"\n        Return the geo hash string for each item of ``values`` members of\n        the specified key identified by the ``name`` argument.\n\n        For more information see https://redis.io/commands/geohash\n        \"\"\"\n        return self.execute_command(\"GEOHASH\", name, *values, keys=[name])\n\n    def geopos(self, name: KeyT, *values: FieldT) -> ResponseT:\n        \"\"\"\n        Return the positions of each item of ``values`` as members of\n        the specified key identified by the ``name`` argument. Each position\n        is represented by the pairs lon and lat.\n\n        For more information see https://redis.io/commands/geopos\n        \"\"\"\n        return self.execute_command(\"GEOPOS\", name, *values, keys=[name])\n\n    def georadius(\n        self,\n        name: KeyT,\n        longitude: float,\n        latitude: float,\n        radius: float,\n        unit: Optional[str] = None,\n        withdist: bool = False,\n        withcoord: bool = False,\n        withhash: bool = False,\n        count: Optional[int] = None,\n        sort: Optional[str] = None,\n        store: Optional[KeyT] = None,\n        store_dist: Optional[KeyT] = None,\n        any: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Return the members of the specified key identified by the\n        ``name`` argument which are within the borders of the area specified\n        with the ``latitude`` and ``longitude`` location and the maximum\n        distance from the center specified by the ``radius`` value.\n\n        The units must be one of the following : m, km mi, ft. By default\n\n        ``withdist`` indicates to return the distances of each place.\n\n        ``withcoord`` indicates to return the latitude and longitude of\n        each place.\n\n        ``withhash`` indicates to return the geohash string of each place.\n\n        ``count`` indicates to return the number of elements up to N.\n\n        ``sort`` indicates to return the places in a sorted way, ASC for\n        nearest to fairest and DESC for fairest to nearest.\n\n        ``store`` indicates to save the places names in a sorted set named\n        with a specific key, each element of the destination sorted set is\n        populated with the score got from the original geo sorted set.\n\n        ``store_dist`` indicates to save the places names in a sorted set\n        named with a specific key, instead of ``store`` the sorted set\n        destination score is set with the distance.\n\n        For more information see https://redis.io/commands/georadius\n        \"\"\"\n        return self._georadiusgeneric(\n            \"GEORADIUS\",\n            name,\n            longitude,\n            latitude,\n            radius,\n            unit=unit,\n            withdist=withdist,\n            withcoord=withcoord,\n            withhash=withhash,\n            count=count,\n            sort=sort,\n            store=store,\n            store_dist=store_dist,\n            any=any,\n        )\n\n    def georadiusbymember(\n        self,\n        name: KeyT,\n        member: FieldT,\n        radius: float,\n        unit: Optional[str] = None,\n        withdist: bool = False,\n        withcoord: bool = False,\n        withhash: bool = False,\n        count: Optional[int] = None,\n        sort: Optional[str] = None,\n        store: Union[KeyT, None] = None,\n        store_dist: Union[KeyT, None] = None,\n        any: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        This command is exactly like ``georadius`` with the sole difference\n        that instead of taking, as the center of the area to query, a longitude\n        and latitude value, it takes the name of a member already existing\n        inside the geospatial index represented by the sorted set.\n\n        For more information see https://redis.io/commands/georadiusbymember\n        \"\"\"\n        return self._georadiusgeneric(\n            \"GEORADIUSBYMEMBER\",\n            name,\n            member,\n            radius,\n            unit=unit,\n            withdist=withdist,\n            withcoord=withcoord,\n            withhash=withhash,\n            count=count,\n            sort=sort,\n            store=store,\n            store_dist=store_dist,\n            any=any,\n        )\n\n    def _georadiusgeneric(\n        self, command: str, *args: EncodableT, **kwargs: Union[EncodableT, None]\n    ) -> ResponseT:\n        pieces = list(args)\n        if kwargs[\"unit\"] and kwargs[\"unit\"] not in (\"m\", \"km\", \"mi\", \"ft\"):\n            raise DataError(\"GEORADIUS invalid unit\")\n        elif kwargs[\"unit\"]:\n            pieces.append(kwargs[\"unit\"])\n        else:\n            pieces.append(\"m\")\n\n        if kwargs[\"any\"] and kwargs[\"count\"] is None:\n            raise DataError(\"``any`` can't be provided without ``count``\")\n\n        for arg_name, byte_repr in (\n            (\"withdist\", \"WITHDIST\"),\n            (\"withcoord\", \"WITHCOORD\"),\n            (\"withhash\", \"WITHHASH\"),\n        ):\n            if kwargs[arg_name]:\n                pieces.append(byte_repr)\n\n        if kwargs[\"count\"] is not None:\n            pieces.extend([\"COUNT\", kwargs[\"count\"]])\n            if kwargs[\"any\"]:\n                pieces.append(\"ANY\")\n\n        if kwargs[\"sort\"]:\n            if kwargs[\"sort\"] == \"ASC\":\n                pieces.append(\"ASC\")\n            elif kwargs[\"sort\"] == \"DESC\":\n                pieces.append(\"DESC\")\n            else:\n                raise DataError(\"GEORADIUS invalid sort\")\n\n        if kwargs[\"store\"] and kwargs[\"store_dist\"]:\n            raise DataError(\"GEORADIUS store and store_dist cant be set together\")\n\n        if kwargs[\"store\"]:\n            pieces.extend([b\"STORE\", kwargs[\"store\"]])\n\n        if kwargs[\"store_dist\"]:\n            pieces.extend([b\"STOREDIST\", kwargs[\"store_dist\"]])\n\n        return self.execute_command(command, *pieces, **kwargs)\n\n    def geosearch(\n        self,\n        name: KeyT,\n        member: Union[FieldT, None] = None,\n        longitude: Union[float, None] = None,\n        latitude: Union[float, None] = None,\n        unit: str = \"m\",\n        radius: Union[float, None] = None,\n        width: Union[float, None] = None,\n        height: Union[float, None] = None,\n        sort: Optional[str] = None,\n        count: Optional[int] = None,\n        any: bool = False,\n        withcoord: bool = False,\n        withdist: bool = False,\n        withhash: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        Return the members of specified key identified by the\n        ``name`` argument, which are within the borders of the\n        area specified by a given shape. This command extends the\n        GEORADIUS command, so in addition to searching within circular\n        areas, it supports searching within rectangular areas.\n\n        This command should be used in place of the deprecated\n        GEORADIUS and GEORADIUSBYMEMBER commands.\n\n        ``member`` Use the position of the given existing\n         member in the sorted set. Can't be given with ``longitude``\n         and ``latitude``.\n\n        ``longitude`` and ``latitude`` Use the position given by\n        this coordinates. Can't be given with ``member``\n        ``radius`` Similar to GEORADIUS, search inside circular\n        area according the given radius. Can't be given with\n        ``height`` and ``width``.\n        ``height`` and ``width`` Search inside an axis-aligned\n        rectangle, determined by the given height and width.\n        Can't be given with ``radius``\n\n        ``unit`` must be one of the following : m, km, mi, ft.\n        `m` for meters (the default value), `km` for kilometers,\n        `mi` for miles and `ft` for feet.\n\n        ``sort`` indicates to return the places in a sorted way,\n        ASC for nearest to furthest and DESC for furthest to nearest.\n\n        ``count`` limit the results to the first count matching items.\n\n        ``any`` is set to True, the command will return as soon as\n        enough matches are found. Can't be provided without ``count``\n\n        ``withdist`` indicates to return the distances of each place.\n        ``withcoord`` indicates to return the latitude and longitude of\n        each place.\n\n        ``withhash`` indicates to return the geohash string of each place.\n\n        For more information see https://redis.io/commands/geosearch\n        \"\"\"\n\n        return self._geosearchgeneric(\n            \"GEOSEARCH\",\n            name,\n            member=member,\n            longitude=longitude,\n            latitude=latitude,\n            unit=unit,\n            radius=radius,\n            width=width,\n            height=height,\n            sort=sort,\n            count=count,\n            any=any,\n            withcoord=withcoord,\n            withdist=withdist,\n            withhash=withhash,\n            store=None,\n            store_dist=None,\n        )\n\n    def geosearchstore(\n        self,\n        dest: KeyT,\n        name: KeyT,\n        member: Optional[FieldT] = None,\n        longitude: Optional[float] = None,\n        latitude: Optional[float] = None,\n        unit: str = \"m\",\n        radius: Optional[float] = None,\n        width: Optional[float] = None,\n        height: Optional[float] = None,\n        sort: Optional[str] = None,\n        count: Optional[int] = None,\n        any: bool = False,\n        storedist: bool = False,\n    ) -> ResponseT:\n        \"\"\"\n        This command is like GEOSEARCH, but stores the result in\n        ``dest``. By default, it stores the results in the destination\n        sorted set with their geospatial information.\n        if ``store_dist`` set to True, the command will stores the\n        items in a sorted set populated with their distance from the\n        center of the circle or box, as a floating-point number.\n\n        For more information see https://redis.io/commands/geosearchstore\n        \"\"\"\n        return self._geosearchgeneric(\n            \"GEOSEARCHSTORE\",\n            dest,\n            name,\n            member=member,\n            longitude=longitude,\n            latitude=latitude,\n            unit=unit,\n            radius=radius,\n            width=width,\n            height=height,\n            sort=sort,\n            count=count,\n            any=any,\n            withcoord=None,\n            withdist=None,\n            withhash=None,\n            store=None,\n            store_dist=storedist,\n        )\n\n    def _geosearchgeneric(\n        self, command: str, *args: EncodableT, **kwargs: Union[EncodableT, None]\n    ) -> ResponseT:\n        pieces = list(args)\n\n        # FROMMEMBER or FROMLONLAT\n        if kwargs[\"member\"] is None:\n            if kwargs[\"longitude\"] is None or kwargs[\"latitude\"] is None:\n                raise DataError(\"GEOSEARCH must have member or longitude and latitude\")\n        if kwargs[\"member\"]:\n            if kwargs[\"longitude\"] or kwargs[\"latitude\"]:\n                raise DataError(\n                    \"GEOSEARCH member and longitude or latitude cant be set together\"\n                )\n            pieces.extend([b\"FROMMEMBER\", kwargs[\"member\"]])\n        if kwargs[\"longitude\"] is not None and kwargs[\"latitude\"] is not None:\n            pieces.extend([b\"FROMLONLAT\", kwargs[\"longitude\"], kwargs[\"latitude\"]])\n\n        # BYRADIUS or BYBOX\n        if kwargs[\"radius\"] is None:\n            if kwargs[\"width\"] is None or kwargs[\"height\"] is None:\n                raise DataError(\"GEOSEARCH must have radius or width and height\")\n        if kwargs[\"unit\"] is None:\n            raise DataError(\"GEOSEARCH must have unit\")\n        if kwargs[\"unit\"].lower() not in (\"m\", \"km\", \"mi\", \"ft\"):\n            raise DataError(\"GEOSEARCH invalid unit\")\n        if kwargs[\"radius\"]:\n            if kwargs[\"width\"] or kwargs[\"height\"]:\n                raise DataError(\n                    \"GEOSEARCH radius and width or height cant be set together\"\n                )\n            pieces.extend([b\"BYRADIUS\", kwargs[\"radius\"], kwargs[\"unit\"]])\n        if kwargs[\"width\"] and kwargs[\"height\"]:\n            pieces.extend([b\"BYBOX\", kwargs[\"width\"], kwargs[\"height\"], kwargs[\"unit\"]])\n\n        # sort\n        if kwargs[\"sort\"]:\n            if kwargs[\"sort\"].upper() == \"ASC\":\n                pieces.append(b\"ASC\")\n            elif kwargs[\"sort\"].upper() == \"DESC\":\n                pieces.append(b\"DESC\")\n            else:\n                raise DataError(\"GEOSEARCH invalid sort\")\n\n        # count any\n        if kwargs[\"count\"]:\n            pieces.extend([b\"COUNT\", kwargs[\"count\"]])\n            if kwargs[\"any\"]:\n                pieces.append(b\"ANY\")\n        elif kwargs[\"any\"]:\n            raise DataError(\"GEOSEARCH ``any`` can't be provided without count\")\n\n        # other properties\n        for arg_name, byte_repr in (\n            (\"withdist\", b\"WITHDIST\"),\n            (\"withcoord\", b\"WITHCOORD\"),\n            (\"withhash\", b\"WITHHASH\"),\n            (\"store_dist\", b\"STOREDIST\"),\n        ):\n            if kwargs[arg_name]:\n                pieces.append(byte_repr)\n\n        kwargs[\"keys\"] = [args[0] if command == \"GEOSEARCH\" else args[1]]\n\n        return self.execute_command(command, *pieces, **kwargs)\n\n\nAsyncGeoCommands = GeoCommands\n\n\nclass ModuleCommands(CommandsProtocol):\n    \"\"\"\n    Redis Module commands.\n    see: https://redis.io/topics/modules-intro\n    \"\"\"\n\n    def module_load(self, path, *args) -> ResponseT:\n        \"\"\"\n        Loads the module from ``path``.\n        Passes all ``*args`` to the module, during loading.\n        Raises ``ModuleError`` if a module is not found at ``path``.\n\n        For more information see https://redis.io/commands/module-load\n        \"\"\"\n        return self.execute_command(\"MODULE LOAD\", path, *args)\n\n    def module_loadex(\n        self,\n        path: str,\n        options: Optional[List[str]] = None,\n        args: Optional[List[str]] = None,\n    ) -> ResponseT:\n        \"\"\"\n        Loads a module from a dynamic library at runtime with configuration directives.\n\n        For more information see https://redis.io/commands/module-loadex\n        \"\"\"\n        pieces = []\n        if options is not None:\n            pieces.append(\"CONFIG\")\n            pieces.extend(options)\n        if args is not None:\n            pieces.append(\"ARGS\")\n            pieces.extend(args)\n\n        return self.execute_command(\"MODULE LOADEX\", path, *pieces)\n\n    def module_unload(self, name) -> ResponseT:\n        \"\"\"\n        Unloads the module ``name``.\n        Raises ``ModuleError`` if ``name`` is not in loaded modules.\n\n        For more information see https://redis.io/commands/module-unload\n        \"\"\"\n        return self.execute_command(\"MODULE UNLOAD\", name)\n\n    def module_list(self) -> ResponseT:\n        \"\"\"\n        Returns a list of dictionaries containing the name and version of\n        all loaded modules.\n\n        For more information see https://redis.io/commands/module-list\n        \"\"\"\n        return self.execute_command(\"MODULE LIST\")\n\n    def command_info(self) -> None:\n        raise NotImplementedError(\n            \"COMMAND INFO is intentionally not implemented in the client.\"\n        )\n\n    def command_count(self) -> ResponseT:\n        return self.execute_command(\"COMMAND COUNT\")\n\n    def command_getkeys(self, *args) -> ResponseT:\n        return self.execute_command(\"COMMAND GETKEYS\", *args)\n\n    def command(self) -> ResponseT:\n        return self.execute_command(\"COMMAND\")\n\n\nclass AsyncModuleCommands(ModuleCommands):\n    async def command_info(self) -> None:\n        return super().command_info()\n\n\nclass ClusterCommands(CommandsProtocol):\n    \"\"\"\n    Class for Redis Cluster commands\n    \"\"\"\n\n    def cluster(self, cluster_arg, *args, **kwargs) -> ResponseT:\n        return self.execute_command(f\"CLUSTER {cluster_arg.upper()}\", *args, **kwargs)\n\n    def readwrite(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Disables read queries for a connection to a Redis Cluster slave node.\n\n        For more information see https://redis.io/commands/readwrite\n        \"\"\"\n        return self.execute_command(\"READWRITE\", **kwargs)\n\n    def readonly(self, **kwargs) -> ResponseT:\n        \"\"\"\n        Enables read queries for a connection to a Redis Cluster replica node.\n\n        For more information see https://redis.io/commands/readonly\n        \"\"\"\n        return self.execute_command(\"READONLY\", **kwargs)\n\n\nAsyncClusterCommands = ClusterCommands\n\n\nclass FunctionCommands:\n    \"\"\"\n    Redis Function commands\n    \"\"\"\n\n    def function_load(\n        self, code: str, replace: Optional[bool] = False\n    ) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Load a library to Redis.\n        :param code: the source code (must start with\n        Shebang statement that provides a metadata about the library)\n        :param replace: changes the behavior to overwrite the existing library\n        with the new contents.\n        Return the library name that was loaded.\n\n        For more information see https://redis.io/commands/function-load\n        \"\"\"\n        pieces = [\"REPLACE\"] if replace else []\n        pieces.append(code)\n        return self.execute_command(\"FUNCTION LOAD\", *pieces)\n\n    def function_delete(self, library: str) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Delete the library called ``library`` and all its functions.\n\n        For more information see https://redis.io/commands/function-delete\n        \"\"\"\n        return self.execute_command(\"FUNCTION DELETE\", library)\n\n    def function_flush(self, mode: str = \"SYNC\") -> Union[Awaitable[str], str]:\n        \"\"\"\n        Deletes all the libraries.\n\n        For more information see https://redis.io/commands/function-flush\n        \"\"\"\n        return self.execute_command(\"FUNCTION FLUSH\", mode)\n\n    def function_list(\n        self, library: Optional[str] = \"*\", withcode: Optional[bool] = False\n    ) -> Union[Awaitable[List], List]:\n        \"\"\"\n        Return information about the functions and libraries.\n\n        Args:\n\n            library: specify a pattern for matching library names\n            withcode: cause the server to include the libraries source implementation\n                in the reply\n        \"\"\"\n        args = [\"LIBRARYNAME\", library]\n        if withcode:\n            args.append(\"WITHCODE\")\n        return self.execute_command(\"FUNCTION LIST\", *args)\n\n    def _fcall(\n        self, command: str, function, numkeys: int, *keys_and_args: Any\n    ) -> Union[Awaitable[str], str]:\n        return self.execute_command(command, function, numkeys, *keys_and_args)\n\n    def fcall(\n        self, function, numkeys: int, *keys_and_args: Any\n    ) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Invoke a function.\n\n        For more information see https://redis.io/commands/fcall\n        \"\"\"\n        return self._fcall(\"FCALL\", function, numkeys, *keys_and_args)\n\n    def fcall_ro(\n        self, function, numkeys: int, *keys_and_args: Any\n    ) -> Union[Awaitable[str], str]:\n        \"\"\"\n        This is a read-only variant of the FCALL command that cannot\n        execute commands that modify data.\n\n        For more information see https://redis.io/commands/fcall_ro\n        \"\"\"\n        return self._fcall(\"FCALL_RO\", function, numkeys, *keys_and_args)\n\n    def function_dump(self) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Return the serialized payload of loaded libraries.\n\n        For more information see https://redis.io/commands/function-dump\n        \"\"\"\n        from redis.client import NEVER_DECODE\n\n        options = {}\n        options[NEVER_DECODE] = []\n\n        return self.execute_command(\"FUNCTION DUMP\", **options)\n\n    def function_restore(\n        self, payload: str, policy: Optional[str] = \"APPEND\"\n    ) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Restore libraries from the serialized ``payload``.\n        You can use the optional policy argument to provide a policy\n        for handling existing libraries.\n\n        For more information see https://redis.io/commands/function-restore\n        \"\"\"\n        return self.execute_command(\"FUNCTION RESTORE\", payload, policy)\n\n    def function_kill(self) -> Union[Awaitable[str], str]:\n        \"\"\"\n        Kill a function that is currently executing.\n\n        For more information see https://redis.io/commands/function-kill\n        \"\"\"\n        return self.execute_command(\"FUNCTION KILL\")\n\n    def function_stats(self) -> Union[Awaitable[List], List]:\n        \"\"\"\n        Return information about the function that's currently running\n        and information about the available execution engines.\n\n        For more information see https://redis.io/commands/function-stats\n        \"\"\"\n        return self.execute_command(\"FUNCTION STATS\")\n\n\nAsyncFunctionCommands = FunctionCommands\n\n\nclass DataAccessCommands(\n    BasicKeyCommands,\n    HyperlogCommands,\n    HashCommands,\n    GeoCommands,\n    ListCommands,\n    ScanCommands,\n    SetCommands,\n    StreamCommands,\n    SortedSetCommands,\n):\n    \"\"\"\n    A class containing all of the implemented data access redis commands.\n    This class is to be used as a mixin for synchronous Redis clients.\n    \"\"\"\n\n\nclass AsyncDataAccessCommands(\n    AsyncBasicKeyCommands,\n    AsyncHyperlogCommands,\n    AsyncHashCommands,\n    AsyncGeoCommands,\n    AsyncListCommands,\n    AsyncScanCommands,\n    AsyncSetCommands,\n    AsyncStreamCommands,\n    AsyncSortedSetCommands,\n):\n    \"\"\"\n    A class containing all of the implemented data access redis commands.\n    This class is to be used as a mixin for asynchronous Redis clients.\n    \"\"\"\n\n\nclass CoreCommands(\n    ACLCommands,\n    ClusterCommands,\n    DataAccessCommands,\n    ManagementCommands,\n    ModuleCommands,\n    PubSubCommands,\n    ScriptCommands,\n    FunctionCommands,\n):\n    \"\"\"\n    A class containing all of the implemented redis commands. This class is\n    to be used as a mixin for synchronous Redis clients.\n    \"\"\"\n\n\nclass AsyncCoreCommands(\n    AsyncACLCommands,\n    AsyncClusterCommands,\n    AsyncDataAccessCommands,\n    AsyncManagementCommands,\n    AsyncModuleCommands,\n    AsyncPubSubCommands,\n    AsyncScriptCommands,\n    AsyncFunctionCommands,\n):\n    \"\"\"\n    A class containing all of the implemented redis commands. This class is\n    to be used as a mixin for asynchronous Redis clients.\n    \"\"\"\n", 6734], "/usr/local/lib/python3.11/site-packages/redis/utils.py": ["import datetime\nimport logging\nimport textwrap\nfrom collections.abc import Callable\nfrom contextlib import contextmanager\nfrom functools import wraps\nfrom typing import Any, Dict, List, Mapping, Optional, TypeVar, Union\n\nfrom redis.exceptions import DataError\nfrom redis.typing import AbsExpiryT, EncodableT, ExpiryT\n\ntry:\n    import hiredis  # noqa\n\n    # Only support Hiredis >= 3.0:\n    hiredis_version = hiredis.__version__.split(\".\")\n    HIREDIS_AVAILABLE = int(hiredis_version[0]) > 3 or (\n        int(hiredis_version[0]) == 3 and int(hiredis_version[1]) >= 2\n    )\n    if not HIREDIS_AVAILABLE:\n        raise ImportError(\"hiredis package should be >= 3.2.0\")\nexcept ImportError:\n    HIREDIS_AVAILABLE = False\n\ntry:\n    import ssl  # noqa\n\n    SSL_AVAILABLE = True\nexcept ImportError:\n    SSL_AVAILABLE = False\n\ntry:\n    import cryptography  # noqa\n\n    CRYPTOGRAPHY_AVAILABLE = True\nexcept ImportError:\n    CRYPTOGRAPHY_AVAILABLE = False\n\nfrom importlib import metadata\n\n\ndef from_url(url, **kwargs):\n    \"\"\"\n    Returns an active Redis client generated from the given database URL.\n\n    Will attempt to extract the database id from the path url fragment, if\n    none is provided.\n    \"\"\"\n    from redis.client import Redis\n\n    return Redis.from_url(url, **kwargs)\n\n\n@contextmanager\ndef pipeline(redis_obj):\n    p = redis_obj.pipeline()\n    yield p\n    p.execute()\n\n\ndef str_if_bytes(value: Union[str, bytes]) -> str:\n    return (\n        value.decode(\"utf-8\", errors=\"replace\") if isinstance(value, bytes) else value\n    )\n\n\ndef safe_str(value):\n    return str(str_if_bytes(value))\n\n\ndef dict_merge(*dicts: Mapping[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Merge all provided dicts into 1 dict.\n    *dicts : `dict`\n        dictionaries to merge\n    \"\"\"\n    merged = {}\n\n    for d in dicts:\n        merged.update(d)\n\n    return merged\n\n\ndef list_keys_to_dict(key_list, callback):\n    return dict.fromkeys(key_list, callback)\n\n\ndef merge_result(command, res):\n    \"\"\"\n    Merge all items in `res` into a list.\n\n    This command is used when sending a command to multiple nodes\n    and the result from each node should be merged into a single list.\n\n    res : 'dict'\n    \"\"\"\n    result = set()\n\n    for v in res.values():\n        for value in v:\n            result.add(value)\n\n    return list(result)\n\n\ndef warn_deprecated(name, reason=\"\", version=\"\", stacklevel=2):\n    import warnings\n\n    msg = f\"Call to deprecated {name}.\"\n    if reason:\n        msg += f\" ({reason})\"\n    if version:\n        msg += f\" -- Deprecated since version {version}.\"\n    warnings.warn(msg, category=DeprecationWarning, stacklevel=stacklevel)\n\n\ndef deprecated_function(reason=\"\", version=\"\", name=None):\n    \"\"\"\n    Decorator to mark a function as deprecated.\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            warn_deprecated(name or func.__name__, reason, version, stacklevel=3)\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n\n\ndef warn_deprecated_arg_usage(\n    arg_name: Union[list, str],\n    function_name: str,\n    reason: str = \"\",\n    version: str = \"\",\n    stacklevel: int = 2,\n):\n    import warnings\n\n    msg = (\n        f\"Call to '{function_name}' function with deprecated\"\n        f\" usage of input argument/s '{arg_name}'.\"\n    )\n    if reason:\n        msg += f\" ({reason})\"\n    if version:\n        msg += f\" -- Deprecated since version {version}.\"\n    warnings.warn(msg, category=DeprecationWarning, stacklevel=stacklevel)\n\n\nC = TypeVar(\"C\", bound=Callable)\n\n\ndef deprecated_args(\n    args_to_warn: list = [\"*\"],\n    allowed_args: list = [],\n    reason: str = \"\",\n    version: str = \"\",\n) -> Callable[[C], C]:\n    \"\"\"\n    Decorator to mark specified args of a function as deprecated.\n    If '*' is in args_to_warn, all arguments will be marked as deprecated.\n    \"\"\"\n\n    def decorator(func: C) -> C:\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            # Get function argument names\n            arg_names = func.__code__.co_varnames[: func.__code__.co_argcount]\n\n            provided_args = dict(zip(arg_names, args))\n            provided_args.update(kwargs)\n\n            provided_args.pop(\"self\", None)\n            for allowed_arg in allowed_args:\n                provided_args.pop(allowed_arg, None)\n\n            for arg in args_to_warn:\n                if arg == \"*\" and len(provided_args) > 0:\n                    warn_deprecated_arg_usage(\n                        list(provided_args.keys()),\n                        func.__name__,\n                        reason,\n                        version,\n                        stacklevel=3,\n                    )\n                elif arg in provided_args:\n                    warn_deprecated_arg_usage(\n                        arg, func.__name__, reason, version, stacklevel=3\n                    )\n\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n\n\ndef _set_info_logger():\n    \"\"\"\n    Set up a logger that log info logs to stdout.\n    (This is used by the default push response handler)\n    \"\"\"\n    if \"push_response\" not in logging.root.manager.loggerDict.keys():\n        logger = logging.getLogger(\"push_response\")\n        logger.setLevel(logging.INFO)\n        handler = logging.StreamHandler()\n        handler.setLevel(logging.INFO)\n        logger.addHandler(handler)\n\n\ndef get_lib_version():\n    try:\n        libver = metadata.version(\"redis\")\n    except metadata.PackageNotFoundError:\n        libver = \"99.99.99\"\n    return libver\n\n\ndef format_error_message(host_error: str, exception: BaseException) -> str:\n    if not exception.args:\n        return f\"Error connecting to {host_error}.\"\n    elif len(exception.args) == 1:\n        return f\"Error {exception.args[0]} connecting to {host_error}.\"\n    else:\n        return (\n            f\"Error {exception.args[0]} connecting to {host_error}. \"\n            f\"{exception.args[1]}.\"\n        )\n\n\ndef compare_versions(version1: str, version2: str) -> int:\n    \"\"\"\n    Compare two versions.\n\n    :return: -1 if version1 > version2\n             0 if both versions are equal\n             1 if version1 < version2\n    \"\"\"\n\n    num_versions1 = list(map(int, version1.split(\".\")))\n    num_versions2 = list(map(int, version2.split(\".\")))\n\n    if len(num_versions1) > len(num_versions2):\n        diff = len(num_versions1) - len(num_versions2)\n        for _ in range(diff):\n            num_versions2.append(0)\n    elif len(num_versions1) < len(num_versions2):\n        diff = len(num_versions2) - len(num_versions1)\n        for _ in range(diff):\n            num_versions1.append(0)\n\n    for i, ver in enumerate(num_versions1):\n        if num_versions1[i] > num_versions2[i]:\n            return -1\n        elif num_versions1[i] < num_versions2[i]:\n            return 1\n\n    return 0\n\n\ndef ensure_string(key):\n    if isinstance(key, bytes):\n        return key.decode(\"utf-8\")\n    elif isinstance(key, str):\n        return key\n    else:\n        raise TypeError(\"Key must be either a string or bytes\")\n\n\ndef extract_expire_flags(\n    ex: Optional[ExpiryT] = None,\n    px: Optional[ExpiryT] = None,\n    exat: Optional[AbsExpiryT] = None,\n    pxat: Optional[AbsExpiryT] = None,\n) -> List[EncodableT]:\n    exp_options: list[EncodableT] = []\n    if ex is not None:\n        exp_options.append(\"EX\")\n        if isinstance(ex, datetime.timedelta):\n            exp_options.append(int(ex.total_seconds()))\n        elif isinstance(ex, int):\n            exp_options.append(ex)\n        elif isinstance(ex, str) and ex.isdigit():\n            exp_options.append(int(ex))\n        else:\n            raise DataError(\"ex must be datetime.timedelta or int\")\n    elif px is not None:\n        exp_options.append(\"PX\")\n        if isinstance(px, datetime.timedelta):\n            exp_options.append(int(px.total_seconds() * 1000))\n        elif isinstance(px, int):\n            exp_options.append(px)\n        else:\n            raise DataError(\"px must be datetime.timedelta or int\")\n    elif exat is not None:\n        if isinstance(exat, datetime.datetime):\n            exat = int(exat.timestamp())\n        exp_options.extend([\"EXAT\", exat])\n    elif pxat is not None:\n        if isinstance(pxat, datetime.datetime):\n            pxat = int(pxat.timestamp() * 1000)\n        exp_options.extend([\"PXAT\", pxat])\n\n    return exp_options\n\n\ndef truncate_text(txt, max_length=100):\n    return textwrap.shorten(\n        text=txt, width=max_length, placeholder=\"...\", break_long_words=True\n    )\n", 314], "/usr/local/lib/python3.11/asyncio/timeouts.py": ["import enum\n\nfrom types import TracebackType\nfrom typing import final, Optional, Type\n\nfrom . import events\nfrom . import exceptions\nfrom . import tasks\n\n\n__all__ = (\n    \"Timeout\",\n    \"timeout\",\n    \"timeout_at\",\n)\n\n\nclass _State(enum.Enum):\n    CREATED = \"created\"\n    ENTERED = \"active\"\n    EXPIRING = \"expiring\"\n    EXPIRED = \"expired\"\n    EXITED = \"finished\"\n\n\n@final\nclass Timeout:\n    \"\"\"Asynchronous context manager for cancelling overdue coroutines.\n\n    Use `timeout()` or `timeout_at()` rather than instantiating this class directly.\n    \"\"\"\n\n    def __init__(self, when: Optional[float]) -> None:\n        \"\"\"Schedule a timeout that will trigger at a given loop time.\n\n        - If `when` is `None`, the timeout will never trigger.\n        - If `when < loop.time()`, the timeout will trigger on the next\n          iteration of the event loop.\n        \"\"\"\n        self._state = _State.CREATED\n\n        self._timeout_handler: Optional[events.TimerHandle] = None\n        self._task: Optional[tasks.Task] = None\n        self._when = when\n\n    def when(self) -> Optional[float]:\n        \"\"\"Return the current deadline.\"\"\"\n        return self._when\n\n    def reschedule(self, when: Optional[float]) -> None:\n        \"\"\"Reschedule the timeout.\"\"\"\n        if self._state is not _State.ENTERED:\n            if self._state is _State.CREATED:\n                raise RuntimeError(\"Timeout has not been entered\")\n            raise RuntimeError(\n                f\"Cannot change state of {self._state.value} Timeout\",\n            )\n\n        self._when = when\n\n        if self._timeout_handler is not None:\n            self._timeout_handler.cancel()\n\n        if when is None:\n            self._timeout_handler = None\n        else:\n            loop = events.get_running_loop()\n            if when <= loop.time():\n                self._timeout_handler = loop.call_soon(self._on_timeout)\n            else:\n                self._timeout_handler = loop.call_at(when, self._on_timeout)\n\n    def expired(self) -> bool:\n        \"\"\"Is timeout expired during execution?\"\"\"\n        return self._state in (_State.EXPIRING, _State.EXPIRED)\n\n    def __repr__(self) -> str:\n        info = ['']\n        if self._state is _State.ENTERED:\n            when = round(self._when, 3) if self._when is not None else None\n            info.append(f\"when={when}\")\n        info_str = ' '.join(info)\n        return f\"<Timeout [{self._state.value}]{info_str}>\"\n\n    async def __aenter__(self) -> \"Timeout\":\n        if self._state is not _State.CREATED:\n            raise RuntimeError(\"Timeout has already been entered\")\n        task = tasks.current_task()\n        if task is None:\n            raise RuntimeError(\"Timeout should be used inside a task\")\n        self._state = _State.ENTERED\n        self._task = task\n        self._cancelling = self._task.cancelling()\n        self.reschedule(self._when)\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> Optional[bool]:\n        assert self._state in (_State.ENTERED, _State.EXPIRING)\n\n        if self._timeout_handler is not None:\n            self._timeout_handler.cancel()\n            self._timeout_handler = None\n\n        if self._state is _State.EXPIRING:\n            self._state = _State.EXPIRED\n\n            if self._task.uncancel() <= self._cancelling and exc_type is exceptions.CancelledError:\n                # Since there are no new cancel requests, we're\n                # handling this.\n                raise TimeoutError from exc_val\n        elif self._state is _State.ENTERED:\n            self._state = _State.EXITED\n\n        return None\n\n    def _on_timeout(self) -> None:\n        assert self._state is _State.ENTERED\n        self._task.cancel()\n        self._state = _State.EXPIRING\n        # drop the reference early\n        self._timeout_handler = None\n\n\ndef timeout(delay: Optional[float]) -> Timeout:\n    \"\"\"Timeout async context manager.\n\n    Useful in cases when you want to apply timeout logic around block\n    of code or in cases when asyncio.wait_for is not suitable. For example:\n\n    >>> async with asyncio.timeout(10):  # 10 seconds timeout\n    ...     await long_running_task()\n\n\n    delay - value in seconds or None to disable timeout logic\n\n    long_running_task() is interrupted by raising asyncio.CancelledError,\n    the top-most affected timeout() context manager converts CancelledError\n    into TimeoutError.\n    \"\"\"\n    loop = events.get_running_loop()\n    return Timeout(loop.time() + delay if delay is not None else None)\n\n\ndef timeout_at(when: Optional[float]) -> Timeout:\n    \"\"\"Schedule the timeout at absolute time.\n\n    Like timeout() but argument gives absolute time in the same clock system\n    as loop.time().\n\n    Please note: it is not POSIX time but a time with\n    undefined starting base, e.g. the time of the system power on.\n\n    >>> async with asyncio.timeout_at(loop.time() + 10):\n    ...     await long_running_task()\n\n\n    when - a deadline when timeout occurs or None to disable timeout logic\n\n    long_running_task() is interrupted by raising asyncio.CancelledError,\n    the top-most affected timeout() context manager converts CancelledError\n    into TimeoutError.\n    \"\"\"\n    return Timeout(when)\n", 168], "/usr/local/lib/python3.11/site-packages/redis/backoff.py": ["import random\nfrom abc import ABC, abstractmethod\n\n# Maximum backoff between each retry in seconds\nDEFAULT_CAP = 0.512\n# Minimum backoff between each retry in seconds\nDEFAULT_BASE = 0.008\n\n\nclass AbstractBackoff(ABC):\n    \"\"\"Backoff interface\"\"\"\n\n    def reset(self):\n        \"\"\"\n        Reset internal state before an operation.\n        `reset` is called once at the beginning of\n        every call to `Retry.call_with_retry`\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def compute(self, failures: int) -> float:\n        \"\"\"Compute backoff in seconds upon failure\"\"\"\n        pass\n\n\nclass ConstantBackoff(AbstractBackoff):\n    \"\"\"Constant backoff upon failure\"\"\"\n\n    def __init__(self, backoff: float) -> None:\n        \"\"\"`backoff`: backoff time in seconds\"\"\"\n        self._backoff = backoff\n\n    def __hash__(self) -> int:\n        return hash((self._backoff,))\n\n    def __eq__(self, other) -> bool:\n        if not isinstance(other, ConstantBackoff):\n            return NotImplemented\n\n        return self._backoff == other._backoff\n\n    def compute(self, failures: int) -> float:\n        return self._backoff\n\n\nclass NoBackoff(ConstantBackoff):\n    \"\"\"No backoff upon failure\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__(0)\n\n\nclass ExponentialBackoff(AbstractBackoff):\n    \"\"\"Exponential backoff upon failure\"\"\"\n\n    def __init__(self, cap: float = DEFAULT_CAP, base: float = DEFAULT_BASE):\n        \"\"\"\n        `cap`: maximum backoff time in seconds\n        `base`: base backoff time in seconds\n        \"\"\"\n        self._cap = cap\n        self._base = base\n\n    def __hash__(self) -> int:\n        return hash((self._base, self._cap))\n\n    def __eq__(self, other) -> bool:\n        if not isinstance(other, ExponentialBackoff):\n            return NotImplemented\n\n        return self._base == other._base and self._cap == other._cap\n\n    def compute(self, failures: int) -> float:\n        return min(self._cap, self._base * 2**failures)\n\n\nclass FullJitterBackoff(AbstractBackoff):\n    \"\"\"Full jitter backoff upon failure\"\"\"\n\n    def __init__(self, cap: float = DEFAULT_CAP, base: float = DEFAULT_BASE) -> None:\n        \"\"\"\n        `cap`: maximum backoff time in seconds\n        `base`: base backoff time in seconds\n        \"\"\"\n        self._cap = cap\n        self._base = base\n\n    def __hash__(self) -> int:\n        return hash((self._base, self._cap))\n\n    def __eq__(self, other) -> bool:\n        if not isinstance(other, FullJitterBackoff):\n            return NotImplemented\n\n        return self._base == other._base and self._cap == other._cap\n\n    def compute(self, failures: int) -> float:\n        return random.uniform(0, min(self._cap, self._base * 2**failures))\n\n\nclass EqualJitterBackoff(AbstractBackoff):\n    \"\"\"Equal jitter backoff upon failure\"\"\"\n\n    def __init__(self, cap: float = DEFAULT_CAP, base: float = DEFAULT_BASE) -> None:\n        \"\"\"\n        `cap`: maximum backoff time in seconds\n        `base`: base backoff time in seconds\n        \"\"\"\n        self._cap = cap\n        self._base = base\n\n    def __hash__(self) -> int:\n        return hash((self._base, self._cap))\n\n    def __eq__(self, other) -> bool:\n        if not isinstance(other, EqualJitterBackoff):\n            return NotImplemented\n\n        return self._base == other._base and self._cap == other._cap\n\n    def compute(self, failures: int) -> float:\n        temp = min(self._cap, self._base * 2**failures) / 2\n        return temp + random.uniform(0, temp)\n\n\nclass DecorrelatedJitterBackoff(AbstractBackoff):\n    \"\"\"Decorrelated jitter backoff upon failure\"\"\"\n\n    def __init__(self, cap: float = DEFAULT_CAP, base: float = DEFAULT_BASE) -> None:\n        \"\"\"\n        `cap`: maximum backoff time in seconds\n        `base`: base backoff time in seconds\n        \"\"\"\n        self._cap = cap\n        self._base = base\n        self._previous_backoff = 0\n\n    def __hash__(self) -> int:\n        return hash((self._base, self._cap))\n\n    def __eq__(self, other) -> bool:\n        if not isinstance(other, DecorrelatedJitterBackoff):\n            return NotImplemented\n\n        return self._base == other._base and self._cap == other._cap\n\n    def reset(self) -> None:\n        self._previous_backoff = 0\n\n    def compute(self, failures: int) -> float:\n        max_backoff = max(self._base, self._previous_backoff * 3)\n        temp = random.uniform(self._base, max_backoff)\n        self._previous_backoff = min(self._cap, temp)\n        return self._previous_backoff\n\n\nclass ExponentialWithJitterBackoff(AbstractBackoff):\n    \"\"\"Exponential backoff upon failure, with jitter\"\"\"\n\n    def __init__(self, cap: float = DEFAULT_CAP, base: float = DEFAULT_BASE) -> None:\n        \"\"\"\n        `cap`: maximum backoff time in seconds\n        `base`: base backoff time in seconds\n        \"\"\"\n        self._cap = cap\n        self._base = base\n\n    def __hash__(self) -> int:\n        return hash((self._base, self._cap))\n\n    def __eq__(self, other) -> bool:\n        if not isinstance(other, ExponentialWithJitterBackoff):\n            return NotImplemented\n\n        return self._base == other._base and self._cap == other._cap\n\n    def compute(self, failures: int) -> float:\n        return min(self._cap, random.random() * self._base * 2**failures)\n\n\ndef default_backoff():\n    return EqualJitterBackoff()\n", 183], "/usr/local/lib/python3.11/site-packages/redis/_parsers/encoders.py": ["from ..exceptions import DataError\n\n\nclass Encoder:\n    \"Encode strings to bytes-like and decode bytes-like to strings\"\n\n    __slots__ = \"encoding\", \"encoding_errors\", \"decode_responses\"\n\n    def __init__(self, encoding, encoding_errors, decode_responses):\n        self.encoding = encoding\n        self.encoding_errors = encoding_errors\n        self.decode_responses = decode_responses\n\n    def encode(self, value):\n        \"Return a bytestring or bytes-like representation of the value\"\n        if isinstance(value, (bytes, memoryview)):\n            return value\n        elif isinstance(value, bool):\n            # special case bool since it is a subclass of int\n            raise DataError(\n                \"Invalid input of type: 'bool'. Convert to a \"\n                \"bytes, string, int or float first.\"\n            )\n        elif isinstance(value, (int, float)):\n            value = repr(value).encode()\n        elif not isinstance(value, str):\n            # a value we don't know how to deal with. throw an error\n            typename = type(value).__name__\n            raise DataError(\n                f\"Invalid input of type: '{typename}'. \"\n                f\"Convert to a bytes, string, int or float first.\"\n            )\n        if isinstance(value, str):\n            value = value.encode(self.encoding, self.encoding_errors)\n        return value\n\n    def decode(self, value, force=False):\n        \"Return a unicode string from the bytes-like representation\"\n        if self.decode_responses or force:\n            if isinstance(value, memoryview):\n                value = value.tobytes()\n            if isinstance(value, bytes):\n                value = value.decode(self.encoding, self.encoding_errors)\n        return value\n", 44], "/usr/local/lib/python3.11/email/utils.py": ["# Copyright (C) 2001-2010 Python Software Foundation\n# Author: Barry Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"Miscellaneous utilities.\"\"\"\n\n__all__ = [\n    'collapse_rfc2231_value',\n    'decode_params',\n    'decode_rfc2231',\n    'encode_rfc2231',\n    'formataddr',\n    'formatdate',\n    'format_datetime',\n    'getaddresses',\n    'make_msgid',\n    'mktime_tz',\n    'parseaddr',\n    'parsedate',\n    'parsedate_tz',\n    'parsedate_to_datetime',\n    'unquote',\n    ]\n\nimport os\nimport re\nimport time\nimport random\nimport socket\nimport datetime\nimport urllib.parse\n\nfrom email._parseaddr import quote\nfrom email._parseaddr import AddressList as _AddressList\nfrom email._parseaddr import mktime_tz\n\nfrom email._parseaddr import parsedate, parsedate_tz, _parsedate_tz\n\n# Intrapackage imports\nfrom email.charset import Charset\n\nCOMMASPACE = ', '\nEMPTYSTRING = ''\nUEMPTYSTRING = ''\nCRLF = '\\r\\n'\nTICK = \"'\"\n\nspecialsre = re.compile(r'[][\\\\()<>@,:;\".]')\nescapesre = re.compile(r'[\\\\\"]')\n\n\ndef _has_surrogates(s):\n    \"\"\"Return True if s may contain surrogate-escaped binary data.\"\"\"\n    # This check is based on the fact that unless there are surrogates, utf8\n    # (Python's default encoding) can encode any string.  This is the fastest\n    # way to check for surrogates, see bpo-11454 (moved to gh-55663) for timings.\n    try:\n        s.encode()\n        return False\n    except UnicodeEncodeError:\n        return True\n\n# How to deal with a string containing bytes before handing it to the\n# application through the 'normal' interface.\ndef _sanitize(string):\n    # Turn any escaped bytes into unicode 'unknown' char.  If the escaped\n    # bytes happen to be utf-8 they will instead get decoded, even if they\n    # were invalid in the charset the source was supposed to be in.  This\n    # seems like it is not a bad thing; a defect was still registered.\n    original_bytes = string.encode('utf-8', 'surrogateescape')\n    return original_bytes.decode('utf-8', 'replace')\n\n\n\n# Helpers\n\ndef formataddr(pair, charset='utf-8'):\n    \"\"\"The inverse of parseaddr(), this takes a 2-tuple of the form\n    (realname, email_address) and returns the string value suitable\n    for an RFC 2822 From, To or Cc header.\n\n    If the first element of pair is false, then the second element is\n    returned unmodified.\n\n    The optional charset is the character set that is used to encode\n    realname in case realname is not ASCII safe.  Can be an instance of str or\n    a Charset-like object which has a header_encode method.  Default is\n    'utf-8'.\n    \"\"\"\n    name, address = pair\n    # The address MUST (per RFC) be ascii, so raise a UnicodeError if it isn't.\n    address.encode('ascii')\n    if name:\n        try:\n            name.encode('ascii')\n        except UnicodeEncodeError:\n            if isinstance(charset, str):\n                charset = Charset(charset)\n            encoded_name = charset.header_encode(name)\n            return \"%s <%s>\" % (encoded_name, address)\n        else:\n            quotes = ''\n            if specialsre.search(name):\n                quotes = '\"'\n            name = escapesre.sub(r'\\\\\\g<0>', name)\n            return '%s%s%s <%s>' % (quotes, name, quotes, address)\n    return address\n\n\ndef _iter_escaped_chars(addr):\n    pos = 0\n    escape = False\n    for pos, ch in enumerate(addr):\n        if escape:\n            yield (pos, '\\\\' + ch)\n            escape = False\n        elif ch == '\\\\':\n            escape = True\n        else:\n            yield (pos, ch)\n    if escape:\n        yield (pos, '\\\\')\n\n\ndef _strip_quoted_realnames(addr):\n    \"\"\"Strip real names between quotes.\"\"\"\n    if '\"' not in addr:\n        # Fast path\n        return addr\n\n    start = 0\n    open_pos = None\n    result = []\n    for pos, ch in _iter_escaped_chars(addr):\n        if ch == '\"':\n            if open_pos is None:\n                open_pos = pos\n            else:\n                if start != open_pos:\n                    result.append(addr[start:open_pos])\n                start = pos + 1\n                open_pos = None\n\n    if start < len(addr):\n        result.append(addr[start:])\n\n    return ''.join(result)\n\n\nsupports_strict_parsing = True\n\ndef getaddresses(fieldvalues, *, strict=True):\n    \"\"\"Return a list of (REALNAME, EMAIL) or ('','') for each fieldvalue.\n\n    When parsing fails for a fieldvalue, a 2-tuple of ('', '') is returned in\n    its place.\n\n    If strict is true, use a strict parser which rejects malformed inputs.\n    \"\"\"\n\n    # If strict is true, if the resulting list of parsed addresses is greater\n    # than the number of fieldvalues in the input list, a parsing error has\n    # occurred and consequently a list containing a single empty 2-tuple [('',\n    # '')] is returned in its place. This is done to avoid invalid output.\n    #\n    # Malformed input: getaddresses(['alice@example.com <bob@example.com>'])\n    # Invalid output: [('', 'alice@example.com'), ('', 'bob@example.com')]\n    # Safe output: [('', '')]\n\n    if not strict:\n        all = COMMASPACE.join(str(v) for v in fieldvalues)\n        a = _AddressList(all)\n        return a.addresslist\n\n    fieldvalues = [str(v) for v in fieldvalues]\n    fieldvalues = _pre_parse_validation(fieldvalues)\n    addr = COMMASPACE.join(fieldvalues)\n    a = _AddressList(addr)\n    result = _post_parse_validation(a.addresslist)\n\n    # Treat output as invalid if the number of addresses is not equal to the\n    # expected number of addresses.\n    n = 0\n    for v in fieldvalues:\n        # When a comma is used in the Real Name part it is not a deliminator.\n        # So strip those out before counting the commas.\n        v = _strip_quoted_realnames(v)\n        # Expected number of addresses: 1 + number of commas\n        n += 1 + v.count(',')\n    if len(result) != n:\n        return [('', '')]\n\n    return result\n\n\ndef _check_parenthesis(addr):\n    # Ignore parenthesis in quoted real names.\n    addr = _strip_quoted_realnames(addr)\n\n    opens = 0\n    for pos, ch in _iter_escaped_chars(addr):\n        if ch == '(':\n            opens += 1\n        elif ch == ')':\n            opens -= 1\n            if opens < 0:\n                return False\n    return (opens == 0)\n\n\ndef _pre_parse_validation(email_header_fields):\n    accepted_values = []\n    for v in email_header_fields:\n        if not _check_parenthesis(v):\n            v = \"('', '')\"\n        accepted_values.append(v)\n\n    return accepted_values\n\n\ndef _post_parse_validation(parsed_email_header_tuples):\n    accepted_values = []\n    # The parser would have parsed a correctly formatted domain-literal\n    # The existence of an [ after parsing indicates a parsing failure\n    for v in parsed_email_header_tuples:\n        if '[' in v[1]:\n            v = ('', '')\n        accepted_values.append(v)\n\n    return accepted_values\n\n\ndef _format_timetuple_and_zone(timetuple, zone):\n    return '%s, %02d %s %04d %02d:%02d:%02d %s' % (\n        ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'][timetuple[6]],\n        timetuple[2],\n        ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n         'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'][timetuple[1] - 1],\n        timetuple[0], timetuple[3], timetuple[4], timetuple[5],\n        zone)\n\ndef formatdate(timeval=None, localtime=False, usegmt=False):\n    \"\"\"Returns a date string as specified by RFC 2822, e.g.:\n\n    Fri, 09 Nov 2001 01:08:47 -0000\n\n    Optional timeval if given is a floating point time value as accepted by\n    gmtime() and localtime(), otherwise the current time is used.\n\n    Optional localtime is a flag that when True, interprets timeval, and\n    returns a date relative to the local timezone instead of UTC, properly\n    taking daylight savings time into account.\n\n    Optional argument usegmt means that the timezone is written out as\n    an ascii string, not numeric one (so \"GMT\" instead of \"+0000\"). This\n    is needed for HTTP, and is only used when localtime==False.\n    \"\"\"\n    # Note: we cannot use strftime() because that honors the locale and RFC\n    # 2822 requires that day and month names be the English abbreviations.\n    if timeval is None:\n        timeval = time.time()\n    if localtime or usegmt:\n        dt = datetime.datetime.fromtimestamp(timeval, datetime.timezone.utc)\n    else:\n        dt = datetime.datetime.utcfromtimestamp(timeval)\n    if localtime:\n        dt = dt.astimezone()\n        usegmt = False\n    return format_datetime(dt, usegmt)\n\ndef format_datetime(dt, usegmt=False):\n    \"\"\"Turn a datetime into a date string as specified in RFC 2822.\n\n    If usegmt is True, dt must be an aware datetime with an offset of zero.  In\n    this case 'GMT' will be rendered instead of the normal +0000 required by\n    RFC2822.  This is to support HTTP headers involving date stamps.\n    \"\"\"\n    now = dt.timetuple()\n    if usegmt:\n        if dt.tzinfo is None or dt.tzinfo != datetime.timezone.utc:\n            raise ValueError(\"usegmt option requires a UTC datetime\")\n        zone = 'GMT'\n    elif dt.tzinfo is None:\n        zone = '-0000'\n    else:\n        zone = dt.strftime(\"%z\")\n    return _format_timetuple_and_zone(now, zone)\n\n\ndef make_msgid(idstring=None, domain=None):\n    \"\"\"Returns a string suitable for RFC 2822 compliant Message-ID, e.g:\n\n    <142480216486.20800.16526388040877946887@nightshade.la.mastaler.com>\n\n    Optional idstring if given is a string used to strengthen the\n    uniqueness of the message id.  Optional domain if given provides the\n    portion of the message id after the '@'.  It defaults to the locally\n    defined hostname.\n    \"\"\"\n    timeval = int(time.time()*100)\n    pid = os.getpid()\n    randint = random.getrandbits(64)\n    if idstring is None:\n        idstring = ''\n    else:\n        idstring = '.' + idstring\n    if domain is None:\n        domain = socket.getfqdn()\n    msgid = '<%d.%d.%d%s@%s>' % (timeval, pid, randint, idstring, domain)\n    return msgid\n\n\ndef parsedate_to_datetime(data):\n    parsed_date_tz = _parsedate_tz(data)\n    if parsed_date_tz is None:\n        raise ValueError('Invalid date value or format \"%s\"' % str(data))\n    *dtuple, tz = parsed_date_tz\n    if tz is None:\n        return datetime.datetime(*dtuple[:6])\n    return datetime.datetime(*dtuple[:6],\n            tzinfo=datetime.timezone(datetime.timedelta(seconds=tz)))\n\n\ndef parseaddr(addr, *, strict=True):\n    \"\"\"\n    Parse addr into its constituent realname and email address parts.\n\n    Return a tuple of realname and email address, unless the parse fails, in\n    which case return a 2-tuple of ('', '').\n\n    If strict is True, use a strict parser which rejects malformed inputs.\n    \"\"\"\n    if not strict:\n        addrs = _AddressList(addr).addresslist\n        if not addrs:\n            return ('', '')\n        return addrs[0]\n\n    if isinstance(addr, list):\n        addr = addr[0]\n\n    if not isinstance(addr, str):\n        return ('', '')\n\n    addr = _pre_parse_validation([addr])[0]\n    addrs = _post_parse_validation(_AddressList(addr).addresslist)\n\n    if not addrs or len(addrs) > 1:\n        return ('', '')\n\n    return addrs[0]\n\n\n# rfc822.unquote() doesn't properly de-backslash-ify in Python pre-2.3.\ndef unquote(str):\n    \"\"\"Remove quotes from a string.\"\"\"\n    if len(str) > 1:\n        if str.startswith('\"') and str.endswith('\"'):\n            return str[1:-1].replace('\\\\\\\\', '\\\\').replace('\\\\\"', '\"')\n        if str.startswith('<') and str.endswith('>'):\n            return str[1:-1]\n    return str\n\n\n\n# RFC2231-related functions - parameter encoding and decoding\ndef decode_rfc2231(s):\n    \"\"\"Decode string according to RFC 2231\"\"\"\n    parts = s.split(TICK, 2)\n    if len(parts) <= 2:\n        return None, None, s\n    return parts\n\n\ndef encode_rfc2231(s, charset=None, language=None):\n    \"\"\"Encode string according to RFC 2231.\n\n    If neither charset nor language is given, then s is returned as-is.  If\n    charset is given but not language, the string is encoded using the empty\n    string for language.\n    \"\"\"\n    s = urllib.parse.quote(s, safe='', encoding=charset or 'ascii')\n    if charset is None and language is None:\n        return s\n    if language is None:\n        language = ''\n    return \"%s'%s'%s\" % (charset, language, s)\n\n\nrfc2231_continuation = re.compile(r'^(?P<name>\\w+)\\*((?P<num>[0-9]+)\\*?)?$',\n    re.ASCII)\n\ndef decode_params(params):\n    \"\"\"Decode parameters list according to RFC 2231.\n\n    params is a sequence of 2-tuples containing (param name, string value).\n    \"\"\"\n    new_params = [params[0]]\n    # Map parameter's name to a list of continuations.  The values are a\n    # 3-tuple of the continuation number, the string value, and a flag\n    # specifying whether a particular segment is %-encoded.\n    rfc2231_params = {}\n    for name, value in params[1:]:\n        encoded = name.endswith('*')\n        value = unquote(value)\n        mo = rfc2231_continuation.match(name)\n        if mo:\n            name, num = mo.group('name', 'num')\n            if num is not None:\n                num = int(num)\n            rfc2231_params.setdefault(name, []).append((num, value, encoded))\n        else:\n            new_params.append((name, '\"%s\"' % quote(value)))\n    if rfc2231_params:\n        for name, continuations in rfc2231_params.items():\n            value = []\n            extended = False\n            # Sort by number\n            continuations.sort()\n            # And now append all values in numerical order, converting\n            # %-encodings for the encoded segments.  If any of the\n            # continuation names ends in a *, then the entire string, after\n            # decoding segments and concatenating, must have the charset and\n            # language specifiers at the beginning of the string.\n            for num, s, encoded in continuations:\n                if encoded:\n                    # Decode as \"latin-1\", so the characters in s directly\n                    # represent the percent-encoded octet values.\n                    # collapse_rfc2231_value treats this as an octet sequence.\n                    s = urllib.parse.unquote(s, encoding=\"latin-1\")\n                    extended = True\n                value.append(s)\n            value = quote(EMPTYSTRING.join(value))\n            if extended:\n                charset, language, value = decode_rfc2231(value)\n                new_params.append((name, (charset, language, '\"%s\"' % value)))\n            else:\n                new_params.append((name, '\"%s\"' % value))\n    return new_params\n\ndef collapse_rfc2231_value(value, errors='replace',\n                           fallback_charset='us-ascii'):\n    if not isinstance(value, tuple) or len(value) != 3:\n        return unquote(value)\n    # While value comes to us as a unicode string, we need it to be a bytes\n    # object.  We do not want bytes() normal utf-8 decoder, we want a straight\n    # interpretation of the string as character bytes.\n    charset, language, text = value\n    if charset is None:\n        # Issue 17369: if charset/lang is None, decode_rfc2231 couldn't parse\n        # the value, so use the fallback_charset.\n        charset = fallback_charset\n    rawbytes = bytes(text, 'raw-unicode-escape')\n    try:\n        return str(rawbytes, charset, errors)\n    except LookupError:\n        # charset is not a known codec.\n        return unquote(text)\n\n\n#\n# datetime doesn't provide a localtime function yet, so provide one.  Code\n# adapted from the patch in issue 9527.  This may not be perfect, but it is\n# better than not having it.\n#\n\ndef localtime(dt=None, isdst=-1):\n    \"\"\"Return local time as an aware datetime object.\n\n    If called without arguments, return current time.  Otherwise *dt*\n    argument should be a datetime instance, and it is converted to the\n    local time zone according to the system time zone database.  If *dt* is\n    naive (that is, dt.tzinfo is None), it is assumed to be in local time.\n    In this case, a positive or zero value for *isdst* causes localtime to\n    presume initially that summer time (for example, Daylight Saving Time)\n    is or is not (respectively) in effect for the specified time.  A\n    negative value for *isdst* causes the localtime() function to attempt\n    to divine whether summer time is in effect for the specified time.\n\n    \"\"\"\n    if dt is None:\n        return datetime.datetime.now(datetime.timezone.utc).astimezone()\n    if dt.tzinfo is not None:\n        return dt.astimezone()\n    # We have a naive datetime.  Convert to a (localtime) timetuple and pass to\n    # system mktime together with the isdst hint.  System mktime will return\n    # seconds since epoch.\n    tm = dt.timetuple()[:-1] + (isdst,)\n    seconds = time.mktime(tm)\n    localtm = time.localtime(seconds)\n    try:\n        delta = datetime.timedelta(seconds=localtm.tm_gmtoff)\n        tz = datetime.timezone(delta, localtm.tm_zone)\n    except AttributeError:\n        # Compute UTC offset and compare with the value implied by tm_isdst.\n        # If the values match, use the zone name implied by tm_isdst.\n        delta = dt - datetime.datetime(*time.gmtime(seconds)[:6])\n        dst = time.daylight and localtm.tm_isdst > 0\n        gmtoff = -(time.altzone if dst else time.timezone)\n        if delta == datetime.timedelta(seconds=gmtoff):\n            tz = datetime.timezone(delta, time.tzname[dst])\n        else:\n            tz = datetime.timezone(delta)\n    return dt.replace(tzinfo=tz)\n", 504], "/usr/local/lib/python3.11/functools.py": ["\"\"\"functools.py - Tools for working with functions and callable objects\n\"\"\"\n# Python module wrapper for _functools C module\n# to allow utilities written in Python to be added\n# to the functools module.\n# Written by Nick Coghlan <ncoghlan at gmail.com>,\n# Raymond Hettinger <python at rcn.com>,\n# and \u0141ukasz Langa <lukasz at langa.pl>.\n#   Copyright (C) 2006-2013 Python Software Foundation.\n# See C source code for _functools credits/copyright\n\n__all__ = ['update_wrapper', 'wraps', 'WRAPPER_ASSIGNMENTS', 'WRAPPER_UPDATES',\n           'total_ordering', 'cache', 'cmp_to_key', 'lru_cache', 'reduce',\n           'partial', 'partialmethod', 'singledispatch', 'singledispatchmethod',\n           'cached_property']\n\nfrom abc import get_cache_token\nfrom collections import namedtuple\n# import types, weakref  # Deferred to single_dispatch()\nfrom reprlib import recursive_repr\nfrom _thread import RLock\nfrom types import GenericAlias\n\n\n################################################################################\n### update_wrapper() and wraps() decorator\n################################################################################\n\n# update_wrapper() and wraps() are tools to help write\n# wrapper functions that can handle naive introspection\n\nWRAPPER_ASSIGNMENTS = ('__module__', '__name__', '__qualname__', '__doc__',\n                       '__annotations__')\nWRAPPER_UPDATES = ('__dict__',)\ndef update_wrapper(wrapper,\n                   wrapped,\n                   assigned = WRAPPER_ASSIGNMENTS,\n                   updated = WRAPPER_UPDATES):\n    \"\"\"Update a wrapper function to look like the wrapped function\n\n       wrapper is the function to be updated\n       wrapped is the original function\n       assigned is a tuple naming the attributes assigned directly\n       from the wrapped function to the wrapper function (defaults to\n       functools.WRAPPER_ASSIGNMENTS)\n       updated is a tuple naming the attributes of the wrapper that\n       are updated with the corresponding attribute from the wrapped\n       function (defaults to functools.WRAPPER_UPDATES)\n    \"\"\"\n    for attr in assigned:\n        try:\n            value = getattr(wrapped, attr)\n        except AttributeError:\n            pass\n        else:\n            setattr(wrapper, attr, value)\n    for attr in updated:\n        getattr(wrapper, attr).update(getattr(wrapped, attr, {}))\n    # Issue #17482: set __wrapped__ last so we don't inadvertently copy it\n    # from the wrapped function when updating __dict__\n    wrapper.__wrapped__ = wrapped\n    # Return the wrapper so this can be used as a decorator via partial()\n    return wrapper\n\ndef wraps(wrapped,\n          assigned = WRAPPER_ASSIGNMENTS,\n          updated = WRAPPER_UPDATES):\n    \"\"\"Decorator factory to apply update_wrapper() to a wrapper function\n\n       Returns a decorator that invokes update_wrapper() with the decorated\n       function as the wrapper argument and the arguments to wraps() as the\n       remaining arguments. Default arguments are as for update_wrapper().\n       This is a convenience function to simplify applying partial() to\n       update_wrapper().\n    \"\"\"\n    return partial(update_wrapper, wrapped=wrapped,\n                   assigned=assigned, updated=updated)\n\n\n################################################################################\n### total_ordering class decorator\n################################################################################\n\n# The total ordering functions all invoke the root magic method directly\n# rather than using the corresponding operator.  This avoids possible\n# infinite recursion that could occur when the operator dispatch logic\n# detects a NotImplemented result and then calls a reflected method.\n\ndef _gt_from_lt(self, other):\n    'Return a > b.  Computed by @total_ordering from (not a < b) and (a != b).'\n    op_result = type(self).__lt__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result and self != other\n\ndef _le_from_lt(self, other):\n    'Return a <= b.  Computed by @total_ordering from (a < b) or (a == b).'\n    op_result = type(self).__lt__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return op_result or self == other\n\ndef _ge_from_lt(self, other):\n    'Return a >= b.  Computed by @total_ordering from (not a < b).'\n    op_result = type(self).__lt__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result\n\ndef _ge_from_le(self, other):\n    'Return a >= b.  Computed by @total_ordering from (not a <= b) or (a == b).'\n    op_result = type(self).__le__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result or self == other\n\ndef _lt_from_le(self, other):\n    'Return a < b.  Computed by @total_ordering from (a <= b) and (a != b).'\n    op_result = type(self).__le__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return op_result and self != other\n\ndef _gt_from_le(self, other):\n    'Return a > b.  Computed by @total_ordering from (not a <= b).'\n    op_result = type(self).__le__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result\n\ndef _lt_from_gt(self, other):\n    'Return a < b.  Computed by @total_ordering from (not a > b) and (a != b).'\n    op_result = type(self).__gt__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result and self != other\n\ndef _ge_from_gt(self, other):\n    'Return a >= b.  Computed by @total_ordering from (a > b) or (a == b).'\n    op_result = type(self).__gt__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return op_result or self == other\n\ndef _le_from_gt(self, other):\n    'Return a <= b.  Computed by @total_ordering from (not a > b).'\n    op_result = type(self).__gt__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result\n\ndef _le_from_ge(self, other):\n    'Return a <= b.  Computed by @total_ordering from (not a >= b) or (a == b).'\n    op_result = type(self).__ge__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result or self == other\n\ndef _gt_from_ge(self, other):\n    'Return a > b.  Computed by @total_ordering from (a >= b) and (a != b).'\n    op_result = type(self).__ge__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return op_result and self != other\n\ndef _lt_from_ge(self, other):\n    'Return a < b.  Computed by @total_ordering from (not a >= b).'\n    op_result = type(self).__ge__(self, other)\n    if op_result is NotImplemented:\n        return op_result\n    return not op_result\n\n_convert = {\n    '__lt__': [('__gt__', _gt_from_lt),\n               ('__le__', _le_from_lt),\n               ('__ge__', _ge_from_lt)],\n    '__le__': [('__ge__', _ge_from_le),\n               ('__lt__', _lt_from_le),\n               ('__gt__', _gt_from_le)],\n    '__gt__': [('__lt__', _lt_from_gt),\n               ('__ge__', _ge_from_gt),\n               ('__le__', _le_from_gt)],\n    '__ge__': [('__le__', _le_from_ge),\n               ('__gt__', _gt_from_ge),\n               ('__lt__', _lt_from_ge)]\n}\n\ndef total_ordering(cls):\n    \"\"\"Class decorator that fills in missing ordering methods\"\"\"\n    # Find user-defined comparisons (not those inherited from object).\n    roots = {op for op in _convert if getattr(cls, op, None) is not getattr(object, op, None)}\n    if not roots:\n        raise ValueError('must define at least one ordering operation: < > <= >=')\n    root = max(roots)       # prefer __lt__ to __le__ to __gt__ to __ge__\n    for opname, opfunc in _convert[root]:\n        if opname not in roots:\n            opfunc.__name__ = opname\n            setattr(cls, opname, opfunc)\n    return cls\n\n\n################################################################################\n### cmp_to_key() function converter\n################################################################################\n\ndef cmp_to_key(mycmp):\n    \"\"\"Convert a cmp= function into a key= function\"\"\"\n    class K(object):\n        __slots__ = ['obj']\n        def __init__(self, obj):\n            self.obj = obj\n        def __lt__(self, other):\n            return mycmp(self.obj, other.obj) < 0\n        def __gt__(self, other):\n            return mycmp(self.obj, other.obj) > 0\n        def __eq__(self, other):\n            return mycmp(self.obj, other.obj) == 0\n        def __le__(self, other):\n            return mycmp(self.obj, other.obj) <= 0\n        def __ge__(self, other):\n            return mycmp(self.obj, other.obj) >= 0\n        __hash__ = None\n    return K\n\ntry:\n    from _functools import cmp_to_key\nexcept ImportError:\n    pass\n\n\n################################################################################\n### reduce() sequence to a single item\n################################################################################\n\n_initial_missing = object()\n\ndef reduce(function, sequence, initial=_initial_missing):\n    \"\"\"\n    reduce(function, iterable[, initial]) -> value\n\n    Apply a function of two arguments cumulatively to the items of a sequence\n    or iterable, from left to right, so as to reduce the iterable to a single\n    value.  For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates\n    ((((1+2)+3)+4)+5).  If initial is present, it is placed before the items\n    of the iterable in the calculation, and serves as a default when the\n    iterable is empty.\n    \"\"\"\n\n    it = iter(sequence)\n\n    if initial is _initial_missing:\n        try:\n            value = next(it)\n        except StopIteration:\n            raise TypeError(\n                \"reduce() of empty iterable with no initial value\") from None\n    else:\n        value = initial\n\n    for element in it:\n        value = function(value, element)\n\n    return value\n\ntry:\n    from _functools import reduce\nexcept ImportError:\n    pass\n\n\n################################################################################\n### partial() argument application\n################################################################################\n\n# Purely functional, no descriptor behaviour\nclass partial:\n    \"\"\"New function with partial application of the given arguments\n    and keywords.\n    \"\"\"\n\n    __slots__ = \"func\", \"args\", \"keywords\", \"__dict__\", \"__weakref__\"\n\n    def __new__(cls, func, /, *args, **keywords):\n        if not callable(func):\n            raise TypeError(\"the first argument must be callable\")\n\n        if hasattr(func, \"func\"):\n            args = func.args + args\n            keywords = {**func.keywords, **keywords}\n            func = func.func\n\n        self = super(partial, cls).__new__(cls)\n\n        self.func = func\n        self.args = args\n        self.keywords = keywords\n        return self\n\n    def __call__(self, /, *args, **keywords):\n        keywords = {**self.keywords, **keywords}\n        return self.func(*self.args, *args, **keywords)\n\n    @recursive_repr()\n    def __repr__(self):\n        qualname = type(self).__qualname__\n        args = [repr(self.func)]\n        args.extend(repr(x) for x in self.args)\n        args.extend(f\"{k}={v!r}\" for (k, v) in self.keywords.items())\n        if type(self).__module__ == \"functools\":\n            return f\"functools.{qualname}({', '.join(args)})\"\n        return f\"{qualname}({', '.join(args)})\"\n\n    def __reduce__(self):\n        return type(self), (self.func,), (self.func, self.args,\n               self.keywords or None, self.__dict__ or None)\n\n    def __setstate__(self, state):\n        if not isinstance(state, tuple):\n            raise TypeError(\"argument to __setstate__ must be a tuple\")\n        if len(state) != 4:\n            raise TypeError(f\"expected 4 items in state, got {len(state)}\")\n        func, args, kwds, namespace = state\n        if (not callable(func) or not isinstance(args, tuple) or\n           (kwds is not None and not isinstance(kwds, dict)) or\n           (namespace is not None and not isinstance(namespace, dict))):\n            raise TypeError(\"invalid partial state\")\n\n        args = tuple(args) # just in case it's a subclass\n        if kwds is None:\n            kwds = {}\n        elif type(kwds) is not dict: # XXX does it need to be *exactly* dict?\n            kwds = dict(kwds)\n        if namespace is None:\n            namespace = {}\n\n        self.__dict__ = namespace\n        self.func = func\n        self.args = args\n        self.keywords = kwds\n\ntry:\n    from _functools import partial\nexcept ImportError:\n    pass\n\n# Descriptor version\nclass partialmethod(object):\n    \"\"\"Method descriptor with partial application of the given arguments\n    and keywords.\n\n    Supports wrapping existing descriptors and handles non-descriptor\n    callables as instance methods.\n    \"\"\"\n\n    def __init__(self, func, /, *args, **keywords):\n        if not callable(func) and not hasattr(func, \"__get__\"):\n            raise TypeError(\"{!r} is not callable or a descriptor\"\n                                 .format(func))\n\n        # func could be a descriptor like classmethod which isn't callable,\n        # so we can't inherit from partial (it verifies func is callable)\n        if isinstance(func, partialmethod):\n            # flattening is mandatory in order to place cls/self before all\n            # other arguments\n            # it's also more efficient since only one function will be called\n            self.func = func.func\n            self.args = func.args + args\n            self.keywords = {**func.keywords, **keywords}\n        else:\n            self.func = func\n            self.args = args\n            self.keywords = keywords\n\n    def __repr__(self):\n        args = \", \".join(map(repr, self.args))\n        keywords = \", \".join(\"{}={!r}\".format(k, v)\n                                 for k, v in self.keywords.items())\n        format_string = \"{module}.{cls}({func}, {args}, {keywords})\"\n        return format_string.format(module=self.__class__.__module__,\n                                    cls=self.__class__.__qualname__,\n                                    func=self.func,\n                                    args=args,\n                                    keywords=keywords)\n\n    def _make_unbound_method(self):\n        def _method(cls_or_self, /, *args, **keywords):\n            keywords = {**self.keywords, **keywords}\n            return self.func(cls_or_self, *self.args, *args, **keywords)\n        _method.__isabstractmethod__ = self.__isabstractmethod__\n        _method._partialmethod = self\n        return _method\n\n    def __get__(self, obj, cls=None):\n        get = getattr(self.func, \"__get__\", None)\n        result = None\n        if get is not None:\n            new_func = get(obj, cls)\n            if new_func is not self.func:\n                # Assume __get__ returning something new indicates the\n                # creation of an appropriate callable\n                result = partial(new_func, *self.args, **self.keywords)\n                try:\n                    result.__self__ = new_func.__self__\n                except AttributeError:\n                    pass\n        if result is None:\n            # If the underlying descriptor didn't do anything, treat this\n            # like an instance method\n            result = self._make_unbound_method().__get__(obj, cls)\n        return result\n\n    @property\n    def __isabstractmethod__(self):\n        return getattr(self.func, \"__isabstractmethod__\", False)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\n# Helper functions\n\ndef _unwrap_partial(func):\n    while isinstance(func, partial):\n        func = func.func\n    return func\n\n################################################################################\n### LRU Cache function decorator\n################################################################################\n\n_CacheInfo = namedtuple(\"CacheInfo\", [\"hits\", \"misses\", \"maxsize\", \"currsize\"])\n\nclass _HashedSeq(list):\n    \"\"\" This class guarantees that hash() will be called no more than once\n        per element.  This is important because the lru_cache() will hash\n        the key multiple times on a cache miss.\n\n    \"\"\"\n\n    __slots__ = 'hashvalue'\n\n    def __init__(self, tup, hash=hash):\n        self[:] = tup\n        self.hashvalue = hash(tup)\n\n    def __hash__(self):\n        return self.hashvalue\n\ndef _make_key(args, kwds, typed,\n             kwd_mark = (object(),),\n             fasttypes = {int, str},\n             tuple=tuple, type=type, len=len):\n    \"\"\"Make a cache key from optionally typed positional and keyword arguments\n\n    The key is constructed in a way that is flat as possible rather than\n    as a nested structure that would take more memory.\n\n    If there is only a single argument and its data type is known to cache\n    its hash value, then that argument is returned without a wrapper.  This\n    saves space and improves lookup speed.\n\n    \"\"\"\n    # All of code below relies on kwds preserving the order input by the user.\n    # Formerly, we sorted() the kwds before looping.  The new way is *much*\n    # faster; however, it means that f(x=1, y=2) will now be treated as a\n    # distinct call from f(y=2, x=1) which will be cached separately.\n    key = args\n    if kwds:\n        key += kwd_mark\n        for item in kwds.items():\n            key += item\n    if typed:\n        key += tuple(type(v) for v in args)\n        if kwds:\n            key += tuple(type(v) for v in kwds.values())\n    elif len(key) == 1 and type(key[0]) in fasttypes:\n        return key[0]\n    return _HashedSeq(key)\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"Least-recently-used cache decorator.\n\n    If *maxsize* is set to None, the LRU features are disabled and the cache\n    can grow without bound.\n\n    If *typed* is True, arguments of different types will be cached separately.\n    For example, f(3.0) and f(3) will be treated as distinct calls with\n    distinct results.\n\n    Arguments to the cached function must be hashable.\n\n    View the cache statistics named tuple (hits, misses, maxsize, currsize)\n    with f.cache_info().  Clear the cache and statistics with f.cache_clear().\n    Access the underlying function with f.__wrapped__.\n\n    See:  https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)\n\n    \"\"\"\n\n    # Users should only access the lru_cache through its public API:\n    #       cache_info, cache_clear, and f.__wrapped__\n    # The internals of the lru_cache are encapsulated for thread safety and\n    # to allow the implementation to change (including a possible C version).\n\n    if isinstance(maxsize, int):\n        # Negative maxsize is treated as 0\n        if maxsize < 0:\n            maxsize = 0\n    elif callable(maxsize) and isinstance(typed, bool):\n        # The user_function was passed in directly via the maxsize argument\n        user_function, maxsize = maxsize, 128\n        wrapper = _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo)\n        wrapper.cache_parameters = lambda : {'maxsize': maxsize, 'typed': typed}\n        return update_wrapper(wrapper, user_function)\n    elif maxsize is not None:\n        raise TypeError(\n            'Expected first argument to be an integer, a callable, or None')\n\n    def decorating_function(user_function):\n        wrapper = _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo)\n        wrapper.cache_parameters = lambda : {'maxsize': maxsize, 'typed': typed}\n        return update_wrapper(wrapper, user_function)\n\n    return decorating_function\n\ndef _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo):\n    # Constants shared by all lru cache instances:\n    sentinel = object()          # unique object used to signal cache misses\n    make_key = _make_key         # build a key from the function arguments\n    PREV, NEXT, KEY, RESULT = 0, 1, 2, 3   # names for the link fields\n\n    cache = {}\n    hits = misses = 0\n    full = False\n    cache_get = cache.get    # bound method to lookup a key or return None\n    cache_len = cache.__len__  # get cache size without calling len()\n    lock = RLock()           # because linkedlist updates aren't threadsafe\n    root = []                # root of the circular doubly linked list\n    root[:] = [root, root, None, None]     # initialize by pointing to self\n\n    if maxsize == 0:\n\n        def wrapper(*args, **kwds):\n            # No caching -- just a statistics update\n            nonlocal misses\n            misses += 1\n            result = user_function(*args, **kwds)\n            return result\n\n    elif maxsize is None:\n\n        def wrapper(*args, **kwds):\n            # Simple caching without ordering or size limit\n            nonlocal hits, misses\n            key = make_key(args, kwds, typed)\n            result = cache_get(key, sentinel)\n            if result is not sentinel:\n                hits += 1\n                return result\n            misses += 1\n            result = user_function(*args, **kwds)\n            cache[key] = result\n            return result\n\n    else:\n\n        def wrapper(*args, **kwds):\n            # Size limited caching that tracks accesses by recency\n            nonlocal root, hits, misses, full\n            key = make_key(args, kwds, typed)\n            with lock:\n                link = cache_get(key)\n                if link is not None:\n                    # Move the link to the front of the circular queue\n                    link_prev, link_next, _key, result = link\n                    link_prev[NEXT] = link_next\n                    link_next[PREV] = link_prev\n                    last = root[PREV]\n                    last[NEXT] = root[PREV] = link\n                    link[PREV] = last\n                    link[NEXT] = root\n                    hits += 1\n                    return result\n                misses += 1\n            result = user_function(*args, **kwds)\n            with lock:\n                if key in cache:\n                    # Getting here means that this same key was added to the\n                    # cache while the lock was released.  Since the link\n                    # update is already done, we need only return the\n                    # computed result and update the count of misses.\n                    pass\n                elif full:\n                    # Use the old root to store the new key and result.\n                    oldroot = root\n                    oldroot[KEY] = key\n                    oldroot[RESULT] = result\n                    # Empty the oldest link and make it the new root.\n                    # Keep a reference to the old key and old result to\n                    # prevent their ref counts from going to zero during the\n                    # update. That will prevent potentially arbitrary object\n                    # clean-up code (i.e. __del__) from running while we're\n                    # still adjusting the links.\n                    root = oldroot[NEXT]\n                    oldkey = root[KEY]\n                    oldresult = root[RESULT]\n                    root[KEY] = root[RESULT] = None\n                    # Now update the cache dictionary.\n                    del cache[oldkey]\n                    # Save the potentially reentrant cache[key] assignment\n                    # for last, after the root and links have been put in\n                    # a consistent state.\n                    cache[key] = oldroot\n                else:\n                    # Put result in a new link at the front of the queue.\n                    last = root[PREV]\n                    link = [last, root, key, result]\n                    last[NEXT] = root[PREV] = cache[key] = link\n                    # Use the cache_len bound method instead of the len() function\n                    # which could potentially be wrapped in an lru_cache itself.\n                    full = (cache_len() >= maxsize)\n            return result\n\n    def cache_info():\n        \"\"\"Report cache statistics\"\"\"\n        with lock:\n            return _CacheInfo(hits, misses, maxsize, cache_len())\n\n    def cache_clear():\n        \"\"\"Clear the cache and cache statistics\"\"\"\n        nonlocal hits, misses, full\n        with lock:\n            cache.clear()\n            root[:] = [root, root, None, None]\n            hits = misses = 0\n            full = False\n\n    wrapper.cache_info = cache_info\n    wrapper.cache_clear = cache_clear\n    return wrapper\n\ntry:\n    from _functools import _lru_cache_wrapper\nexcept ImportError:\n    pass\n\n\n################################################################################\n### cache -- simplified access to the infinity cache\n################################################################################\n\ndef cache(user_function, /):\n    'Simple lightweight unbounded cache.  Sometimes called \"memoize\".'\n    return lru_cache(maxsize=None)(user_function)\n\n\n################################################################################\n### singledispatch() - single-dispatch generic function decorator\n################################################################################\n\ndef _c3_merge(sequences):\n    \"\"\"Merges MROs in *sequences* to a single MRO using the C3 algorithm.\n\n    Adapted from https://www.python.org/download/releases/2.3/mro/.\n\n    \"\"\"\n    result = []\n    while True:\n        sequences = [s for s in sequences if s]   # purge empty sequences\n        if not sequences:\n            return result\n        for s1 in sequences:   # find merge candidates among seq heads\n            candidate = s1[0]\n            for s2 in sequences:\n                if candidate in s2[1:]:\n                    candidate = None\n                    break      # reject the current head, it appears later\n            else:\n                break\n        if candidate is None:\n            raise RuntimeError(\"Inconsistent hierarchy\")\n        result.append(candidate)\n        # remove the chosen candidate\n        for seq in sequences:\n            if seq[0] == candidate:\n                del seq[0]\n\ndef _c3_mro(cls, abcs=None):\n    \"\"\"Computes the method resolution order using extended C3 linearization.\n\n    If no *abcs* are given, the algorithm works exactly like the built-in C3\n    linearization used for method resolution.\n\n    If given, *abcs* is a list of abstract base classes that should be inserted\n    into the resulting MRO. Unrelated ABCs are ignored and don't end up in the\n    result. The algorithm inserts ABCs where their functionality is introduced,\n    i.e. issubclass(cls, abc) returns True for the class itself but returns\n    False for all its direct base classes. Implicit ABCs for a given class\n    (either registered or inferred from the presence of a special method like\n    __len__) are inserted directly after the last ABC explicitly listed in the\n    MRO of said class. If two implicit ABCs end up next to each other in the\n    resulting MRO, their ordering depends on the order of types in *abcs*.\n\n    \"\"\"\n    for i, base in enumerate(reversed(cls.__bases__)):\n        if hasattr(base, '__abstractmethods__'):\n            boundary = len(cls.__bases__) - i\n            break   # Bases up to the last explicit ABC are considered first.\n    else:\n        boundary = 0\n    abcs = list(abcs) if abcs else []\n    explicit_bases = list(cls.__bases__[:boundary])\n    abstract_bases = []\n    other_bases = list(cls.__bases__[boundary:])\n    for base in abcs:\n        if issubclass(cls, base) and not any(\n                issubclass(b, base) for b in cls.__bases__\n            ):\n            # If *cls* is the class that introduces behaviour described by\n            # an ABC *base*, insert said ABC to its MRO.\n            abstract_bases.append(base)\n    for base in abstract_bases:\n        abcs.remove(base)\n    explicit_c3_mros = [_c3_mro(base, abcs=abcs) for base in explicit_bases]\n    abstract_c3_mros = [_c3_mro(base, abcs=abcs) for base in abstract_bases]\n    other_c3_mros = [_c3_mro(base, abcs=abcs) for base in other_bases]\n    return _c3_merge(\n        [[cls]] +\n        explicit_c3_mros + abstract_c3_mros + other_c3_mros +\n        [explicit_bases] + [abstract_bases] + [other_bases]\n    )\n\ndef _compose_mro(cls, types):\n    \"\"\"Calculates the method resolution order for a given class *cls*.\n\n    Includes relevant abstract base classes (with their respective bases) from\n    the *types* iterable. Uses a modified C3 linearization algorithm.\n\n    \"\"\"\n    bases = set(cls.__mro__)\n    # Remove entries which are already present in the __mro__ or unrelated.\n    def is_related(typ):\n        return (typ not in bases and hasattr(typ, '__mro__')\n                                 and not isinstance(typ, GenericAlias)\n                                 and issubclass(cls, typ))\n    types = [n for n in types if is_related(n)]\n    # Remove entries which are strict bases of other entries (they will end up\n    # in the MRO anyway.\n    def is_strict_base(typ):\n        for other in types:\n            if typ != other and typ in other.__mro__:\n                return True\n        return False\n    types = [n for n in types if not is_strict_base(n)]\n    # Subclasses of the ABCs in *types* which are also implemented by\n    # *cls* can be used to stabilize ABC ordering.\n    type_set = set(types)\n    mro = []\n    for typ in types:\n        found = []\n        for sub in typ.__subclasses__():\n            if sub not in bases and issubclass(cls, sub):\n                found.append([s for s in sub.__mro__ if s in type_set])\n        if not found:\n            mro.append(typ)\n            continue\n        # Favor subclasses with the biggest number of useful bases\n        found.sort(key=len, reverse=True)\n        for sub in found:\n            for subcls in sub:\n                if subcls not in mro:\n                    mro.append(subcls)\n    return _c3_mro(cls, abcs=mro)\n\ndef _find_impl(cls, registry):\n    \"\"\"Returns the best matching implementation from *registry* for type *cls*.\n\n    Where there is no registered implementation for a specific type, its method\n    resolution order is used to find a more generic implementation.\n\n    Note: if *registry* does not contain an implementation for the base\n    *object* type, this function may return None.\n\n    \"\"\"\n    mro = _compose_mro(cls, registry.keys())\n    match = None\n    for t in mro:\n        if match is not None:\n            # If *match* is an implicit ABC but there is another unrelated,\n            # equally matching implicit ABC, refuse the temptation to guess.\n            if (t in registry and t not in cls.__mro__\n                              and match not in cls.__mro__\n                              and not issubclass(match, t)):\n                raise RuntimeError(\"Ambiguous dispatch: {} or {}\".format(\n                    match, t))\n            break\n        if t in registry:\n            match = t\n    return registry.get(match)\n\ndef singledispatch(func):\n    \"\"\"Single-dispatch generic function decorator.\n\n    Transforms a function into a generic function, which can have different\n    behaviours depending upon the type of its first argument. The decorated\n    function acts as the default implementation, and additional\n    implementations can be registered using the register() attribute of the\n    generic function.\n    \"\"\"\n    # There are many programs that use functools without singledispatch, so we\n    # trade-off making singledispatch marginally slower for the benefit of\n    # making start-up of such applications slightly faster.\n    import types, weakref\n\n    registry = {}\n    dispatch_cache = weakref.WeakKeyDictionary()\n    cache_token = None\n\n    def dispatch(cls):\n        \"\"\"generic_func.dispatch(cls) -> <function implementation>\n\n        Runs the dispatch algorithm to return the best available implementation\n        for the given *cls* registered on *generic_func*.\n\n        \"\"\"\n        nonlocal cache_token\n        if cache_token is not None:\n            current_token = get_cache_token()\n            if cache_token != current_token:\n                dispatch_cache.clear()\n                cache_token = current_token\n        try:\n            impl = dispatch_cache[cls]\n        except KeyError:\n            try:\n                impl = registry[cls]\n            except KeyError:\n                impl = _find_impl(cls, registry)\n            dispatch_cache[cls] = impl\n        return impl\n\n    def _is_union_type(cls):\n        from typing import get_origin, Union\n        return get_origin(cls) in {Union, types.UnionType}\n\n    def _is_valid_dispatch_type(cls):\n        if isinstance(cls, type):\n            return True\n        from typing import get_args\n        return (_is_union_type(cls) and\n                all(isinstance(arg, type) for arg in get_args(cls)))\n\n    def register(cls, func=None):\n        \"\"\"generic_func.register(cls, func) -> func\n\n        Registers a new implementation for the given *cls* on a *generic_func*.\n\n        \"\"\"\n        nonlocal cache_token\n        if _is_valid_dispatch_type(cls):\n            if func is None:\n                return lambda f: register(cls, f)\n        else:\n            if func is not None:\n                raise TypeError(\n                    f\"Invalid first argument to `register()`. \"\n                    f\"{cls!r} is not a class or union type.\"\n                )\n            ann = getattr(cls, '__annotations__', {})\n            if not ann:\n                raise TypeError(\n                    f\"Invalid first argument to `register()`: {cls!r}. \"\n                    f\"Use either `@register(some_class)` or plain `@register` \"\n                    f\"on an annotated function.\"\n                )\n            func = cls\n\n            # only import typing if annotation parsing is necessary\n            from typing import get_type_hints\n            argname, cls = next(iter(get_type_hints(func).items()))\n            if not _is_valid_dispatch_type(cls):\n                if _is_union_type(cls):\n                    raise TypeError(\n                        f\"Invalid annotation for {argname!r}. \"\n                        f\"{cls!r} not all arguments are classes.\"\n                    )\n                else:\n                    raise TypeError(\n                        f\"Invalid annotation for {argname!r}. \"\n                        f\"{cls!r} is not a class.\"\n                    )\n\n        if _is_union_type(cls):\n            from typing import get_args\n\n            for arg in get_args(cls):\n                registry[arg] = func\n        else:\n            registry[cls] = func\n        if cache_token is None and hasattr(cls, '__abstractmethods__'):\n            cache_token = get_cache_token()\n        dispatch_cache.clear()\n        return func\n\n    def wrapper(*args, **kw):\n        if not args:\n            raise TypeError(f'{funcname} requires at least '\n                            '1 positional argument')\n\n        return dispatch(args[0].__class__)(*args, **kw)\n\n    funcname = getattr(func, '__name__', 'singledispatch function')\n    registry[object] = func\n    wrapper.register = register\n    wrapper.dispatch = dispatch\n    wrapper.registry = types.MappingProxyType(registry)\n    wrapper._clear_cache = dispatch_cache.clear\n    update_wrapper(wrapper, func)\n    return wrapper\n\n\n# Descriptor version\nclass singledispatchmethod:\n    \"\"\"Single-dispatch generic method descriptor.\n\n    Supports wrapping existing descriptors and handles non-descriptor\n    callables as instance methods.\n    \"\"\"\n\n    def __init__(self, func):\n        if not callable(func) and not hasattr(func, \"__get__\"):\n            raise TypeError(f\"{func!r} is not callable or a descriptor\")\n\n        self.dispatcher = singledispatch(func)\n        self.func = func\n\n    def register(self, cls, method=None):\n        \"\"\"generic_method.register(cls, func) -> func\n\n        Registers a new implementation for the given *cls* on a *generic_method*.\n        \"\"\"\n        return self.dispatcher.register(cls, func=method)\n\n    def __get__(self, obj, cls=None):\n        def _method(*args, **kwargs):\n            method = self.dispatcher.dispatch(args[0].__class__)\n            return method.__get__(obj, cls)(*args, **kwargs)\n\n        _method.__isabstractmethod__ = self.__isabstractmethod__\n        _method.register = self.register\n        update_wrapper(_method, self.func)\n        return _method\n\n    @property\n    def __isabstractmethod__(self):\n        return getattr(self.func, '__isabstractmethod__', False)\n\n\n################################################################################\n### cached_property() - computed once per instance, cached as attribute\n################################################################################\n\n_NOT_FOUND = object()\n\n\nclass cached_property:\n    def __init__(self, func):\n        self.func = func\n        self.attrname = None\n        self.__doc__ = func.__doc__\n        self.lock = RLock()\n\n    def __set_name__(self, owner, name):\n        if self.attrname is None:\n            self.attrname = name\n        elif name != self.attrname:\n            raise TypeError(\n                \"Cannot assign the same cached_property to two different names \"\n                f\"({self.attrname!r} and {name!r}).\"\n            )\n\n    def __get__(self, instance, owner=None):\n        if instance is None:\n            return self\n        if self.attrname is None:\n            raise TypeError(\n                \"Cannot use cached_property instance without calling __set_name__ on it.\")\n        try:\n            cache = instance.__dict__\n        except AttributeError:  # not all objects have __dict__ (e.g. class defines slots)\n            msg = (\n                f\"No '__dict__' attribute on {type(instance).__name__!r} \"\n                f\"instance to cache {self.attrname!r} property.\"\n            )\n            raise TypeError(msg) from None\n        val = cache.get(self.attrname, _NOT_FOUND)\n        if val is _NOT_FOUND:\n            with self.lock:\n                # check if another thread filled cache while we awaited lock\n                val = cache.get(self.attrname, _NOT_FOUND)\n                if val is _NOT_FOUND:\n                    val = self.func(instance)\n                    try:\n                        cache[self.attrname] = val\n                    except TypeError:\n                        msg = (\n                            f\"The '__dict__' attribute on {type(instance).__name__!r} instance \"\n                            f\"does not support item assignment for caching {self.attrname!r} property.\"\n                        )\n                        raise TypeError(msg) from None\n        return val\n\n    __class_getitem__ = classmethod(GenericAlias)\n", 1012], "/usr/local/lib/python3.11/threading.py": ["\"\"\"Thread module emulating a subset of Java's threading model.\"\"\"\n\nimport os as _os\nimport sys as _sys\nimport _thread\nimport functools\n\nfrom time import monotonic as _time\nfrom _weakrefset import WeakSet\nfrom itertools import islice as _islice, count as _count\ntry:\n    from _collections import deque as _deque\nexcept ImportError:\n    from collections import deque as _deque\n\n# Note regarding PEP 8 compliant names\n#  This threading model was originally inspired by Java, and inherited\n# the convention of camelCase function and method names from that\n# language. Those original names are not in any imminent danger of\n# being deprecated (even for Py3k),so this module provides them as an\n# alias for the PEP 8 compliant names\n# Note that using the new PEP 8 compliant names facilitates substitution\n# with the multiprocessing module, which doesn't provide the old\n# Java inspired names.\n\n__all__ = ['get_ident', 'active_count', 'Condition', 'current_thread',\n           'enumerate', 'main_thread', 'TIMEOUT_MAX',\n           'Event', 'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore', 'Thread',\n           'Barrier', 'BrokenBarrierError', 'Timer', 'ThreadError',\n           'setprofile', 'settrace', 'local', 'stack_size',\n           'excepthook', 'ExceptHookArgs', 'gettrace', 'getprofile']\n\n# Rename some stuff so \"from threading import *\" is safe\n_start_new_thread = _thread.start_new_thread\n_allocate_lock = _thread.allocate_lock\n_set_sentinel = _thread._set_sentinel\nget_ident = _thread.get_ident\ntry:\n    get_native_id = _thread.get_native_id\n    _HAVE_THREAD_NATIVE_ID = True\n    __all__.append('get_native_id')\nexcept AttributeError:\n    _HAVE_THREAD_NATIVE_ID = False\nThreadError = _thread.error\ntry:\n    _CRLock = _thread.RLock\nexcept AttributeError:\n    _CRLock = None\nTIMEOUT_MAX = _thread.TIMEOUT_MAX\ndel _thread\n\n\n# Support for profile and trace hooks\n\n_profile_hook = None\n_trace_hook = None\n\ndef setprofile(func):\n    \"\"\"Set a profile function for all threads started from the threading module.\n\n    The func will be passed to sys.setprofile() for each thread, before its\n    run() method is called.\n\n    \"\"\"\n    global _profile_hook\n    _profile_hook = func\n\ndef getprofile():\n    \"\"\"Get the profiler function as set by threading.setprofile().\"\"\"\n    return _profile_hook\n\ndef settrace(func):\n    \"\"\"Set a trace function for all threads started from the threading module.\n\n    The func will be passed to sys.settrace() for each thread, before its run()\n    method is called.\n\n    \"\"\"\n    global _trace_hook\n    _trace_hook = func\n\ndef gettrace():\n    \"\"\"Get the trace function as set by threading.settrace().\"\"\"\n    return _trace_hook\n\n# Synchronization classes\n\nLock = _allocate_lock\n\ndef RLock(*args, **kwargs):\n    \"\"\"Factory function that returns a new reentrant lock.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it again\n    without blocking; the thread must release it once for each time it has\n    acquired it.\n\n    \"\"\"\n    if _CRLock is None:\n        return _PyRLock(*args, **kwargs)\n    return _CRLock(*args, **kwargs)\n\nclass _RLock:\n    \"\"\"This class implements reentrant lock objects.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it\n    again without blocking; the thread must release it once for each time it\n    has acquired it.\n\n    \"\"\"\n\n    def __init__(self):\n        self._block = _allocate_lock()\n        self._owner = None\n        self._count = 0\n\n    def __repr__(self):\n        owner = self._owner\n        try:\n            owner = _active[owner].name\n        except KeyError:\n            pass\n        return \"<%s %s.%s object owner=%r count=%d at %s>\" % (\n            \"locked\" if self._block.locked() else \"unlocked\",\n            self.__class__.__module__,\n            self.__class__.__qualname__,\n            owner,\n            self._count,\n            hex(id(self))\n        )\n\n    def _at_fork_reinit(self):\n        self._block._at_fork_reinit()\n        self._owner = None\n        self._count = 0\n\n    def acquire(self, blocking=True, timeout=-1):\n        \"\"\"Acquire a lock, blocking or non-blocking.\n\n        When invoked without arguments: if this thread already owns the lock,\n        increment the recursion level by one, and return immediately. Otherwise,\n        if another thread owns the lock, block until the lock is unlocked. Once\n        the lock is unlocked (not owned by any thread), then grab ownership, set\n        the recursion level to one, and return. If more than one thread is\n        blocked waiting until the lock is unlocked, only one at a time will be\n        able to grab ownership of the lock. There is no return value in this\n        case.\n\n        When invoked with the blocking argument set to true, do the same thing\n        as when called without arguments, and return true.\n\n        When invoked with the blocking argument set to false, do not block. If a\n        call without an argument would block, return false immediately;\n        otherwise, do the same thing as when called without arguments, and\n        return true.\n\n        When invoked with the floating-point timeout argument set to a positive\n        value, block for at most the number of seconds specified by timeout\n        and as long as the lock cannot be acquired.  Return true if the lock has\n        been acquired, false if the timeout has elapsed.\n\n        \"\"\"\n        me = get_ident()\n        if self._owner == me:\n            self._count += 1\n            return 1\n        rc = self._block.acquire(blocking, timeout)\n        if rc:\n            self._owner = me\n            self._count = 1\n        return rc\n\n    __enter__ = acquire\n\n    def release(self):\n        \"\"\"Release a lock, decrementing the recursion level.\n\n        If after the decrement it is zero, reset the lock to unlocked (not owned\n        by any thread), and if any other threads are blocked waiting for the\n        lock to become unlocked, allow exactly one of them to proceed. If after\n        the decrement the recursion level is still nonzero, the lock remains\n        locked and owned by the calling thread.\n\n        Only call this method when the calling thread owns the lock. A\n        RuntimeError is raised if this method is called when the lock is\n        unlocked.\n\n        There is no return value.\n\n        \"\"\"\n        if self._owner != get_ident():\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        self._count = count = self._count - 1\n        if not count:\n            self._owner = None\n            self._block.release()\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n    # Internal methods used by condition variables\n\n    def _acquire_restore(self, state):\n        self._block.acquire()\n        self._count, self._owner = state\n\n    def _release_save(self):\n        if self._count == 0:\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        count = self._count\n        self._count = 0\n        owner = self._owner\n        self._owner = None\n        self._block.release()\n        return (count, owner)\n\n    def _is_owned(self):\n        return self._owner == get_ident()\n\n    # Internal method used for reentrancy checks\n\n    def _recursion_count(self):\n        if self._owner != get_ident():\n            return 0\n        return self._count\n\n_PyRLock = _RLock\n\n\nclass Condition:\n    \"\"\"Class that implements a condition variable.\n\n    A condition variable allows one or more threads to wait until they are\n    notified by another thread.\n\n    If the lock argument is given and not None, it must be a Lock or RLock\n    object, and it is used as the underlying lock. Otherwise, a new RLock object\n    is created and used as the underlying lock.\n\n    \"\"\"\n\n    def __init__(self, lock=None):\n        if lock is None:\n            lock = RLock()\n        self._lock = lock\n        # Export the lock's acquire() and release() methods\n        self.acquire = lock.acquire\n        self.release = lock.release\n        # If the lock defines _release_save() and/or _acquire_restore(),\n        # these override the default implementations (which just call\n        # release() and acquire() on the lock).  Ditto for _is_owned().\n        try:\n            self._release_save = lock._release_save\n        except AttributeError:\n            pass\n        try:\n            self._acquire_restore = lock._acquire_restore\n        except AttributeError:\n            pass\n        try:\n            self._is_owned = lock._is_owned\n        except AttributeError:\n            pass\n        self._waiters = _deque()\n\n    def _at_fork_reinit(self):\n        self._lock._at_fork_reinit()\n        self._waiters.clear()\n\n    def __enter__(self):\n        return self._lock.__enter__()\n\n    def __exit__(self, *args):\n        return self._lock.__exit__(*args)\n\n    def __repr__(self):\n        return \"<Condition(%s, %d)>\" % (self._lock, len(self._waiters))\n\n    def _release_save(self):\n        self._lock.release()           # No state to save\n\n    def _acquire_restore(self, x):\n        self._lock.acquire()           # Ignore saved state\n\n    def _is_owned(self):\n        # Return True if lock is owned by current_thread.\n        # This method is called only if _lock doesn't have _is_owned().\n        if self._lock.acquire(False):\n            self._lock.release()\n            return False\n        else:\n            return True\n\n    def wait(self, timeout=None):\n        \"\"\"Wait until notified or until a timeout occurs.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method releases the underlying lock, and then blocks until it is\n        awakened by a notify() or notify_all() call for the same condition\n        variable in another thread, or until the optional timeout occurs. Once\n        awakened or timed out, it re-acquires the lock and returns.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        When the underlying lock is an RLock, it is not released using its\n        release() method, since this may not actually unlock the lock when it\n        was acquired multiple times recursively. Instead, an internal interface\n        of the RLock class is used, which really unlocks it even when it has\n        been recursively acquired several times. Another internal interface is\n        then used to restore the recursion level when the lock is reacquired.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot wait on un-acquired lock\")\n        waiter = _allocate_lock()\n        waiter.acquire()\n        self._waiters.append(waiter)\n        saved_state = self._release_save()\n        gotit = False\n        try:    # restore state no matter what (e.g., KeyboardInterrupt)\n            if timeout is None:\n                waiter.acquire()\n                gotit = True\n            else:\n                if timeout > 0:\n                    gotit = waiter.acquire(True, timeout)\n                else:\n                    gotit = waiter.acquire(False)\n            return gotit\n        finally:\n            self._acquire_restore(saved_state)\n            if not gotit:\n                try:\n                    self._waiters.remove(waiter)\n                except ValueError:\n                    pass\n\n    def wait_for(self, predicate, timeout=None):\n        \"\"\"Wait until a condition evaluates to True.\n\n        predicate should be a callable which result will be interpreted as a\n        boolean value.  A timeout may be provided giving the maximum time to\n        wait.\n\n        \"\"\"\n        endtime = None\n        waittime = timeout\n        result = predicate()\n        while not result:\n            if waittime is not None:\n                if endtime is None:\n                    endtime = _time() + waittime\n                else:\n                    waittime = endtime - _time()\n                    if waittime <= 0:\n                        break\n            self.wait(waittime)\n            result = predicate()\n        return result\n\n    def notify(self, n=1):\n        \"\"\"Wake up one or more threads waiting on this condition, if any.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method wakes up at most n of the threads waiting for the condition\n        variable; it is a no-op if no threads are waiting.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot notify on un-acquired lock\")\n        waiters = self._waiters\n        while waiters and n > 0:\n            waiter = waiters[0]\n            try:\n                waiter.release()\n            except RuntimeError:\n                # gh-92530: The previous call of notify() released the lock,\n                # but was interrupted before removing it from the queue.\n                # It can happen if a signal handler raises an exception,\n                # like CTRL+C which raises KeyboardInterrupt.\n                pass\n            else:\n                n -= 1\n            try:\n                waiters.remove(waiter)\n            except ValueError:\n                pass\n\n    def notify_all(self):\n        \"\"\"Wake up all threads waiting on this condition.\n\n        If the calling thread has not acquired the lock when this method\n        is called, a RuntimeError is raised.\n\n        \"\"\"\n        self.notify(len(self._waiters))\n\n    def notifyAll(self):\n        \"\"\"Wake up all threads waiting on this condition.\n\n        This method is deprecated, use notify_all() instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('notifyAll() is deprecated, use notify_all() instead',\n                      DeprecationWarning, stacklevel=2)\n        self.notify_all()\n\n\nclass Semaphore:\n    \"\"\"This class implements semaphore objects.\n\n    Semaphores manage a counter representing the number of release() calls minus\n    the number of acquire() calls, plus an initial value. The acquire() method\n    blocks if necessary until it can return without making the counter\n    negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    # After Tim Peters' semaphore class, but not quite the same (no maximum)\n\n    def __init__(self, value=1):\n        if value < 0:\n            raise ValueError(\"semaphore initial value must be >= 0\")\n        self._cond = Condition(Lock())\n        self._value = value\n\n    def __repr__(self):\n        cls = self.__class__\n        return (f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}:\"\n                f\" value={self._value}>\")\n\n    def acquire(self, blocking=True, timeout=None):\n        \"\"\"Acquire a semaphore, decrementing the internal counter by one.\n\n        When invoked without arguments: if the internal counter is larger than\n        zero on entry, decrement it by one and return immediately. If it is zero\n        on entry, block, waiting until some other thread has called release() to\n        make it larger than zero. This is done with proper interlocking so that\n        if multiple acquire() calls are blocked, release() will wake exactly one\n        of them up. The implementation may pick one at random, so the order in\n        which blocked threads are awakened should not be relied on. There is no\n        return value in this case.\n\n        When invoked with blocking set to true, do the same thing as when called\n        without arguments, and return true.\n\n        When invoked with blocking set to false, do not block. If a call without\n        an argument would block, return false immediately; otherwise, do the\n        same thing as when called without arguments, and return true.\n\n        When invoked with a timeout other than None, it will block for at\n        most timeout seconds.  If acquire does not complete successfully in\n        that interval, return false.  Return true otherwise.\n\n        \"\"\"\n        if not blocking and timeout is not None:\n            raise ValueError(\"can't specify timeout for non-blocking acquire\")\n        rc = False\n        endtime = None\n        with self._cond:\n            while self._value == 0:\n                if not blocking:\n                    break\n                if timeout is not None:\n                    if endtime is None:\n                        endtime = _time() + timeout\n                    else:\n                        timeout = endtime - _time()\n                        if timeout <= 0:\n                            break\n                self._cond.wait(timeout)\n            else:\n                self._value -= 1\n                rc = True\n        return rc\n\n    __enter__ = acquire\n\n    def release(self, n=1):\n        \"\"\"Release a semaphore, incrementing the internal counter by one or more.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        \"\"\"\n        if n < 1:\n            raise ValueError('n must be one or more')\n        with self._cond:\n            self._value += n\n            for i in range(n):\n                self._cond.notify()\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n\nclass BoundedSemaphore(Semaphore):\n    \"\"\"Implements a bounded semaphore.\n\n    A bounded semaphore checks to make sure its current value doesn't exceed its\n    initial value. If it does, ValueError is raised. In most situations\n    semaphores are used to guard resources with limited capacity.\n\n    If the semaphore is released too many times it's a sign of a bug. If not\n    given, value defaults to 1.\n\n    Like regular semaphores, bounded semaphores manage a counter representing\n    the number of release() calls minus the number of acquire() calls, plus an\n    initial value. The acquire() method blocks if necessary until it can return\n    without making the counter negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    def __init__(self, value=1):\n        Semaphore.__init__(self, value)\n        self._initial_value = value\n\n    def __repr__(self):\n        cls = self.__class__\n        return (f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}:\"\n                f\" value={self._value}/{self._initial_value}>\")\n\n    def release(self, n=1):\n        \"\"\"Release a semaphore, incrementing the internal counter by one or more.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        If the number of releases exceeds the number of acquires,\n        raise a ValueError.\n\n        \"\"\"\n        if n < 1:\n            raise ValueError('n must be one or more')\n        with self._cond:\n            if self._value + n > self._initial_value:\n                raise ValueError(\"Semaphore released too many times\")\n            self._value += n\n            for i in range(n):\n                self._cond.notify()\n\n\nclass Event:\n    \"\"\"Class implementing event objects.\n\n    Events manage a flag that can be set to true with the set() method and reset\n    to false with the clear() method. The wait() method blocks until the flag is\n    true.  The flag is initially false.\n\n    \"\"\"\n\n    # After Tim Peters' event class (without is_posted())\n\n    def __init__(self):\n        self._cond = Condition(Lock())\n        self._flag = False\n\n    def __repr__(self):\n        cls = self.__class__\n        status = 'set' if self._flag else 'unset'\n        return f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}: {status}>\"\n\n    def _at_fork_reinit(self):\n        # Private method called by Thread._reset_internal_locks()\n        self._cond._at_fork_reinit()\n\n    def is_set(self):\n        \"\"\"Return true if and only if the internal flag is true.\"\"\"\n        return self._flag\n\n    def isSet(self):\n        \"\"\"Return true if and only if the internal flag is true.\n\n        This method is deprecated, use is_set() instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('isSet() is deprecated, use is_set() instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.is_set()\n\n    def set(self):\n        \"\"\"Set the internal flag to true.\n\n        All threads waiting for it to become true are awakened. Threads\n        that call wait() once the flag is true will not block at all.\n\n        \"\"\"\n        with self._cond:\n            self._flag = True\n            self._cond.notify_all()\n\n    def clear(self):\n        \"\"\"Reset the internal flag to false.\n\n        Subsequently, threads calling wait() will block until set() is called to\n        set the internal flag to true again.\n\n        \"\"\"\n        with self._cond:\n            self._flag = False\n\n    def wait(self, timeout=None):\n        \"\"\"Block until the internal flag is true.\n\n        If the internal flag is true on entry, return immediately. Otherwise,\n        block until another thread calls set() to set the flag to true, or until\n        the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        This method returns the internal flag on exit, so it will always return\n        True except if a timeout is given and the operation times out.\n\n        \"\"\"\n        with self._cond:\n            signaled = self._flag\n            if not signaled:\n                signaled = self._cond.wait(timeout)\n            return signaled\n\n\n# A barrier class.  Inspired in part by the pthread_barrier_* api and\n# the CyclicBarrier class from Java.  See\n# http://sourceware.org/pthreads-win32/manual/pthread_barrier_init.html and\n# http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/\n#        CyclicBarrier.html\n# for information.\n# We maintain two main states, 'filling' and 'draining' enabling the barrier\n# to be cyclic.  Threads are not allowed into it until it has fully drained\n# since the previous cycle.  In addition, a 'resetting' state exists which is\n# similar to 'draining' except that threads leave with a BrokenBarrierError,\n# and a 'broken' state in which all threads get the exception.\nclass Barrier:\n    \"\"\"Implements a Barrier.\n\n    Useful for synchronizing a fixed number of threads at known synchronization\n    points.  Threads block on 'wait()' and are simultaneously awoken once they\n    have all made that call.\n\n    \"\"\"\n\n    def __init__(self, parties, action=None, timeout=None):\n        \"\"\"Create a barrier, initialised to 'parties' threads.\n\n        'action' is a callable which, when supplied, will be called by one of\n        the threads after they have all entered the barrier and just prior to\n        releasing them all. If a 'timeout' is provided, it is used as the\n        default for all subsequent 'wait()' calls.\n\n        \"\"\"\n        self._cond = Condition(Lock())\n        self._action = action\n        self._timeout = timeout\n        self._parties = parties\n        self._state = 0  # 0 filling, 1 draining, -1 resetting, -2 broken\n        self._count = 0\n\n    def __repr__(self):\n        cls = self.__class__\n        if self.broken:\n            return f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}: broken>\"\n        return (f\"<{cls.__module__}.{cls.__qualname__} at {id(self):#x}:\"\n                f\" waiters={self.n_waiting}/{self.parties}>\")\n\n    def wait(self, timeout=None):\n        \"\"\"Wait for the barrier.\n\n        When the specified number of threads have started waiting, they are all\n        simultaneously awoken. If an 'action' was provided for the barrier, one\n        of the threads will have executed that callback prior to returning.\n        Returns an individual index number from 0 to 'parties-1'.\n\n        \"\"\"\n        if timeout is None:\n            timeout = self._timeout\n        with self._cond:\n            self._enter() # Block while the barrier drains.\n            index = self._count\n            self._count += 1\n            try:\n                if index + 1 == self._parties:\n                    # We release the barrier\n                    self._release()\n                else:\n                    # We wait until someone releases us\n                    self._wait(timeout)\n                return index\n            finally:\n                self._count -= 1\n                # Wake up any threads waiting for barrier to drain.\n                self._exit()\n\n    # Block until the barrier is ready for us, or raise an exception\n    # if it is broken.\n    def _enter(self):\n        while self._state in (-1, 1):\n            # It is draining or resetting, wait until done\n            self._cond.wait()\n        #see if the barrier is in a broken state\n        if self._state < 0:\n            raise BrokenBarrierError\n        assert self._state == 0\n\n    # Optionally run the 'action' and release the threads waiting\n    # in the barrier.\n    def _release(self):\n        try:\n            if self._action:\n                self._action()\n            # enter draining state\n            self._state = 1\n            self._cond.notify_all()\n        except:\n            #an exception during the _action handler.  Break and reraise\n            self._break()\n            raise\n\n    # Wait in the barrier until we are released.  Raise an exception\n    # if the barrier is reset or broken.\n    def _wait(self, timeout):\n        if not self._cond.wait_for(lambda : self._state != 0, timeout):\n            #timed out.  Break the barrier\n            self._break()\n            raise BrokenBarrierError\n        if self._state < 0:\n            raise BrokenBarrierError\n        assert self._state == 1\n\n    # If we are the last thread to exit the barrier, signal any threads\n    # waiting for the barrier to drain.\n    def _exit(self):\n        if self._count == 0:\n            if self._state in (-1, 1):\n                #resetting or draining\n                self._state = 0\n                self._cond.notify_all()\n\n    def reset(self):\n        \"\"\"Reset the barrier to the initial state.\n\n        Any threads currently waiting will get the BrokenBarrier exception\n        raised.\n\n        \"\"\"\n        with self._cond:\n            if self._count > 0:\n                if self._state == 0:\n                    #reset the barrier, waking up threads\n                    self._state = -1\n                elif self._state == -2:\n                    #was broken, set it to reset state\n                    #which clears when the last thread exits\n                    self._state = -1\n            else:\n                self._state = 0\n            self._cond.notify_all()\n\n    def abort(self):\n        \"\"\"Place the barrier into a 'broken' state.\n\n        Useful in case of error.  Any currently waiting threads and threads\n        attempting to 'wait()' will have BrokenBarrierError raised.\n\n        \"\"\"\n        with self._cond:\n            self._break()\n\n    def _break(self):\n        # An internal error was detected.  The barrier is set to\n        # a broken state all parties awakened.\n        self._state = -2\n        self._cond.notify_all()\n\n    @property\n    def parties(self):\n        \"\"\"Return the number of threads required to trip the barrier.\"\"\"\n        return self._parties\n\n    @property\n    def n_waiting(self):\n        \"\"\"Return the number of threads currently waiting at the barrier.\"\"\"\n        # We don't need synchronization here since this is an ephemeral result\n        # anyway.  It returns the correct value in the steady state.\n        if self._state == 0:\n            return self._count\n        return 0\n\n    @property\n    def broken(self):\n        \"\"\"Return True if the barrier is in a broken state.\"\"\"\n        return self._state == -2\n\n# exception raised by the Barrier class\nclass BrokenBarrierError(RuntimeError):\n    pass\n\n\n# Helper to generate new thread names\n_counter = _count(1).__next__\ndef _newname(name_template):\n    return name_template % _counter()\n\n# Active thread administration.\n#\n# bpo-44422: Use a reentrant lock to allow reentrant calls to functions like\n# threading.enumerate().\n_active_limbo_lock = RLock()\n_active = {}    # maps thread id to Thread object\n_limbo = {}\n_dangling = WeakSet()\n\n# Set of Thread._tstate_lock locks of non-daemon threads used by _shutdown()\n# to wait until all Python thread states get deleted:\n# see Thread._set_tstate_lock().\n_shutdown_locks_lock = _allocate_lock()\n_shutdown_locks = set()\n\ndef _maintain_shutdown_locks():\n    \"\"\"\n    Drop any shutdown locks that don't correspond to running threads anymore.\n\n    Calling this from time to time avoids an ever-growing _shutdown_locks\n    set when Thread objects are not joined explicitly. See bpo-37788.\n\n    This must be called with _shutdown_locks_lock acquired.\n    \"\"\"\n    # If a lock was released, the corresponding thread has exited\n    to_remove = [lock for lock in _shutdown_locks if not lock.locked()]\n    _shutdown_locks.difference_update(to_remove)\n\n\n# Main class for threads\n\nclass Thread:\n    \"\"\"A class that represents a thread of control.\n\n    This class can be safely subclassed in a limited fashion. There are two ways\n    to specify the activity: by passing a callable object to the constructor, or\n    by overriding the run() method in a subclass.\n\n    \"\"\"\n\n    _initialized = False\n\n    def __init__(self, group=None, target=None, name=None,\n                 args=(), kwargs=None, *, daemon=None):\n        \"\"\"This constructor should always be called with keyword arguments. Arguments are:\n\n        *group* should be None; reserved for future extension when a ThreadGroup\n        class is implemented.\n\n        *target* is the callable object to be invoked by the run()\n        method. Defaults to None, meaning nothing is called.\n\n        *name* is the thread name. By default, a unique name is constructed of\n        the form \"Thread-N\" where N is a small decimal number.\n\n        *args* is a list or tuple of arguments for the target invocation. Defaults to ().\n\n        *kwargs* is a dictionary of keyword arguments for the target\n        invocation. Defaults to {}.\n\n        If a subclass overrides the constructor, it must make sure to invoke\n        the base class constructor (Thread.__init__()) before doing anything\n        else to the thread.\n\n        \"\"\"\n        assert group is None, \"group argument must be None for now\"\n        if kwargs is None:\n            kwargs = {}\n        if name:\n            name = str(name)\n        else:\n            name = _newname(\"Thread-%d\")\n            if target is not None:\n                try:\n                    target_name = target.__name__\n                    name += f\" ({target_name})\"\n                except AttributeError:\n                    pass\n\n        self._target = target\n        self._name = name\n        self._args = args\n        self._kwargs = kwargs\n        if daemon is not None:\n            self._daemonic = daemon\n        else:\n            self._daemonic = current_thread().daemon\n        self._ident = None\n        if _HAVE_THREAD_NATIVE_ID:\n            self._native_id = None\n        self._tstate_lock = None\n        self._started = Event()\n        self._is_stopped = False\n        self._initialized = True\n        # Copy of sys.stderr used by self._invoke_excepthook()\n        self._stderr = _sys.stderr\n        self._invoke_excepthook = _make_invoke_excepthook()\n        # For debugging and _after_fork()\n        _dangling.add(self)\n\n    def _reset_internal_locks(self, is_alive):\n        # private!  Called by _after_fork() to reset our internal locks as\n        # they may be in an invalid state leading to a deadlock or crash.\n        self._started._at_fork_reinit()\n        if is_alive:\n            # bpo-42350: If the fork happens when the thread is already stopped\n            # (ex: after threading._shutdown() has been called), _tstate_lock\n            # is None. Do nothing in this case.\n            if self._tstate_lock is not None:\n                self._tstate_lock._at_fork_reinit()\n                self._tstate_lock.acquire()\n        else:\n            # The thread isn't alive after fork: it doesn't have a tstate\n            # anymore.\n            self._is_stopped = True\n            self._tstate_lock = None\n\n    def __repr__(self):\n        assert self._initialized, \"Thread.__init__() was not called\"\n        status = \"initial\"\n        if self._started.is_set():\n            status = \"started\"\n        self.is_alive() # easy way to get ._is_stopped set when appropriate\n        if self._is_stopped:\n            status = \"stopped\"\n        if self._daemonic:\n            status += \" daemon\"\n        if self._ident is not None:\n            status += \" %s\" % self._ident\n        return \"<%s(%s, %s)>\" % (self.__class__.__name__, self._name, status)\n\n    def start(self):\n        \"\"\"Start the thread's activity.\n\n        It must be called at most once per thread object. It arranges for the\n        object's run() method to be invoked in a separate thread of control.\n\n        This method will raise a RuntimeError if called more than once on the\n        same thread object.\n\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"thread.__init__() not called\")\n\n        if self._started.is_set():\n            raise RuntimeError(\"threads can only be started once\")\n\n        with _active_limbo_lock:\n            _limbo[self] = self\n        try:\n            _start_new_thread(self._bootstrap, ())\n        except Exception:\n            with _active_limbo_lock:\n                del _limbo[self]\n            raise\n        self._started.wait()\n\n    def run(self):\n        \"\"\"Method representing the thread's activity.\n\n        You may override this method in a subclass. The standard run() method\n        invokes the callable object passed to the object's constructor as the\n        target argument, if any, with sequential and keyword arguments taken\n        from the args and kwargs arguments, respectively.\n\n        \"\"\"\n        try:\n            if self._target is not None:\n                self._target(*self._args, **self._kwargs)\n        finally:\n            # Avoid a refcycle if the thread is running a function with\n            # an argument that has a member that points to the thread.\n            del self._target, self._args, self._kwargs\n\n    def _bootstrap(self):\n        # Wrapper around the real bootstrap code that ignores\n        # exceptions during interpreter cleanup.  Those typically\n        # happen when a daemon thread wakes up at an unfortunate\n        # moment, finds the world around it destroyed, and raises some\n        # random exception *** while trying to report the exception in\n        # _bootstrap_inner() below ***.  Those random exceptions\n        # don't help anybody, and they confuse users, so we suppress\n        # them.  We suppress them only when it appears that the world\n        # indeed has already been destroyed, so that exceptions in\n        # _bootstrap_inner() during normal business hours are properly\n        # reported.  Also, we only suppress them for daemonic threads;\n        # if a non-daemonic encounters this, something else is wrong.\n        try:\n            self._bootstrap_inner()\n        except:\n            if self._daemonic and _sys is None:\n                return\n            raise\n\n    def _set_ident(self):\n        self._ident = get_ident()\n\n    if _HAVE_THREAD_NATIVE_ID:\n        def _set_native_id(self):\n            self._native_id = get_native_id()\n\n    def _set_tstate_lock(self):\n        \"\"\"\n        Set a lock object which will be released by the interpreter when\n        the underlying thread state (see pystate.h) gets deleted.\n        \"\"\"\n        self._tstate_lock = _set_sentinel()\n        self._tstate_lock.acquire()\n\n        if not self.daemon:\n            with _shutdown_locks_lock:\n                _maintain_shutdown_locks()\n                _shutdown_locks.add(self._tstate_lock)\n\n    def _bootstrap_inner(self):\n        try:\n            self._set_ident()\n            self._set_tstate_lock()\n            if _HAVE_THREAD_NATIVE_ID:\n                self._set_native_id()\n            self._started.set()\n            with _active_limbo_lock:\n                _active[self._ident] = self\n                del _limbo[self]\n\n            if _trace_hook:\n                _sys.settrace(_trace_hook)\n            if _profile_hook:\n                _sys.setprofile(_profile_hook)\n\n            try:\n                self.run()\n            except:\n                self._invoke_excepthook(self)\n        finally:\n            self._delete()\n\n    def _stop(self):\n        # After calling ._stop(), .is_alive() returns False and .join() returns\n        # immediately.  ._tstate_lock must be released before calling ._stop().\n        #\n        # Normal case:  C code at the end of the thread's life\n        # (release_sentinel in _threadmodule.c) releases ._tstate_lock, and\n        # that's detected by our ._wait_for_tstate_lock(), called by .join()\n        # and .is_alive().  Any number of threads _may_ call ._stop()\n        # simultaneously (for example, if multiple threads are blocked in\n        # .join() calls), and they're not serialized.  That's harmless -\n        # they'll just make redundant rebindings of ._is_stopped and\n        # ._tstate_lock.  Obscure:  we rebind ._tstate_lock last so that the\n        # \"assert self._is_stopped\" in ._wait_for_tstate_lock() always works\n        # (the assert is executed only if ._tstate_lock is None).\n        #\n        # Special case:  _main_thread releases ._tstate_lock via this\n        # module's _shutdown() function.\n        lock = self._tstate_lock\n        if lock is not None:\n            assert not lock.locked()\n        self._is_stopped = True\n        self._tstate_lock = None\n        if not self.daemon:\n            with _shutdown_locks_lock:\n                # Remove our lock and other released locks from _shutdown_locks\n                _maintain_shutdown_locks()\n\n    def _delete(self):\n        \"Remove current thread from the dict of currently running threads.\"\n        with _active_limbo_lock:\n            del _active[get_ident()]\n            # There must not be any python code between the previous line\n            # and after the lock is released.  Otherwise a tracing function\n            # could try to acquire the lock again in the same thread, (in\n            # current_thread()), and would block.\n\n    def join(self, timeout=None):\n        \"\"\"Wait until the thread terminates.\n\n        This blocks the calling thread until the thread whose join() method is\n        called terminates -- either normally or through an unhandled exception\n        or until the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof). As join() always returns None, you must call\n        is_alive() after join() to decide whether a timeout happened -- if the\n        thread is still alive, the join() call timed out.\n\n        When the timeout argument is not present or None, the operation will\n        block until the thread terminates.\n\n        A thread can be join()ed many times.\n\n        join() raises a RuntimeError if an attempt is made to join the current\n        thread as that would cause a deadlock. It is also an error to join() a\n        thread before it has been started and attempts to do so raises the same\n        exception.\n\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if not self._started.is_set():\n            raise RuntimeError(\"cannot join thread before it is started\")\n        if self is current_thread():\n            raise RuntimeError(\"cannot join current thread\")\n\n        if timeout is None:\n            self._wait_for_tstate_lock()\n        else:\n            # the behavior of a negative timeout isn't documented, but\n            # historically .join(timeout=x) for x<0 has acted as if timeout=0\n            self._wait_for_tstate_lock(timeout=max(timeout, 0))\n\n    def _wait_for_tstate_lock(self, block=True, timeout=-1):\n        # Issue #18808: wait for the thread state to be gone.\n        # At the end of the thread's life, after all knowledge of the thread\n        # is removed from C data structures, C code releases our _tstate_lock.\n        # This method passes its arguments to _tstate_lock.acquire().\n        # If the lock is acquired, the C code is done, and self._stop() is\n        # called.  That sets ._is_stopped to True, and ._tstate_lock to None.\n        lock = self._tstate_lock\n        if lock is None:\n            # already determined that the C code is done\n            assert self._is_stopped\n            return\n\n        try:\n            if lock.acquire(block, timeout):\n                lock.release()\n                self._stop()\n        except:\n            if lock.locked():\n                # bpo-45274: lock.acquire() acquired the lock, but the function\n                # was interrupted with an exception before reaching the\n                # lock.release(). It can happen if a signal handler raises an\n                # exception, like CTRL+C which raises KeyboardInterrupt.\n                lock.release()\n                self._stop()\n            raise\n\n    @property\n    def name(self):\n        \"\"\"A string used for identification purposes only.\n\n        It has no semantics. Multiple threads may be given the same name. The\n        initial name is set by the constructor.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        assert self._initialized, \"Thread.__init__() not called\"\n        self._name = str(name)\n\n    @property\n    def ident(self):\n        \"\"\"Thread identifier of this thread or None if it has not been started.\n\n        This is a nonzero integer. See the get_ident() function. Thread\n        identifiers may be recycled when a thread exits and another thread is\n        created. The identifier is available even after the thread has exited.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._ident\n\n    if _HAVE_THREAD_NATIVE_ID:\n        @property\n        def native_id(self):\n            \"\"\"Native integral thread ID of this thread, or None if it has not been started.\n\n            This is a non-negative integer. See the get_native_id() function.\n            This represents the Thread ID as reported by the kernel.\n\n            \"\"\"\n            assert self._initialized, \"Thread.__init__() not called\"\n            return self._native_id\n\n    def is_alive(self):\n        \"\"\"Return whether the thread is alive.\n\n        This method returns True just before the run() method starts until just\n        after the run() method terminates. See also the module function\n        enumerate().\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        if self._is_stopped or not self._started.is_set():\n            return False\n        self._wait_for_tstate_lock(False)\n        return not self._is_stopped\n\n    @property\n    def daemon(self):\n        \"\"\"A boolean value indicating whether this thread is a daemon thread.\n\n        This must be set before start() is called, otherwise RuntimeError is\n        raised. Its initial value is inherited from the creating thread; the\n        main thread is not a daemon thread and therefore all threads created in\n        the main thread default to daemon = False.\n\n        The entire Python program exits when only daemon threads are left.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._daemonic\n\n    @daemon.setter\n    def daemon(self, daemonic):\n        if not self._initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if self._started.is_set():\n            raise RuntimeError(\"cannot set daemon status of active thread\")\n        self._daemonic = daemonic\n\n    def isDaemon(self):\n        \"\"\"Return whether this thread is a daemon.\n\n        This method is deprecated, use the daemon attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('isDaemon() is deprecated, get the daemon attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.daemon\n\n    def setDaemon(self, daemonic):\n        \"\"\"Set whether this thread is a daemon.\n\n        This method is deprecated, use the .daemon property instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('setDaemon() is deprecated, set the daemon attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        self.daemon = daemonic\n\n    def getName(self):\n        \"\"\"Return a string used for identification purposes only.\n\n        This method is deprecated, use the name attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('getName() is deprecated, get the name attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.name\n\n    def setName(self, name):\n        \"\"\"Set the name string for this thread.\n\n        This method is deprecated, use the name attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('setName() is deprecated, set the name attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        self.name = name\n\n\ntry:\n    from _thread import (_excepthook as excepthook,\n                         _ExceptHookArgs as ExceptHookArgs)\nexcept ImportError:\n    # Simple Python implementation if _thread._excepthook() is not available\n    from traceback import print_exception as _print_exception\n    from collections import namedtuple\n\n    _ExceptHookArgs = namedtuple(\n        'ExceptHookArgs',\n        'exc_type exc_value exc_traceback thread')\n\n    def ExceptHookArgs(args):\n        return _ExceptHookArgs(*args)\n\n    def excepthook(args, /):\n        \"\"\"\n        Handle uncaught Thread.run() exception.\n        \"\"\"\n        if args.exc_type == SystemExit:\n            # silently ignore SystemExit\n            return\n\n        if _sys is not None and _sys.stderr is not None:\n            stderr = _sys.stderr\n        elif args.thread is not None:\n            stderr = args.thread._stderr\n            if stderr is None:\n                # do nothing if sys.stderr is None and sys.stderr was None\n                # when the thread was created\n                return\n        else:\n            # do nothing if sys.stderr is None and args.thread is None\n            return\n\n        if args.thread is not None:\n            name = args.thread.name\n        else:\n            name = get_ident()\n        print(f\"Exception in thread {name}:\",\n              file=stderr, flush=True)\n        _print_exception(args.exc_type, args.exc_value, args.exc_traceback,\n                         file=stderr)\n        stderr.flush()\n\n\n# Original value of threading.excepthook\n__excepthook__ = excepthook\n\n\ndef _make_invoke_excepthook():\n    # Create a local namespace to ensure that variables remain alive\n    # when _invoke_excepthook() is called, even if it is called late during\n    # Python shutdown. It is mostly needed for daemon threads.\n\n    old_excepthook = excepthook\n    old_sys_excepthook = _sys.excepthook\n    if old_excepthook is None:\n        raise RuntimeError(\"threading.excepthook is None\")\n    if old_sys_excepthook is None:\n        raise RuntimeError(\"sys.excepthook is None\")\n\n    sys_exc_info = _sys.exc_info\n    local_print = print\n    local_sys = _sys\n\n    def invoke_excepthook(thread):\n        global excepthook\n        try:\n            hook = excepthook\n            if hook is None:\n                hook = old_excepthook\n\n            args = ExceptHookArgs([*sys_exc_info(), thread])\n\n            hook(args)\n        except Exception as exc:\n            exc.__suppress_context__ = True\n            del exc\n\n            if local_sys is not None and local_sys.stderr is not None:\n                stderr = local_sys.stderr\n            else:\n                stderr = thread._stderr\n\n            local_print(\"Exception in threading.excepthook:\",\n                        file=stderr, flush=True)\n\n            if local_sys is not None and local_sys.excepthook is not None:\n                sys_excepthook = local_sys.excepthook\n            else:\n                sys_excepthook = old_sys_excepthook\n\n            sys_excepthook(*sys_exc_info())\n        finally:\n            # Break reference cycle (exception stored in a variable)\n            args = None\n\n    return invoke_excepthook\n\n\n# The timer class was contributed by Itamar Shtull-Trauring\n\nclass Timer(Thread):\n    \"\"\"Call a function after a specified number of seconds:\n\n            t = Timer(30.0, f, args=None, kwargs=None)\n            t.start()\n            t.cancel()     # stop the timer's action if it's still waiting\n\n    \"\"\"\n\n    def __init__(self, interval, function, args=None, kwargs=None):\n        Thread.__init__(self)\n        self.interval = interval\n        self.function = function\n        self.args = args if args is not None else []\n        self.kwargs = kwargs if kwargs is not None else {}\n        self.finished = Event()\n\n    def cancel(self):\n        \"\"\"Stop the timer if it hasn't finished yet.\"\"\"\n        self.finished.set()\n\n    def run(self):\n        self.finished.wait(self.interval)\n        if not self.finished.is_set():\n            self.function(*self.args, **self.kwargs)\n        self.finished.set()\n\n\n# Special thread class to represent the main thread\n\nclass _MainThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=\"MainThread\", daemon=False)\n        self._set_tstate_lock()\n        self._started.set()\n        self._set_ident()\n        if _HAVE_THREAD_NATIVE_ID:\n            self._set_native_id()\n        with _active_limbo_lock:\n            _active[self._ident] = self\n\n\n# Dummy thread class to represent threads not started here.\n# These aren't garbage collected when they die, nor can they be waited for.\n# If they invoke anything in threading.py that calls current_thread(), they\n# leave an entry in the _active dict forever after.\n# Their purpose is to return *something* from current_thread().\n# They are marked as daemon threads so we won't wait for them\n# when we exit (conform previous semantics).\n\nclass _DummyThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=_newname(\"Dummy-%d\"), daemon=True)\n\n        self._started.set()\n        self._set_ident()\n        if _HAVE_THREAD_NATIVE_ID:\n            self._set_native_id()\n        with _active_limbo_lock:\n            _active[self._ident] = self\n\n    def _stop(self):\n        pass\n\n    def is_alive(self):\n        assert not self._is_stopped and self._started.is_set()\n        return True\n\n    def join(self, timeout=None):\n        assert False, \"cannot join a dummy thread\"\n\n\n# Global API functions\n\ndef current_thread():\n    \"\"\"Return the current Thread object, corresponding to the caller's thread of control.\n\n    If the caller's thread of control was not created through the threading\n    module, a dummy thread object with limited functionality is returned.\n\n    \"\"\"\n    try:\n        return _active[get_ident()]\n    except KeyError:\n        return _DummyThread()\n\ndef currentThread():\n    \"\"\"Return the current Thread object, corresponding to the caller's thread of control.\n\n    This function is deprecated, use current_thread() instead.\n\n    \"\"\"\n    import warnings\n    warnings.warn('currentThread() is deprecated, use current_thread() instead',\n                  DeprecationWarning, stacklevel=2)\n    return current_thread()\n\ndef active_count():\n    \"\"\"Return the number of Thread objects currently alive.\n\n    The returned count is equal to the length of the list returned by\n    enumerate().\n\n    \"\"\"\n    with _active_limbo_lock:\n        return len(_active) + len(_limbo)\n\ndef activeCount():\n    \"\"\"Return the number of Thread objects currently alive.\n\n    This function is deprecated, use active_count() instead.\n\n    \"\"\"\n    import warnings\n    warnings.warn('activeCount() is deprecated, use active_count() instead',\n                  DeprecationWarning, stacklevel=2)\n    return active_count()\n\ndef _enumerate():\n    # Same as enumerate(), but without the lock. Internal use only.\n    return list(_active.values()) + list(_limbo.values())\n\ndef enumerate():\n    \"\"\"Return a list of all Thread objects currently alive.\n\n    The list includes daemonic threads, dummy thread objects created by\n    current_thread(), and the main thread. It excludes terminated threads and\n    threads that have not yet been started.\n\n    \"\"\"\n    with _active_limbo_lock:\n        return list(_active.values()) + list(_limbo.values())\n\n\n_threading_atexits = []\n_SHUTTING_DOWN = False\n\ndef _register_atexit(func, *arg, **kwargs):\n    \"\"\"CPython internal: register *func* to be called before joining threads.\n\n    The registered *func* is called with its arguments just before all\n    non-daemon threads are joined in `_shutdown()`. It provides a similar\n    purpose to `atexit.register()`, but its functions are called prior to\n    threading shutdown instead of interpreter shutdown.\n\n    For similarity to atexit, the registered functions are called in reverse.\n    \"\"\"\n    if _SHUTTING_DOWN:\n        raise RuntimeError(\"can't register atexit after shutdown\")\n\n    call = functools.partial(func, *arg, **kwargs)\n    _threading_atexits.append(call)\n\n\nfrom _thread import stack_size\n\n# Create the main thread object,\n# and make it available for the interpreter\n# (Py_Main) as threading._shutdown.\n\n_main_thread = _MainThread()\n\ndef _shutdown():\n    \"\"\"\n    Wait until the Python thread state of all non-daemon threads get deleted.\n    \"\"\"\n    # Obscure:  other threads may be waiting to join _main_thread.  That's\n    # dubious, but some code does it.  We can't wait for C code to release\n    # the main thread's tstate_lock - that won't happen until the interpreter\n    # is nearly dead.  So we release it here.  Note that just calling _stop()\n    # isn't enough:  other threads may already be waiting on _tstate_lock.\n    if _main_thread._is_stopped:\n        # _shutdown() was already called\n        return\n\n    global _SHUTTING_DOWN\n    _SHUTTING_DOWN = True\n\n    # Call registered threading atexit functions before threads are joined.\n    # Order is reversed, similar to atexit.\n    for atexit_call in reversed(_threading_atexits):\n        atexit_call()\n\n    # Main thread\n    if _main_thread.ident == get_ident():\n        tlock = _main_thread._tstate_lock\n        # The main thread isn't finished yet, so its thread state lock can't\n        # have been released.\n        assert tlock is not None\n        assert tlock.locked()\n        tlock.release()\n        _main_thread._stop()\n    else:\n        # bpo-1596321: _shutdown() must be called in the main thread.\n        # If the threading module was not imported by the main thread,\n        # _main_thread is the thread which imported the threading module.\n        # In this case, ignore _main_thread, similar behavior than for threads\n        # spawned by C libraries or using _thread.start_new_thread().\n        pass\n\n    # Join all non-deamon threads\n    while True:\n        with _shutdown_locks_lock:\n            locks = list(_shutdown_locks)\n            _shutdown_locks.clear()\n\n        if not locks:\n            break\n\n        for lock in locks:\n            # mimic Thread.join()\n            lock.acquire()\n            lock.release()\n\n        # new threads can be spawned while we were waiting for the other\n        # threads to complete\n\n\ndef main_thread():\n    \"\"\"Return the main thread object.\n\n    In normal conditions, the main thread is the thread from which the\n    Python interpreter was started.\n    \"\"\"\n    return _main_thread\n\n# get thread-local implementation, either from the thread\n# module, or from the python fallback\n\ntry:\n    from _thread import _local as local\nexcept ImportError:\n    from _threading_local import local\n\n\ndef _after_fork():\n    \"\"\"\n    Cleanup threading module state that should not exist after a fork.\n    \"\"\"\n    # Reset _active_limbo_lock, in case we forked while the lock was held\n    # by another (non-forked) thread.  http://bugs.python.org/issue874900\n    global _active_limbo_lock, _main_thread\n    global _shutdown_locks_lock, _shutdown_locks\n    _active_limbo_lock = RLock()\n\n    # fork() only copied the current thread; clear references to others.\n    new_active = {}\n\n    try:\n        current = _active[get_ident()]\n    except KeyError:\n        # fork() was called in a thread which was not spawned\n        # by threading.Thread. For example, a thread spawned\n        # by thread.start_new_thread().\n        current = _MainThread()\n\n    _main_thread = current\n\n    # reset _shutdown() locks: threads re-register their _tstate_lock below\n    _shutdown_locks_lock = _allocate_lock()\n    _shutdown_locks = set()\n\n    with _active_limbo_lock:\n        # Dangling thread instances must still have their locks reset,\n        # because someone may join() them.\n        threads = set(_enumerate())\n        threads.update(_dangling)\n        for thread in threads:\n            # Any lock/condition variable may be currently locked or in an\n            # invalid state, so we reinitialize them.\n            if thread is current:\n                # There is only one active thread. We reset the ident to\n                # its new value since it can have changed.\n                thread._reset_internal_locks(True)\n                ident = get_ident()\n                if isinstance(thread, _DummyThread):\n                    thread.__class__ = _MainThread\n                    thread._name = 'MainThread'\n                    thread._daemonic = False\n                    thread._set_tstate_lock()\n                thread._ident = ident\n                new_active[ident] = thread\n            else:\n                # All the others are already stopped.\n                thread._reset_internal_locks(False)\n                thread._stop()\n\n        _limbo.clear()\n        _active.clear()\n        _active.update(new_active)\n        assert len(_active) == 1\n\n\nif hasattr(_os, \"register_at_fork\"):\n    _os.register_at_fork(after_in_child=_after_fork)\n", 1673], "/usr/local/lib/python3.11/concurrent/futures/_base.py": ["# Copyright 2009 Brian Quinlan. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n__author__ = 'Brian Quinlan (brian@sweetapp.com)'\n\nimport collections\nimport logging\nimport threading\nimport time\nimport types\n\nFIRST_COMPLETED = 'FIRST_COMPLETED'\nFIRST_EXCEPTION = 'FIRST_EXCEPTION'\nALL_COMPLETED = 'ALL_COMPLETED'\n_AS_COMPLETED = '_AS_COMPLETED'\n\n# Possible future states (for internal use by the futures package).\nPENDING = 'PENDING'\nRUNNING = 'RUNNING'\n# The future was cancelled by the user...\nCANCELLED = 'CANCELLED'\n# ...and _Waiter.add_cancelled() was called by a worker.\nCANCELLED_AND_NOTIFIED = 'CANCELLED_AND_NOTIFIED'\nFINISHED = 'FINISHED'\n\n_FUTURE_STATES = [\n    PENDING,\n    RUNNING,\n    CANCELLED,\n    CANCELLED_AND_NOTIFIED,\n    FINISHED\n]\n\n_STATE_TO_DESCRIPTION_MAP = {\n    PENDING: \"pending\",\n    RUNNING: \"running\",\n    CANCELLED: \"cancelled\",\n    CANCELLED_AND_NOTIFIED: \"cancelled\",\n    FINISHED: \"finished\"\n}\n\n# Logger for internal use by the futures package.\nLOGGER = logging.getLogger(\"concurrent.futures\")\n\nclass Error(Exception):\n    \"\"\"Base class for all future-related exceptions.\"\"\"\n    pass\n\nclass CancelledError(Error):\n    \"\"\"The Future was cancelled.\"\"\"\n    pass\n\nTimeoutError = TimeoutError  # make local alias for the standard exception\n\nclass InvalidStateError(Error):\n    \"\"\"The operation is not allowed in this state.\"\"\"\n    pass\n\nclass _Waiter(object):\n    \"\"\"Provides the event that wait() and as_completed() block on.\"\"\"\n    def __init__(self):\n        self.event = threading.Event()\n        self.finished_futures = []\n\n    def add_result(self, future):\n        self.finished_futures.append(future)\n\n    def add_exception(self, future):\n        self.finished_futures.append(future)\n\n    def add_cancelled(self, future):\n        self.finished_futures.append(future)\n\nclass _AsCompletedWaiter(_Waiter):\n    \"\"\"Used by as_completed().\"\"\"\n\n    def __init__(self):\n        super(_AsCompletedWaiter, self).__init__()\n        self.lock = threading.Lock()\n\n    def add_result(self, future):\n        with self.lock:\n            super(_AsCompletedWaiter, self).add_result(future)\n            self.event.set()\n\n    def add_exception(self, future):\n        with self.lock:\n            super(_AsCompletedWaiter, self).add_exception(future)\n            self.event.set()\n\n    def add_cancelled(self, future):\n        with self.lock:\n            super(_AsCompletedWaiter, self).add_cancelled(future)\n            self.event.set()\n\nclass _FirstCompletedWaiter(_Waiter):\n    \"\"\"Used by wait(return_when=FIRST_COMPLETED).\"\"\"\n\n    def add_result(self, future):\n        super().add_result(future)\n        self.event.set()\n\n    def add_exception(self, future):\n        super().add_exception(future)\n        self.event.set()\n\n    def add_cancelled(self, future):\n        super().add_cancelled(future)\n        self.event.set()\n\nclass _AllCompletedWaiter(_Waiter):\n    \"\"\"Used by wait(return_when=FIRST_EXCEPTION and ALL_COMPLETED).\"\"\"\n\n    def __init__(self, num_pending_calls, stop_on_exception):\n        self.num_pending_calls = num_pending_calls\n        self.stop_on_exception = stop_on_exception\n        self.lock = threading.Lock()\n        super().__init__()\n\n    def _decrement_pending_calls(self):\n        with self.lock:\n            self.num_pending_calls -= 1\n            if not self.num_pending_calls:\n                self.event.set()\n\n    def add_result(self, future):\n        super().add_result(future)\n        self._decrement_pending_calls()\n\n    def add_exception(self, future):\n        super().add_exception(future)\n        if self.stop_on_exception:\n            self.event.set()\n        else:\n            self._decrement_pending_calls()\n\n    def add_cancelled(self, future):\n        super().add_cancelled(future)\n        self._decrement_pending_calls()\n\nclass _AcquireFutures(object):\n    \"\"\"A context manager that does an ordered acquire of Future conditions.\"\"\"\n\n    def __init__(self, futures):\n        self.futures = sorted(futures, key=id)\n\n    def __enter__(self):\n        for future in self.futures:\n            future._condition.acquire()\n\n    def __exit__(self, *args):\n        for future in self.futures:\n            future._condition.release()\n\ndef _create_and_install_waiters(fs, return_when):\n    if return_when == _AS_COMPLETED:\n        waiter = _AsCompletedWaiter()\n    elif return_when == FIRST_COMPLETED:\n        waiter = _FirstCompletedWaiter()\n    else:\n        pending_count = sum(\n                f._state not in [CANCELLED_AND_NOTIFIED, FINISHED] for f in fs)\n\n        if return_when == FIRST_EXCEPTION:\n            waiter = _AllCompletedWaiter(pending_count, stop_on_exception=True)\n        elif return_when == ALL_COMPLETED:\n            waiter = _AllCompletedWaiter(pending_count, stop_on_exception=False)\n        else:\n            raise ValueError(\"Invalid return condition: %r\" % return_when)\n\n    for f in fs:\n        f._waiters.append(waiter)\n\n    return waiter\n\n\ndef _yield_finished_futures(fs, waiter, ref_collect):\n    \"\"\"\n    Iterate on the list *fs*, yielding finished futures one by one in\n    reverse order.\n    Before yielding a future, *waiter* is removed from its waiters\n    and the future is removed from each set in the collection of sets\n    *ref_collect*.\n\n    The aim of this function is to avoid keeping stale references after\n    the future is yielded and before the iterator resumes.\n    \"\"\"\n    while fs:\n        f = fs[-1]\n        for futures_set in ref_collect:\n            futures_set.remove(f)\n        with f._condition:\n            f._waiters.remove(waiter)\n        del f\n        # Careful not to keep a reference to the popped value\n        yield fs.pop()\n\n\ndef as_completed(fs, timeout=None):\n    \"\"\"An iterator over the given futures that yields each as it completes.\n\n    Args:\n        fs: The sequence of Futures (possibly created by different Executors) to\n            iterate over.\n        timeout: The maximum number of seconds to wait. If None, then there\n            is no limit on the wait time.\n\n    Returns:\n        An iterator that yields the given Futures as they complete (finished or\n        cancelled). If any given Futures are duplicated, they will be returned\n        once.\n\n    Raises:\n        TimeoutError: If the entire result iterator could not be generated\n            before the given timeout.\n    \"\"\"\n    if timeout is not None:\n        end_time = timeout + time.monotonic()\n\n    fs = set(fs)\n    total_futures = len(fs)\n    with _AcquireFutures(fs):\n        finished = set(\n                f for f in fs\n                if f._state in [CANCELLED_AND_NOTIFIED, FINISHED])\n        pending = fs - finished\n        waiter = _create_and_install_waiters(fs, _AS_COMPLETED)\n    finished = list(finished)\n    try:\n        yield from _yield_finished_futures(finished, waiter,\n                                           ref_collect=(fs,))\n\n        while pending:\n            if timeout is None:\n                wait_timeout = None\n            else:\n                wait_timeout = end_time - time.monotonic()\n                if wait_timeout < 0:\n                    raise TimeoutError(\n                            '%d (of %d) futures unfinished' % (\n                            len(pending), total_futures))\n\n            waiter.event.wait(wait_timeout)\n\n            with waiter.lock:\n                finished = waiter.finished_futures\n                waiter.finished_futures = []\n                waiter.event.clear()\n\n            # reverse to keep finishing order\n            finished.reverse()\n            yield from _yield_finished_futures(finished, waiter,\n                                               ref_collect=(fs, pending))\n\n    finally:\n        # Remove waiter from unfinished futures\n        for f in fs:\n            with f._condition:\n                f._waiters.remove(waiter)\n\nDoneAndNotDoneFutures = collections.namedtuple(\n        'DoneAndNotDoneFutures', 'done not_done')\ndef wait(fs, timeout=None, return_when=ALL_COMPLETED):\n    \"\"\"Wait for the futures in the given sequence to complete.\n\n    Args:\n        fs: The sequence of Futures (possibly created by different Executors) to\n            wait upon.\n        timeout: The maximum number of seconds to wait. If None, then there\n            is no limit on the wait time.\n        return_when: Indicates when this function should return. The options\n            are:\n\n            FIRST_COMPLETED - Return when any future finishes or is\n                              cancelled.\n            FIRST_EXCEPTION - Return when any future finishes by raising an\n                              exception. If no future raises an exception\n                              then it is equivalent to ALL_COMPLETED.\n            ALL_COMPLETED -   Return when all futures finish or are cancelled.\n\n    Returns:\n        A named 2-tuple of sets. The first set, named 'done', contains the\n        futures that completed (is finished or cancelled) before the wait\n        completed. The second set, named 'not_done', contains uncompleted\n        futures. Duplicate futures given to *fs* are removed and will be\n        returned only once.\n    \"\"\"\n    fs = set(fs)\n    with _AcquireFutures(fs):\n        done = {f for f in fs\n                   if f._state in [CANCELLED_AND_NOTIFIED, FINISHED]}\n        not_done = fs - done\n        if (return_when == FIRST_COMPLETED) and done:\n            return DoneAndNotDoneFutures(done, not_done)\n        elif (return_when == FIRST_EXCEPTION) and done:\n            if any(f for f in done\n                   if not f.cancelled() and f.exception() is not None):\n                return DoneAndNotDoneFutures(done, not_done)\n\n        if len(done) == len(fs):\n            return DoneAndNotDoneFutures(done, not_done)\n\n        waiter = _create_and_install_waiters(fs, return_when)\n\n    waiter.event.wait(timeout)\n    for f in fs:\n        with f._condition:\n            f._waiters.remove(waiter)\n\n    done.update(waiter.finished_futures)\n    return DoneAndNotDoneFutures(done, fs - done)\n\n\ndef _result_or_cancel(fut, timeout=None):\n    try:\n        try:\n            return fut.result(timeout)\n        finally:\n            fut.cancel()\n    finally:\n        # Break a reference cycle with the exception in self._exception\n        del fut\n\n\nclass Future(object):\n    \"\"\"Represents the result of an asynchronous computation.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes the future. Should not be called by clients.\"\"\"\n        self._condition = threading.Condition()\n        self._state = PENDING\n        self._result = None\n        self._exception = None\n        self._waiters = []\n        self._done_callbacks = []\n\n    def _invoke_callbacks(self):\n        for callback in self._done_callbacks:\n            try:\n                callback(self)\n            except Exception:\n                LOGGER.exception('exception calling callback for %r', self)\n\n    def __repr__(self):\n        with self._condition:\n            if self._state == FINISHED:\n                if self._exception:\n                    return '<%s at %#x state=%s raised %s>' % (\n                        self.__class__.__name__,\n                        id(self),\n                        _STATE_TO_DESCRIPTION_MAP[self._state],\n                        self._exception.__class__.__name__)\n                else:\n                    return '<%s at %#x state=%s returned %s>' % (\n                        self.__class__.__name__,\n                        id(self),\n                        _STATE_TO_DESCRIPTION_MAP[self._state],\n                        self._result.__class__.__name__)\n            return '<%s at %#x state=%s>' % (\n                    self.__class__.__name__,\n                    id(self),\n                   _STATE_TO_DESCRIPTION_MAP[self._state])\n\n    def cancel(self):\n        \"\"\"Cancel the future if possible.\n\n        Returns True if the future was cancelled, False otherwise. A future\n        cannot be cancelled if it is running or has already completed.\n        \"\"\"\n        with self._condition:\n            if self._state in [RUNNING, FINISHED]:\n                return False\n\n            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n                return True\n\n            self._state = CANCELLED\n            self._condition.notify_all()\n\n        self._invoke_callbacks()\n        return True\n\n    def cancelled(self):\n        \"\"\"Return True if the future was cancelled.\"\"\"\n        with self._condition:\n            return self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]\n\n    def running(self):\n        \"\"\"Return True if the future is currently executing.\"\"\"\n        with self._condition:\n            return self._state == RUNNING\n\n    def done(self):\n        \"\"\"Return True if the future was cancelled or finished executing.\"\"\"\n        with self._condition:\n            return self._state in [CANCELLED, CANCELLED_AND_NOTIFIED, FINISHED]\n\n    def __get_result(self):\n        if self._exception:\n            try:\n                raise self._exception\n            finally:\n                # Break a reference cycle with the exception in self._exception\n                self = None\n        else:\n            return self._result\n\n    def add_done_callback(self, fn):\n        \"\"\"Attaches a callable that will be called when the future finishes.\n\n        Args:\n            fn: A callable that will be called with this future as its only\n                argument when the future completes or is cancelled. The callable\n                will always be called by a thread in the same process in which\n                it was added. If the future has already completed or been\n                cancelled then the callable will be called immediately. These\n                callables are called in the order that they were added.\n        \"\"\"\n        with self._condition:\n            if self._state not in [CANCELLED, CANCELLED_AND_NOTIFIED, FINISHED]:\n                self._done_callbacks.append(fn)\n                return\n        try:\n            fn(self)\n        except Exception:\n            LOGGER.exception('exception calling callback for %r', self)\n\n    def result(self, timeout=None):\n        \"\"\"Return the result of the call that the future represents.\n\n        Args:\n            timeout: The number of seconds to wait for the result if the future\n                isn't done. If None, then there is no limit on the wait time.\n\n        Returns:\n            The result of the call that the future represents.\n\n        Raises:\n            CancelledError: If the future was cancelled.\n            TimeoutError: If the future didn't finish executing before the given\n                timeout.\n            Exception: If the call raised then that exception will be raised.\n        \"\"\"\n        try:\n            with self._condition:\n                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n                    raise CancelledError()\n                elif self._state == FINISHED:\n                    return self.__get_result()\n\n                self._condition.wait(timeout)\n\n                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n                    raise CancelledError()\n                elif self._state == FINISHED:\n                    return self.__get_result()\n                else:\n                    raise TimeoutError()\n        finally:\n            # Break a reference cycle with the exception in self._exception\n            self = None\n\n    def exception(self, timeout=None):\n        \"\"\"Return the exception raised by the call that the future represents.\n\n        Args:\n            timeout: The number of seconds to wait for the exception if the\n                future isn't done. If None, then there is no limit on the wait\n                time.\n\n        Returns:\n            The exception raised by the call that the future represents or None\n            if the call completed without raising.\n\n        Raises:\n            CancelledError: If the future was cancelled.\n            TimeoutError: If the future didn't finish executing before the given\n                timeout.\n        \"\"\"\n\n        with self._condition:\n            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n                raise CancelledError()\n            elif self._state == FINISHED:\n                return self._exception\n\n            self._condition.wait(timeout)\n\n            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n                raise CancelledError()\n            elif self._state == FINISHED:\n                return self._exception\n            else:\n                raise TimeoutError()\n\n    # The following methods should only be used by Executors and in tests.\n    def set_running_or_notify_cancel(self):\n        \"\"\"Mark the future as running or process any cancel notifications.\n\n        Should only be used by Executor implementations and unit tests.\n\n        If the future has been cancelled (cancel() was called and returned\n        True) then any threads waiting on the future completing (though calls\n        to as_completed() or wait()) are notified and False is returned.\n\n        If the future was not cancelled then it is put in the running state\n        (future calls to running() will return True) and True is returned.\n\n        This method should be called by Executor implementations before\n        executing the work associated with this future. If this method returns\n        False then the work should not be executed.\n\n        Returns:\n            False if the Future was cancelled, True otherwise.\n\n        Raises:\n            RuntimeError: if this method was already called or if set_result()\n                or set_exception() was called.\n        \"\"\"\n        with self._condition:\n            if self._state == CANCELLED:\n                self._state = CANCELLED_AND_NOTIFIED\n                for waiter in self._waiters:\n                    waiter.add_cancelled(self)\n                # self._condition.notify_all() is not necessary because\n                # self.cancel() triggers a notification.\n                return False\n            elif self._state == PENDING:\n                self._state = RUNNING\n                return True\n            else:\n                LOGGER.critical('Future %s in unexpected state: %s',\n                                id(self),\n                                self._state)\n                raise RuntimeError('Future in unexpected state')\n\n    def set_result(self, result):\n        \"\"\"Sets the return value of work associated with the future.\n\n        Should only be used by Executor implementations and unit tests.\n        \"\"\"\n        with self._condition:\n            if self._state in {CANCELLED, CANCELLED_AND_NOTIFIED, FINISHED}:\n                raise InvalidStateError('{}: {!r}'.format(self._state, self))\n            self._result = result\n            self._state = FINISHED\n            for waiter in self._waiters:\n                waiter.add_result(self)\n            self._condition.notify_all()\n        self._invoke_callbacks()\n\n    def set_exception(self, exception):\n        \"\"\"Sets the result of the future as being the given exception.\n\n        Should only be used by Executor implementations and unit tests.\n        \"\"\"\n        with self._condition:\n            if self._state in {CANCELLED, CANCELLED_AND_NOTIFIED, FINISHED}:\n                raise InvalidStateError('{}: {!r}'.format(self._state, self))\n            self._exception = exception\n            self._state = FINISHED\n            for waiter in self._waiters:\n                waiter.add_exception(self)\n            self._condition.notify_all()\n        self._invoke_callbacks()\n\n    __class_getitem__ = classmethod(types.GenericAlias)\n\nclass Executor(object):\n    \"\"\"This is an abstract base class for concrete asynchronous executors.\"\"\"\n\n    def submit(self, fn, /, *args, **kwargs):\n        \"\"\"Submits a callable to be executed with the given arguments.\n\n        Schedules the callable to be executed as fn(*args, **kwargs) and returns\n        a Future instance representing the execution of the callable.\n\n        Returns:\n            A Future representing the given call.\n        \"\"\"\n        raise NotImplementedError()\n\n    def map(self, fn, *iterables, timeout=None, chunksize=1):\n        \"\"\"Returns an iterator equivalent to map(fn, iter).\n\n        Args:\n            fn: A callable that will take as many arguments as there are\n                passed iterables.\n            timeout: The maximum number of seconds to wait. If None, then there\n                is no limit on the wait time.\n            chunksize: The size of the chunks the iterable will be broken into\n                before being passed to a child process. This argument is only\n                used by ProcessPoolExecutor; it is ignored by\n                ThreadPoolExecutor.\n\n        Returns:\n            An iterator equivalent to: map(func, *iterables) but the calls may\n            be evaluated out-of-order.\n\n        Raises:\n            TimeoutError: If the entire result iterator could not be generated\n                before the given timeout.\n            Exception: If fn(*args) raises for any values.\n        \"\"\"\n        if timeout is not None:\n            end_time = timeout + time.monotonic()\n\n        fs = [self.submit(fn, *args) for args in zip(*iterables)]\n\n        # Yield must be hidden in closure so that the futures are submitted\n        # before the first iterator value is required.\n        def result_iterator():\n            try:\n                # reverse to keep finishing order\n                fs.reverse()\n                while fs:\n                    # Careful not to keep a reference to the popped future\n                    if timeout is None:\n                        yield _result_or_cancel(fs.pop())\n                    else:\n                        yield _result_or_cancel(fs.pop(), end_time - time.monotonic())\n            finally:\n                for future in fs:\n                    future.cancel()\n        return result_iterator()\n\n    def shutdown(self, wait=True, *, cancel_futures=False):\n        \"\"\"Clean-up the resources associated with the Executor.\n\n        It is safe to call this method several times. Otherwise, no other\n        methods can be called after this one.\n\n        Args:\n            wait: If True then shutdown will not return until all running\n                futures have finished executing and the resources used by the\n                executor have been reclaimed.\n            cancel_futures: If True then shutdown will cancel all pending\n                futures. Futures that are completed or running will not be\n                cancelled.\n        \"\"\"\n        pass\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.shutdown(wait=True)\n        return False\n\n\nclass BrokenExecutor(RuntimeError):\n    \"\"\"\n    Raised when a executor has become non-functional after a severe failure.\n    \"\"\"\n", 654], "/usr/local/lib/python3.11/concurrent/futures/thread.py": ["# Copyright 2009 Brian Quinlan. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Implements ThreadPoolExecutor.\"\"\"\n\n__author__ = 'Brian Quinlan (brian@sweetapp.com)'\n\nfrom concurrent.futures import _base\nimport itertools\nimport queue\nimport threading\nimport types\nimport weakref\nimport os\n\n\n_threads_queues = weakref.WeakKeyDictionary()\n_shutdown = False\n# Lock that ensures that new workers are not created while the interpreter is\n# shutting down. Must be held while mutating _threads_queues and _shutdown.\n_global_shutdown_lock = threading.Lock()\n\ndef _python_exit():\n    global _shutdown\n    with _global_shutdown_lock:\n        _shutdown = True\n    items = list(_threads_queues.items())\n    for t, q in items:\n        q.put(None)\n    for t, q in items:\n        t.join()\n\n# Register for `_python_exit()` to be called just before joining all\n# non-daemon threads. This is used instead of `atexit.register()` for\n# compatibility with subinterpreters, which no longer support daemon threads.\n# See bpo-39812 for context.\nthreading._register_atexit(_python_exit)\n\n# At fork, reinitialize the `_global_shutdown_lock` lock in the child process\nif hasattr(os, 'register_at_fork'):\n    os.register_at_fork(before=_global_shutdown_lock.acquire,\n                        after_in_child=_global_shutdown_lock._at_fork_reinit,\n                        after_in_parent=_global_shutdown_lock.release)\n\n\nclass _WorkItem(object):\n    def __init__(self, future, fn, args, kwargs):\n        self.future = future\n        self.fn = fn\n        self.args = args\n        self.kwargs = kwargs\n\n    def run(self):\n        if not self.future.set_running_or_notify_cancel():\n            return\n\n        try:\n            result = self.fn(*self.args, **self.kwargs)\n        except BaseException as exc:\n            self.future.set_exception(exc)\n            # Break a reference cycle with the exception 'exc'\n            self = None\n        else:\n            self.future.set_result(result)\n\n    __class_getitem__ = classmethod(types.GenericAlias)\n\n\ndef _worker(executor_reference, work_queue, initializer, initargs):\n    if initializer is not None:\n        try:\n            initializer(*initargs)\n        except BaseException:\n            _base.LOGGER.critical('Exception in initializer:', exc_info=True)\n            executor = executor_reference()\n            if executor is not None:\n                executor._initializer_failed()\n            return\n    try:\n        while True:\n            work_item = work_queue.get(block=True)\n            if work_item is not None:\n                work_item.run()\n                # Delete references to object. See issue16284\n                del work_item\n\n                # attempt to increment idle count\n                executor = executor_reference()\n                if executor is not None:\n                    executor._idle_semaphore.release()\n                del executor\n                continue\n\n            executor = executor_reference()\n            # Exit if:\n            #   - The interpreter is shutting down OR\n            #   - The executor that owns the worker has been collected OR\n            #   - The executor that owns the worker has been shutdown.\n            if _shutdown or executor is None or executor._shutdown:\n                # Flag the executor as shutting down as early as possible if it\n                # is not gc-ed yet.\n                if executor is not None:\n                    executor._shutdown = True\n                # Notice other workers\n                work_queue.put(None)\n                return\n            del executor\n    except BaseException:\n        _base.LOGGER.critical('Exception in worker', exc_info=True)\n\n\nclass BrokenThreadPool(_base.BrokenExecutor):\n    \"\"\"\n    Raised when a worker thread in a ThreadPoolExecutor failed initializing.\n    \"\"\"\n\n\nclass ThreadPoolExecutor(_base.Executor):\n\n    # Used to assign unique thread names when thread_name_prefix is not supplied.\n    _counter = itertools.count().__next__\n\n    def __init__(self, max_workers=None, thread_name_prefix='',\n                 initializer=None, initargs=()):\n        \"\"\"Initializes a new ThreadPoolExecutor instance.\n\n        Args:\n            max_workers: The maximum number of threads that can be used to\n                execute the given calls.\n            thread_name_prefix: An optional name prefix to give our threads.\n            initializer: A callable used to initialize worker threads.\n            initargs: A tuple of arguments to pass to the initializer.\n        \"\"\"\n        if max_workers is None:\n            # ThreadPoolExecutor is often used to:\n            # * CPU bound task which releases GIL\n            # * I/O bound task (which releases GIL, of course)\n            #\n            # We use cpu_count + 4 for both types of tasks.\n            # But we limit it to 32 to avoid consuming surprisingly large resource\n            # on many core machine.\n            max_workers = min(32, (os.cpu_count() or 1) + 4)\n        if max_workers <= 0:\n            raise ValueError(\"max_workers must be greater than 0\")\n\n        if initializer is not None and not callable(initializer):\n            raise TypeError(\"initializer must be a callable\")\n\n        self._max_workers = max_workers\n        self._work_queue = queue.SimpleQueue()\n        self._idle_semaphore = threading.Semaphore(0)\n        self._threads = set()\n        self._broken = False\n        self._shutdown = False\n        self._shutdown_lock = threading.Lock()\n        self._thread_name_prefix = (thread_name_prefix or\n                                    (\"ThreadPoolExecutor-%d\" % self._counter()))\n        self._initializer = initializer\n        self._initargs = initargs\n\n    def submit(self, fn, /, *args, **kwargs):\n        with self._shutdown_lock, _global_shutdown_lock:\n            if self._broken:\n                raise BrokenThreadPool(self._broken)\n\n            if self._shutdown:\n                raise RuntimeError('cannot schedule new futures after shutdown')\n            if _shutdown:\n                raise RuntimeError('cannot schedule new futures after '\n                                   'interpreter shutdown')\n\n            f = _base.Future()\n            w = _WorkItem(f, fn, args, kwargs)\n\n            self._work_queue.put(w)\n            self._adjust_thread_count()\n            return f\n    submit.__doc__ = _base.Executor.submit.__doc__\n\n    def _adjust_thread_count(self):\n        # if idle threads are available, don't spin new threads\n        if self._idle_semaphore.acquire(timeout=0):\n            return\n\n        # When the executor gets lost, the weakref callback will wake up\n        # the worker threads.\n        def weakref_cb(_, q=self._work_queue):\n            q.put(None)\n\n        num_threads = len(self._threads)\n        if num_threads < self._max_workers:\n            thread_name = '%s_%d' % (self._thread_name_prefix or self,\n                                     num_threads)\n            t = threading.Thread(name=thread_name, target=_worker,\n                                 args=(weakref.ref(self, weakref_cb),\n                                       self._work_queue,\n                                       self._initializer,\n                                       self._initargs))\n            t.start()\n            self._threads.add(t)\n            _threads_queues[t] = self._work_queue\n\n    def _initializer_failed(self):\n        with self._shutdown_lock:\n            self._broken = ('A thread initializer failed, the thread pool '\n                            'is not usable anymore')\n            # Drain work queue and mark pending futures failed\n            while True:\n                try:\n                    work_item = self._work_queue.get_nowait()\n                except queue.Empty:\n                    break\n                if work_item is not None:\n                    work_item.future.set_exception(BrokenThreadPool(self._broken))\n\n    def shutdown(self, wait=True, *, cancel_futures=False):\n        with self._shutdown_lock:\n            self._shutdown = True\n            if cancel_futures:\n                # Drain all work items from the queue, and then cancel their\n                # associated futures.\n                while True:\n                    try:\n                        work_item = self._work_queue.get_nowait()\n                    except queue.Empty:\n                        break\n                    if work_item is not None:\n                        work_item.future.cancel()\n\n            # Send a wake-up to prevent threads calling\n            # _work_queue.get(block=True) from permanently blocking.\n            self._work_queue.put(None)\n        if wait:\n            for t in self._threads:\n                t.join()\n    shutdown.__doc__ = _base.Executor.shutdown.__doc__\n", 236], "/usr/local/lib/python3.11/asyncio/threads.py": ["\"\"\"High-level support for working with threads in asyncio\"\"\"\n\nimport functools\nimport contextvars\n\nfrom . import events\n\n\n__all__ = \"to_thread\",\n\n\nasync def to_thread(func, /, *args, **kwargs):\n    \"\"\"Asynchronously run function *func* in a separate thread.\n\n    Any *args and **kwargs supplied for this function are directly passed\n    to *func*. Also, the current :class:`contextvars.Context` is propagated,\n    allowing context variables from the main thread to be accessed in the\n    separate thread.\n\n    Return a coroutine that can be awaited to get the eventual result of *func*.\n    \"\"\"\n    loop = events.get_running_loop()\n    ctx = contextvars.copy_context()\n    func_call = functools.partial(ctx.run, func, *args, **kwargs)\n    return await loop.run_in_executor(None, func_call)\n", 25], "/usr/local/lib/python3.11/site-packages/prometheus_client/utils.py": ["import math\nfrom typing import Union\n\nINF = float(\"inf\")\nMINUS_INF = float(\"-inf\")\nNaN = float(\"NaN\")\n\n\ndef floatToGoString(d):\n    d = float(d)\n    if d == INF:\n        return '+Inf'\n    elif d == MINUS_INF:\n        return '-Inf'\n    elif math.isnan(d):\n        return 'NaN'\n    else:\n        s = repr(d)\n        dot = s.find('.')\n        # Go switches to exponents sooner than Python.\n        # We only need to care about positive values for le/quantile.\n        if d > 0 and dot > 6:\n            mantissa = f'{s[0]}.{s[1:dot]}{s[dot + 1:]}'.rstrip('0.')\n            return f'{mantissa}e+0{dot - 1}'\n        return s\n\n\ndef parse_version(version_str: str) -> tuple[Union[int, str], ...]:\n    version: list[Union[int, str]] = []\n    for part in version_str.split('.'):\n        try:\n            version.append(int(part))\n        except ValueError:\n            version.append(part)\n\n    return tuple(version)\n", 36], "/usr/local/lib/python3.11/logging/__init__.py": ["# Copyright 2001-2019 by Vinay Sajip. All Rights Reserved.\n#\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose and without fee is hereby granted,\n# provided that the above copyright notice appear in all copies and that\n# both that copyright notice and this permission notice appear in\n# supporting documentation, and that the name of Vinay Sajip\n# not be used in advertising or publicity pertaining to distribution\n# of the software without specific, written prior permission.\n# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\n# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR\n# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER\n# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n\"\"\"\nLogging package for Python. Based on PEP 282 and comments thereto in\ncomp.lang.python.\n\nCopyright (C) 2001-2019 Vinay Sajip. All Rights Reserved.\n\nTo use, simply 'import logging' and log away!\n\"\"\"\n\nimport sys, os, time, io, re, traceback, warnings, weakref, collections.abc\n\nfrom types import GenericAlias\nfrom string import Template\nfrom string import Formatter as StrFormatter\n\n\n__all__ = ['BASIC_FORMAT', 'BufferingFormatter', 'CRITICAL', 'DEBUG', 'ERROR',\n           'FATAL', 'FileHandler', 'Filter', 'Formatter', 'Handler', 'INFO',\n           'LogRecord', 'Logger', 'LoggerAdapter', 'NOTSET', 'NullHandler',\n           'StreamHandler', 'WARN', 'WARNING', 'addLevelName', 'basicConfig',\n           'captureWarnings', 'critical', 'debug', 'disable', 'error',\n           'exception', 'fatal', 'getLevelName', 'getLogger', 'getLoggerClass',\n           'info', 'log', 'makeLogRecord', 'setLoggerClass', 'shutdown',\n           'warn', 'warning', 'getLogRecordFactory', 'setLogRecordFactory',\n           'lastResort', 'raiseExceptions', 'getLevelNamesMapping']\n\nimport threading\n\n__author__  = \"Vinay Sajip <vinay_sajip@red-dove.com>\"\n__status__  = \"production\"\n# The following module attributes are no longer updated.\n__version__ = \"0.5.1.2\"\n__date__    = \"07 February 2010\"\n\n#---------------------------------------------------------------------------\n#   Miscellaneous module data\n#---------------------------------------------------------------------------\n\n#\n#_startTime is used as the base when calculating the relative time of events\n#\n_startTime = time.time()\n\n#\n#raiseExceptions is used to see if exceptions during handling should be\n#propagated\n#\nraiseExceptions = True\n\n#\n# If you don't want threading information in the log, set this to zero\n#\nlogThreads = True\n\n#\n# If you don't want multiprocessing information in the log, set this to zero\n#\nlogMultiprocessing = True\n\n#\n# If you don't want process information in the log, set this to zero\n#\nlogProcesses = True\n\n#---------------------------------------------------------------------------\n#   Level related stuff\n#---------------------------------------------------------------------------\n#\n# Default levels and level names, these can be replaced with any positive set\n# of values having corresponding names. There is a pseudo-level, NOTSET, which\n# is only really there as a lower limit for user-defined levels. Handlers and\n# loggers are initialized with NOTSET so that they will log all messages, even\n# at user-defined levels.\n#\n\nCRITICAL = 50\nFATAL = CRITICAL\nERROR = 40\nWARNING = 30\nWARN = WARNING\nINFO = 20\nDEBUG = 10\nNOTSET = 0\n\n_levelToName = {\n    CRITICAL: 'CRITICAL',\n    ERROR: 'ERROR',\n    WARNING: 'WARNING',\n    INFO: 'INFO',\n    DEBUG: 'DEBUG',\n    NOTSET: 'NOTSET',\n}\n_nameToLevel = {\n    'CRITICAL': CRITICAL,\n    'FATAL': FATAL,\n    'ERROR': ERROR,\n    'WARN': WARNING,\n    'WARNING': WARNING,\n    'INFO': INFO,\n    'DEBUG': DEBUG,\n    'NOTSET': NOTSET,\n}\n\ndef getLevelNamesMapping():\n    return _nameToLevel.copy()\n\ndef getLevelName(level):\n    \"\"\"\n    Return the textual or numeric representation of logging level 'level'.\n\n    If the level is one of the predefined levels (CRITICAL, ERROR, WARNING,\n    INFO, DEBUG) then you get the corresponding string. If you have\n    associated levels with names using addLevelName then the name you have\n    associated with 'level' is returned.\n\n    If a numeric value corresponding to one of the defined levels is passed\n    in, the corresponding string representation is returned.\n\n    If a string representation of the level is passed in, the corresponding\n    numeric value is returned.\n\n    If no matching numeric or string value is passed in, the string\n    'Level %s' % level is returned.\n    \"\"\"\n    # See Issues #22386, #27937 and #29220 for why it's this way\n    result = _levelToName.get(level)\n    if result is not None:\n        return result\n    result = _nameToLevel.get(level)\n    if result is not None:\n        return result\n    return \"Level %s\" % level\n\ndef addLevelName(level, levelName):\n    \"\"\"\n    Associate 'levelName' with 'level'.\n\n    This is used when converting levels to text during message formatting.\n    \"\"\"\n    _acquireLock()\n    try:    #unlikely to cause an exception, but you never know...\n        _levelToName[level] = levelName\n        _nameToLevel[levelName] = level\n    finally:\n        _releaseLock()\n\nif hasattr(sys, \"_getframe\"):\n    currentframe = lambda: sys._getframe(1)\nelse: #pragma: no cover\n    def currentframe():\n        \"\"\"Return the frame object for the caller's stack frame.\"\"\"\n        try:\n            raise Exception\n        except Exception:\n            return sys.exc_info()[2].tb_frame.f_back\n\n#\n# _srcfile is used when walking the stack to check when we've got the first\n# caller stack frame, by skipping frames whose filename is that of this\n# module's source. It therefore should contain the filename of this module's\n# source file.\n#\n# Ordinarily we would use __file__ for this, but frozen modules don't always\n# have __file__ set, for some reason (see Issue #21736). Thus, we get the\n# filename from a handy code object from a function defined in this module.\n# (There's no particular reason for picking addLevelName.)\n#\n\n_srcfile = os.path.normcase(addLevelName.__code__.co_filename)\n\n# _srcfile is only used in conjunction with sys._getframe().\n# Setting _srcfile to None will prevent findCaller() from being called. This\n# way, you can avoid the overhead of fetching caller information.\n\n# The following is based on warnings._is_internal_frame. It makes sure that\n# frames of the import mechanism are skipped when logging at module level and\n# using a stacklevel value greater than one.\ndef _is_internal_frame(frame):\n    \"\"\"Signal whether the frame is a CPython or logging module internal.\"\"\"\n    filename = os.path.normcase(frame.f_code.co_filename)\n    return filename == _srcfile or (\n        \"importlib\" in filename and \"_bootstrap\" in filename\n    )\n\n\ndef _checkLevel(level):\n    if isinstance(level, int):\n        rv = level\n    elif str(level) == level:\n        if level not in _nameToLevel:\n            raise ValueError(\"Unknown level: %r\" % level)\n        rv = _nameToLevel[level]\n    else:\n        raise TypeError(\"Level not an integer or a valid string: %r\"\n                        % (level,))\n    return rv\n\n#---------------------------------------------------------------------------\n#   Thread-related stuff\n#---------------------------------------------------------------------------\n\n#\n#_lock is used to serialize access to shared data structures in this module.\n#This needs to be an RLock because fileConfig() creates and configures\n#Handlers, and so might arbitrary user threads. Since Handler code updates the\n#shared dictionary _handlers, it needs to acquire the lock. But if configuring,\n#the lock would already have been acquired - so we need an RLock.\n#The same argument applies to Loggers and Manager.loggerDict.\n#\n_lock = threading.RLock()\n\ndef _acquireLock():\n    \"\"\"\n    Acquire the module-level lock for serializing access to shared data.\n\n    This should be released with _releaseLock().\n    \"\"\"\n    if _lock:\n        _lock.acquire()\n\ndef _releaseLock():\n    \"\"\"\n    Release the module-level lock acquired by calling _acquireLock().\n    \"\"\"\n    if _lock:\n        _lock.release()\n\n\n# Prevent a held logging lock from blocking a child from logging.\n\nif not hasattr(os, 'register_at_fork'):  # Windows and friends.\n    def _register_at_fork_reinit_lock(instance):\n        pass  # no-op when os.register_at_fork does not exist.\nelse:\n    # A collection of instances with a _at_fork_reinit method (logging.Handler)\n    # to be called in the child after forking.  The weakref avoids us keeping\n    # discarded Handler instances alive.\n    _at_fork_reinit_lock_weakset = weakref.WeakSet()\n\n    def _register_at_fork_reinit_lock(instance):\n        _acquireLock()\n        try:\n            _at_fork_reinit_lock_weakset.add(instance)\n        finally:\n            _releaseLock()\n\n    def _after_at_fork_child_reinit_locks():\n        for handler in _at_fork_reinit_lock_weakset:\n            handler._at_fork_reinit()\n\n        # _acquireLock() was called in the parent before forking.\n        # The lock is reinitialized to unlocked state.\n        _lock._at_fork_reinit()\n\n    os.register_at_fork(before=_acquireLock,\n                        after_in_child=_after_at_fork_child_reinit_locks,\n                        after_in_parent=_releaseLock)\n\n\n#---------------------------------------------------------------------------\n#   The logging record\n#---------------------------------------------------------------------------\n\nclass LogRecord(object):\n    \"\"\"\n    A LogRecord instance represents an event being logged.\n\n    LogRecord instances are created every time something is logged. They\n    contain all the information pertinent to the event being logged. The\n    main information passed in is in msg and args, which are combined\n    using str(msg) % args to create the message field of the record. The\n    record also includes information such as when the record was created,\n    the source line where the logging call was made, and any exception\n    information to be logged.\n    \"\"\"\n    def __init__(self, name, level, pathname, lineno,\n                 msg, args, exc_info, func=None, sinfo=None, **kwargs):\n        \"\"\"\n        Initialize a logging record with interesting information.\n        \"\"\"\n        ct = time.time()\n        self.name = name\n        self.msg = msg\n        #\n        # The following statement allows passing of a dictionary as a sole\n        # argument, so that you can do something like\n        #  logging.debug(\"a %(a)d b %(b)s\", {'a':1, 'b':2})\n        # Suggested by Stefan Behnel.\n        # Note that without the test for args[0], we get a problem because\n        # during formatting, we test to see if the arg is present using\n        # 'if self.args:'. If the event being logged is e.g. 'Value is %d'\n        # and if the passed arg fails 'if self.args:' then no formatting\n        # is done. For example, logger.warning('Value is %d', 0) would log\n        # 'Value is %d' instead of 'Value is 0'.\n        # For the use case of passing a dictionary, this should not be a\n        # problem.\n        # Issue #21172: a request was made to relax the isinstance check\n        # to hasattr(args[0], '__getitem__'). However, the docs on string\n        # formatting still seem to suggest a mapping object is required.\n        # Thus, while not removing the isinstance check, it does now look\n        # for collections.abc.Mapping rather than, as before, dict.\n        if (args and len(args) == 1 and isinstance(args[0], collections.abc.Mapping)\n            and args[0]):\n            args = args[0]\n        self.args = args\n        self.levelname = getLevelName(level)\n        self.levelno = level\n        self.pathname = pathname\n        try:\n            self.filename = os.path.basename(pathname)\n            self.module = os.path.splitext(self.filename)[0]\n        except (TypeError, ValueError, AttributeError):\n            self.filename = pathname\n            self.module = \"Unknown module\"\n        self.exc_info = exc_info\n        self.exc_text = None      # used to cache the traceback text\n        self.stack_info = sinfo\n        self.lineno = lineno\n        self.funcName = func\n        self.created = ct\n        self.msecs = int((ct - int(ct)) * 1000) + 0.0  # see gh-89047\n        self.relativeCreated = (self.created - _startTime) * 1000\n        if logThreads:\n            self.thread = threading.get_ident()\n            self.threadName = threading.current_thread().name\n        else: # pragma: no cover\n            self.thread = None\n            self.threadName = None\n        if not logMultiprocessing: # pragma: no cover\n            self.processName = None\n        else:\n            self.processName = 'MainProcess'\n            mp = sys.modules.get('multiprocessing')\n            if mp is not None:\n                # Errors may occur if multiprocessing has not finished loading\n                # yet - e.g. if a custom import hook causes third-party code\n                # to run when multiprocessing calls import. See issue 8200\n                # for an example\n                try:\n                    self.processName = mp.current_process().name\n                except Exception: #pragma: no cover\n                    pass\n        if logProcesses and hasattr(os, 'getpid'):\n            self.process = os.getpid()\n        else:\n            self.process = None\n\n    def __repr__(self):\n        return '<LogRecord: %s, %s, %s, %s, \"%s\">'%(self.name, self.levelno,\n            self.pathname, self.lineno, self.msg)\n\n    def getMessage(self):\n        \"\"\"\n        Return the message for this LogRecord.\n\n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.\n        \"\"\"\n        msg = str(self.msg)\n        if self.args:\n            msg = msg % self.args\n        return msg\n\n#\n#   Determine which class to use when instantiating log records.\n#\n_logRecordFactory = LogRecord\n\ndef setLogRecordFactory(factory):\n    \"\"\"\n    Set the factory to be used when instantiating a log record.\n\n    :param factory: A callable which will be called to instantiate\n    a log record.\n    \"\"\"\n    global _logRecordFactory\n    _logRecordFactory = factory\n\ndef getLogRecordFactory():\n    \"\"\"\n    Return the factory to be used when instantiating a log record.\n    \"\"\"\n\n    return _logRecordFactory\n\ndef makeLogRecord(dict):\n    \"\"\"\n    Make a LogRecord whose attributes are defined by the specified dictionary,\n    This function is useful for converting a logging event received over\n    a socket connection (which is sent as a dictionary) into a LogRecord\n    instance.\n    \"\"\"\n    rv = _logRecordFactory(None, None, \"\", 0, \"\", (), None, None)\n    rv.__dict__.update(dict)\n    return rv\n\n\n#---------------------------------------------------------------------------\n#   Formatter classes and functions\n#---------------------------------------------------------------------------\n_str_formatter = StrFormatter()\ndel StrFormatter\n\n\nclass PercentStyle(object):\n\n    default_format = '%(message)s'\n    asctime_format = '%(asctime)s'\n    asctime_search = '%(asctime)'\n    validation_pattern = re.compile(r'%\\(\\w+\\)[#0+ -]*(\\*|\\d+)?(\\.(\\*|\\d+))?[diouxefgcrsa%]', re.I)\n\n    def __init__(self, fmt, *, defaults=None):\n        self._fmt = fmt or self.default_format\n        self._defaults = defaults\n\n    def usesTime(self):\n        return self._fmt.find(self.asctime_search) >= 0\n\n    def validate(self):\n        \"\"\"Validate the input format, ensure it matches the correct style\"\"\"\n        if not self.validation_pattern.search(self._fmt):\n            raise ValueError(\"Invalid format '%s' for '%s' style\" % (self._fmt, self.default_format[0]))\n\n    def _format(self, record):\n        if defaults := self._defaults:\n            values = defaults | record.__dict__\n        else:\n            values = record.__dict__\n        return self._fmt % values\n\n    def format(self, record):\n        try:\n            return self._format(record)\n        except KeyError as e:\n            raise ValueError('Formatting field not found in record: %s' % e)\n\n\nclass StrFormatStyle(PercentStyle):\n    default_format = '{message}'\n    asctime_format = '{asctime}'\n    asctime_search = '{asctime'\n\n    fmt_spec = re.compile(r'^(.?[<>=^])?[+ -]?#?0?(\\d+|{\\w+})?[,_]?(\\.(\\d+|{\\w+}))?[bcdefgnosx%]?$', re.I)\n    field_spec = re.compile(r'^(\\d+|\\w+)(\\.\\w+|\\[[^]]+\\])*$')\n\n    def _format(self, record):\n        if defaults := self._defaults:\n            values = defaults | record.__dict__\n        else:\n            values = record.__dict__\n        return self._fmt.format(**values)\n\n    def validate(self):\n        \"\"\"Validate the input format, ensure it is the correct string formatting style\"\"\"\n        fields = set()\n        try:\n            for _, fieldname, spec, conversion in _str_formatter.parse(self._fmt):\n                if fieldname:\n                    if not self.field_spec.match(fieldname):\n                        raise ValueError('invalid field name/expression: %r' % fieldname)\n                    fields.add(fieldname)\n                if conversion and conversion not in 'rsa':\n                    raise ValueError('invalid conversion: %r' % conversion)\n                if spec and not self.fmt_spec.match(spec):\n                    raise ValueError('bad specifier: %r' % spec)\n        except ValueError as e:\n            raise ValueError('invalid format: %s' % e)\n        if not fields:\n            raise ValueError('invalid format: no fields')\n\n\nclass StringTemplateStyle(PercentStyle):\n    default_format = '${message}'\n    asctime_format = '${asctime}'\n    asctime_search = '${asctime}'\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._tpl = Template(self._fmt)\n\n    def usesTime(self):\n        fmt = self._fmt\n        return fmt.find('$asctime') >= 0 or fmt.find(self.asctime_search) >= 0\n\n    def validate(self):\n        pattern = Template.pattern\n        fields = set()\n        for m in pattern.finditer(self._fmt):\n            d = m.groupdict()\n            if d['named']:\n                fields.add(d['named'])\n            elif d['braced']:\n                fields.add(d['braced'])\n            elif m.group(0) == '$':\n                raise ValueError('invalid format: bare \\'$\\' not allowed')\n        if not fields:\n            raise ValueError('invalid format: no fields')\n\n    def _format(self, record):\n        if defaults := self._defaults:\n            values = defaults | record.__dict__\n        else:\n            values = record.__dict__\n        return self._tpl.substitute(**values)\n\n\nBASIC_FORMAT = \"%(levelname)s:%(name)s:%(message)s\"\n\n_STYLES = {\n    '%': (PercentStyle, BASIC_FORMAT),\n    '{': (StrFormatStyle, '{levelname}:{name}:{message}'),\n    '$': (StringTemplateStyle, '${levelname}:${name}:${message}'),\n}\n\nclass Formatter(object):\n    \"\"\"\n    Formatter instances are used to convert a LogRecord to text.\n\n    Formatters need to know how a LogRecord is constructed. They are\n    responsible for converting a LogRecord to (usually) a string which can\n    be interpreted by either a human or an external system. The base Formatter\n    allows a formatting string to be specified. If none is supplied, the\n    style-dependent default value, \"%(message)s\", \"{message}\", or\n    \"${message}\", is used.\n\n    The Formatter can be initialized with a format string which makes use of\n    knowledge of the LogRecord attributes - e.g. the default value mentioned\n    above makes use of the fact that the user's message and arguments are pre-\n    formatted into a LogRecord's message attribute. Currently, the useful\n    attributes in a LogRecord are described by:\n\n    %(name)s            Name of the logger (logging channel)\n    %(levelno)s         Numeric logging level for the message (DEBUG, INFO,\n                        WARNING, ERROR, CRITICAL)\n    %(levelname)s       Text logging level for the message (\"DEBUG\", \"INFO\",\n                        \"WARNING\", \"ERROR\", \"CRITICAL\")\n    %(pathname)s        Full pathname of the source file where the logging\n                        call was issued (if available)\n    %(filename)s        Filename portion of pathname\n    %(module)s          Module (name portion of filename)\n    %(lineno)d          Source line number where the logging call was issued\n                        (if available)\n    %(funcName)s        Function name\n    %(created)f         Time when the LogRecord was created (time.time()\n                        return value)\n    %(asctime)s         Textual time when the LogRecord was created\n    %(msecs)d           Millisecond portion of the creation time\n    %(relativeCreated)d Time in milliseconds when the LogRecord was created,\n                        relative to the time the logging module was loaded\n                        (typically at application startup time)\n    %(thread)d          Thread ID (if available)\n    %(threadName)s      Thread name (if available)\n    %(process)d         Process ID (if available)\n    %(message)s         The result of record.getMessage(), computed just as\n                        the record is emitted\n    \"\"\"\n\n    converter = time.localtime\n\n    def __init__(self, fmt=None, datefmt=None, style='%', validate=True, *,\n                 defaults=None):\n        \"\"\"\n        Initialize the formatter with specified format strings.\n\n        Initialize the formatter either with the specified format string, or a\n        default as described above. Allow for specialized date formatting with\n        the optional datefmt argument. If datefmt is omitted, you get an\n        ISO8601-like (or RFC 3339-like) format.\n\n        Use a style parameter of '%', '{' or '$' to specify that you want to\n        use one of %-formatting, :meth:`str.format` (``{}``) formatting or\n        :class:`string.Template` formatting in your format string.\n\n        .. versionchanged:: 3.2\n           Added the ``style`` parameter.\n        \"\"\"\n        if style not in _STYLES:\n            raise ValueError('Style must be one of: %s' % ','.join(\n                             _STYLES.keys()))\n        self._style = _STYLES[style][0](fmt, defaults=defaults)\n        if validate:\n            self._style.validate()\n\n        self._fmt = self._style._fmt\n        self.datefmt = datefmt\n\n    default_time_format = '%Y-%m-%d %H:%M:%S'\n    default_msec_format = '%s,%03d'\n\n    def formatTime(self, record, datefmt=None):\n        \"\"\"\n        Return the creation time of the specified LogRecord as formatted text.\n\n        This method should be called from format() by a formatter which\n        wants to make use of a formatted time. This method can be overridden\n        in formatters to provide for any specific requirement, but the\n        basic behaviour is as follows: if datefmt (a string) is specified,\n        it is used with time.strftime() to format the creation time of the\n        record. Otherwise, an ISO8601-like (or RFC 3339-like) format is used.\n        The resulting string is returned. This function uses a user-configurable\n        function to convert the creation time to a tuple. By default,\n        time.localtime() is used; to change this for a particular formatter\n        instance, set the 'converter' attribute to a function with the same\n        signature as time.localtime() or time.gmtime(). To change it for all\n        formatters, for example if you want all logging times to be shown in GMT,\n        set the 'converter' attribute in the Formatter class.\n        \"\"\"\n        ct = self.converter(record.created)\n        if datefmt:\n            s = time.strftime(datefmt, ct)\n        else:\n            s = time.strftime(self.default_time_format, ct)\n            if self.default_msec_format:\n                s = self.default_msec_format % (s, record.msecs)\n        return s\n\n    def formatException(self, ei):\n        \"\"\"\n        Format and return the specified exception information as a string.\n\n        This default implementation just uses\n        traceback.print_exception()\n        \"\"\"\n        sio = io.StringIO()\n        tb = ei[2]\n        # See issues #9427, #1553375. Commented out for now.\n        #if getattr(self, 'fullstack', False):\n        #    traceback.print_stack(tb.tb_frame.f_back, file=sio)\n        traceback.print_exception(ei[0], ei[1], tb, None, sio)\n        s = sio.getvalue()\n        sio.close()\n        if s[-1:] == \"\\n\":\n            s = s[:-1]\n        return s\n\n    def usesTime(self):\n        \"\"\"\n        Check if the format uses the creation time of the record.\n        \"\"\"\n        return self._style.usesTime()\n\n    def formatMessage(self, record):\n        return self._style.format(record)\n\n    def formatStack(self, stack_info):\n        \"\"\"\n        This method is provided as an extension point for specialized\n        formatting of stack information.\n\n        The input data is a string as returned from a call to\n        :func:`traceback.print_stack`, but with the last trailing newline\n        removed.\n\n        The base implementation just returns the value passed in.\n        \"\"\"\n        return stack_info\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record as text.\n\n        The record's attribute dictionary is used as the operand to a\n        string formatting operation which yields the returned string.\n        Before formatting the dictionary, a couple of preparatory steps\n        are carried out. The message attribute of the record is computed\n        using LogRecord.getMessage(). If the formatting string uses the\n        time (as determined by a call to usesTime(), formatTime() is\n        called to format the event time. If there is exception information,\n        it is formatted using formatException() and appended to the message.\n        \"\"\"\n        record.message = record.getMessage()\n        if self.usesTime():\n            record.asctime = self.formatTime(record, self.datefmt)\n        s = self.formatMessage(record)\n        if record.exc_info:\n            # Cache the traceback text to avoid converting it multiple times\n            # (it's constant anyway)\n            if not record.exc_text:\n                record.exc_text = self.formatException(record.exc_info)\n        if record.exc_text:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            s = s + record.exc_text\n        if record.stack_info:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            s = s + self.formatStack(record.stack_info)\n        return s\n\n#\n#   The default formatter to use when no other is specified\n#\n_defaultFormatter = Formatter()\n\nclass BufferingFormatter(object):\n    \"\"\"\n    A formatter suitable for formatting a number of records.\n    \"\"\"\n    def __init__(self, linefmt=None):\n        \"\"\"\n        Optionally specify a formatter which will be used to format each\n        individual record.\n        \"\"\"\n        if linefmt:\n            self.linefmt = linefmt\n        else:\n            self.linefmt = _defaultFormatter\n\n    def formatHeader(self, records):\n        \"\"\"\n        Return the header string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def formatFooter(self, records):\n        \"\"\"\n        Return the footer string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def format(self, records):\n        \"\"\"\n        Format the specified records and return the result as a string.\n        \"\"\"\n        rv = \"\"\n        if len(records) > 0:\n            rv = rv + self.formatHeader(records)\n            for record in records:\n                rv = rv + self.linefmt.format(record)\n            rv = rv + self.formatFooter(records)\n        return rv\n\n#---------------------------------------------------------------------------\n#   Filter classes and functions\n#---------------------------------------------------------------------------\n\nclass Filter(object):\n    \"\"\"\n    Filter instances are used to perform arbitrary filtering of LogRecords.\n\n    Loggers and Handlers can optionally use Filter instances to filter\n    records as desired. The base filter class only allows events which are\n    below a certain point in the logger hierarchy. For example, a filter\n    initialized with \"A.B\" will allow events logged by loggers \"A.B\",\n    \"A.B.C\", \"A.B.C.D\", \"A.B.D\" etc. but not \"A.BB\", \"B.A.B\" etc. If\n    initialized with the empty string, all events are passed.\n    \"\"\"\n    def __init__(self, name=''):\n        \"\"\"\n        Initialize a filter.\n\n        Initialize with the name of the logger which, together with its\n        children, will have its events allowed through the filter. If no\n        name is specified, allow every event.\n        \"\"\"\n        self.name = name\n        self.nlen = len(name)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if the specified record is to be logged.\n\n        Returns True if the record should be logged, or False otherwise.\n        If deemed appropriate, the record may be modified in-place.\n        \"\"\"\n        if self.nlen == 0:\n            return True\n        elif self.name == record.name:\n            return True\n        elif record.name.find(self.name, 0, self.nlen) != 0:\n            return False\n        return (record.name[self.nlen] == \".\")\n\nclass Filterer(object):\n    \"\"\"\n    A base class for loggers and handlers which allows them to share\n    common code.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the list of filters to be an empty list.\n        \"\"\"\n        self.filters = []\n\n    def addFilter(self, filter):\n        \"\"\"\n        Add the specified filter to this handler.\n        \"\"\"\n        if not (filter in self.filters):\n            self.filters.append(filter)\n\n    def removeFilter(self, filter):\n        \"\"\"\n        Remove the specified filter from this handler.\n        \"\"\"\n        if filter in self.filters:\n            self.filters.remove(filter)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if a record is loggable by consulting all the filters.\n\n        The default is to allow the record to be logged; any filter can veto\n        this and the record is then dropped. Returns a zero value if a record\n        is to be dropped, else non-zero.\n\n        .. versionchanged:: 3.2\n\n           Allow filters to be just callables.\n        \"\"\"\n        rv = True\n        for f in self.filters:\n            if hasattr(f, 'filter'):\n                result = f.filter(record)\n            else:\n                result = f(record) # assume callable - will raise if not\n            if not result:\n                rv = False\n                break\n        return rv\n\n#---------------------------------------------------------------------------\n#   Handler classes and functions\n#---------------------------------------------------------------------------\n\n_handlers = weakref.WeakValueDictionary()  #map of handler names to handlers\n_handlerList = [] # added to allow handlers to be removed in reverse of order initialized\n\ndef _removeHandlerRef(wr):\n    \"\"\"\n    Remove a handler reference from the internal cleanup list.\n    \"\"\"\n    # This function can be called during module teardown, when globals are\n    # set to None. It can also be called from another thread. So we need to\n    # pre-emptively grab the necessary globals and check if they're None,\n    # to prevent race conditions and failures during interpreter shutdown.\n    acquire, release, handlers = _acquireLock, _releaseLock, _handlerList\n    if acquire and release and handlers:\n        acquire()\n        try:\n            handlers.remove(wr)\n        except ValueError:\n            pass\n        finally:\n            release()\n\ndef _addHandlerRef(handler):\n    \"\"\"\n    Add a handler to the internal cleanup list using a weak reference.\n    \"\"\"\n    _acquireLock()\n    try:\n        _handlerList.append(weakref.ref(handler, _removeHandlerRef))\n    finally:\n        _releaseLock()\n\nclass Handler(Filterer):\n    \"\"\"\n    Handler instances dispatch logging events to specific destinations.\n\n    The base handler class. Acts as a placeholder which defines the Handler\n    interface. Handlers can optionally use Formatter instances to format\n    records as desired. By default, no formatter is specified; in this case,\n    the 'raw' message as determined by record.message is logged.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initializes the instance - basically setting the formatter to None\n        and the filter list to empty.\n        \"\"\"\n        Filterer.__init__(self)\n        self._name = None\n        self.level = _checkLevel(level)\n        self.formatter = None\n        self._closed = False\n        # Add the handler to the global _handlerList (for cleanup on shutdown)\n        _addHandlerRef(self)\n        self.createLock()\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, name):\n        _acquireLock()\n        try:\n            if self._name in _handlers:\n                del _handlers[self._name]\n            self._name = name\n            if name:\n                _handlers[name] = self\n        finally:\n            _releaseLock()\n\n    name = property(get_name, set_name)\n\n    def createLock(self):\n        \"\"\"\n        Acquire a thread lock for serializing access to the underlying I/O.\n        \"\"\"\n        self.lock = threading.RLock()\n        _register_at_fork_reinit_lock(self)\n\n    def _at_fork_reinit(self):\n        self.lock._at_fork_reinit()\n\n    def acquire(self):\n        \"\"\"\n        Acquire the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.acquire()\n\n    def release(self):\n        \"\"\"\n        Release the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.release()\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this handler.  level must be an int or a str.\n        \"\"\"\n        self.level = _checkLevel(level)\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record.\n\n        If a formatter is set, use it. Otherwise, use the default formatter\n        for the module.\n        \"\"\"\n        if self.formatter:\n            fmt = self.formatter\n        else:\n            fmt = _defaultFormatter\n        return fmt.format(record)\n\n    def emit(self, record):\n        \"\"\"\n        Do whatever it takes to actually log the specified logging record.\n\n        This version is intended to be implemented by subclasses and so\n        raises a NotImplementedError.\n        \"\"\"\n        raise NotImplementedError('emit must be implemented '\n                                  'by Handler subclasses')\n\n    def handle(self, record):\n        \"\"\"\n        Conditionally emit the specified logging record.\n\n        Emission depends on filters which may have been added to the handler.\n        Wrap the actual emission of the record with acquisition/release of\n        the I/O thread lock. Returns whether the filter passed the record for\n        emission.\n        \"\"\"\n        rv = self.filter(record)\n        if rv:\n            self.acquire()\n            try:\n                self.emit(record)\n            finally:\n                self.release()\n        return rv\n\n    def setFormatter(self, fmt):\n        \"\"\"\n        Set the formatter for this handler.\n        \"\"\"\n        self.formatter = fmt\n\n    def flush(self):\n        \"\"\"\n        Ensure all logging output has been flushed.\n\n        This version does nothing and is intended to be implemented by\n        subclasses.\n        \"\"\"\n        pass\n\n    def close(self):\n        \"\"\"\n        Tidy up any resources used by the handler.\n\n        This version removes the handler from an internal map of handlers,\n        _handlers, which is used for handler lookup by name. Subclasses\n        should ensure that this gets called from overridden close()\n        methods.\n        \"\"\"\n        #get the module data lock, as we're updating a shared structure.\n        _acquireLock()\n        try:    #unlikely to raise an exception, but you never know...\n            self._closed = True\n            if self._name and self._name in _handlers:\n                del _handlers[self._name]\n        finally:\n            _releaseLock()\n\n    def handleError(self, record):\n        \"\"\"\n        Handle errors which occur during an emit() call.\n\n        This method should be called from handlers when an exception is\n        encountered during an emit() call. If raiseExceptions is false,\n        exceptions get silently ignored. This is what is mostly wanted\n        for a logging system - most users will not care about errors in\n        the logging system, they are more interested in application errors.\n        You could, however, replace this with a custom handler if you wish.\n        The record which was being processed is passed in to this method.\n        \"\"\"\n        if raiseExceptions and sys.stderr:  # see issue 13807\n            t, v, tb = sys.exc_info()\n            try:\n                sys.stderr.write('--- Logging error ---\\n')\n                traceback.print_exception(t, v, tb, None, sys.stderr)\n                sys.stderr.write('Call stack:\\n')\n                # Walk the stack frame up until we're out of logging,\n                # so as to print the calling context.\n                frame = tb.tb_frame\n                while (frame and os.path.dirname(frame.f_code.co_filename) ==\n                       __path__[0]):\n                    frame = frame.f_back\n                if frame:\n                    traceback.print_stack(frame, file=sys.stderr)\n                else:\n                    # couldn't find the right stack frame, for some reason\n                    sys.stderr.write('Logged from file %s, line %s\\n' % (\n                                     record.filename, record.lineno))\n                # Issue 18671: output logging message and arguments\n                try:\n                    sys.stderr.write('Message: %r\\n'\n                                     'Arguments: %s\\n' % (record.msg,\n                                                          record.args))\n                except RecursionError:  # See issue 36272\n                    raise\n                except Exception:\n                    sys.stderr.write('Unable to print the message and arguments'\n                                     ' - possible formatting error.\\nUse the'\n                                     ' traceback above to help find the error.\\n'\n                                    )\n            except OSError: #pragma: no cover\n                pass    # see issue 5971\n            finally:\n                del t, v, tb\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        return '<%s (%s)>' % (self.__class__.__name__, level)\n\nclass StreamHandler(Handler):\n    \"\"\"\n    A handler class which writes logging records, appropriately formatted,\n    to a stream. Note that this class does not close the stream, as\n    sys.stdout or sys.stderr may be used.\n    \"\"\"\n\n    terminator = '\\n'\n\n    def __init__(self, stream=None):\n        \"\"\"\n        Initialize the handler.\n\n        If stream is not specified, sys.stderr is used.\n        \"\"\"\n        Handler.__init__(self)\n        if stream is None:\n            stream = sys.stderr\n        self.stream = stream\n\n    def flush(self):\n        \"\"\"\n        Flushes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            if self.stream and hasattr(self.stream, \"flush\"):\n                self.stream.flush()\n        finally:\n            self.release()\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If a formatter is specified, it is used to format the record.\n        The record is then written to the stream with a trailing newline.  If\n        exception information is present, it is formatted using\n        traceback.print_exception and appended to the stream.  If the stream\n        has an 'encoding' attribute, it is used to determine how to do the\n        output to the stream.\n        \"\"\"\n        try:\n            msg = self.format(record)\n            stream = self.stream\n            # issue 35046: merged two stream.writes into one.\n            stream.write(msg + self.terminator)\n            self.flush()\n        except RecursionError:  # See issue 36272\n            raise\n        except Exception:\n            self.handleError(record)\n\n    def setStream(self, stream):\n        \"\"\"\n        Sets the StreamHandler's stream to the specified value,\n        if it is different.\n\n        Returns the old stream, if the stream was changed, or None\n        if it wasn't.\n        \"\"\"\n        if stream is self.stream:\n            result = None\n        else:\n            result = self.stream\n            self.acquire()\n            try:\n                self.flush()\n                self.stream = stream\n            finally:\n                self.release()\n        return result\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        name = getattr(self.stream, 'name', '')\n        #  bpo-36015: name can be an int\n        name = str(name)\n        if name:\n            name += ' '\n        return '<%s %s(%s)>' % (self.__class__.__name__, name, level)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass FileHandler(StreamHandler):\n    \"\"\"\n    A handler class which writes formatted logging records to disk files.\n    \"\"\"\n    def __init__(self, filename, mode='a', encoding=None, delay=False, errors=None):\n        \"\"\"\n        Open the specified file and use it as the stream for logging.\n        \"\"\"\n        # Issue #27493: add support for Path objects to be passed in\n        filename = os.fspath(filename)\n        #keep the absolute path, otherwise derived classes which use this\n        #may come a cropper when the current directory changes\n        self.baseFilename = os.path.abspath(filename)\n        self.mode = mode\n        self.encoding = encoding\n        if \"b\" not in mode:\n            self.encoding = io.text_encoding(encoding)\n        self.errors = errors\n        self.delay = delay\n        # bpo-26789: FileHandler keeps a reference to the builtin open()\n        # function to be able to open or reopen the file during Python\n        # finalization.\n        self._builtin_open = open\n        if delay:\n            #We don't open the stream, but we still need to call the\n            #Handler constructor to set level, formatter, lock etc.\n            Handler.__init__(self)\n            self.stream = None\n        else:\n            StreamHandler.__init__(self, self._open())\n\n    def close(self):\n        \"\"\"\n        Closes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            try:\n                if self.stream:\n                    try:\n                        self.flush()\n                    finally:\n                        stream = self.stream\n                        self.stream = None\n                        if hasattr(stream, \"close\"):\n                            stream.close()\n            finally:\n                # Issue #19523: call unconditionally to\n                # prevent a handler leak when delay is set\n                # Also see Issue #42378: we also rely on\n                # self._closed being set to True there\n                StreamHandler.close(self)\n        finally:\n            self.release()\n\n    def _open(self):\n        \"\"\"\n        Open the current base file with the (original) mode and encoding.\n        Return the resulting stream.\n        \"\"\"\n        open_func = self._builtin_open\n        return open_func(self.baseFilename, self.mode,\n                         encoding=self.encoding, errors=self.errors)\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If the stream was not opened because 'delay' was specified in the\n        constructor, open it before calling the superclass's emit.\n\n        If stream is not open, current mode is 'w' and `_closed=True`, record\n        will not be emitted (see Issue #42378).\n        \"\"\"\n        if self.stream is None:\n            if self.mode != 'w' or not self._closed:\n                self.stream = self._open()\n        if self.stream:\n            StreamHandler.emit(self, record)\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        return '<%s %s (%s)>' % (self.__class__.__name__, self.baseFilename, level)\n\n\nclass _StderrHandler(StreamHandler):\n    \"\"\"\n    This class is like a StreamHandler using sys.stderr, but always uses\n    whatever sys.stderr is currently set to rather than the value of\n    sys.stderr at handler construction time.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initialize the handler.\n        \"\"\"\n        Handler.__init__(self, level)\n\n    @property\n    def stream(self):\n        return sys.stderr\n\n\n_defaultLastResort = _StderrHandler(WARNING)\nlastResort = _defaultLastResort\n\n#---------------------------------------------------------------------------\n#   Manager classes and functions\n#---------------------------------------------------------------------------\n\nclass PlaceHolder(object):\n    \"\"\"\n    PlaceHolder instances are used in the Manager logger hierarchy to take\n    the place of nodes for which no loggers have been defined. This class is\n    intended for internal use only and not as part of the public API.\n    \"\"\"\n    def __init__(self, alogger):\n        \"\"\"\n        Initialize with the specified logger being a child of this placeholder.\n        \"\"\"\n        self.loggerMap = { alogger : None }\n\n    def append(self, alogger):\n        \"\"\"\n        Add the specified logger as a child of this placeholder.\n        \"\"\"\n        if alogger not in self.loggerMap:\n            self.loggerMap[alogger] = None\n\n#\n#   Determine which class to use when instantiating loggers.\n#\n\ndef setLoggerClass(klass):\n    \"\"\"\n    Set the class to be used when instantiating a logger. The class should\n    define __init__() such that only a name argument is required, and the\n    __init__() should call Logger.__init__()\n    \"\"\"\n    if klass != Logger:\n        if not issubclass(klass, Logger):\n            raise TypeError(\"logger not derived from logging.Logger: \"\n                            + klass.__name__)\n    global _loggerClass\n    _loggerClass = klass\n\ndef getLoggerClass():\n    \"\"\"\n    Return the class to be used when instantiating a logger.\n    \"\"\"\n    return _loggerClass\n\nclass Manager(object):\n    \"\"\"\n    There is [under normal circumstances] just one Manager instance, which\n    holds the hierarchy of loggers.\n    \"\"\"\n    def __init__(self, rootnode):\n        \"\"\"\n        Initialize the manager with the root node of the logger hierarchy.\n        \"\"\"\n        self.root = rootnode\n        self.disable = 0\n        self.emittedNoHandlerWarning = False\n        self.loggerDict = {}\n        self.loggerClass = None\n        self.logRecordFactory = None\n\n    @property\n    def disable(self):\n        return self._disable\n\n    @disable.setter\n    def disable(self, value):\n        self._disable = _checkLevel(value)\n\n    def getLogger(self, name):\n        \"\"\"\n        Get a logger with the specified name (channel name), creating it\n        if it doesn't yet exist. This name is a dot-separated hierarchical\n        name, such as \"a\", \"a.b\", \"a.b.c\" or similar.\n\n        If a PlaceHolder existed for the specified name [i.e. the logger\n        didn't exist but a child of it did], replace it with the created\n        logger and fix up the parent/child references which pointed to the\n        placeholder to now point to the logger.\n        \"\"\"\n        rv = None\n        if not isinstance(name, str):\n            raise TypeError('A logger name must be a string')\n        _acquireLock()\n        try:\n            if name in self.loggerDict:\n                rv = self.loggerDict[name]\n                if isinstance(rv, PlaceHolder):\n                    ph = rv\n                    rv = (self.loggerClass or _loggerClass)(name)\n                    rv.manager = self\n                    self.loggerDict[name] = rv\n                    self._fixupChildren(ph, rv)\n                    self._fixupParents(rv)\n            else:\n                rv = (self.loggerClass or _loggerClass)(name)\n                rv.manager = self\n                self.loggerDict[name] = rv\n                self._fixupParents(rv)\n        finally:\n            _releaseLock()\n        return rv\n\n    def setLoggerClass(self, klass):\n        \"\"\"\n        Set the class to be used when instantiating a logger with this Manager.\n        \"\"\"\n        if klass != Logger:\n            if not issubclass(klass, Logger):\n                raise TypeError(\"logger not derived from logging.Logger: \"\n                                + klass.__name__)\n        self.loggerClass = klass\n\n    def setLogRecordFactory(self, factory):\n        \"\"\"\n        Set the factory to be used when instantiating a log record with this\n        Manager.\n        \"\"\"\n        self.logRecordFactory = factory\n\n    def _fixupParents(self, alogger):\n        \"\"\"\n        Ensure that there are either loggers or placeholders all the way\n        from the specified logger to the root of the logger hierarchy.\n        \"\"\"\n        name = alogger.name\n        i = name.rfind(\".\")\n        rv = None\n        while (i > 0) and not rv:\n            substr = name[:i]\n            if substr not in self.loggerDict:\n                self.loggerDict[substr] = PlaceHolder(alogger)\n            else:\n                obj = self.loggerDict[substr]\n                if isinstance(obj, Logger):\n                    rv = obj\n                else:\n                    assert isinstance(obj, PlaceHolder)\n                    obj.append(alogger)\n            i = name.rfind(\".\", 0, i - 1)\n        if not rv:\n            rv = self.root\n        alogger.parent = rv\n\n    def _fixupChildren(self, ph, alogger):\n        \"\"\"\n        Ensure that children of the placeholder ph are connected to the\n        specified logger.\n        \"\"\"\n        name = alogger.name\n        namelen = len(name)\n        for c in ph.loggerMap.keys():\n            #The if means ... if not c.parent.name.startswith(nm)\n            if c.parent.name[:namelen] != name:\n                alogger.parent = c.parent\n                c.parent = alogger\n\n    def _clear_cache(self):\n        \"\"\"\n        Clear the cache for all loggers in loggerDict\n        Called when level changes are made\n        \"\"\"\n\n        _acquireLock()\n        for logger in self.loggerDict.values():\n            if isinstance(logger, Logger):\n                logger._cache.clear()\n        self.root._cache.clear()\n        _releaseLock()\n\n#---------------------------------------------------------------------------\n#   Logger classes and functions\n#---------------------------------------------------------------------------\n\nclass Logger(Filterer):\n    \"\"\"\n    Instances of the Logger class represent a single logging channel. A\n    \"logging channel\" indicates an area of an application. Exactly how an\n    \"area\" is defined is up to the application developer. Since an\n    application can have any number of areas, logging channels are identified\n    by a unique string. Application areas can be nested (e.g. an area\n    of \"input processing\" might include sub-areas \"read CSV files\", \"read\n    XLS files\" and \"read Gnumeric files\"). To cater for this natural nesting,\n    channel names are organized into a namespace hierarchy where levels are\n    separated by periods, much like the Java or Python package namespace. So\n    in the instance given above, channel names might be \"input\" for the upper\n    level, and \"input.csv\", \"input.xls\" and \"input.gnu\" for the sub-levels.\n    There is no arbitrary limit to the depth of nesting.\n    \"\"\"\n    def __init__(self, name, level=NOTSET):\n        \"\"\"\n        Initialize the logger with a name and an optional level.\n        \"\"\"\n        Filterer.__init__(self)\n        self.name = name\n        self.level = _checkLevel(level)\n        self.parent = None\n        self.propagate = True\n        self.handlers = []\n        self.disabled = False\n        self._cache = {}\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this logger.  level must be an int or a str.\n        \"\"\"\n        self.level = _checkLevel(level)\n        self.manager._clear_cache()\n\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'DEBUG'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.debug(\"Houston, we have a %s\", \"thorny problem\", exc_info=True)\n        \"\"\"\n        if self.isEnabledFor(DEBUG):\n            self._log(DEBUG, msg, args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'INFO'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.info(\"Houston, we have a %s\", \"interesting problem\", exc_info=True)\n        \"\"\"\n        if self.isEnabledFor(INFO):\n            self._log(INFO, msg, args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'WARNING'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.warning(\"Houston, we have a %s\", \"bit of a problem\", exc_info=True)\n        \"\"\"\n        if self.isEnabledFor(WARNING):\n            self._log(WARNING, msg, args, **kwargs)\n\n    def warn(self, msg, *args, **kwargs):\n        warnings.warn(\"The 'warn' method is deprecated, \"\n            \"use 'warning' instead\", DeprecationWarning, 2)\n        self.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'ERROR'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.error(\"Houston, we have a %s\", \"major problem\", exc_info=True)\n        \"\"\"\n        if self.isEnabledFor(ERROR):\n            self._log(ERROR, msg, args, **kwargs)\n\n    def exception(self, msg, *args, exc_info=True, **kwargs):\n        \"\"\"\n        Convenience method for logging an ERROR with exception information.\n        \"\"\"\n        self.error(msg, *args, exc_info=exc_info, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'CRITICAL'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.critical(\"Houston, we have a %s\", \"major disaster\", exc_info=True)\n        \"\"\"\n        if self.isEnabledFor(CRITICAL):\n            self._log(CRITICAL, msg, args, **kwargs)\n\n    def fatal(self, msg, *args, **kwargs):\n        \"\"\"\n        Don't use this method, use critical() instead.\n        \"\"\"\n        self.critical(msg, *args, **kwargs)\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with the integer severity 'level'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.log(level, \"We have a %s\", \"mysterious problem\", exc_info=True)\n        \"\"\"\n        if not isinstance(level, int):\n            if raiseExceptions:\n                raise TypeError(\"level must be an integer\")\n            else:\n                return\n        if self.isEnabledFor(level):\n            self._log(level, msg, args, **kwargs)\n\n    def findCaller(self, stack_info=False, stacklevel=1):\n        \"\"\"\n        Find the stack frame of the caller so that we can note the source\n        file name, line number and function name.\n        \"\"\"\n        f = currentframe()\n        #On some versions of IronPython, currentframe() returns None if\n        #IronPython isn't run with -X:Frames.\n        if f is None:\n            return \"(unknown file)\", 0, \"(unknown function)\", None\n        while stacklevel > 0:\n            next_f = f.f_back\n            if next_f is None:\n                ## We've got options here.\n                ## If we want to use the last (deepest) frame:\n                break\n                ## If we want to mimic the warnings module:\n                #return (\"sys\", 1, \"(unknown function)\", None)\n                ## If we want to be pedantic:\n                #raise ValueError(\"call stack is not deep enough\")\n            f = next_f\n            if not _is_internal_frame(f):\n                stacklevel -= 1\n        co = f.f_code\n        sinfo = None\n        if stack_info:\n            with io.StringIO() as sio:\n                sio.write(\"Stack (most recent call last):\\n\")\n                traceback.print_stack(f, file=sio)\n                sinfo = sio.getvalue()\n                if sinfo[-1] == '\\n':\n                    sinfo = sinfo[:-1]\n        return co.co_filename, f.f_lineno, co.co_name, sinfo\n\n    def makeRecord(self, name, level, fn, lno, msg, args, exc_info,\n                   func=None, extra=None, sinfo=None):\n        \"\"\"\n        A factory method which can be overridden in subclasses to create\n        specialized LogRecords.\n        \"\"\"\n        rv = _logRecordFactory(name, level, fn, lno, msg, args, exc_info, func,\n                             sinfo)\n        if extra is not None:\n            for key in extra:\n                if (key in [\"message\", \"asctime\"]) or (key in rv.__dict__):\n                    raise KeyError(\"Attempt to overwrite %r in LogRecord\" % key)\n                rv.__dict__[key] = extra[key]\n        return rv\n\n    def _log(self, level, msg, args, exc_info=None, extra=None, stack_info=False,\n             stacklevel=1):\n        \"\"\"\n        Low-level logging routine which creates a LogRecord and then calls\n        all the handlers of this logger to handle the record.\n        \"\"\"\n        sinfo = None\n        if _srcfile:\n            #IronPython doesn't track Python frames, so findCaller raises an\n            #exception on some versions of IronPython. We trap it here so that\n            #IronPython can use logging.\n            try:\n                fn, lno, func, sinfo = self.findCaller(stack_info, stacklevel)\n            except ValueError: # pragma: no cover\n                fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        else: # pragma: no cover\n            fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        if exc_info:\n            if isinstance(exc_info, BaseException):\n                exc_info = (type(exc_info), exc_info, exc_info.__traceback__)\n            elif not isinstance(exc_info, tuple):\n                exc_info = sys.exc_info()\n        record = self.makeRecord(self.name, level, fn, lno, msg, args,\n                                 exc_info, func, extra, sinfo)\n        self.handle(record)\n\n    def handle(self, record):\n        \"\"\"\n        Call the handlers for the specified record.\n\n        This method is used for unpickled records received from a socket, as\n        well as those created locally. Logger-level filtering is applied.\n        \"\"\"\n        if (not self.disabled) and self.filter(record):\n            self.callHandlers(record)\n\n    def addHandler(self, hdlr):\n        \"\"\"\n        Add the specified handler to this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if not (hdlr in self.handlers):\n                self.handlers.append(hdlr)\n        finally:\n            _releaseLock()\n\n    def removeHandler(self, hdlr):\n        \"\"\"\n        Remove the specified handler from this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if hdlr in self.handlers:\n                self.handlers.remove(hdlr)\n        finally:\n            _releaseLock()\n\n    def hasHandlers(self):\n        \"\"\"\n        See if this logger has any handlers configured.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. Return True if a handler was found, else False.\n        Stop searching up the hierarchy whenever a logger with the \"propagate\"\n        attribute set to zero is found - that will be the last logger which\n        is checked for the existence of handlers.\n        \"\"\"\n        c = self\n        rv = False\n        while c:\n            if c.handlers:\n                rv = True\n                break\n            if not c.propagate:\n                break\n            else:\n                c = c.parent\n        return rv\n\n    def callHandlers(self, record):\n        \"\"\"\n        Pass a record to all relevant handlers.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. If no handler was found, output a one-off error\n        message to sys.stderr. Stop searching up the hierarchy whenever a\n        logger with the \"propagate\" attribute set to zero is found - that\n        will be the last logger whose handlers are called.\n        \"\"\"\n        c = self\n        found = 0\n        while c:\n            for hdlr in c.handlers:\n                found = found + 1\n                if record.levelno >= hdlr.level:\n                    hdlr.handle(record)\n            if not c.propagate:\n                c = None    #break out\n            else:\n                c = c.parent\n        if (found == 0):\n            if lastResort:\n                if record.levelno >= lastResort.level:\n                    lastResort.handle(record)\n            elif raiseExceptions and not self.manager.emittedNoHandlerWarning:\n                sys.stderr.write(\"No handlers could be found for logger\"\n                                 \" \\\"%s\\\"\\n\" % self.name)\n                self.manager.emittedNoHandlerWarning = True\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for this logger.\n\n        Loop through this logger and its parents in the logger hierarchy,\n        looking for a non-zero logging level. Return the first one found.\n        \"\"\"\n        logger = self\n        while logger:\n            if logger.level:\n                return logger.level\n            logger = logger.parent\n        return NOTSET\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        if self.disabled:\n            return False\n\n        try:\n            return self._cache[level]\n        except KeyError:\n            _acquireLock()\n            try:\n                if self.manager.disable >= level:\n                    is_enabled = self._cache[level] = False\n                else:\n                    is_enabled = self._cache[level] = (\n                        level >= self.getEffectiveLevel()\n                    )\n            finally:\n                _releaseLock()\n            return is_enabled\n\n    def getChild(self, suffix):\n        \"\"\"\n        Get a logger which is a descendant to this one.\n\n        This is a convenience method, such that\n\n        logging.getLogger('abc').getChild('def.ghi')\n\n        is the same as\n\n        logging.getLogger('abc.def.ghi')\n\n        It's useful, for example, when the parent logger is named using\n        __name__ rather than a literal string.\n        \"\"\"\n        if self.root is not self:\n            suffix = '.'.join((self.name, suffix))\n        return self.manager.getLogger(suffix)\n\n    def __repr__(self):\n        level = getLevelName(self.getEffectiveLevel())\n        return '<%s %s (%s)>' % (self.__class__.__name__, self.name, level)\n\n    def __reduce__(self):\n        if getLogger(self.name) is not self:\n            import pickle\n            raise pickle.PicklingError('logger cannot be pickled')\n        return getLogger, (self.name,)\n\n\nclass RootLogger(Logger):\n    \"\"\"\n    A root logger is not that different to any other logger, except that\n    it must have a logging level and there is only one instance of it in\n    the hierarchy.\n    \"\"\"\n    def __init__(self, level):\n        \"\"\"\n        Initialize the logger with the name \"root\".\n        \"\"\"\n        Logger.__init__(self, \"root\", level)\n\n    def __reduce__(self):\n        return getLogger, ()\n\n_loggerClass = Logger\n\nclass LoggerAdapter(object):\n    \"\"\"\n    An adapter for loggers which makes it easier to specify contextual\n    information in logging output.\n    \"\"\"\n\n    def __init__(self, logger, extra=None):\n        \"\"\"\n        Initialize the adapter with a logger and a dict-like object which\n        provides contextual information. This constructor signature allows\n        easy stacking of LoggerAdapters, if so desired.\n\n        You can effectively pass keyword arguments as shown in the\n        following example:\n\n        adapter = LoggerAdapter(someLogger, dict(p1=v1, p2=\"v2\"))\n        \"\"\"\n        self.logger = logger\n        self.extra = extra\n\n    def process(self, msg, kwargs):\n        \"\"\"\n        Process the logging message and keyword arguments passed in to\n        a logging call to insert contextual information. You can either\n        manipulate the message itself, the keyword args or both. Return\n        the message and kwargs modified (or not) to suit your needs.\n\n        Normally, you'll only need to override this one method in a\n        LoggerAdapter subclass for your specific needs.\n        \"\"\"\n        kwargs[\"extra\"] = self.extra\n        return msg, kwargs\n\n    #\n    # Boilerplate convenience methods\n    #\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a debug call to the underlying logger.\n        \"\"\"\n        self.log(DEBUG, msg, *args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an info call to the underlying logger.\n        \"\"\"\n        self.log(INFO, msg, *args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a warning call to the underlying logger.\n        \"\"\"\n        self.log(WARNING, msg, *args, **kwargs)\n\n    def warn(self, msg, *args, **kwargs):\n        warnings.warn(\"The 'warn' method is deprecated, \"\n            \"use 'warning' instead\", DeprecationWarning, 2)\n        self.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an error call to the underlying logger.\n        \"\"\"\n        self.log(ERROR, msg, *args, **kwargs)\n\n    def exception(self, msg, *args, exc_info=True, **kwargs):\n        \"\"\"\n        Delegate an exception call to the underlying logger.\n        \"\"\"\n        self.log(ERROR, msg, *args, exc_info=exc_info, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a critical call to the underlying logger.\n        \"\"\"\n        self.log(CRITICAL, msg, *args, **kwargs)\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a log call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        if self.isEnabledFor(level):\n            msg, kwargs = self.process(msg, kwargs)\n            self.logger.log(level, msg, *args, **kwargs)\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        return self.logger.isEnabledFor(level)\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the specified level on the underlying logger.\n        \"\"\"\n        self.logger.setLevel(level)\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for the underlying logger.\n        \"\"\"\n        return self.logger.getEffectiveLevel()\n\n    def hasHandlers(self):\n        \"\"\"\n        See if the underlying logger has any handlers.\n        \"\"\"\n        return self.logger.hasHandlers()\n\n    def _log(self, level, msg, args, **kwargs):\n        \"\"\"\n        Low-level log implementation, proxied to allow nested logger adapters.\n        \"\"\"\n        return self.logger._log(level, msg, args, **kwargs)\n\n    @property\n    def manager(self):\n        return self.logger.manager\n\n    @manager.setter\n    def manager(self, value):\n        self.logger.manager = value\n\n    @property\n    def name(self):\n        return self.logger.name\n\n    def __repr__(self):\n        logger = self.logger\n        level = getLevelName(logger.getEffectiveLevel())\n        return '<%s %s (%s)>' % (self.__class__.__name__, logger.name, level)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\nroot = RootLogger(WARNING)\nLogger.root = root\nLogger.manager = Manager(Logger.root)\n\n#---------------------------------------------------------------------------\n# Configuration classes and functions\n#---------------------------------------------------------------------------\n\ndef basicConfig(**kwargs):\n    \"\"\"\n    Do basic configuration for the logging system.\n\n    This function does nothing if the root logger already has handlers\n    configured, unless the keyword argument *force* is set to ``True``.\n    It is a convenience method intended for use by simple scripts\n    to do one-shot configuration of the logging package.\n\n    The default behaviour is to create a StreamHandler which writes to\n    sys.stderr, set a formatter using the BASIC_FORMAT format string, and\n    add the handler to the root logger.\n\n    A number of optional keyword arguments may be specified, which can alter\n    the default behaviour.\n\n    filename  Specifies that a FileHandler be created, using the specified\n              filename, rather than a StreamHandler.\n    filemode  Specifies the mode to open the file, if filename is specified\n              (if filemode is unspecified, it defaults to 'a').\n    format    Use the specified format string for the handler.\n    datefmt   Use the specified date/time format.\n    style     If a format string is specified, use this to specify the\n              type of format string (possible values '%', '{', '$', for\n              %-formatting, :meth:`str.format` and :class:`string.Template`\n              - defaults to '%').\n    level     Set the root logger level to the specified level.\n    stream    Use the specified stream to initialize the StreamHandler. Note\n              that this argument is incompatible with 'filename' - if both\n              are present, 'stream' is ignored.\n    handlers  If specified, this should be an iterable of already created\n              handlers, which will be added to the root logger. Any handler\n              in the list which does not have a formatter assigned will be\n              assigned the formatter created in this function.\n    force     If this keyword  is specified as true, any existing handlers\n              attached to the root logger are removed and closed, before\n              carrying out the configuration as specified by the other\n              arguments.\n    encoding  If specified together with a filename, this encoding is passed to\n              the created FileHandler, causing it to be used when the file is\n              opened.\n    errors    If specified together with a filename, this value is passed to the\n              created FileHandler, causing it to be used when the file is\n              opened in text mode. If not specified, the default value is\n              `backslashreplace`.\n\n    Note that you could specify a stream created using open(filename, mode)\n    rather than passing the filename and mode in. However, it should be\n    remembered that StreamHandler does not close its stream (since it may be\n    using sys.stdout or sys.stderr), whereas FileHandler closes its stream\n    when the handler is closed.\n\n    .. versionchanged:: 3.2\n       Added the ``style`` parameter.\n\n    .. versionchanged:: 3.3\n       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for\n       incompatible arguments (e.g. ``handlers`` specified together with\n       ``filename``/``filemode``, or ``filename``/``filemode`` specified\n       together with ``stream``, or ``handlers`` specified together with\n       ``stream``.\n\n    .. versionchanged:: 3.8\n       Added the ``force`` parameter.\n\n    .. versionchanged:: 3.9\n       Added the ``encoding`` and ``errors`` parameters.\n    \"\"\"\n    # Add thread safety in case someone mistakenly calls\n    # basicConfig() from multiple threads\n    _acquireLock()\n    try:\n        force = kwargs.pop('force', False)\n        encoding = kwargs.pop('encoding', None)\n        errors = kwargs.pop('errors', 'backslashreplace')\n        if force:\n            for h in root.handlers[:]:\n                root.removeHandler(h)\n                h.close()\n        if len(root.handlers) == 0:\n            handlers = kwargs.pop(\"handlers\", None)\n            if handlers is None:\n                if \"stream\" in kwargs and \"filename\" in kwargs:\n                    raise ValueError(\"'stream' and 'filename' should not be \"\n                                     \"specified together\")\n            else:\n                if \"stream\" in kwargs or \"filename\" in kwargs:\n                    raise ValueError(\"'stream' or 'filename' should not be \"\n                                     \"specified together with 'handlers'\")\n            if handlers is None:\n                filename = kwargs.pop(\"filename\", None)\n                mode = kwargs.pop(\"filemode\", 'a')\n                if filename:\n                    if 'b' in mode:\n                        errors = None\n                    else:\n                        encoding = io.text_encoding(encoding)\n                    h = FileHandler(filename, mode,\n                                    encoding=encoding, errors=errors)\n                else:\n                    stream = kwargs.pop(\"stream\", None)\n                    h = StreamHandler(stream)\n                handlers = [h]\n            dfs = kwargs.pop(\"datefmt\", None)\n            style = kwargs.pop(\"style\", '%')\n            if style not in _STYLES:\n                raise ValueError('Style must be one of: %s' % ','.join(\n                                 _STYLES.keys()))\n            fs = kwargs.pop(\"format\", _STYLES[style][1])\n            fmt = Formatter(fs, dfs, style)\n            for h in handlers:\n                if h.formatter is None:\n                    h.setFormatter(fmt)\n                root.addHandler(h)\n            level = kwargs.pop(\"level\", None)\n            if level is not None:\n                root.setLevel(level)\n            if kwargs:\n                keys = ', '.join(kwargs.keys())\n                raise ValueError('Unrecognised argument(s): %s' % keys)\n    finally:\n        _releaseLock()\n\n#---------------------------------------------------------------------------\n# Utility functions at module level.\n# Basically delegate everything to the root logger.\n#---------------------------------------------------------------------------\n\ndef getLogger(name=None):\n    \"\"\"\n    Return a logger with the specified name, creating it if necessary.\n\n    If no name is specified, return the root logger.\n    \"\"\"\n    if not name or isinstance(name, str) and name == root.name:\n        return root\n    return Logger.manager.getLogger(name)\n\ndef critical(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'CRITICAL' on the root logger. If the logger\n    has no handlers, call basicConfig() to add a console handler with a\n    pre-defined format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.critical(msg, *args, **kwargs)\n\ndef fatal(msg, *args, **kwargs):\n    \"\"\"\n    Don't use this function, use critical() instead.\n    \"\"\"\n    critical(msg, *args, **kwargs)\n\ndef error(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.error(msg, *args, **kwargs)\n\ndef exception(msg, *args, exc_info=True, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger, with exception\n    information. If the logger has no handlers, basicConfig() is called to add\n    a console handler with a pre-defined format.\n    \"\"\"\n    error(msg, *args, exc_info=exc_info, **kwargs)\n\ndef warning(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'WARNING' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.warning(msg, *args, **kwargs)\n\ndef warn(msg, *args, **kwargs):\n    warnings.warn(\"The 'warn' function is deprecated, \"\n        \"use 'warning' instead\", DeprecationWarning, 2)\n    warning(msg, *args, **kwargs)\n\ndef info(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'INFO' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.info(msg, *args, **kwargs)\n\ndef debug(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'DEBUG' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.debug(msg, *args, **kwargs)\n\ndef log(level, msg, *args, **kwargs):\n    \"\"\"\n    Log 'msg % args' with the integer severity 'level' on the root logger. If\n    the logger has no handlers, call basicConfig() to add a console handler\n    with a pre-defined format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.log(level, msg, *args, **kwargs)\n\ndef disable(level=CRITICAL):\n    \"\"\"\n    Disable all logging calls of severity 'level' and below.\n    \"\"\"\n    root.manager.disable = level\n    root.manager._clear_cache()\n\ndef shutdown(handlerList=_handlerList):\n    \"\"\"\n    Perform any cleanup actions in the logging system (e.g. flushing\n    buffers).\n\n    Should be called at application exit.\n    \"\"\"\n    for wr in reversed(handlerList[:]):\n        #errors might occur, for example, if files are locked\n        #we just ignore them if raiseExceptions is not set\n        try:\n            h = wr()\n            if h:\n                try:\n                    h.acquire()\n                    h.flush()\n                    h.close()\n                except (OSError, ValueError):\n                    # Ignore errors which might be caused\n                    # because handlers have been closed but\n                    # references to them are still around at\n                    # application exit.\n                    pass\n                finally:\n                    h.release()\n        except: # ignore everything, as we're shutting down\n            if raiseExceptions:\n                raise\n            #else, swallow\n\n#Let's try and shutdown automatically on application exit...\nimport atexit\natexit.register(shutdown)\n\n# Null handler\n\nclass NullHandler(Handler):\n    \"\"\"\n    This handler does nothing. It's intended to be used to avoid the\n    \"No handlers could be found for logger XXX\" one-off warning. This is\n    important for library code, which may contain code to log events. If a user\n    of the library does not configure logging, the one-off warning might be\n    produced; to avoid this, the library developer simply needs to instantiate\n    a NullHandler and add it to the top-level logger of the library module or\n    package.\n    \"\"\"\n    def handle(self, record):\n        \"\"\"Stub.\"\"\"\n\n    def emit(self, record):\n        \"\"\"Stub.\"\"\"\n\n    def createLock(self):\n        self.lock = None\n\n    def _at_fork_reinit(self):\n        pass\n\n# Warnings integration\n\n_warnings_showwarning = None\n\ndef _showwarning(message, category, filename, lineno, file=None, line=None):\n    \"\"\"\n    Implementation of showwarnings which redirects to logging, which will first\n    check to see if the file parameter is None. If a file is specified, it will\n    delegate to the original warnings implementation of showwarning. Otherwise,\n    it will call warnings.formatwarning and will log the resulting string to a\n    warnings logger named \"py.warnings\" with level logging.WARNING.\n    \"\"\"\n    if file is not None:\n        if _warnings_showwarning is not None:\n            _warnings_showwarning(message, category, filename, lineno, file, line)\n    else:\n        s = warnings.formatwarning(message, category, filename, lineno, line)\n        logger = getLogger(\"py.warnings\")\n        if not logger.handlers:\n            logger.addHandler(NullHandler())\n        # bpo-46557: Log str(s) as msg instead of logger.warning(\"%s\", s)\n        # since some log aggregation tools group logs by the msg arg\n        logger.warning(str(s))\n\ndef captureWarnings(capture):\n    \"\"\"\n    If capture is true, redirect all warnings to the logging package.\n    If capture is False, ensure that warnings are not redirected to logging\n    but to their original destinations.\n    \"\"\"\n    global _warnings_showwarning\n    if capture:\n        if _warnings_showwarning is None:\n            _warnings_showwarning = warnings.showwarning\n            warnings.showwarning = _showwarning\n    else:\n        if _warnings_showwarning is not None:\n            warnings.showwarning = _warnings_showwarning\n            _warnings_showwarning = None\n", 2266], "<frozen posixpath>": ["\"\"\"Common operations on Posix pathnames.\n\nInstead of importing this module directly, import os and refer to\nthis module as os.path.  The \"os.path\" name is an alias for this\nmodule on Posix systems; on other systems (e.g. Windows),\nos.path provides the same operations in a manner specific to that\nplatform, and is an alias to another module (e.g. ntpath).\n\nSome of this can actually be useful on non-Posix systems too, e.g.\nfor manipulation of the pathname component of URLs.\n\"\"\"\n\n# Strings representing various path-related bits and pieces.\n# These are primarily for export; internally, they are hardcoded.\n# Should be set before imports for resolving cyclic dependency.\ncurdir = '.'\npardir = '..'\nextsep = '.'\nsep = '/'\npathsep = ':'\ndefpath = '/bin:/usr/bin'\naltsep = None\ndevnull = '/dev/null'\n\nimport os\nimport sys\nimport stat\nimport genericpath\nfrom genericpath import *\n\n__all__ = [\"normcase\",\"isabs\",\"join\",\"splitdrive\",\"split\",\"splitext\",\n           \"basename\",\"dirname\",\"commonprefix\",\"getsize\",\"getmtime\",\n           \"getatime\",\"getctime\",\"islink\",\"exists\",\"lexists\",\"isdir\",\"isfile\",\n           \"ismount\", \"expanduser\",\"expandvars\",\"normpath\",\"abspath\",\n           \"samefile\",\"sameopenfile\",\"samestat\",\n           \"curdir\",\"pardir\",\"sep\",\"pathsep\",\"defpath\",\"altsep\",\"extsep\",\n           \"devnull\",\"realpath\",\"supports_unicode_filenames\",\"relpath\",\n           \"commonpath\", \"ALLOW_MISSING\"]\n\n\ndef _get_sep(path):\n    if isinstance(path, bytes):\n        return b'/'\n    else:\n        return '/'\n\n# Normalize the case of a pathname.  Trivial in Posix, string.lower on Mac.\n# On MS-DOS this may also turn slashes into backslashes; however, other\n# normalizations (such as optimizing '../' away) are not allowed\n# (another function should be defined to do that).\n\ndef normcase(s):\n    \"\"\"Normalize case of pathname.  Has no effect under Posix\"\"\"\n    return os.fspath(s)\n\n\n# Return whether a path is absolute.\n# Trivial in Posix, harder on the Mac or MS-DOS.\n\ndef isabs(s):\n    \"\"\"Test whether a path is absolute\"\"\"\n    s = os.fspath(s)\n    sep = _get_sep(s)\n    return s.startswith(sep)\n\n\n# Join pathnames.\n# Ignore the previous parts if a part is absolute.\n# Insert a '/' unless the first part is empty or already ends in '/'.\n\ndef join(a, *p):\n    \"\"\"Join two or more pathname components, inserting '/' as needed.\n    If any component is an absolute path, all previous path components\n    will be discarded.  An empty last part will result in a path that\n    ends with a separator.\"\"\"\n    a = os.fspath(a)\n    sep = _get_sep(a)\n    path = a\n    try:\n        if not p:\n            path[:0] + sep  #23780: Ensure compatible data type even if p is null.\n        for b in map(os.fspath, p):\n            if b.startswith(sep):\n                path = b\n            elif not path or path.endswith(sep):\n                path += b\n            else:\n                path += sep + b\n    except (TypeError, AttributeError, BytesWarning):\n        genericpath._check_arg_types('join', a, *p)\n        raise\n    return path\n\n\n# Split a path in head (everything up to the last '/') and tail (the\n# rest).  If the path ends in '/', tail will be empty.  If there is no\n# '/' in the path, head  will be empty.\n# Trailing '/'es are stripped from head unless it is the root.\n\ndef split(p):\n    \"\"\"Split a pathname.  Returns tuple \"(head, tail)\" where \"tail\" is\n    everything after the final slash.  Either part may be empty.\"\"\"\n    p = os.fspath(p)\n    sep = _get_sep(p)\n    i = p.rfind(sep) + 1\n    head, tail = p[:i], p[i:]\n    if head and head != sep*len(head):\n        head = head.rstrip(sep)\n    return head, tail\n\n\n# Split a path in root and extension.\n# The extension is everything starting at the last dot in the last\n# pathname component; the root is everything before that.\n# It is always true that root + ext == p.\n\ndef splitext(p):\n    p = os.fspath(p)\n    if isinstance(p, bytes):\n        sep = b'/'\n        extsep = b'.'\n    else:\n        sep = '/'\n        extsep = '.'\n    return genericpath._splitext(p, sep, None, extsep)\nsplitext.__doc__ = genericpath._splitext.__doc__\n\n# Split a pathname into a drive specification and the rest of the\n# path.  Useful on DOS/Windows/NT; on Unix, the drive is always empty.\n\ndef splitdrive(p):\n    \"\"\"Split a pathname into drive and path. On Posix, drive is always\n    empty.\"\"\"\n    p = os.fspath(p)\n    return p[:0], p\n\n\n# Return the tail (basename) part of a path, same as split(path)[1].\n\ndef basename(p):\n    \"\"\"Returns the final component of a pathname\"\"\"\n    p = os.fspath(p)\n    sep = _get_sep(p)\n    i = p.rfind(sep) + 1\n    return p[i:]\n\n\n# Return the head (dirname) part of a path, same as split(path)[0].\n\ndef dirname(p):\n    \"\"\"Returns the directory component of a pathname\"\"\"\n    p = os.fspath(p)\n    sep = _get_sep(p)\n    i = p.rfind(sep) + 1\n    head = p[:i]\n    if head and head != sep*len(head):\n        head = head.rstrip(sep)\n    return head\n\n\n# Is a path a symbolic link?\n# This will always return false on systems where os.lstat doesn't exist.\n\ndef islink(path):\n    \"\"\"Test whether a path is a symbolic link\"\"\"\n    try:\n        st = os.lstat(path)\n    except (OSError, ValueError, AttributeError):\n        return False\n    return stat.S_ISLNK(st.st_mode)\n\n# Being true for dangling symbolic links is also useful.\n\ndef lexists(path):\n    \"\"\"Test whether a path exists.  Returns True for broken symbolic links\"\"\"\n    try:\n        os.lstat(path)\n    except (OSError, ValueError):\n        return False\n    return True\n\n\n# Is a path a mount point?\n# (Does this work for all UNIXes?  Is it even guaranteed to work by Posix?)\n\ndef ismount(path):\n    \"\"\"Test whether a path is a mount point\"\"\"\n    try:\n        s1 = os.lstat(path)\n    except (OSError, ValueError):\n        # It doesn't exist -- so not a mount point. :-)\n        return False\n    else:\n        # A symlink can never be a mount point\n        if stat.S_ISLNK(s1.st_mode):\n            return False\n\n    path = os.fspath(path)\n    if isinstance(path, bytes):\n        parent = join(path, b'..')\n    else:\n        parent = join(path, '..')\n    parent = realpath(parent)\n    try:\n        s2 = os.lstat(parent)\n    except (OSError, ValueError):\n        return False\n\n    dev1 = s1.st_dev\n    dev2 = s2.st_dev\n    if dev1 != dev2:\n        return True     # path/.. on a different device as path\n    ino1 = s1.st_ino\n    ino2 = s2.st_ino\n    if ino1 == ino2:\n        return True     # path/.. is the same i-node as path\n    return False\n\n\n# Expand paths beginning with '~' or '~user'.\n# '~' means $HOME; '~user' means that user's home directory.\n# If the path doesn't begin with '~', or if the user or $HOME is unknown,\n# the path is returned unchanged (leaving error reporting to whatever\n# function is called with the expanded path as argument).\n# See also module 'glob' for expansion of *, ? and [...] in pathnames.\n# (A function should also be defined to do full *sh-style environment\n# variable expansion.)\n\ndef expanduser(path):\n    \"\"\"Expand ~ and ~user constructions.  If user or $HOME is unknown,\n    do nothing.\"\"\"\n    path = os.fspath(path)\n    if isinstance(path, bytes):\n        tilde = b'~'\n    else:\n        tilde = '~'\n    if not path.startswith(tilde):\n        return path\n    sep = _get_sep(path)\n    i = path.find(sep, 1)\n    if i < 0:\n        i = len(path)\n    if i == 1:\n        if 'HOME' not in os.environ:\n            try:\n                import pwd\n            except ImportError:\n                # pwd module unavailable, return path unchanged\n                return path\n            try:\n                userhome = pwd.getpwuid(os.getuid()).pw_dir\n            except KeyError:\n                # bpo-10496: if the current user identifier doesn't exist in the\n                # password database, return the path unchanged\n                return path\n        else:\n            userhome = os.environ['HOME']\n    else:\n        try:\n            import pwd\n        except ImportError:\n            # pwd module unavailable, return path unchanged\n            return path\n        name = path[1:i]\n        if isinstance(name, bytes):\n            name = str(name, 'ASCII')\n        try:\n            pwent = pwd.getpwnam(name)\n        except KeyError:\n            # bpo-10496: if the user name from the path doesn't exist in the\n            # password database, return the path unchanged\n            return path\n        userhome = pwent.pw_dir\n    # if no user home, return the path unchanged on VxWorks\n    if userhome is None and sys.platform == \"vxworks\":\n        return path\n    if isinstance(path, bytes):\n        userhome = os.fsencode(userhome)\n        root = b'/'\n    else:\n        root = '/'\n    userhome = userhome.rstrip(root)\n    return (userhome + path[i:]) or root\n\n\n# Expand paths containing shell variable substitutions.\n# This expands the forms $variable and ${variable} only.\n# Non-existent variables are left unchanged.\n\n_varprog = None\n_varprogb = None\n\ndef expandvars(path):\n    \"\"\"Expand shell variables of form $var and ${var}.  Unknown variables\n    are left unchanged.\"\"\"\n    path = os.fspath(path)\n    global _varprog, _varprogb\n    if isinstance(path, bytes):\n        if b'$' not in path:\n            return path\n        if not _varprogb:\n            import re\n            _varprogb = re.compile(br'\\$(\\w+|\\{[^}]*\\})', re.ASCII)\n        search = _varprogb.search\n        start = b'{'\n        end = b'}'\n        environ = getattr(os, 'environb', None)\n    else:\n        if '$' not in path:\n            return path\n        if not _varprog:\n            import re\n            _varprog = re.compile(r'\\$(\\w+|\\{[^}]*\\})', re.ASCII)\n        search = _varprog.search\n        start = '{'\n        end = '}'\n        environ = os.environ\n    i = 0\n    while True:\n        m = search(path, i)\n        if not m:\n            break\n        i, j = m.span(0)\n        name = m.group(1)\n        if name.startswith(start) and name.endswith(end):\n            name = name[1:-1]\n        try:\n            if environ is None:\n                value = os.fsencode(os.environ[os.fsdecode(name)])\n            else:\n                value = environ[name]\n        except KeyError:\n            i = j\n        else:\n            tail = path[j:]\n            path = path[:i] + value\n            i = len(path)\n            path += tail\n    return path\n\n\n# Normalize a path, e.g. A//B, A/./B and A/foo/../B all become A/B.\n# It should be understood that this may change the meaning of the path\n# if it contains symbolic links!\n\ntry:\n    from posix import _path_normpath\n\nexcept ImportError:\n    def normpath(path):\n        \"\"\"Normalize path, eliminating double slashes, etc.\"\"\"\n        path = os.fspath(path)\n        if isinstance(path, bytes):\n            sep = b'/'\n            empty = b''\n            dot = b'.'\n            dotdot = b'..'\n        else:\n            sep = '/'\n            empty = ''\n            dot = '.'\n            dotdot = '..'\n        if path == empty:\n            return dot\n        initial_slashes = path.startswith(sep)\n        # POSIX allows one or two initial slashes, but treats three or more\n        # as single slash.\n        # (see https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap04.html#tag_04_13)\n        if (initial_slashes and\n            path.startswith(sep*2) and not path.startswith(sep*3)):\n            initial_slashes = 2\n        comps = path.split(sep)\n        new_comps = []\n        for comp in comps:\n            if comp in (empty, dot):\n                continue\n            if (comp != dotdot or (not initial_slashes and not new_comps) or\n                 (new_comps and new_comps[-1] == dotdot)):\n                new_comps.append(comp)\n            elif new_comps:\n                new_comps.pop()\n        comps = new_comps\n        path = sep.join(comps)\n        if initial_slashes:\n            path = sep*initial_slashes + path\n        return path or dot\n\nelse:\n    def normpath(path):\n        \"\"\"Normalize path, eliminating double slashes, etc.\"\"\"\n        path = os.fspath(path)\n        if isinstance(path, bytes):\n            return os.fsencode(_path_normpath(os.fsdecode(path))) or b\".\"\n        return _path_normpath(path) or \".\"\n\n\ndef abspath(path):\n    \"\"\"Return an absolute path.\"\"\"\n    path = os.fspath(path)\n    if not isabs(path):\n        if isinstance(path, bytes):\n            cwd = os.getcwdb()\n        else:\n            cwd = os.getcwd()\n        path = join(cwd, path)\n    return normpath(path)\n\n\n# Return a canonical path (i.e. the absolute location of a file on the\n# filesystem).\n\ndef realpath(filename, *, strict=False):\n    \"\"\"Return the canonical path of the specified filename, eliminating any\nsymbolic links encountered in the path.\"\"\"\n    filename = os.fspath(filename)\n    path, ok = _joinrealpath(filename[:0], filename, strict, {})\n    return abspath(path)\n\n# Join two paths, normalizing and eliminating any symbolic links\n# encountered in the second path.\ndef _joinrealpath(path, rest, strict, seen):\n    if isinstance(path, bytes):\n        sep = b'/'\n        curdir = b'.'\n        pardir = b'..'\n    else:\n        sep = '/'\n        curdir = '.'\n        pardir = '..'\n        getcwd = os.getcwd\n    if strict is ALLOW_MISSING:\n        ignored_error = FileNotFoundError\n    elif strict:\n        ignored_error = ()\n    else:\n        ignored_error = OSError\n\n    maxlinks = None\n\n    if isabs(rest):\n        rest = rest[1:]\n        path = sep\n\n    while rest:\n        name, _, rest = rest.partition(sep)\n        if not name or name == curdir:\n            # current dir\n            continue\n        if name == pardir:\n            # parent dir\n            if path:\n                path, name = split(path)\n                if name == pardir:\n                    path = join(path, pardir, pardir)\n            else:\n                path = pardir\n            continue\n        newpath = join(path, name)\n        try:\n            st = os.lstat(newpath)\n        except ignored_error:\n            is_link = False\n        else:\n            is_link = stat.S_ISLNK(st.st_mode)\n        if not is_link:\n            path = newpath\n            continue\n        # Resolve the symbolic link\n        if newpath in seen:\n            # Already seen this path\n            path = seen[newpath]\n            if path is not None:\n                # use cached value\n                continue\n            # The symlink is not resolved, so we must have a symlink loop.\n            if strict:\n                # Raise OSError(errno.ELOOP)\n                os.stat(newpath)\n            else:\n                # Return already resolved part + rest of the path unchanged.\n                return join(newpath, rest), False\n        seen[newpath] = None # not resolved symlink\n        path, ok = _joinrealpath(path, os.readlink(newpath), strict, seen)\n        if not ok:\n            return join(path, rest), False\n        seen[newpath] = path # resolved symlink\n\n    return path, True\n\n\nsupports_unicode_filenames = (sys.platform == 'darwin')\n\ndef relpath(path, start=None):\n    \"\"\"Return a relative version of a path\"\"\"\n\n    if not path:\n        raise ValueError(\"no path specified\")\n\n    path = os.fspath(path)\n    if isinstance(path, bytes):\n        curdir = b'.'\n        sep = b'/'\n        pardir = b'..'\n    else:\n        curdir = '.'\n        sep = '/'\n        pardir = '..'\n\n    if start is None:\n        start = curdir\n    else:\n        start = os.fspath(start)\n\n    try:\n        start_list = [x for x in abspath(start).split(sep) if x]\n        path_list = [x for x in abspath(path).split(sep) if x]\n        # Work out how much of the filepath is shared by start and path.\n        i = len(commonprefix([start_list, path_list]))\n\n        rel_list = [pardir] * (len(start_list)-i) + path_list[i:]\n        if not rel_list:\n            return curdir\n        return join(*rel_list)\n    except (TypeError, AttributeError, BytesWarning, DeprecationWarning):\n        genericpath._check_arg_types('relpath', path, start)\n        raise\n\n\n# Return the longest common sub-path of the sequence of paths given as input.\n# The paths are not normalized before comparing them (this is the\n# responsibility of the caller). Any trailing separator is stripped from the\n# returned path.\n\ndef commonpath(paths):\n    \"\"\"Given a sequence of path names, returns the longest common sub-path.\"\"\"\n\n    if not paths:\n        raise ValueError('commonpath() arg is an empty sequence')\n\n    paths = tuple(map(os.fspath, paths))\n    if isinstance(paths[0], bytes):\n        sep = b'/'\n        curdir = b'.'\n    else:\n        sep = '/'\n        curdir = '.'\n\n    try:\n        split_paths = [path.split(sep) for path in paths]\n\n        try:\n            isabs, = set(p[:1] == sep for p in paths)\n        except ValueError:\n            raise ValueError(\"Can't mix absolute and relative paths\") from None\n\n        split_paths = [[c for c in s if c and c != curdir] for s in split_paths]\n        s1 = min(split_paths)\n        s2 = max(split_paths)\n        common = s1\n        for i, c in enumerate(s1):\n            if c != s2[i]:\n                common = s1[:i]\n                break\n\n        prefix = sep if isabs else sep[:0]\n        return prefix + sep.join(common)\n    except (TypeError, AttributeError):\n        genericpath._check_arg_types('commonpath', *paths)\n        raise\n", 569], "<frozen genericpath>": ["\"\"\"\nPath operations common to more than one OS\nDo not use directly.  The OS specific modules import the appropriate\nfunctions from this module themselves.\n\"\"\"\nimport os\nimport stat\n\n__all__ = ['commonprefix', 'exists', 'getatime', 'getctime', 'getmtime',\n           'getsize', 'isdir', 'isfile', 'samefile', 'sameopenfile',\n           'samestat', 'ALLOW_MISSING']\n\n\n# Does a path exist?\n# This is false for dangling symbolic links on systems that support them.\ndef exists(path):\n    \"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\n    try:\n        os.stat(path)\n    except (OSError, ValueError):\n        return False\n    return True\n\n\n# This follows symbolic links, so both islink() and isdir() can be true\n# for the same path on systems that support symlinks\ndef isfile(path):\n    \"\"\"Test whether a path is a regular file\"\"\"\n    try:\n        st = os.stat(path)\n    except (OSError, ValueError):\n        return False\n    return stat.S_ISREG(st.st_mode)\n\n\n# Is a path a directory?\n# This follows symbolic links, so both islink() and isdir()\n# can be true for the same path on systems that support symlinks\ndef isdir(s):\n    \"\"\"Return true if the pathname refers to an existing directory.\"\"\"\n    try:\n        st = os.stat(s)\n    except (OSError, ValueError):\n        return False\n    return stat.S_ISDIR(st.st_mode)\n\n\ndef getsize(filename):\n    \"\"\"Return the size of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_size\n\n\ndef getmtime(filename):\n    \"\"\"Return the last modification time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_mtime\n\n\ndef getatime(filename):\n    \"\"\"Return the last access time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_atime\n\n\ndef getctime(filename):\n    \"\"\"Return the metadata change time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_ctime\n\n\n# Return the longest prefix of all list elements.\ndef commonprefix(m):\n    \"Given a list of pathnames, returns the longest common leading component\"\n    if not m: return ''\n    # Some people pass in a list of pathname parts to operate in an OS-agnostic\n    # fashion; don't try to translate in that case as that's an abuse of the\n    # API and they are already doing what they need to be OS-agnostic and so\n    # they most likely won't be using an os.PathLike object in the sublists.\n    if not isinstance(m[0], (list, tuple)):\n        m = tuple(map(os.fspath, m))\n    s1 = min(m)\n    s2 = max(m)\n    for i, c in enumerate(s1):\n        if c != s2[i]:\n            return s1[:i]\n    return s1\n\n# Are two stat buffers (obtained from stat, fstat or lstat)\n# describing the same file?\ndef samestat(s1, s2):\n    \"\"\"Test whether two stat buffers reference the same file\"\"\"\n    return (s1.st_ino == s2.st_ino and\n            s1.st_dev == s2.st_dev)\n\n\n# Are two filenames really pointing to the same file?\ndef samefile(f1, f2):\n    \"\"\"Test whether two pathnames reference the same actual file or directory\n\n    This is determined by the device number and i-node number and\n    raises an exception if an os.stat() call on either pathname fails.\n    \"\"\"\n    s1 = os.stat(f1)\n    s2 = os.stat(f2)\n    return samestat(s1, s2)\n\n\n# Are two open files really referencing the same file?\n# (Not necessarily the same file descriptor!)\ndef sameopenfile(fp1, fp2):\n    \"\"\"Test whether two open file objects reference the same file\"\"\"\n    s1 = os.fstat(fp1)\n    s2 = os.fstat(fp2)\n    return samestat(s1, s2)\n\n\n# Split a path in root and extension.\n# The extension is everything starting at the last dot in the last\n# pathname component; the root is everything before that.\n# It is always true that root + ext == p.\n\n# Generic implementation of splitext, to be parametrized with\n# the separators\ndef _splitext(p, sep, altsep, extsep):\n    \"\"\"Split the extension from a pathname.\n\n    Extension is everything from the last dot to the end, ignoring\n    leading dots.  Returns \"(root, ext)\"; ext may be empty.\"\"\"\n    # NOTE: This code must work for text and bytes strings.\n\n    sepIndex = p.rfind(sep)\n    if altsep:\n        altsepIndex = p.rfind(altsep)\n        sepIndex = max(sepIndex, altsepIndex)\n\n    dotIndex = p.rfind(extsep)\n    if dotIndex > sepIndex:\n        # skip all leading dots\n        filenameIndex = sepIndex + 1\n        while filenameIndex < dotIndex:\n            if p[filenameIndex:filenameIndex+1] != extsep:\n                return p[:dotIndex], p[dotIndex:]\n            filenameIndex += 1\n\n    return p, p[:0]\n\ndef _check_arg_types(funcname, *args):\n    hasstr = hasbytes = False\n    for s in args:\n        if isinstance(s, str):\n            hasstr = True\n        elif isinstance(s, bytes):\n            hasbytes = True\n        else:\n            raise TypeError(f'{funcname}() argument must be str, bytes, or '\n                            f'os.PathLike object, not {s.__class__.__name__!r}') from None\n    if hasstr and hasbytes:\n        raise TypeError(\"Can't mix strings and bytes in path components\") from None\n\n# A singleton with a true boolean value.\n@object.__new__\nclass ALLOW_MISSING:\n    \"\"\"Special value for use in realpath().\"\"\"\n    def __repr__(self):\n        return 'os.path.ALLOW_MISSING'\n    def __reduce__(self):\n        return self.__class__.__name__\n", 164], "/usr/local/lib/python3.11/multiprocessing/process.py": ["#\n# Module providing the `Process` class which emulates `threading.Thread`\n#\n# multiprocessing/process.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# Licensed to PSF under a Contributor Agreement.\n#\n\n__all__ = ['BaseProcess', 'current_process', 'active_children',\n           'parent_process']\n\n#\n# Imports\n#\n\nimport os\nimport sys\nimport signal\nimport itertools\nimport threading\nfrom _weakrefset import WeakSet\n\n#\n#\n#\n\ntry:\n    ORIGINAL_DIR = os.path.abspath(os.getcwd())\nexcept OSError:\n    ORIGINAL_DIR = None\n\n#\n# Public functions\n#\n\ndef current_process():\n    '''\n    Return process object representing the current process\n    '''\n    return _current_process\n\ndef active_children():\n    '''\n    Return list of process objects corresponding to live child processes\n    '''\n    _cleanup()\n    return list(_children)\n\n\ndef parent_process():\n    '''\n    Return process object representing the parent process\n    '''\n    return _parent_process\n\n#\n#\n#\n\ndef _cleanup():\n    # check for processes which have finished\n    for p in list(_children):\n        if (child_popen := p._popen) and child_popen.poll() is not None:\n            _children.discard(p)\n\n#\n# The `Process` class\n#\n\nclass BaseProcess(object):\n    '''\n    Process objects represent activity that is run in a separate process\n\n    The class is analogous to `threading.Thread`\n    '''\n    def _Popen(self):\n        raise NotImplementedError\n\n    def __init__(self, group=None, target=None, name=None, args=(), kwargs={},\n                 *, daemon=None):\n        assert group is None, 'group argument must be None for now'\n        count = next(_process_counter)\n        self._identity = _current_process._identity + (count,)\n        self._config = _current_process._config.copy()\n        self._parent_pid = os.getpid()\n        self._parent_name = _current_process.name\n        self._popen = None\n        self._closed = False\n        self._target = target\n        self._args = tuple(args)\n        self._kwargs = dict(kwargs)\n        self._name = name or type(self).__name__ + '-' + \\\n                     ':'.join(str(i) for i in self._identity)\n        if daemon is not None:\n            self.daemon = daemon\n        _dangling.add(self)\n\n    def _check_closed(self):\n        if self._closed:\n            raise ValueError(\"process object is closed\")\n\n    def run(self):\n        '''\n        Method to be run in sub-process; can be overridden in sub-class\n        '''\n        if self._target:\n            self._target(*self._args, **self._kwargs)\n\n    def start(self):\n        '''\n        Start child process\n        '''\n        self._check_closed()\n        assert self._popen is None, 'cannot start a process twice'\n        assert self._parent_pid == os.getpid(), \\\n               'can only start a process object created by current process'\n        assert not _current_process._config.get('daemon'), \\\n               'daemonic processes are not allowed to have children'\n        _cleanup()\n        self._popen = self._Popen(self)\n        self._sentinel = self._popen.sentinel\n        # Avoid a refcycle if the target function holds an indirect\n        # reference to the process object (see bpo-30775)\n        del self._target, self._args, self._kwargs\n        _children.add(self)\n\n    def terminate(self):\n        '''\n        Terminate process; sends SIGTERM signal or uses TerminateProcess()\n        '''\n        self._check_closed()\n        self._popen.terminate()\n\n    def kill(self):\n        '''\n        Terminate process; sends SIGKILL signal or uses TerminateProcess()\n        '''\n        self._check_closed()\n        self._popen.kill()\n\n    def join(self, timeout=None):\n        '''\n        Wait until child process terminates\n        '''\n        self._check_closed()\n        assert self._parent_pid == os.getpid(), 'can only join a child process'\n        assert self._popen is not None, 'can only join a started process'\n        res = self._popen.wait(timeout)\n        if res is not None:\n            _children.discard(self)\n\n    def is_alive(self):\n        '''\n        Return whether process is alive\n        '''\n        self._check_closed()\n        if self is _current_process:\n            return True\n        assert self._parent_pid == os.getpid(), 'can only test a child process'\n\n        if self._popen is None:\n            return False\n\n        returncode = self._popen.poll()\n        if returncode is None:\n            return True\n        else:\n            _children.discard(self)\n            return False\n\n    def close(self):\n        '''\n        Close the Process object.\n\n        This method releases resources held by the Process object.  It is\n        an error to call this method if the child process is still running.\n        '''\n        if self._popen is not None:\n            if self._popen.poll() is None:\n                raise ValueError(\"Cannot close a process while it is still running. \"\n                                 \"You should first call join() or terminate().\")\n            self._popen.close()\n            self._popen = None\n            del self._sentinel\n            _children.discard(self)\n        self._closed = True\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        assert isinstance(name, str), 'name must be a string'\n        self._name = name\n\n    @property\n    def daemon(self):\n        '''\n        Return whether process is a daemon\n        '''\n        return self._config.get('daemon', False)\n\n    @daemon.setter\n    def daemon(self, daemonic):\n        '''\n        Set whether process is a daemon\n        '''\n        assert self._popen is None, 'process has already started'\n        self._config['daemon'] = daemonic\n\n    @property\n    def authkey(self):\n        return self._config['authkey']\n\n    @authkey.setter\n    def authkey(self, authkey):\n        '''\n        Set authorization key of process\n        '''\n        self._config['authkey'] = AuthenticationString(authkey)\n\n    @property\n    def exitcode(self):\n        '''\n        Return exit code of process or `None` if it has yet to stop\n        '''\n        self._check_closed()\n        if self._popen is None:\n            return self._popen\n        return self._popen.poll()\n\n    @property\n    def ident(self):\n        '''\n        Return identifier (PID) of process or `None` if it has yet to start\n        '''\n        self._check_closed()\n        if self is _current_process:\n            return os.getpid()\n        else:\n            return self._popen and self._popen.pid\n\n    pid = ident\n\n    @property\n    def sentinel(self):\n        '''\n        Return a file descriptor (Unix) or handle (Windows) suitable for\n        waiting for process termination.\n        '''\n        self._check_closed()\n        try:\n            return self._sentinel\n        except AttributeError:\n            raise ValueError(\"process not started\") from None\n\n    def __repr__(self):\n        exitcode = None\n        if self is _current_process:\n            status = 'started'\n        elif self._closed:\n            status = 'closed'\n        elif self._parent_pid != os.getpid():\n            status = 'unknown'\n        elif self._popen is None:\n            status = 'initial'\n        else:\n            exitcode = self._popen.poll()\n            if exitcode is not None:\n                status = 'stopped'\n            else:\n                status = 'started'\n\n        info = [type(self).__name__, 'name=%r' % self._name]\n        if self._popen is not None:\n            info.append('pid=%s' % self._popen.pid)\n        info.append('parent=%s' % self._parent_pid)\n        info.append(status)\n        if exitcode is not None:\n            exitcode = _exitcode_to_name.get(exitcode, exitcode)\n            info.append('exitcode=%s' % exitcode)\n        if self.daemon:\n            info.append('daemon')\n        return '<%s>' % ' '.join(info)\n\n    ##\n\n    def _bootstrap(self, parent_sentinel=None):\n        from . import util, context\n        global _current_process, _parent_process, _process_counter, _children\n\n        try:\n            if self._start_method is not None:\n                context._force_start_method(self._start_method)\n            _process_counter = itertools.count(1)\n            _children = set()\n            util._close_stdin()\n            old_process = _current_process\n            _current_process = self\n            _parent_process = _ParentProcess(\n                self._parent_name, self._parent_pid, parent_sentinel)\n            if threading._HAVE_THREAD_NATIVE_ID:\n                threading.main_thread()._set_native_id()\n            try:\n                self._after_fork()\n            finally:\n                # delay finalization of the old process object until after\n                # _run_after_forkers() is executed\n                del old_process\n            util.info('child process calling self.run()')\n            try:\n                self.run()\n                exitcode = 0\n            finally:\n                util._exit_function()\n        except SystemExit as e:\n            if e.code is None:\n                exitcode = 0\n            elif isinstance(e.code, int):\n                exitcode = e.code\n            else:\n                sys.stderr.write(str(e.code) + '\\n')\n                exitcode = 1\n        except:\n            exitcode = 1\n            import traceback\n            sys.stderr.write('Process %s:\\n' % self.name)\n            traceback.print_exc()\n        finally:\n            threading._shutdown()\n            util.info('process exiting with exitcode %d' % exitcode)\n            util._flush_std_streams()\n\n        return exitcode\n\n    @staticmethod\n    def _after_fork():\n        from . import util\n        util._finalizer_registry.clear()\n        util._run_after_forkers()\n\n\n#\n# We subclass bytes to avoid accidental transmission of auth keys over network\n#\n\nclass AuthenticationString(bytes):\n    def __reduce__(self):\n        from .context import get_spawning_popen\n        if get_spawning_popen() is None:\n            raise TypeError(\n                'Pickling an AuthenticationString object is '\n                'disallowed for security reasons'\n                )\n        return AuthenticationString, (bytes(self),)\n\n\n#\n# Create object representing the parent process\n#\n\nclass _ParentProcess(BaseProcess):\n\n    def __init__(self, name, pid, sentinel):\n        self._identity = ()\n        self._name = name\n        self._pid = pid\n        self._parent_pid = None\n        self._popen = None\n        self._closed = False\n        self._sentinel = sentinel\n        self._config = {}\n\n    def is_alive(self):\n        from multiprocessing.connection import wait\n        return not wait([self._sentinel], timeout=0)\n\n    @property\n    def ident(self):\n        return self._pid\n\n    def join(self, timeout=None):\n        '''\n        Wait until parent process terminates\n        '''\n        from multiprocessing.connection import wait\n        wait([self._sentinel], timeout=timeout)\n\n    pid = ident\n\n#\n# Create object representing the main process\n#\n\nclass _MainProcess(BaseProcess):\n\n    def __init__(self):\n        self._identity = ()\n        self._name = 'MainProcess'\n        self._parent_pid = None\n        self._popen = None\n        self._closed = False\n        self._config = {'authkey': AuthenticationString(os.urandom(32)),\n                        'semprefix': '/mp'}\n        # Note that some versions of FreeBSD only allow named\n        # semaphores to have names of up to 14 characters.  Therefore\n        # we choose a short prefix.\n        #\n        # On MacOSX in a sandbox it may be necessary to use a\n        # different prefix -- see #19478.\n        #\n        # Everything in self._config will be inherited by descendant\n        # processes.\n\n    def close(self):\n        pass\n\n\n_parent_process = None\n_current_process = _MainProcess()\n_process_counter = itertools.count(1)\n_children = set()\ndel _MainProcess\n\n#\n# Give names to some return codes\n#\n\n_exitcode_to_name = {}\n\nfor name, signum in list(signal.__dict__.items()):\n    if name[:3]=='SIG' and '_' not in name:\n        _exitcode_to_name[-signum] = f'-{name}'\ndel name, signum\n\n# For debug and leak testing\n_dangling = WeakSet()\n", 439]}, "functions": {"DataCache.get (/app/app/services/cache.py:10)": ["/app/app/services/cache.py", 10], "get_settings (/app/app/core/config.py:131)": ["/app/app/core/config.py", 131], "Lock.__init__ (/usr/local/lib/python3.11/asyncio/locks.py:78)": ["/usr/local/lib/python3.11/asyncio/locks.py", 78], "CircuitBreaker.__init__ (/app/app/utils/circuit_breaker.py:42)": ["/app/app/utils/circuit_breaker.py", 42], "ProviderCircuitBreakers.get_breaker (/app/app/utils/circuit_breaker.py:142)": ["/app/app/utils/circuit_breaker.py", 142], "CircuitBreakerManager._get_breaker (/app/app/circuit_breaker.py:26)": ["/app/app/circuit_breaker.py", 26], "Enum.name (/usr/local/lib/python3.11/enum.py:1252)": ["/usr/local/lib/python3.11/enum.py", 1252], "property.__get__ (/usr/local/lib/python3.11/enum.py:193)": ["/usr/local/lib/python3.11/enum.py", 193], "CircuitBreakerManager.get_state (/app/app/circuit_breaker.py:19)": ["/app/app/circuit_breaker.py", 19], "ProviderRegistry._breaker_score (/app/app/providers/registry.py:66)": ["/app/app/providers/registry.py", 66], "ProviderRegistry.health_score (/app/app/providers/registry.py:74)": ["/app/app/providers/registry.py", 74], "ProviderRegistry.rank.<locals>.<dictcomp> (/app/app/providers/registry.py:121)": ["/app/app/providers/registry.py", 121], "ProviderRegistry.rank.<locals>.<lambda> (/app/app/providers/registry.py:122)": ["/app/app/providers/registry.py", 122], "ProviderRegistry.rank (/app/app/providers/registry.py:90)": ["/app/app/providers/registry.py", 90], "MetricWrapperBase.labels.<locals>.<genexpr> (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:172)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py", 172], "_build_full_name (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:25)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py", 25], "get_legacy_validation (/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py:17)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py", 17], "_validate_labelname (/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py:75)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py", 75], "_validate_labelnames (/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py:103)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py", 103], "_validate_metric_name (/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py:34)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/validation.py", 34], "MetricWrapperBase._is_parent (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:80)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py", 80], "MetricWrapperBase._is_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:67)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py", 67], "MutexValue.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:13)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/values.py", 13], "Counter._metric_init (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:281)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py", 281], "MetricWrapperBase.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:102)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py", 102], "MetricWrapperBase.labels (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:134)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py", 134], "MetricWrapperBase._raise_if_not_observable (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:73)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py", 73], "MutexValue.inc (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:18)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/values.py", 18], "Counter.inc (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:286)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py", 286], "Gauge._metric_init (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:389)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py", 389], "Gauge.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:362)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py", 362], "MutexValue.set (/usr/local/lib/python3.11/site-packages/prometheus_client/values.py:22)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/values.py", 22], "Gauge.set (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:409)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py", 409], "ProviderRegistry.record_selection (/app/app/providers/registry.py:132)": ["/app/app/providers/registry.py", 132], "MarketDataService.get_stock_price.<locals>.<lambda> (/app/app/services/market_data.py:147)": ["/app/app/services/market_data.py", 147], "Lock.acquire (/usr/local/lib/python3.11/asyncio/locks.py:93)": ["/usr/local/lib/python3.11/asyncio/locks.py", 93], "_ContextManagerMixin.__aenter__ (/usr/local/lib/python3.11/asyncio/locks.py:14)": ["/usr/local/lib/python3.11/asyncio/locks.py", 14], "Lock._wake_up_first (/usr/local/lib/python3.11/asyncio/locks.py:142)": ["/usr/local/lib/python3.11/asyncio/locks.py", 142], "Lock.release (/usr/local/lib/python3.11/asyncio/locks.py:125)": ["/usr/local/lib/python3.11/asyncio/locks.py", 125], "_ContextManagerMixin.__aexit__ (/usr/local/lib/python3.11/asyncio/locks.py:20)": ["/usr/local/lib/python3.11/asyncio/locks.py", 20], "isfuture (/usr/local/lib/python3.11/asyncio/base_futures.py:14)": ["/usr/local/lib/python3.11/asyncio/base_futures.py", 14], "iscoroutine (/usr/local/lib/python3.11/asyncio/coroutines.py:34)": ["/usr/local/lib/python3.11/asyncio/coroutines.py", 34], "WeakSet.add (/usr/local/lib/python3.11/_weakrefset.py:85)": ["/usr/local/lib/python3.11/_weakrefset.py", 85], "_ensure_future (/usr/local/lib/python3.11/asyncio/tasks.py:662)": ["/usr/local/lib/python3.11/asyncio/tasks.py", 662], "ensure_future (/usr/local/lib/python3.11/asyncio/tasks.py:654)": ["/usr/local/lib/python3.11/asyncio/tasks.py", 654], "wait_for (/usr/local/lib/python3.11/asyncio/tasks.py:436)": ["/usr/local/lib/python3.11/asyncio/tasks.py", 436], "CircuitBreaker.call (/app/app/utils/circuit_breaker.py:48)": ["/app/app/utils/circuit_breaker.py", 48], "DataProvider.get_price_safe (/app/app/providers/base.py:80)": ["/app/app/providers/base.py", 80], "MarketDataService._try_providers (/app/app/services/market_data.py:92)": ["/app/app/services/market_data.py", 92], "MarketDataService.get_stock_price (/app/app/services/market_data.py:134)": ["/app/app/services/market_data.py", 134], "sleep (/usr/local/lib/python3.11/asyncio/tasks.py:637)": ["/usr/local/lib/python3.11/asyncio/tasks.py", 637], "FinnhubProvider.get_price (/app/app/providers/finnhub_provider.py:17)": ["/app/app/providers/finnhub_provider.py", 17], "DataProvider._get_price_impl (/app/app/providers/base.py:125)": ["/app/app/providers/base.py", 125], "_set_result_unless_cancelled (/usr/local/lib/python3.11/asyncio/futures.py:311)": ["/usr/local/lib/python3.11/asyncio/futures.py", 311], "Server.on_tick (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:233)": ["/usr/local/lib/python3.11/site-packages/uvicorn/server.py", 233], "Server.main_loop (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:224)": ["/usr/local/lib/python3.11/site-packages/uvicorn/server.py", 224], "Server._serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:73)": ["/usr/local/lib/python3.11/site-packages/uvicorn/server.py", 73], "Server.serve (/usr/local/lib/python3.11/site-packages/uvicorn/server.py:69)": ["/usr/local/lib/python3.11/site-packages/uvicorn/server.py", 69], "StreamReaderProtocol._stream_reader (/usr/local/lib/python3.11/asyncio/streams.py:211)": ["/usr/local/lib/python3.11/asyncio/streams.py", 211], "StreamReader._wakeup_waiter (/usr/local/lib/python3.11/asyncio/streams.py:472)": ["/usr/local/lib/python3.11/asyncio/streams.py", 472], "StreamReader.feed_data (/usr/local/lib/python3.11/asyncio/streams.py:497)": ["/usr/local/lib/python3.11/asyncio/streams.py", 497], "StreamReaderProtocol.data_received (/usr/local/lib/python3.11/asyncio/streams.py:284)": ["/usr/local/lib/python3.11/asyncio/streams.py", 284], "StreamReader._wait_for_data (/usr/local/lib/python3.11/asyncio/streams.py:519)": ["/usr/local/lib/python3.11/asyncio/streams.py", 519], "StreamReader._maybe_resume_transport (/usr/local/lib/python3.11/asyncio/streams.py:484)": ["/usr/local/lib/python3.11/asyncio/streams.py", 484], "StreamReader.readuntil (/usr/local/lib/python3.11/asyncio/streams.py:578)": ["/usr/local/lib/python3.11/asyncio/streams.py", 578], "StreamReader.readline (/usr/local/lib/python3.11/asyncio/streams.py:547)": ["/usr/local/lib/python3.11/asyncio/streams.py", 547], "_AsyncRESPBase._readline (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:273)": ["/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py", 273], "_AsyncRESP2Parser._read_response (/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py:87)": ["/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py", 87], "_AsyncRESPBase._clear (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:225)": ["/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py", 225], "_AsyncRESP2Parser.read_response (/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py:74)": ["/usr/local/lib/python3.11/site-packages/redis/_parsers/resp2.py", 74], "AbstractConnection.read_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:572)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 572], "CaseInsensitiveDict.__contains__ (/usr/local/lib/python3.11/site-packages/redis/client.py:88)": ["/usr/local/lib/python3.11/site-packages/redis/client.py", 88], "cast (/usr/local/lib/python3.11/typing.py:2287)": ["/usr/local/lib/python3.11/typing.py", 2287], "CaseInsensitiveDict.__getitem__ (/usr/local/lib/python3.11/site-packages/redis/client.py:94)": ["/usr/local/lib/python3.11/site-packages/redis/client.py", 94], "<lambda> (/usr/local/lib/python3.11/site-packages/redis/_parsers/helpers.py:806)": ["/usr/local/lib/python3.11/site-packages/redis/_parsers/helpers.py", 806], "ABCMeta.__instancecheck__ (<frozen abc>:117)": ["<frozen abc>", 117], "isawaitable (/usr/local/lib/python3.11/inspect.py:449)": ["/usr/local/lib/python3.11/inspect.py", 449], "Redis.parse_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:689)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py", 689], "Redis._send_command_parse_response (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:647)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py", 647], "Retry.call_with_retry (/usr/local/lib/python3.11/site-packages/redis/asyncio/retry.py:37)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/retry.py", 37], "AfterConnectionReleasedEvent.__init__ (/usr/local/lib/python3.11/site-packages/redis/event.py:98)": ["/usr/local/lib/python3.11/site-packages/redis/event.py", 98], "AfterConnectionReleasedEvent.connection (/usr/local/lib/python3.11/site-packages/redis/event.py:101)": ["/usr/local/lib/python3.11/site-packages/redis/event.py", 101], "AbstractConnection.re_auth (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:717)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 717], "AsyncReAuthConnectionListener.listen (/usr/local/lib/python3.11/site-packages/redis/event.py:243)": ["/usr/local/lib/python3.11/site-packages/redis/event.py", 243], "EventDispatcher.dispatch_async (/usr/local/lib/python3.11/site-packages/redis/event.py:86)": ["/usr/local/lib/python3.11/site-packages/redis/event.py", 86], "ConnectionPool.release (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1196)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 1196], "Redis.execute_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:667)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py", 667], "CollectorJobs.pop_job (/app/app/services/data_collector.py:61)": ["/app/app/services/data_collector.py", 61], "DataCollectorService.consume_jobs (/app/app/services/data_collector.py:184)": ["/app/app/services/data_collector.py", 184], "list_or_args (/usr/local/lib/python3.11/site-packages/redis/commands/helpers.py:10)": ["/usr/local/lib/python3.11/site-packages/redis/commands/helpers.py", 10], "ListCommands.brpop (/usr/local/lib/python3.11/site-packages/redis/commands/core.py:2560)": ["/usr/local/lib/python3.11/site-packages/redis/commands/core.py", 2560], "Redis.initialize (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:399)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py", 399], "deprecated_args.<locals>.decorator.<locals>.wrapper (/usr/local/lib/python3.11/site-packages/redis/utils.py:169)": ["/usr/local/lib/python3.11/site-packages/redis/utils.py", 169], "Lock.acquire.<locals>.<genexpr> (/usr/local/lib/python3.11/asyncio/locks.py:100)": ["/usr/local/lib/python3.11/asyncio/locks.py", 100], "ConnectionPool.get_available_connection (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1156)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 1156], "AbstractConnection.is_connected (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:259)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 259], "AbstractConnection.connect_check_health (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:298)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 298], "AbstractConnection.connect (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:294)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 294], "Timeout.__init__ (/usr/local/lib/python3.11/asyncio/timeouts.py:33)": ["/usr/local/lib/python3.11/asyncio/timeouts.py", 33], "timeout (/usr/local/lib/python3.11/asyncio/timeouts.py:129)": ["/usr/local/lib/python3.11/asyncio/timeouts.py", 129], "current_task (/usr/local/lib/python3.11/asyncio/tasks.py:35)": ["/usr/local/lib/python3.11/asyncio/tasks.py", 35], "Timeout.reschedule (/usr/local/lib/python3.11/asyncio/timeouts.py:50)": ["/usr/local/lib/python3.11/asyncio/timeouts.py", 50], "Timeout.__aenter__ (/usr/local/lib/python3.11/asyncio/timeouts.py:85)": ["/usr/local/lib/python3.11/asyncio/timeouts.py", 85], "StreamReader.at_eof (/usr/local/lib/python3.11/asyncio/streams.py:493)": ["/usr/local/lib/python3.11/asyncio/streams.py", 493], "Timeout.__aexit__ (/usr/local/lib/python3.11/asyncio/timeouts.py:97)": ["/usr/local/lib/python3.11/asyncio/timeouts.py", 97], "_AsyncRESPBase.can_read_destructive (/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py:242)": ["/usr/local/lib/python3.11/site-packages/redis/_parsers/base.py", 242], "AbstractConnection.can_read_destructive (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:563)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 563], "ConnectionPool.ensure_connection (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1180)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 1180], "ConnectionPool.get_connection (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:1139)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 1139], "AbstractBackoff.reset (/usr/local/lib/python3.11/site-packages/redis/backoff.py:13)": ["/usr/local/lib/python3.11/site-packages/redis/backoff.py", 13], "Redis.execute_command.<locals>.<lambda> (/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py:678)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/client.py", 678], "Encoder.encode (/usr/local/lib/python3.11/site-packages/redis/_parsers/encoders.py:14)": ["/usr/local/lib/python3.11/site-packages/redis/_parsers/encoders.py", 14], "AbstractConnection.pack_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:630)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 630], "AbstractConnection.check_health (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:504)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 504], "StreamWriter.writelines (/usr/local/lib/python3.11/asyncio/streams.py:348)": ["/usr/local/lib/python3.11/asyncio/streams.py", 348], "StreamReader.exception (/usr/local/lib/python3.11/asyncio/streams.py:460)": ["/usr/local/lib/python3.11/asyncio/streams.py", 460], "FlowControlMixin._drain_helper (/usr/local/lib/python3.11/asyncio/streams.py:164)": ["/usr/local/lib/python3.11/asyncio/streams.py", 164], "StreamWriter.drain (/usr/local/lib/python3.11/asyncio/streams.py:369)": ["/usr/local/lib/python3.11/asyncio/streams.py", 369], "AbstractConnection.send_packed_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:516)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 516], "AbstractConnection.send_command (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:557)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 557], "Connection._host_error (/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py:781)": ["/usr/local/lib/python3.11/site-packages/redis/asyncio/connection.py", 781], "_format_timetuple_and_zone (/usr/local/lib/python3.11/email/utils.py:233)": ["/usr/local/lib/python3.11/email/utils.py", 233], "format_datetime (/usr/local/lib/python3.11/email/utils.py:271)": ["/usr/local/lib/python3.11/email/utils.py", 271], "formatdate (/usr/local/lib/python3.11/email/utils.py:242)": ["/usr/local/lib/python3.11/email/utils.py", 242], "ismethod (/usr/local/lib/python3.11/inspect.py:300)": ["/usr/local/lib/python3.11/inspect.py", 300], "_unwrap_partial (/usr/local/lib/python3.11/functools.py:421)": ["/usr/local/lib/python3.11/functools.py", 421], "isfunction (/usr/local/lib/python3.11/inspect.py:378)": ["/usr/local/lib/python3.11/inspect.py", 378], "isclass (/usr/local/lib/python3.11/inspect.py:292)": ["/usr/local/lib/python3.11/inspect.py", 292], "_signature_is_functionlike (/usr/local/lib/python3.11/inspect.py:2075)": ["/usr/local/lib/python3.11/inspect.py", 2075], "_has_code_flag (/usr/local/lib/python3.11/inspect.py:391)": ["/usr/local/lib/python3.11/inspect.py", 391], "iscoroutinefunction (/usr/local/lib/python3.11/inspect.py:409)": ["/usr/local/lib/python3.11/inspect.py", 409], "iscoroutinefunction (/usr/local/lib/python3.11/asyncio/coroutines.py:21)": ["/usr/local/lib/python3.11/asyncio/coroutines.py", 21], "RLock (/usr/local/lib/python3.11/threading.py:90)": ["/usr/local/lib/python3.11/threading.py", 90], "Condition.__init__ (/usr/local/lib/python3.11/threading.py:243)": ["/usr/local/lib/python3.11/threading.py", 243], "Future.__init__ (/usr/local/lib/python3.11/concurrent/futures/_base.py:328)": ["/usr/local/lib/python3.11/concurrent/futures/_base.py", 328], "_WorkItem.__init__ (/usr/local/lib/python3.11/concurrent/futures/thread.py:47)": ["/usr/local/lib/python3.11/concurrent/futures/thread.py", 47], "Condition.__enter__ (/usr/local/lib/python3.11/threading.py:271)": ["/usr/local/lib/python3.11/threading.py", 271], "Condition.__exit__ (/usr/local/lib/python3.11/threading.py:274)": ["/usr/local/lib/python3.11/threading.py", 274], "Semaphore.acquire (/usr/local/lib/python3.11/threading.py:440)": ["/usr/local/lib/python3.11/threading.py", 440], "ThreadPoolExecutor._adjust_thread_count (/usr/local/lib/python3.11/concurrent/futures/thread.py:180)": ["/usr/local/lib/python3.11/concurrent/futures/thread.py", 180], "ThreadPoolExecutor.submit (/usr/local/lib/python3.11/concurrent/futures/thread.py:161)": ["/usr/local/lib/python3.11/concurrent/futures/thread.py", 161], "_get_loop (/usr/local/lib/python3.11/asyncio/futures.py:299)": ["/usr/local/lib/python3.11/asyncio/futures.py", 299], "Future.add_done_callback (/usr/local/lib/python3.11/concurrent/futures/_base.py:408)": ["/usr/local/lib/python3.11/concurrent/futures/_base.py", 408], "_chain_future (/usr/local/lib/python3.11/asyncio/futures.py:365)": ["/usr/local/lib/python3.11/asyncio/futures.py", 365], "wrap_future (/usr/local/lib/python3.11/asyncio/futures.py:409)": ["/usr/local/lib/python3.11/asyncio/futures.py", 409], "to_thread (/usr/local/lib/python3.11/asyncio/threads.py:12)": ["/usr/local/lib/python3.11/asyncio/threads.py", 12], "Future.done (/usr/local/lib/python3.11/concurrent/futures/_base.py:393)": ["/usr/local/lib/python3.11/concurrent/futures/_base.py", 393], "Future.cancelled (/usr/local/lib/python3.11/concurrent/futures/_base.py:383)": ["/usr/local/lib/python3.11/concurrent/futures/_base.py", 383], "Future.exception (/usr/local/lib/python3.11/concurrent/futures/_base.py:463)": ["/usr/local/lib/python3.11/concurrent/futures/_base.py", 463], "Future.__get_result (/usr/local/lib/python3.11/concurrent/futures/_base.py:398)": ["/usr/local/lib/python3.11/concurrent/futures/_base.py", 398], "Future.result (/usr/local/lib/python3.11/concurrent/futures/_base.py:428)": ["/usr/local/lib/python3.11/concurrent/futures/_base.py", 428], "_copy_future_state (/usr/local/lib/python3.11/asyncio/futures.py:345)": ["/usr/local/lib/python3.11/asyncio/futures.py", 345], "_chain_future.<locals>._set_state (/usr/local/lib/python3.11/asyncio/futures.py:381)": ["/usr/local/lib/python3.11/asyncio/futures.py", 381], "_chain_future.<locals>._call_check_cancel (/usr/local/lib/python3.11/asyncio/futures.py:387)": ["/usr/local/lib/python3.11/asyncio/futures.py", 387], "DataProvider.mark_available (/app/app/providers/base.py:74)": ["/app/app/providers/base.py", 74], "_release_waiter (/usr/local/lib/python3.11/asyncio/tasks.py:431)": ["/usr/local/lib/python3.11/asyncio/tasks.py", 431], "WeakSet.__init__.<locals>._remove (/usr/local/lib/python3.11/_weakrefset.py:39)": ["/usr/local/lib/python3.11/_weakrefset.py", 39], "RollingStats.update_latency (/app/app/providers/registry.py:28)": ["/app/app/providers/registry.py", 28], "RollingStats.update_error (/app/app/providers/registry.py:31)": ["/app/app/providers/registry.py", 31], "Histogram._prepare_buckets.<locals>.<listcomp> (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:590)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py", 590], "Histogram._prepare_buckets (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:589)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py", 589], "floatToGoString (/usr/local/lib/python3.11/site-packages/prometheus_client/utils.py:9)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/utils.py", 9], "Histogram._metric_init (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:601)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py", 601], "Histogram.__init__ (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:565)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py", 565], "Histogram.observe (/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py:616)": ["/usr/local/lib/python3.11/site-packages/prometheus_client/metrics.py", 616], "ProviderRegistry.record_outcome (/app/app/providers/registry.py:139)": ["/app/app/providers/registry.py", 139], "DataCache.set (/app/app/services/cache.py:19)": ["/app/app/services/cache.py", 19], "Logger.isEnabledFor (/usr/local/lib/python3.11/logging/__init__.py:1734)": ["/usr/local/lib/python3.11/logging/__init__.py", 1734], "<lambda> (/usr/local/lib/python3.11/logging/__init__.py:164)": ["/usr/local/lib/python3.11/logging/__init__.py", 164], "normcase (<frozen posixpath>:52)": ["<frozen posixpath>", 52], "_is_internal_frame (/usr/local/lib/python3.11/logging/__init__.py:194)": ["/usr/local/lib/python3.11/logging/__init__.py", 194], "Logger.findCaller (/usr/local/lib/python3.11/logging/__init__.py:1561)": ["/usr/local/lib/python3.11/logging/__init__.py", 1561], "getLevelName (/usr/local/lib/python3.11/logging/__init__.py:123)": ["/usr/local/lib/python3.11/logging/__init__.py", 123], "_get_sep (<frozen posixpath>:41)": ["<frozen posixpath>", 41], "basename (<frozen posixpath>:140)": ["<frozen posixpath>", 140], "_splitext (<frozen genericpath>:121)": ["<frozen genericpath>", 121], "splitext (<frozen posixpath>:117)": ["<frozen posixpath>", 117], "current_thread (/usr/local/lib/python3.11/threading.py:1453)": ["/usr/local/lib/python3.11/threading.py", 1453], "Thread.name (/usr/local/lib/python3.11/threading.py:1152)": ["/usr/local/lib/python3.11/threading.py", 1152], "current_process (/usr/local/lib/python3.11/multiprocessing/process.py:37)": ["/usr/local/lib/python3.11/multiprocessing/process.py", 37], "BaseProcess.name (/usr/local/lib/python3.11/multiprocessing/process.py:189)": ["/usr/local/lib/python3.11/multiprocessing/process.py", 189], "LogRecord.__init__ (/usr/local/lib/python3.11/logging/__init__.py:292)": ["/usr/local/lib/python3.11/logging/__init__.py", 292], "Logger.makeRecord (/usr/local/lib/python3.11/logging/__init__.py:1595)": ["/usr/local/lib/python3.11/logging/__init__.py", 1595], "Filterer.filter (/usr/local/lib/python3.11/logging/__init__.py:815)": ["/usr/local/lib/python3.11/logging/__init__.py", 815], "Handler.acquire (/usr/local/lib/python3.11/logging/__init__.py:922)": ["/usr/local/lib/python3.11/logging/__init__.py", 922], "LogRecord.getMessage (/usr/local/lib/python3.11/logging/__init__.py:368)": ["/usr/local/lib/python3.11/logging/__init__.py", 368], "PercentStyle.usesTime (/usr/local/lib/python3.11/logging/__init__.py:432)": ["/usr/local/lib/python3.11/logging/__init__.py", 432], "Formatter.usesTime (/usr/local/lib/python3.11/logging/__init__.py:652)": ["/usr/local/lib/python3.11/logging/__init__.py", 652], "PercentStyle._format (/usr/local/lib/python3.11/logging/__init__.py:440)": ["/usr/local/lib/python3.11/logging/__init__.py", 440], "PercentStyle.format (/usr/local/lib/python3.11/logging/__init__.py:447)": ["/usr/local/lib/python3.11/logging/__init__.py", 447], "Formatter.formatMessage (/usr/local/lib/python3.11/logging/__init__.py:658)": ["/usr/local/lib/python3.11/logging/__init__.py", 658], "Formatter.format (/usr/local/lib/python3.11/logging/__init__.py:674)": ["/usr/local/lib/python3.11/logging/__init__.py", 674], "Handler.format (/usr/local/lib/python3.11/logging/__init__.py:942)": ["/usr/local/lib/python3.11/logging/__init__.py", 942], "Handler.release (/usr/local/lib/python3.11/logging/__init__.py:929)": ["/usr/local/lib/python3.11/logging/__init__.py", 929], "StreamHandler.flush (/usr/local/lib/python3.11/logging/__init__.py:1087)": ["/usr/local/lib/python3.11/logging/__init__.py", 1087], "StreamHandler.emit (/usr/local/lib/python3.11/logging/__init__.py:1098)": ["/usr/local/lib/python3.11/logging/__init__.py", 1098], "Handler.handle (/usr/local/lib/python3.11/logging/__init__.py:965)": ["/usr/local/lib/python3.11/logging/__init__.py", 965], "Logger.callHandlers (/usr/local/lib/python3.11/logging/__init__.py:1690)": ["/usr/local/lib/python3.11/logging/__init__.py", 1690], "Logger.handle (/usr/local/lib/python3.11/logging/__init__.py:1636)": ["/usr/local/lib/python3.11/logging/__init__.py", 1636], "Logger._log (/usr/local/lib/python3.11/logging/__init__.py:1610)": ["/usr/local/lib/python3.11/logging/__init__.py", 1610], "Logger.info (/usr/local/lib/python3.11/logging/__init__.py:1479)": ["/usr/local/lib/python3.11/logging/__init__.py", 1479]}}}